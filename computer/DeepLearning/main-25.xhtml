<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h5><a id="bookmark0"></a><span class="font64" style="font-weight:bold;">12.4.3 High-Dimensional Outputs</span></h5>
<p><span class="font64">In many natural language applications, we often want our models to produce words (rather than characters) as the fundamental unit of the output. For large&#160;vocabularies, it can be very computationally expensive to represent an output&#160;distribution over the choice of a word, because the vocabulary size is large. In many&#160;applications, V contains hundreds of thousands of words. The naive approach to&#160;representing such a distribution is to apply an affine transformation from a hidden&#160;representation to the output space, then apply the softmax function. Suppose&#160;we have a vocabulary V with size |V|. The weight matrix describing the linear&#160;component of this affine transformation is very large, because its output dimension&#160;is |V|. This imposes a high memory cost to represent the matrix, and a high&#160;computational cost to multiply by it. Because the softmax is normalized across all&#160;|V| outputs, it is necessary to perform the full matrix multiplication at training&#160;time as well as test time—we cannot calculate only the dot product with the weight&#160;vector for the correct output. The high computational costs of the output layer&#160;thus arise both at training time (to compute the likelihood and its gradient) and&#160;at test time (to compute probabilities for all or selected words). For specialized&#160;loss functions, the gradient can be computed efficiently (Vincent </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015), but&#160;the standard cross-entropy loss applied to a traditional softmax output layer poses&#160;many difficulties.</span></p>
<p><span class="font64">Suppose that h is the top hidden layer used to predict the output probabilities </span><span class="font63" style="font-style:italic;">y</span><span class="font64" style="font-weight:bold;font-style:italic;">.</span><span class="font64"> If we parametrize the transformation from h to </span><span class="font63" style="font-style:italic;">y</span><span class="font64"> with learned weights W&#160;and learned biases b, then the affine-softmax output layer performs the following&#160;computations:</span></p>
<p><span class="font64">a </span><span class="font64" style="font-weight:bold;font-style:italic;">= b</span><span class="font63" style="font-style:italic;">i </span><span class="font64" style="font-weight:bold;font-style:italic;">+ &#160;&#160;&#160;W</span><span class="font63" style="font-style:italic;">ij</span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font63" style="font-style:italic;">j V</span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> e {1.....|V|}, &#160;&#160;&#160;(12.8)</span></p>
<p><span class="font64"><sup>j</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">e</span><span class="font63" style="font-style:italic;"><sup>a</sup> &#160;&#160;&#160;.</span><span class="font64">&#160;&#160;&#160;&#160;.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>Vi</sup> =</span><span class="font64"> &#160;&#160;&#160;|V| <sub>e</sub>a<sup>9</sup>־<sup>12)</sup>&#160;&#160;&#160;&#160;• </span><span class="font64" style="font-weight:bold;font-style:italic;">׳</span><span class="font64"><sup>)</sup></span></p>
<p><span class="font64">i </span><span class="font18">1</span><span class="font64">=׳ <sup>e </sup></span><span class="font18"><sup>4</sup></span></p>
<p><span class="font64">If h contains nh elements then the above operation is O(|V|nh). With nh in the thousands and |V| in the hundreds of thousands, this operation dominates the&#160;computation of most neural language models.</span></p>
<p><span class="font64">12.4.3.1 Use of a Short List</span></p>
<p><span class="font64">The first neural language models (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2001, 2003) dealt with the high cost of using a softmax over a large number of output words by limiting the vocabulary&#160;size to 10,000 or 20,000 words. Schwenk and Gauvain (2002) and Schwenk (2007)&#160;built upon this approach by splitting the vocabulary V into a </span><span class="font64" style="font-weight:bold;font-style:italic;">shortlist</span><span class="font64"> L of most&#160;frequent words (handled by the neural net) and a tail T = V\L of more rare words&#160;(handled by an n-gram model). To be able to combine the two predictions, the&#160;neural net also has to predict the probability that a word appearing after context&#160;C belongs to the tail list. This may be achieved by adding an extra sigmoid output&#160;unit to provide an estimate of </span><span class="font64" style="font-weight:bold;font-style:italic;">P(i</span><span class="font64"> e T | C). The extra output can then be used to&#160;achieve an estimate of the probability distribution over all words in V as follows:</span></p>
<p><span class="font64">P(y </span><span class="font64" style="font-weight:bold;font-style:italic;">= i</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;font-style:italic;">C) =1</span><span class="font64"> i^P</span><span class="font64" style="font-weight:bold;font-style:italic;">(y = i</span><span class="font64"> | C,i e L)(1 - P(i e T | </span><span class="font64" style="font-weight:bold;font-style:italic;">C</span><span class="font64">)) &#160;&#160;&#160;(12.10)</span></p>
<p><span class="font64">+1 i<sub>€</sub>TP(y = i I C,i e T)P(i e T | C) &#160;&#160;&#160;(12.11)</span></p>
<p><span class="font64">where P(y = i | C, i e L) is provided by the neural language model and P (y = i | C, i e T) is provided by the n-gram model. With slight modification, this approach&#160;can also work using an extra output value in the neural language model’s softmax&#160;layer, rather than a separate sigmoid unit.</span></p>
<p><span class="font64">An obvious disadvantage of the short list approach is that the potential generalization advantage of the neural language models is limited to the most frequent words, where, arguably, it is the least useful. This disadvantage has stimulated&#160;the exploration of alternative methods to deal with high-dimensional outputs,&#160;described below.</span></p>
<p><span class="font64" style="font-weight:bold;">12.4.3.2 Hierarchical Softmax</span></p>
<p><span class="font64">A classical approach (Goodman, 2001) to reducing the computational burden of high-dimensional output layers over large vocabulary sets V is to decompose&#160;probabilities hierarchically. Instead of necessitating a number of computations&#160;proportional to |V| (and also proportional to the number of hidden units, nh),&#160;the |V| factor can be reduced to as low as log |V|. Bengio (2002) and Morin and&#160;Bengio (2005) introduced this factorized approach to the context of neural language&#160;models.</span></p>
<p><span class="font64">One can think of this hierarchy as building categories of words, then categories of categories of words, then categories of categories of categories of words, etc.&#160;These nested categories form a tree, with words at the leaves. In a balanced tree,&#160;the tree has depth O(log |V|). The probability of a choosing a word is given by the&#160;product of the probabilities of choosing the branch leading to that word at every&#160;node on a path from the root of the tree to the leaf containing the word. Fig. 12.4&#160;illustrates a simple example. Mnih and Hinton (2009) also describe how to use&#160;multiple paths to identify a single word in order to better model words that have&#160;multiple meanings. Computing the probability of a word then involves summation&#160;over all of the paths that lead to that word.</span></p>
<p><span class="font64">To predict the conditional probabilities required at each node of the tree, we typically use a logistic regression model at each node of the tree, and provide the&#160;same context C as input to all of these models. Because the correct output is&#160;encoded in the training set, we can use supervised learning to train the logistic&#160;regression models. This is typically done using a standard cross-entropy loss,&#160;corresponding to maximizing the log-likelihood of the correct sequence of decisions.</span></p>
<p><span class="font64">Because the output log-likelihood can be computed efficiently (as low as log |V| rather than |V|), its gradients may also be computed efficiently. This includes not&#160;only the gradient with respect to the output parameters but also the gradients&#160;with respect to the hidden layer activations.</span></p>
<p><span class="font64">It is possible but usually not practical to optimize the tree structure to minimize the expected number of computations. Tools from information theory specify how&#160;to choose the optimal binary code given the relative frequencies of the words. To&#160;do so, we could structure the tree so that the number of bits associated with a word&#160;is approximately equal to the logarithm of the frequency of that word. However, in&#160;practice, the computational savings are typically not worth the effort because the&#160;computation of the output probabilities is only one part of the total computation&#160;in the neural language model. For example, suppose there are l fully connected&#160;hidden layers of width </span><span class="font64" style="font-weight:bold;font-style:italic;">nh.</span><span class="font64"> Let </span><span class="font64" style="font-weight:bold;font-style:italic;">nb</span><span class="font64"> be the weighted average of the number of bits</span></p><div><img src="main-142.jpg" alt=""/></div>
<p><span class="font64">Figure 12.4: Illustration of a simple hierarchy of word categories, with </span><span class="font19">8</span><span class="font64"> wordsw</span><span class="font19">0</span><span class="font64">,..., </span><span class="font64" style="font-style:italic;">w</span><span class="font19" style="font-style:italic;">7 </span><span class="font64">organized into a three level hierarchy. The leaves of the tree represent actual specific words.&#160;Internal nodes represent groups of words. Any node can be indexed by the sequence&#160;of binary decisions (0=left, 1=right) to reach the node from the root. Super-class (0)&#160;contains the classes (</span><span class="font19">0</span><span class="font64">, </span><span class="font19">0</span><span class="font64">) and (</span><span class="font19">0</span><span class="font64">, </span><span class="font19">1</span><span class="font64">), which respectively contain the sets of words {wo,w<sub>1</sub>}&#160;and {w</span><span class="font19">2</span><span class="font64">, W</span><span class="font19">3</span><span class="font64">}, and similarly super-class (</span><span class="font19">1</span><span class="font64">) contains the classes (</span><span class="font19">1</span><span class="font64">,</span><span class="font19">0</span><span class="font64">) and (</span><span class="font19">1</span><span class="font64">,</span><span class="font19">1</span><span class="font64">), which&#160;respectively contain the words (w</span><span class="font19">4</span><span class="font64">,w</span><span class="font19">5</span><span class="font64">) and (w</span><span class="font19">6</span><span class="font64">, w</span><span class="font19">7</span><span class="font64">). If the tree is sufficiently balanced,&#160;the maximum depth (number of binary decisions) is on the order of the logarithm of&#160;the number of words | V|: the choice of one out of | V| words can be obtained by doing&#160;O(log |V|) operations (one for each of the nodes on the path from the root). In this example,&#160;computing the probability of a word y can be done by multiplying three probabilities,&#160;associated with the binary decisions to move left or right at each node on the path from&#160;the root to a node y. Let </span><span class="font64" style="font-style:italic;">bi(y)</span><span class="font64"> be the i-th binary decision when traversing the tree&#160;towards the value y. The probability of sampling an output y decomposes into a product&#160;of conditional probabilities, using the chain rule for conditional probabilities, with each&#160;node indexed by the prefix of these bits. For example, node (1, 0) corresponds to the&#160;prefix </span><span class="font64" style="font-style:italic;">(b</span><span class="font19">0</span><span class="font64"> (w</span><span class="font19">4</span><span class="font64">) = </span><span class="font19">1</span><span class="font64">,</span><span class="font19">61</span><span class="font64"> (w</span><span class="font19">4</span><span class="font64">) = </span><span class="font19">0</span><span class="font64">), and the probability of w can be decomposed as follows:</span></p>
<p><span class="font64" style="font-style:italic;">P</span><span class="font64"> (y = w </span><span class="font19">4</span><span class="font64">) = &#160;&#160;&#160;P (bo&#160;&#160;&#160;&#160;= 1, b</span><span class="font19">1</span><span class="font64"> =&#160;&#160;&#160;&#160;0, b</span><span class="font19">2</span><span class="font64"> = 0)&#160;&#160;&#160;&#160;(12.12)</span></p>
<p><span class="font64">= &#160;&#160;&#160;P(b</span><span class="font19"><sub>0</sub></span><span class="font64">&#160;&#160;&#160;&#160;= 1)P(b</span><span class="font19"><sub>1</sub></span><span class="font64">&#160;&#160;&#160;&#160;= 0 | b = 1)P(b</span><span class="font19"><sub>2</sub></span><span class="font64">&#160;&#160;&#160;&#160;= 0 | b</span><span class="font19"><sub>0</sub></span><span class="font64">&#160;&#160;&#160;&#160;= 1,b</span><span class="font19"><sub>1</sub></span><span class="font64"> = 0).&#160;&#160;&#160;&#160;(12.13)</span></p>
<p><span class="font64">required to identify a word, with the weighting given by the frequency of these words. In this example, the number of operations needed to compute the hidden&#160;activations grows as as O(lnh) while the output computations grow as </span><span class="font64" style="font-weight:bold;font-style:italic;">O(nhn</span><span class="font64">).&#160;As long as nb &lt; </span><span class="font64" style="font-weight:bold;font-style:italic;">In</span><span class="font64"> h, we can reduce computation more by shrinking nh than by&#160;shrinking nb. Indeed, nb is often small. Because the size of the vocabulary rarely&#160;exceeds a million words and log</span><span class="font18">2</span><span class="font64"> (</span><span class="font18">10</span><span class="font64"><sup>6</sup>) ~ </span><span class="font18">20</span><span class="font64">, it is possible to reduce nb to about </span><span class="font18">20</span><span class="font64">,&#160;but nh is often much larger, around 10</span><span class="font18"><sup>3</sup></span><span class="font64"> or more. Rather than carefully optimizing&#160;a tree with a branching factor of </span><span class="font18">2</span><span class="font64">, one can instead define a tree with depth two&#160;and a branching factor of y^Vj. Such a tree corresponds to simply defining a set&#160;of mutually exclusive word classes. The simple approach based on a tree of depth&#160;two captures most of the computational benefit of the hierarchical strategy.</span></p>
<p><span class="font64">One question that remains somewhat open is how to best define these word classes, or how to define the word hierarchy in general. Early work used existing&#160;hierarchies (Morin and Bengio, 2005) but the hierarchy can also be learned, ideally&#160;jointly with the neural language model. Learning the hierarchy is difficult. An exact&#160;optimization of the log-likelihood appears intractable because the choice of a word&#160;hierarchy is a discrete one, not amenable to gradient-based optimization. However,&#160;one could use discrete optimization to approximately optimize the partition of&#160;words into word classes.</span></p>
<p><span class="font64">An important advantage of the hierarchical softmax is that it brings computational benefits both at training time and at test time, if at test time we want to compute the probability of specific words.</span></p>
<p><span class="font64">Of course, computing the probability of all jVj words will remain expensive even with the hierarchical softmax. Another important operation is selecting the&#160;most likely word in a given context. Unfortunately the tree structure does not&#160;provide an efficient and exact solution to this problem.</span></p>
<p><span class="font64">A disadvantage is that in practice the hierarchical softmax tends to give worse test results than sampling-based methods we will describe next. This may be due&#160;to a poor choice of word classes.</span></p>
<p><span class="font64" style="font-weight:bold;">12.4.3.3 Importance Sampling</span></p>
<p><span class="font64">One way to speed up the training of neural language models is to avoid explicitly computing the contribution of the gradient from all of the words that do not appear&#160;in the next position. Every incorrect word should have low probability under the&#160;model. It can be computationally costly to enumerate all of these words. Instead,&#160;it is possible to sample only a subset of the words. Using the notation introduced&#160;where a is the vector of pre-softmax activations (or scores), with one element&#160;per word. The first term is the </span><span class="font64" style="font-weight:bold;font-style:italic;">positive phase</span><span class="font64"> term (pushing a<sub>y</sub> up) while the&#160;second term is the </span><span class="font64" style="font-weight:bold;font-style:italic;">negative phase</span><span class="font64"> term (pushing a<sub>i</sub> down for all i, with weight&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">P(i</span><span class="font64"> | C). Since the negative phase term is an expectation, we can estimate it with&#160;a Monte Carlo sample. However, that would require sampling from the model itself.&#160;Sampling from the model requires computing P (i | C) for all i in the vocabulary,&#160;which is precisely what we are trying to avoid.</span></p>
<p><span class="font64">Instead of sampling from the model, one can sample from another distribution, called the proposal distribution (denoted q), and use appropriate weights to correct&#160;for the bias introduced by sampling from the wrong distribution (Bengio and&#160;Senecal, 2003; Bengio and Senecal, 2008). This is an application of a more general&#160;technique called </span><span class="font64" style="font-weight:bold;font-style:italic;">importance sampling,</span><span class="font64"> which will be described in more detail in&#160;Sec. 17.2. Unfortunately, even exact importance sampling is not efficient because it&#160;requires computing weights </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font63" style="font-style:italic;"><sub>i</sub></span><span class="font64" style="font-weight:bold;font-style:italic;">/q</span><span class="font63" style="font-style:italic;"><sub>i</sub></span><span class="font64">, where </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font63" style="font-style:italic;"><sub>i</sub></span><span class="font64"> = P (i | C), which can only be computed&#160;if all the scores a</span><span class="font63" style="font-style:italic;">i</span><span class="font64"> are computed. The solution adopted for this application is called&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">biased importance sampling,</span><span class="font64"> where the importance weights are normalized to sum&#160;to 1. When negative word n<sub>i</sub> is sampled, the associated gradient is weighted by</span></p><div>
<p><span class="font64">(12.18)</span></p></div><div>
<p><span class="font63" style="font-style:italic;">Wi</span></p></div><div>
<p><span class="font64">in Eq. 12.8, the gradient can be written as follows:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"> log P(y | C) &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"> logsoftmaxy (a)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d9</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d9</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d &#160;&#160;&#160;e<sup>a</sup>y</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">= <sub>d9</sub></span><span class="font64" style="font-weight:bold;"><sup>10</sup>g </span><span class="font64">- <sub>£ </sub></span><span class="font64" style="font-weight:bold;font-style:italic;">39&#160;&#160;&#160;&#160;i<sup>e</sup></span></p></div><div>
<p><span class="font63" style="font-style:italic;">i</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span></p>
<p><span class="font64">&lt;99</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">da</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d9</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(</sup>ay <sup>-</sup></span><span class="font64"><sup> 10</sup>gj]e<sup>a</sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>i)</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i</span></p><h3><a id="bookmark1"></a><span class="font64" style="font-variant:small-caps;">E p (ע = i </span><span class="font18">1</span><span class="font64" style="font-variant:small-caps;"> c )</span></h3></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dai</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">39</span></p></div><div>
<p><span class="font64">(12.14)</span></p>
<p><span class="font64">(12.15)</span></p>
<p><span class="font64">(12.16) (12.17)</span></p></div>
<p><span class="font64"><sup>p</sup> n </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>/ q</sup></span><span class="font63" style="font-style:italic;">n i</span></p>
<p><span class="font64">£1 </span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font63" style="font-style:italic;">nj</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>/</sup>q</span><span class="font63" style="font-style:italic;">n</span></p><div>
<p><span class="font64">These weights are used to give the appropriate importance to the </span><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font64"> negative samples from </span><span class="font64" style="font-weight:bold;font-style:italic;">q</span><span class="font64"> used to form the estimated negative phase contribution to the&#160;gradient:</span></p>
<p><span class="font64"><sup>|V|</sup> &#160;&#160;&#160;m</span></p>
<p><span class="font64">EP(iic) </span><span class="font64" style="font-weight:bold;font-style:italic;">% -</span><span class="font64"> m Ewi %. &#160;&#160;&#160;(12.19)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font18">=1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font18">=1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d9</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">m</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d9</span></p></div>
<p><span class="font64">A unigram or a bigram distribution works well as the proposal distribution q. It is easy to estimate the parameters of such a distribution from data. After estimating&#160;the parameters, it is also possible to sample from such a distribution very efficiently.</span></p>
<p><span class="font64">Importance sampling is not only useful for speeding up models with large softmax outputs. More generally, it is useful for accelerating training with large&#160;sparse output layers, where the output is a sparse vector rather than a </span><span class="font18">1</span><span class="font64">-of-n&#160;choice. An example is a </span><span class="font64" style="font-weight:bold;font-style:italic;">bag of words.</span><span class="font64"> A bag of words is a sparse vector v where v<sub>i&#160;</sub>indicates the presence or absence of word i from the vocabulary in the document.&#160;Alternately, v<sub>i</sub> can indicate the number of times that word i appears. Machine&#160;learning models that emit such sparse vectors can be expensive to train for a&#160;variety of reasons. Early in learning, the model may not actually choose to make&#160;the output truly sparse. Moreover, the loss function we use for training might&#160;most naturally be described in terms of comparing every element of the output to&#160;every element of the target. This means that it is not always clear that there is a&#160;computational benefit to using sparse outputs, because the model may choose to&#160;make the majority of the output non-zero and all of these non-zero values need to&#160;be compared to the corresponding training target, even if the training target is zero.&#160;Dauphin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011) demonstrated that such models can be accelerated using&#160;importance sampling. The efficient algorithm minimizes the loss reconstruction for&#160;the “positive words” (those that are non-zero in the target) and an equal number&#160;of “negative words.” The negative words are chosen randomly, using a heuristic to&#160;sample words that are more likely to be mistaken. The bias introduced by this&#160;heuristic oversampling can then be corrected using importance weights.</span></p>
<p><span class="font64">In all of these cases, the computational complexity of gradient estimation for the output layer is reduced to be proportional to the number of negative samples&#160;rather than proportional to the size of the output vector.</span></p>
<p><span class="font64">12.4.3.4 Noise-Contrastive Estimation and Ranking Loss</span></p>
<p><span class="font64">Other approaches based on sampling have been proposed to reduce the computational cost of training neural language models with large vocabularies. An early example is the ranking loss proposed by Collobert and Weston (2008a), which&#160;views the output of the neural language model for each word as a score and tries to&#160;make the score of the correct word </span><span class="font64" style="font-weight:bold;font-style:italic;">ay</span><span class="font64"> be ranked high in comparison to the other&#160;scores </span><span class="font64" style="font-weight:bold;font-style:italic;">a</span><span class="font64"><sub>i</sub>. The ranking loss proposed then is</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">L =</span><span class="font64"> max(0,1 — </span><span class="font64" style="font-weight:bold;font-style:italic;">a<sub>y</sub></span><span class="font64"> + a<sub>i</sub>). &#160;&#160;&#160;(12.20)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i</span></p>
<p><span class="font64">The gradient is zero for the i-th term if the score of the observed word, a<sub>y</sub>, is greater than the score of the negative word a<sub>i</sub> by a margin of 1. One issue with&#160;this criterion is that it does not provide estimated conditional probabilities, which&#160;are useful in some applications, including speech recognition and text generation&#160;(including conditional text generation tasks such as translation).</span></p>
<p><span class="font64">A more recently used training objective for neural language model is noise-contrastive estimation, which is introduced in Sec. 18.6. This approach has been successfully applied to neural language models (Mnih and Teh, 2012; Mnih and&#160;Kavukcuoglu, 2013).</span></p><h5><a id="bookmark2"></a><span class="font64" style="font-weight:bold;">12.4.4 Combining Neural Language Models with n-grams</span></h5>
<p><span class="font64">A major advantage of n-gram models over neural networks is that n-gram models achieve high model capacity (by storing the frequencies of very many tuples)&#160;while requiring very little computation to process an example (by looking up&#160;only a few tuples that match the current context). If we use hash tables or trees&#160;to access the counts, the computation used for n-grams is almost independent&#160;of capacity. In comparison, doubling a neural network’s number of parameters&#160;typically also roughly doubles its computation time. Exceptions include models&#160;that avoid using all parameters on each pass. Embedding layers index only a single&#160;embedding in each pass, so we can increase the vocabulary size without increasing&#160;the computation time per example. Some other models, such as tiled convolutional&#160;networks, can add parameters while reducing the degree of parameter sharing&#160;in order to maintain the same amount of computation. However, typical neural&#160;network layers based on matrix multiplication use an amount of computation&#160;proportional to the number of parameters.</span></p>
<p><span class="font64">One easy way to add capacity is thus to combine both approaches in an ensemble consisting of a neural language model and an n-gram language model (Bengio&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2001, 2003). As with any ensemble, this technique can reduce test error if&#160;the ensemble members make independent mistakes. The field of ensemble learning&#160;provides many ways of combining the ensemble members’ predictions, including&#160;uniform weighting and weights chosen on a validation set. Mikolov </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011a)&#160;extended the ensemble to include not just two models but a large array of models.&#160;It is also possible to pair a neural network with a maximum entropy model and&#160;train both jointly (Mikolov </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011b). This approach can be viewed as training&#160;a neural network with an extra set of inputs that are connected directly to the&#160;output, and not connected to any other part of the model. The extra inputs are&#160;indicators for the presence of particular n-grams in the input context, so these&#160;variables are very high-dimensional and very sparse. The increase in model capacity&#160;is huge—the new portion of the architecture contains up to |sV |<sup>n</sup> parameters—but&#160;the amount of added computation needed to process an input is minimal because&#160;the extra inputs are very sparse.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">12.4.5 Neural Machine Translation</span></h5>
<p><span class="font64">Machine translation is the task of reading a sentence in one natural language and emitting a sentence with the equivalent meaning in another language. Machine&#160;translation systems often involve many components. At a high level, there is&#160;often one component that proposes many candidate translations. Many of these&#160;translations will not be grammatical due to differences between the languages. For&#160;example, many languages put adjectives after nouns, so when translated to English&#160;directly they yield phrases such as “apple red.” The proposal mechanism suggests&#160;many variants of the suggested translation, ideally including “red apple.” A second&#160;component of the translation system, a language model, evaluates the proposed&#160;translations, and can score “red apple” as better than “apple red.”</span></p>
<p><span class="font64">The earliest use of neural networks for machine translation was to upgrade the language model of a translation system by using a neural language model (Schwenk&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006; Schwenk, 2010). Previously, most machine translation systems had&#160;used an n-gram model for this component. The n-gram based models used for&#160;machine translation include not just traditional back-off n-gram models (Jelinek&#160;and Mercer, 1980; Katz, 1987; Chen and Goodman, 1999) but also </span><span class="font64" style="font-weight:bold;font-style:italic;">maximum&#160;entropy language models</span><span class="font64"> (Berger </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1996), in which an affine-softmax layer&#160;predicts the next word given the presence of frequent n-grams in the context.</span></p>
<p><span class="font64">Traditional language models simply report the probability of a natural language sentence. Because machine translation involves producing an output sentence given&#160;an input sentence, it makes sense to extend the natural language model to be&#160;conditional. As described in Sec. 6.2.1.1, it is straightforward to extend a model&#160;that defines a marginal distribution over some variable to define a conditional&#160;distribution over that variable given a context C, where C might be a single variable&#160;or a list of variables. Devlin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) beat the state-of-the-art in some statistical&#160;machine translation benchmarks by using an MLP to score a phrase </span><span class="font18">11</span><span class="font64">, t</span><span class="font18">2</span><span class="font64">,..., t&amp;&#160;in the target language given a phrase s!, s</span><span class="font18">2</span><span class="font64">,..., s<sub>n</sub> in the source language. The&#160;MLP estimates P (t<sub>1</sub>, t</span><span class="font18">2</span><span class="font64">,..., tk | s<sub>1</sub>, s<sub>2</sub>,..., s״). The estimate formed by this MLP&#160;replaces the estimate provided by conditional n-gram models.</span></p>
<p><span class="font64">A drawback of the MLP-based approach is that it requires the sequences to be preprocessed to be of fixed length. To make the translation more flexible, we would&#160;like to use a model that can accommodate variable length inputs and variable&#160;length outputs. An RNN provides this ability. Sec. 10.2.4 describes several ways&#160;of constructing an RNN that represents a conditional distribution over a sequence&#160;given some input, and Sec. 10.4 describes how to accomplish this conditioning&#160;when the input is a sequence. In all cases, one model first reads the input sequence&#160;and emits a data structure that summarizes the input sequence. We call this</span></p><div>
<table border="1">
<tr><td>
<p><span class="font63">/</span></p></td><td style="vertical-align:bottom;">
<p><span class="font63">Output object (English</span></p></td><td>
<p><span class="font63">\</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p><span class="font63">sentence)</span></p></td><td>
<p><span class="font59" style="font-style:italic;">J</span></p></td></tr>
</table>
<p><span class="font63">Decoder</span></p></div><div>
<p><span class="font15">C</span></p></div><div>
<p><span class="font63">Intermediate, semantic representation</span></p></div><div>
<p><span class="font63">Encoder</span></p></div><div>
<p><span class="font15">c</span></p></div><div>
<p><span class="font63">Source object (French sentence or image)</span></p></div><div>
<p><span class="font60">J</span></p></div>
<p><span class="font64">Figure 12.5: The encoder-decoder architecture to map back and forth between a surface representation (such as a sequence of words or an image) and a semantic representation.&#160;By using the output of an encoder of data from one modality (such as the encoder mapping&#160;from French sentences to hidden representations capturing the meaning of sentences) as&#160;the input to a decoder for another modality (such as the decoder mapping from hidden&#160;representations capturing the meaning of sentences to English), we can train systems that&#160;translate from one modality to another. This idea has been applied successfully not just&#160;to machine translation but also to caption generation from images.</span></p>
<p><span class="font64">summary the “context” </span><span class="font64" style="font-weight:bold;font-style:italic;">C.</span><span class="font64"> The context C may be a list of vectors, or it may be a vector or tensor. The model that reads the input to produce C may be an RNN&#160;(Cho </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014a; Sutskever </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014; Jean </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014) or a convolutional&#160;network (Kalchbrenner and Blunsom, 2013). A second model, usually an RNN,&#160;then reads the context C and generates a sentence in the target language. This&#160;general idea of an encoder-decoder framework for machine translation is illustrated&#160;in Fig. 12.5.</span></p>
<p><span class="font64">In order to generate an entire sentence conditioned on the source sentence, the model must have a way to represent the entire source sentence. Earlier models&#160;were only able to represent individual words or phrases. From a representation&#160;learning point of view, it can be useful to learn a representation in which sentences&#160;that have the same meaning have similar representations regardless of whether&#160;they were written in the source language or the target language. This strategy was&#160;explored first using a combination of convolutions and RNNs (Kalchbrenner and&#160;Blunsom, 2013). Later work introduced the use of an RNN for scoring proposed&#160;translations (Cho </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014a) and for generating translated sentences (Sutskever&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014). Jean </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) scaled these models to larger vocabularies.</span></p>
<p><span class="font64" style="font-weight:bold;">Using an Attention Mechanism and Aligning Pieces of Data</span></p><div>
<p><span class="font64" style="font-weight:bold;">12.4.5.1</span></p></div><div><div><img src="main-143.jpg" alt=""/>
<p><span class="font64">Figure 12.6: A modern attention mechanism, as introduced by Bahdanau </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2015), is essentially a weighted average. A context vector c is formed by taking a weighted average&#160;of feature vectors h<sup>(t)</sup> with weights a<sup>(t)</sup>. In some applications, the feature vectors h are&#160;hidden units of a neural network, but they may also be raw input to the model. The&#160;weights a<sup>(t)</sup> are produced by the model itself. They are usually values in the interval&#160;[0,1] and are intended to concentrate around just one h<sup>(t)</sup> so that the weighted average&#160;approximates reading that one specific time step precisely. The weights a<sup>(t)</sup> are usually&#160;produced by applying a softmax function to relevance scores emitted by another portion&#160;of the model. The attention mechanism is more expensive computationally than directly&#160;indexing the desired h<sup>(t)</sup>, but direct indexing cannot be trained with gradient descent. The&#160;attention mechanism based on weighted averages is a smooth, differentiable approximation&#160;that can be trained with existing optimization algorithms.</span></p></div></div>
<p><span class="font64">Using a fixed-size representation to capture all the semantic details of a very long sentence of say 60 words is very difficult. It can be achieved by training a&#160;sufficiently large RNN well enough and for long enough, as demonstrated by Cho&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014a) and Sutskever </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014). However, a more efficient approach is&#160;to read the whole sentence or paragraph (to get the context and the gist of what&#160;is being expressed), then produce the translated words one at a time, each time&#160;focusing on a different part of the input sentence in order to gather the semantic&#160;details that are required to produce the next output word. That is exactly the&#160;idea that Bahdanau </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) first introduced. The attention mechanism used&#160;to focus on specific parts of the input sequence at each time step is illustrated in&#160;Fig. 12.6.</span></p>
<p><span class="font64">We can think of an attention-based system as having three components:</span></p>
<p><span class="font64">1. &#160;&#160;&#160;A process that “</span><span class="font64" style="font-weight:bold;font-style:italic;">reads״</span><span class="font64"> raw data (such as source words in a source sentence),&#160;and converts them into distributed representations, with one feature vector&#160;associated with each word position.</span></p>
<p><span class="font64">2. &#160;&#160;&#160;A list of feature vectors storing the output of the reader. This can be&#160;understood as a “ </span><span class="font64" style="font-weight:bold;font-style:italic;">memory</span><span class="font64">’ containing a sequence of facts, which can be&#160;retrieved later, not necessarily in the same order, without having to visit all&#160;of them.</span></p>
<p><span class="font64">3. &#160;&#160;&#160;A process that “ </span><span class="font64" style="font-weight:bold;font-style:italic;">exploits</span><span class="font64">” the content of the memory to sequentially perform&#160;a task, at each time step having the ability put attention on the content of&#160;one memory element (or a few, with a different weight).</span></p>
<p><span class="font64">The third component generates the translated sentence.</span></p>
<p><span class="font64">When words in a sentence written in one language are aligned with corresponding words in a translated sentence in another language, it becomes possible to relate the corresponding word embeddings. Earlier work showed that one could learn a&#160;kind of translation matrix relating the word embeddings in one language with the&#160;word embeddings in another (Kocisky </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014), yielding lower alignment error&#160;rates than traditional approaches based on the frequency counts in the phrase table.&#160;There is even earlier work on learning cross-lingual word vectors (Klementiev </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2012). Many extensions to this approach are possible. For example, more efficient&#160;cross-lingual alignment (Gouws </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014) allows training on larger datasets.</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">12.4.6 Historical Perspective</span></h5>
<p><span class="font64">The idea of distributed representations for symbols was introduced by Rumelhart </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (1986a) in one of the first explorations of back-propagation, with symbols&#160;corresponding to the identity of family members and the neural network capturing&#160;the relationships between family members, with training examples forming triplets&#160;such as (Colin, Mother, Victoria). The first layer of the neural network learned&#160;a representation of each family member. For example, the features for Colin&#160;might represent which family tree Colin was in, what branch of that tree he was&#160;in, what generation he was from, etc. One can think of the neural network as&#160;computing learned rules relating these attributes together in order to obtain the&#160;desired predictions. The model can then make predictions such as inferring who is&#160;the mother of Colin.</span></p>
<p><span class="font64">The idea of forming an embedding for a symbol was extended to the idea of an embedding for a word by Deerwester </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (1990). These embeddings were learned&#160;using the SVD. Later, embeddings would be learned by neural networks.</span></p>
<p><span class="font64">The history of natural language processing is marked by transitions in the popularity of different ways of representing the input to the model. Following&#160;this early work on symbols or words, some of the earliest applications of neural&#160;networks to NLP (Miikkulainen and Dyer, 1991; Schmidhuber, 1996) represented&#160;the input as a sequence of characters.</span></p>
<p><span class="font64">Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2001) returned the focus to modeling words and introduced neural language models, which produce interpretable word embeddings. These&#160;neural models have scaled up from defining representations of a small set of symbols&#160;in the 1980s to millions of words (including proper nouns and misspellings) in&#160;modern applications. This computational scaling effort led to the invention of the&#160;techniques described above in Sec. 12.4.3.</span></p>
<p><span class="font64">Initially, the use of words as the fundamental units of language models yielded improved language modeling performance (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2001). To this day,&#160;new techniques continually push both character-based models (Sutskever </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2011) and word-based models forward, with recent work (Gillick </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015) even&#160;modeling individual bytes of Unicode characters.</span></p>
<p><span class="font64">The ideas behind neural language models have been extended into several natural language processing applications, such as parsing (Henderson, 2003, 2004;&#160;Collobert, 2011), part-of-speech tagging, semantic role labeling, chunking, etc,&#160;sometimes using a single multi-task learning architecture (Collobert and Weston,&#160;2008a; Collobert </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011a) in which the word embeddings are shared across&#160;tasks.</span></p>
<p><span class="font64">Two-dimensional visualizations of embeddings became a popular tool for analyzing language models following the development of the t-SNE dimensionality reduction algorithm (van der Maaten and Hinton, 2008) and its high-profile application to visualization word embeddings by Joseph Turian in 2009.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">12.5 Other Applications</span></h4>
<p><span class="font64">In this section we cover a few other types of applications of deep learning that are different from the standard object recognition, speech recognition and natural&#160;language processing tasks discussed above. Part III of this book will expand&#160;that scope even further to include tasks requiring the ability to generate rich&#160;high-dimensional samples (unlike “the next word,” in language models).</span></p><h5><a id="bookmark6"></a><span class="font64" style="font-weight:bold;">12.5.1 Recommender Systems</span></h5>
<p><span class="font64">One of the major families of applications of machine learning in the information technology sector is the ability to make recommendations of items to potential&#160;users or customers. Two major types of applications can be distinguished: online&#160;advertising and item recommendations (often these recommendations are still for&#160;the purpose of selling a product). Both rely on predicting the association between&#160;a user and an item, either to predict the probability of some action (the user&#160;buying the product, or some proxy for this action) or the expected gain (which&#160;may depend on the value of the product) if an ad is shown or a recommendation is&#160;made regarding that product to that user. The internet is currently financed in&#160;great part by various forms of online advertising. There are major parts of the&#160;economy that rely on online shopping. Companies including Amazon and eBay&#160;use machine learning, including deep learning, for their product recommendations.&#160;Sometimes, the items are not products that are actually for sale. Examples include&#160;selecting posts to display on social network news feeds, recommending movies to&#160;watch, recommending jokes, recommending advice from experts, matching players&#160;for video games, or matching people in dating services.</span></p>
<p><span class="font64">Often, this association problem is handled like a supervised learning problem: given some information about the item and about the user, predict the proxy of&#160;interest (user clicks on ad, user enters a rating, user clicks on a “like” button, user&#160;buys product, user spends some amount of money on the product, user spends&#160;time visiting a page for the product, etc). This often ends up being either a&#160;regression problem (predicting some conditional expected value) or a probabilistic&#160;classification problem (predicting the conditional probability of some discrete&#160;event).</span></p>
<p><span class="font64">The early work on recommender systems relied on minimal information as inputs for these predictions: the user ID and the item ID. In this context, the only&#160;way to generalize is to rely on the similarity between the patterns of values of the&#160;target variable for different users or for different items. Suppose that user 1 and&#160;user 2 both like items A, B and C. From this, we may infer that user 1 and user 2&#160;have similar tastes. If user 1 likes item D, then this should be a strong cue that&#160;user 2 will also like D. Algorithms based on this principle come under the name of&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">collaborative filtering.</span><span class="font64"> Both non-parametric approaches (such as nearest-neighbor&#160;methods based on the estimated similarity between patterns of preferences) and&#160;parametric methods are possible. Parametric methods often rely on learning a&#160;distributed representation (also called an embedding) for each user and for each&#160;item. Bilinear prediction of the target variable (such as a rating) is a simple&#160;parametric method that is highly successful and often found as a component of&#160;state-of-the-art systems. The prediction is obtained by the dot product between&#160;the user embedding and the item embedding (possibly corrected by constants that&#160;depend only on either the user ID or the item ID). Let R be the matrix containing&#160;our predictions, A a matrix with user embeddings in its rows and B a matrix with&#160;item embeddings in its columns. Let b and c be vectors that contain respectively&#160;a kind of bias for each user (representing how grumpy or positive that user is&#160;in general) and for each item (representing its general popularity). The bilinear&#160;prediction is thus obtained as follows:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">R</span><span class="font63" style="font-style:italic;">u,i</span><span class="font64"> = </span><span class="font64" style="font-weight:bold;font-style:italic;">b</span><span class="font63" style="font-style:italic;"><sub>u</sub></span><span class="font64"> + <sup>c</sup></span><span class="font63" style="font-style:italic;">i</span><span class="font64"> + </span><span class="font64" style="text-decoration:underline;">^ '</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">A</span><span class="font63" style="font-style:italic;"><sub>u</sub>,j</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>B</sup></span><span class="font63" style="font-style:italic;">j,i</span><span class="font64" style="font-weight:bold;font-style:italic;">.</span><span class="font64"> &#160;&#160;&#160;<sup>(12</sup>.<sup>21)</sup></span></p>
<p><span class="font64"><sup>j</sup></span></p>
<p><span class="font64">Typically one wants to minimize the squared error between predicted ratings RU,i and actual ratings </span><span class="font64" style="font-weight:bold;font-style:italic;">R</span><span class="font63" style="font-style:italic;"><sub>Uji</sub></span><span class="font64" style="font-weight:bold;font-style:italic;">.</span><span class="font64"> User embeddings and item embeddings can then be&#160;conveniently visualized when they are first reduced to a low dimension (two or&#160;three), or they can be used to compare users or items against each other, just&#160;like word embeddings. One way to obtain these embeddings is by performing a&#160;singular value decomposition of the matrix </span><span class="font64" style="font-weight:bold;font-style:italic;">R</span><span class="font64"> of actual targets (such as ratings).&#160;This corresponds to factorizing R = </span><span class="font64" style="font-weight:bold;font-style:italic;">UDV'</span><span class="font64"> (or a normalized variant) into the&#160;product of two factors, the lower rank matrices A = UD and B = </span><span class="font64" style="font-weight:bold;font-style:italic;">V'</span><span class="font64">. One&#160;problem with the SVD is that it treats the missing entries in an arbitrary way,&#160;as if they corresponded to a target value of 0. Instead we would like to avoid&#160;paying any cost for the predictions made on missing entries. Fortunately, the&#160;sum of squared errors on the observed ratings can also be easily minimized by&#160;gradient-based optimization. The SVD and the bilinear prediction of Eq. 12.21 both&#160;performed very well in the competition for the Netflix prize (Bennett and Lanning,&#160;2007), aiming at predicting ratings for films, based only on previous ratings by&#160;a large set of anonymous users. Many machine learning experts participated in&#160;this competition, which took place between 2006 and 2009. It raised the level of&#160;research in recommender systems using advanced machine learning and yielded&#160;improvements in recommender systems. Even though it did not win by itself,&#160;the simple bilinear prediction or SVD was a component of the ensemble models&#160;presented by most of the competitors, including the winners (Toscher </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009;&#160;Koren, 2009).</span></p>
<p><span class="font64">Beyond these bilinear models with distributed representations, one of the first uses of neural networks for collaborative filtering is based on the RBM undirected&#160;probabilistic model (Salakhutdinov </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2007). RBMs were an important element&#160;of the ensemble of methods that won the Netflix competition (Toscher </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009;&#160;Koren, 2009). More advanced variants on the idea of factorizing the ratings matrix&#160;have also been explored in the neural networks community (Salakhutdinov and</span></p>
<p><span class="font64">Mnih, 2008).</span></p>
<p><span class="font64">However, there is a basic limitation of collaborative filtering systems: when a new item or a new user is introduced, its lack of rating history means that there&#160;is no way to evaluate its similarity with other items or users (respectively), or&#160;the degree of association between, say, that new user and existing items. This&#160;is called the problem of cold-start recommendations. A general way of solving&#160;the cold-start recommendation problem is to introduce extra information about&#160;the individual users and items. For example, this extra information could be user&#160;profile information or features of each item. Systems that use such information are&#160;called </span><span class="font64" style="font-weight:bold;font-style:italic;">content-based recommender systems.</span><span class="font64"> The mapping from a rich set of user&#160;features or item features to an embedding can be learned through a deep learning&#160;architecture (Huang </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013; Elkahky </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015).</span></p>
<p><span class="font64">Specialized deep learning architectures such as convolutional networks have also been applied to learn to extract features from rich content such as from musical&#160;audio tracks, for music recommendation (van den Oord </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013). In that work,&#160;the convolutional net takes acoustic features as input and computes an embedding&#160;for the associated song. The dot product between this song embedding and the&#160;embedding for a user is then used to predict whether a user will listen to the song.</span></p>
<p><span class="font64">12.5.1.1 Exploration Versus Exploitation</span></p>
<p><span class="font64">When making recommendations to users, an issue arises that goes beyond ordinary supervised learning and into the realm of reinforcement learning. Many recommendation problems are most accurately described theoretically as </span><span class="font64" style="font-weight:bold;font-style:italic;">contextual bandits&#160;</span><span class="font64">(Langford and Zhang, 2008; Lu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010). The issue is that when we use the&#160;recommendation system to collect data, we get a biased and incomplete view of&#160;the preferences of users: we only see the responses of users to the items they were&#160;recommended and not to the other items. In addition, in some cases we may not&#160;get any information on users for whom no recommendation has been made (for&#160;example, with ad auctions, it may be that the price proposed for an ad was below&#160;a minimum price threshold, or does not win the auction, so the ad is not shown at&#160;all). More importantly, we get no information about what outcome would have&#160;resulted from recommending any of the other items. This would be like training a&#160;classifier by picking one class y for each training example x (typically the class&#160;with the highest probability according to the model) and then only getting as&#160;feedback whether this was the correct class or not. Clearly, each example conveys&#160;less information than in the supervised case where the true label </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"> is directly&#160;accessible, so more examples are necessary. Worse, if we are not careful, we could&#160;end up with a system that continues picking the wrong decisions even as more&#160;and more data is collected, because the correct decision initially had a very low&#160;probability: until the learner picks that correct decision, it does not learn about&#160;the correct decision. This is similar to the situation in reinforcement learning&#160;where only the reward for the selected action is observed. In general, reinforcement&#160;learning can involve a sequence of many actions and many rewards. The bandits&#160;scenario is a special case of reinforcement learning, in which the learner takes only&#160;a single action and receives a single reward. The bandit problem is easier in the&#160;sense that the learner knows which reward is associated with which action. In&#160;the general reinforcement learning scenario, a high reward or a low reward might&#160;have been caused by a recent action or by an action in the distant past. The term&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">contextual</span><span class="font64"> bandits refers to the case where the action is taken in the context of&#160;some input variable that can inform the decision. For example, we at least know&#160;the user identity, and we want to pick an item. The mapping from context to&#160;action is also called a </span><span class="font64" style="font-weight:bold;font-style:italic;">policy.</span><span class="font64"> The feedback loop between the learner and the data&#160;distribution (which now depends on the actions of the learner) is a central research&#160;issue in the reinforcement learning and bandits literature.</span></p>
<p><span class="font64">Reinforcement learning requires choosing a tradeoff between </span><span class="font64" style="font-weight:bold;font-style:italic;">exploration</span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">exploitation.</span><span class="font64"> Exploitation refers to taking actions that come from the current, best&#160;version of the learned policy—actions that we know will achieve a high reward.&#160;Exploration refers to taking actions specifically in order to obtain more training&#160;data. If we know that given context x, action </span><span class="font64" style="font-weight:bold;font-style:italic;">a</span><span class="font64"> gives us a reward of 1, we do not&#160;know whether that is the best possible reward. We may want to exploit our current&#160;policy and continue taking action a in order to be relatively sure of obtaining a&#160;reward of 1. However, we may also want to explore by trying action </span><span class="font64" style="font-weight:bold;font-style:italic;">a'.</span><span class="font64"> We do not&#160;know what will happen if we try action a'. We hope to get a reward of 2, but we&#160;run the risk of getting a reward of 0. Either way, we at least gain some knowledge.</span></p>
<p><span class="font64">Exploration can be implemented in many ways, ranging from occasionally taking random actions intended to cover the entire space of possible actions, to&#160;model-based approaches that compute a choice of action based on its expected&#160;reward and the model’s amount of uncertainty about that reward.</span></p>
<p><span class="font64">Many factors determine the extent to which we prefer exploration or exploitation. One of the most prominent factors is the time scale we are interested in. If the&#160;agent has only a short amount of time to accrue reward, then we prefer more&#160;exploitation. If the agent has a long time to accrue reward, then we begin with&#160;more exploration so that future actions can be planned more effectively with more&#160;knowledge. As time progresses and our learned policy improves, we move toward&#160;more exploitation.</span></p>
<p><span class="font64">Supervised learning has no tradeoff between exploration and exploitation because the supervision signal always specifies which output is correct for each&#160;input. There is no need to try out different outputs to determine if one is better&#160;than the model’s current output—we always know that the label is the best output.</span></p>
<p><span class="font64">Another difficulty arising in the context of reinforcement learning, besides the exploration-exploitation trade-off, is the difficulty of evaluating and comparing&#160;different policies. Reinforcement learning involves interaction between the learner&#160;and the environment. This feedback loop means that it is not straightforward to&#160;evaluate the learner’s performance using a fixed set of test set input values. The&#160;policy itself determines which inputs will be seen. Dudik </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011) present&#160;techniques for evaluating contextual bandits.</span></p><h5><a id="bookmark7"></a><span class="font64" style="font-weight:bold;">12.5.2 Knowledge Representation, Reasoning and Question Answering</span></h5>
<p><span class="font64">Deep learning approaches have been very successful in language modeling, machine translation and natural language processing due to the use of embeddings for&#160;symbols (Rumelhart </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1986a) and words (Deerwester </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1990; Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2001). These embeddings represent semantic knowledge about individual words&#160;and concepts. A research frontier is to develop embeddings for phrases and for&#160;relations between words and facts. Search engines already use machine learning for&#160;this purpose but much more remains to be done to improve these more advanced&#160;representations.</span></p>
<p><span class="font64" style="font-weight:bold;">12.5.2.1 Knowledge, Relations and Question Answering</span></p>
<p><span class="font64">indexRelations One interesting research direction is determining how distributed representations can be trained to capture the </span><span class="font64" style="font-weight:bold;font-style:italic;">relations</span><span class="font64"> between two entities. These&#160;relations allow us to formalize facts about objects and how objects interact with&#160;each other.</span></p>
<p><span class="font64">In mathematics, a </span><span class="font64" style="font-weight:bold;font-style:italic;">binary relation</span><span class="font64"> is a set of ordered pairs of objects. Pairs that are in the set are said to have the relation while those who are not in the set&#160;do not. For example, we can define the relation “is less than” on the set of entities&#160;{1, </span><span class="font18">2</span><span class="font64">,3} by defining the set of ordered pairs S = {(1,</span><span class="font18">2</span><span class="font64">), (</span><span class="font18">1</span><span class="font64">, 3), (</span><span class="font18">2</span><span class="font64">, 3)}. Once this&#160;relation is defined, we can use it like a verb. Because (1, 2) G S, we say that 1 is&#160;less than 2. Because (2,1) G S ,we can not say that 2 is less than 1. Of course, the&#160;entities that are related to one another need not be numbers. We could define a&#160;relation is_a_type_of containing tuples like (dog, mammal).</span></p>
<p><span class="font64">In the context of AI, we think of a relation as a sentence in a syntactically simple and highly structured language. The relation plays the role of a verb,&#160;while two arguments to the relation play the role of its subject and object. These&#160;sentences take the form of a triplet of tokens</span></p><div>
<p><span class="font64">(</span><span class="font18">12</span><span class="font64">.</span><span class="font18">22</span><span class="font64">)</span></p></div><div>
<p><span class="font64">(12.23)</span></p></div>
<p><span class="font64">(subject, verb, object)</span></p>
<p><span class="font64">with values</span></p>
<p><span class="font64">(entity ^ relation^ entity </span><span class="font64" style="font-weight:bold;font-style:italic;">k).</span></p>
<p><span class="font64">We can also define an </span><span class="font64" style="font-weight:bold;font-style:italic;">attribute</span><span class="font64">, a concept analogous to a relation, but taking only one argument:</span></p>
<p><span class="font64">(entity^, attribute j). &#160;&#160;&#160;(12.24)</span></p>
<p><span class="font64">For example, we could define the has_fur attribute, and apply it to entities like dog.</span></p>
<p><span class="font64">Many applications require representing relations and reasoning about them. How should we best do this within the context of neural networks?</span></p>
<p><span class="font64">Machine learning models of course require training data. We can infer relations between entities from training datasets consisting of unstructured natural language.&#160;There are also structured databases that identify relations explicitly. A common&#160;structure for these databases is the </span><span class="font64" style="font-weight:bold;font-style:italic;">relational database,</span><span class="font64"> which stores this same&#160;kind of information, albeit not formatted as three token sentences. When a&#160;database is intended to convey commonsense knowledge about everyday life or&#160;expert knowledge about an application area to an artificial intelligence system,&#160;we call the database a </span><span class="font64" style="font-weight:bold;font-style:italic;">knowledge base.</span><span class="font64"> Knowledge bases range from general&#160;ones like Freebase, OpenCyc, WordNet, or Wikibase,</span><span class="font18"><sup>1</sup></span><span class="font64"> etc. to more specialized&#160;knowledge bases, like GeneOntology.</span><span class="font18"><sup>2</sup></span><span class="font64"> Representations for entities and relations&#160;can be learned by considering each triplet in a knowledge base as a training example&#160;and maximizing a training objective that captures their joint distribution (Bordes&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013a).</span></p>
<p><span class="font64">In addition to training data, we also need to define a model family to train. A common approach is to extend neural language models to model entities and&#160;relations. Neural language models learn a vector that provides a distributed&#160;representation of each word. They also learn about interactions between words,&#160;such as which word is likely to come after a sequence of words, by learning functions&#160;of these vectors. We can extend this approach to entities and relations by learning&#160;an embedding vector for each relation. In fact, the parallel between modeling</span></p><div>
<p><span class="font11" style="font-weight:bold;"><sup>1</sup>Respectively available from these web sites: princeton.edu, wikiba.se</span></p></div><div>
<p><span class="font11" style="font-weight:bold;">freebase.com, cyc.com/opencyc, wordnet.</span></p></div>
<p><span class="font11" style="font-weight:bold;">geneontology.org</span></p>
<p><span class="font64">language and modeling knowledge encoded as relations is so close that researchers have trained representations of such entities by using </span><span class="font64" style="font-weight:bold;font-style:italic;">both</span><span class="font64"> knowledge bases </span><span class="font64" style="font-weight:bold;font-style:italic;">and&#160;</span><span class="font64">natural language sentences (Bordes </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011, 2012; Wang </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014a) or&#160;combining data from multiple relational databases (Bordes </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013b). Many&#160;possibilities exist for the particular parametrization associated with such a model.&#160;Early work on learning about relations between entities (Paccanaro and Hinton,&#160;</span><span class="font18">2000</span><span class="font64">) posited highly constrained parametric forms (“linear relational embeddings”),&#160;often using a different form of representation for the relation than for the entities.&#160;For example, Paccanaro and Hinton (2000) and Bordes </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011) used vectors for&#160;entities and matrices for relations, with the idea that a relation acts like an operator&#160;on entities. Alternatively, relations can be considered as any other entity (Bordes&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> </span><span class="font18">2012</span><span class="font64">), allowing us to make statements about relations, but more flexibility is&#160;put in the machinery that combines them in order to model their joint distribution.</span></p>
<p><span class="font64">A practical short-term application of such models is </span><span class="font64" style="font-weight:bold;font-style:italic;">link prediction:</span><span class="font64"> predicting missing arcs in the knowledge graph. This is a form of generalization to new&#160;facts, based on old facts. Most of the knowledge bases that currently exist have&#160;been constructed through manual labor, which tends to leave many and probably&#160;the majority of true relations absent from the knowledge base. See Wang </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(2014b), Lin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) and Garcia-Duran </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) for examples of such an&#160;application.</span></p>
<p><span class="font64">Evaluating the performance of a model on a link prediction task is difficult because we have only a dataset of positive examples (facts that are known to&#160;be true). If the model proposes a fact that is not in the dataset, we are unsure&#160;whether the model has made a mistake or discovered a new, previously unknown&#160;fact. The metrics are thus somewhat imprecise and are based on testing how the&#160;model ranks a held-out of set of known true positive facts compared to other facts&#160;that are less likely to be true. A common way to construct interesting examples&#160;that are probably negative (facts that are probably false) is to begin with a true&#160;fact and create corrupted versions of that fact, for example by replacing one entity&#160;in the relation with a different entity selected at random. The popular precision at&#160;</span><span class="font18">10</span><span class="font64">% metric counts how many times the model ranks a “correct” fact among the&#160;top </span><span class="font18">10</span><span class="font64">% of all corrupted versions of that fact.</span></p>
<p><span class="font64">Another application of knowledge bases and distributed representations for them is </span><span class="font64" style="font-weight:bold;font-style:italic;">word-sense disambiguation</span><span class="font64"> (Navigli and Velardi, 2005; Bordes </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012),&#160;which is the task of deciding which of the senses of a word is the appropriate one,&#160;in some context.</span></p>
<p><span class="font64">Eventually, knowledge of relations combined with a reasoning process and understanding of natural language could allow us to build a general question&#160;answering system. A general question answering system must be able to process&#160;input information and remember important facts, organized in a way that enables&#160;it to retrieve and reason about them later. This remains a difficult open problem&#160;which can only be solved in restricted “toy” environments. Currently, the best&#160;approach to remembering and retrieving specific declarative facts is to use an&#160;explicit memory mechanism, as described in Sec. 10.12. Memory networks were&#160;first proposed to solve a toy question answering task (Weston </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014). Kumar&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) have proposed an extension that uses GRU recurrent nets to read&#160;the input into the memory and to produce the answer given the contents of the&#160;memory.</span></p>
<p><span class="font64">Deep learning has been applied to many other applications besides the ones described here, and will surely be applied to even more after this writing. It would&#160;be impossible to describe anything remotely resembling a comprehensive coverage&#160;of such a topic. This survey provides a representative sample of what is possible&#160;as of this writing.</span></p>
<p><span class="font64">This concludes Part II, which has described modern practices involving deep networks, comprising all of the most successful methods. Generally speaking, these&#160;methods involve using the gradient of a cost function to find the parameters of a&#160;model that approximates some desired function. With enough training data, this&#160;approach is extremely powerful. We now turn to Part III, in which we step into the&#160;territory of research—methods that are designed to work with less training data&#160;or to perform a greater variety of tasks, where the challenges are more difficult&#160;and not as close to being solved as the situations we have described so far.</span></p>
<p><span class="font66" style="font-weight:bold;">Part III</span></p>
<p><span class="font67" style="font-weight:bold;">Deep Learning Research</span></p>
<p><span class="font64">This part of the book describes the more ambitious and advanced approaches to deep learning, currently pursued by the research community.</span></p>
<p><span class="font64">In the previous parts of the book, we have shown how to solve supervised learning problems—how to learn to map one vector to another, given enough&#160;examples of the mapping.</span></p>
<p><span class="font64">Not all problems we might want to solve fall into this category. We may wish to generate new examples, or determine how likely some point is, or handle&#160;missing values and take advantage of a large set of unlabeled examples or examples&#160;from related tasks. A shortcoming of the current state of the art for industrial&#160;applications is that our learning algorithms require large amounts of supervised&#160;data to achieve good accuracy. In this part of the book, we discuss some of&#160;the speculative approaches to reducing the amount of labeled data necessary&#160;for existing models to work well and be applicable across a broader range of&#160;tasks. Accomplishing these goals usually requires some form of unsupervised or&#160;semi-supervised learning.</span></p>
<p><span class="font64">Many deep learning algorithms have been designed to tackle unsupervised learning problems, but none have truly solved the problem in the same way that&#160;deep learning has largely solved the supervised learning problem for a wide variety of&#160;tasks. In this part of the book, we describe the existing approaches to unsupervised&#160;learning and some of the popular thought about how we can make progress in this&#160;field.</span></p>
<p><span class="font64">A central cause of the difficulties with unsupervised learning is the high dimensionality of the random variables being modeled. This brings two distinct challenges: a statistical challenge and a computational challenge. The </span><span class="font64" style="font-weight:bold;font-style:italic;">statistical&#160;challenge</span><span class="font64"> regards generalization: the number of configurations we may want to&#160;distinguish can grow exponentially with the number of dimensions of interest, and&#160;this quickly becomes much larger than the number of examples one can possibly&#160;have (or use with bounded computational resources). The </span><span class="font64" style="font-weight:bold;font-style:italic;">computational challenge&#160;</span><span class="font64">associated with high-dimensional distributions arises because many algorithms for&#160;learning or using a trained model (especially those based on estimating an explicit&#160;probability function) involve intractable computations that grow exponentially&#160;with the number of dimensions.</span></p>
<p><span class="font64">With probabilistic models, this computational challenge arises from the need to perform intractable inference or simply from the need to normalize the distribution.</span></p>
<p><span class="font64">• Intractable inference: inference is discussed mostly in Chapter 19. It regards the question of guessing the probable values of some variables a,&#160;given other variables b, with respect to a model that captures the joint&#160;distribution between a, b and c. In order to even compute such conditional&#160;probabilities one needs to sum over the values of the variables c, as well as&#160;compute a normalization constant which sums over the values of a and c.</span></p>
<p><span class="font64">• Intractable normalization constants (the partition function): the</span></p>
<p><span class="font64">partition function is discussed mostly in Chapter 18. Normalizing constants of probability functions come up in inference (above) as well as in learning.&#160;Many probabilistic models involve such a normalizing constant. Unfortunately, learning such a model often requires computing the gradient of the&#160;logarithm of the partition function with respect to the model parameters.&#160;That computation is generally as intractable as computing the partition&#160;function itself. Monte Carlo Markov chain (MCMC) methods (Chapter 17)&#160;are often used to deal with the partition function (computing it or its gradient). Unfortunately, MCMC methods suffer when the modes of the model&#160;distribution are numerous and well-separated, especially in high-dimensional&#160;spaces (Sec. 17.5).</span></p>
<p><span class="font64">One way to confront these intractable computations is to approximate them, and many approaches have been proposed as discussed in this third part of the&#160;book. Another interesting way, also discussed here, would be to avoid these&#160;intractable computations altogether by design, and methods that do not require&#160;such computations are thus very appealing. Several generative models have been&#160;proposed in recent years, with that motivation. A wide variety of contemporary&#160;approaches to generative modeling are discussed in Chapter 20.</span></p>
<p><span class="font64">Part III is the most important for a researcher—someone who wants to understand the breadth of perspectives that have been brought to the field of deep learning, and push the field forward towards true artificial intelligence.</span></p>
</body>
</html>