<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 20</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Deep Generative Models</span></h2>
<p><span class="font64">In this chapter, we present several of the specific kinds of generative models that can be built and trained using the techniques presented in Chapters 16, 17, 18 and&#160;19. All of these models represent probability distributions over multiple variables&#160;in some way. Some allow the probability distribution function to be evaluated&#160;explicitly. Others do not allow the evaluation of the probability distribution&#160;function, but support operations that implicitly require knowledge of it, such&#160;as drawing samples from the distribution. Some of these models are structured&#160;probabilistic models described in terms of graphs and factors, using the language&#160;of graphical models presented in Chapter 16. Others can not easily be described&#160;in terms of factors, but represent probability distributions nonetheless.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">20.1 Boltzmann Machines</span></h4>
<p><span class="font64">Boltzmann machines were originally introduced as a general “connectionist” approach to learning arbitrary probability distributions over binary vectors (Fahlman </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1983; Ackley </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1985; Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1984; Hinton and Sejnowski, 1986).&#160;Variants of the Boltzmann machine that include other kinds of variables have long&#160;ago surpassed the popularity of the original. In this section we briefly introduce&#160;the binary Boltzmann machine and discuss the issues that come up when trying to&#160;train and perform inference in the model.</span></p>
<p><span class="font64">We define the Boltzmann machine over a d-dimensional binary random vector x £ {0,1 </span><span class="font64" style="font-weight:bold;font-style:italic;">}<sup>d</sup>.</span><span class="font64"> The Boltzmann machine is an energy-based model (Sec. 16.2.4),</span></p>
<p><span class="font64">meaning we define the joint probability distribution using an energy function:</span></p>
<p><span class="font64" style="font-weight:bold;">P (x) = </span><span class="font64" style="font-weight:bold;text-decoration:line-through;"><sup>exp(</sup></span><span class="font64" style="text-decoration:line-through;">־</span><span class="font64" style="font-weight:bold;text-decoration:line-through;"><sup>E (x</sup>»</span><span class="font64" style="font-weight:bold;">, &#160;&#160;&#160;</span><span class="font64">(20.1)</span></p>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;">E </span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) is the energy function and </span><span class="font64" style="font-weight:bold;font-style:italic;">Z</span><span class="font64"> is the partition function that ensures that ^</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>x</sub> P</span><span class="font64" style="font-weight:bold;">(x) = 1</span><span class="font64">. The energy function of the Boltzmann machine is given by</span></p>
<p><span class="font64" style="font-weight:bold;">E(x) = </span><span class="font64">-</span><span class="font64" style="font-weight:bold;">x<sup>T</sup>Ux </span><span class="font64">- </span><span class="font64" style="font-weight:bold;">bx, &#160;&#160;&#160;</span><span class="font64">(</span><span class="font18">20</span><span class="font64">.</span><span class="font18">2</span><span class="font64">)</span></p>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;">U </span><span class="font64">is the “weight” matrix of model parameters and </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">is the vector of bias parameters.</span></p>
<p><span class="font64">In the general setting of the Boltzmann machine, we are given a set of training examples, each of which are </span><span class="font64" style="font-weight:bold;">n</span><span class="font64">-dimensional. Eq. 20.1 describes the joint probability&#160;distribution over the observed variables. While this scenario is certainly viable,&#160;it does limit the kinds of interactions between the observed variables to those&#160;described by the weight matrix. Specifically, it means that the probability of one&#160;unit being on is given by a linear model (logistic regression) from the values of the&#160;other units.</span></p>
<p><span class="font64">The Boltzmann machine becomes more powerful when not all the variables are observed. In this case, the non-observed variables, or </span><span class="font64" style="font-weight:bold;font-style:italic;">latent</span><span class="font64"> variables, can&#160;act similarly to hidden units in a multi-layer perceptron and model higher-order&#160;interactions among the visible units. Just as the addition of hidden units to&#160;convert logistic regression into an MLP results in the MLP being a universal&#160;approximator of functions, a Boltzmann machine with hidden units is no longer&#160;limited to modeling linear relationships between variables. Instead, the Boltzmann&#160;machine becomes a universal approximator of probability mass functions over&#160;discrete variables (Le Roux and Bengio, 2008).</span></p>
<p><span class="font64">Formally, we decompose the units </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">into two subsets: the visible units </span><span class="font64" style="font-weight:bold;">v </span><span class="font64">and the latent (or hidden) units </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">. The energy function becomes</span></p>
<p><span class="font64" style="font-weight:bold;">E(v, h) = </span><span class="font64">-</span><span class="font64" style="font-weight:bold;">v<sup>T</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">Rv -</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">v<sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">Wh -</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">h<sup>T</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">Sh -</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">b<sup>T</sup>v </span><span class="font64">- </span><span class="font64" style="font-weight:bold;">c<sup>T</sup>h. &#160;&#160;&#160;</span><span class="font64">(20.3)</span></p>
<p><span class="font64" style="font-weight:bold;">Boltzmann Machine Learning </span><span class="font64">Learning algorithms for Boltzmann machines are usually based on maximum likelihood. All Boltzmann machines have an&#160;intractable partition function, so the maximum likelihood gradient must be approximated using the techniques described in Chapter 18.</span></p>
<p><span class="font64">One interesting property of Boltzmann machines when trained with learning rules based on maximum likelihood is that the update for a particular weight&#160;connecting two units depends only the statistics of those two units, collected</span></p>
<p><span class="font64">under different distributions: P<sub>mo</sub>d<sub>e</sub>](</span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) and Pd<sub>ata</sub>(</span><span class="font64" style="font-weight:bold;">v</span><span class="font64">)P<sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (</span><span class="font64" style="font-weight:bold;">h </span><span class="font64">| </span><span class="font64" style="font-weight:bold;font-style:italic;">v</span><span class="font64">). The rest of the network participates in shaping those statistics, but the weight can be updated&#160;without knowing anything about the rest of the network or how those statistics were&#160;produced. This means that the learning rule is “local,” which makes Boltzmann&#160;machine learning somewhat biologically plausible. It is conceivable that if each&#160;neuron were a random variable in a Boltzmann machine, then the axons and&#160;dendrites connecting two random variables could learn only by observing the firing&#160;pattern of the cells that they actually physically touch. In particular, in the&#160;positive phase, two units that frequently activate together have their connection&#160;strengthened. This is an example of a Hebbian learning rule (Hebb, 1949) often&#160;summarized with the mnemonic “fire together, wire together.” Hebbian learning&#160;rules are among the oldest hypothesized explanations for learning in biological&#160;systems and remain relevant today (Giudice </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009).</span></p>
<p><span class="font64">Other learning algorithms that use more information than local statistics seem to require us to hypothesize the existence of more machinery than this. For&#160;example, for the brain to implement back-propagation in a multilayer perceptron,&#160;it seems necessary for the brain to maintain a secondary communication network for&#160;transmitting gradient information backwards through the network. Proposals for&#160;biologically plausible implementations (and approximations) of back-propagation&#160;have been made (Hinton, 2007a; Bengio, 2015) but remain to be validated, and&#160;Bengio (2015) links back-propagation of gradients to inference in energy-based&#160;models similar to the Boltzmann machine (but with continuous latent variables).</span></p>
<p><span class="font64">The negative phase of Boltzmann machine learning is somewhat harder to explain from a biological point of view. As argued in Sec. 18.2, dream sleep may&#160;be a form of negative phase sampling. This idea is more speculative though.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">20.2 Restricted Boltzmann Machines</span></h4>
<p><span class="font64">Invented under the name </span><span class="font64" style="font-weight:bold;font-style:italic;">harmonium</span><span class="font64"> (Smolensky, 1986), restricted Boltzmann machines are some of the most common building blocks of deep probabilistic models.&#160;We have briefly described RBMs previously, in Sec. 16.7.1. Here we review the&#160;previous information and go into more detail. RBMs are undirected probabilistic&#160;graphical models containing a layer of observable variables and a single layer of&#160;latent variables. RBMs may be stacked (one on top of the other) to form deeper&#160;models. See Fig. 20.1 for some examples. In particular, Fig. 20.1a shows the graph&#160;structure of the RBM itself. It is a bipartite graph, with no connections permitted&#160;between any variables in the observed layer or between any units in the latent&#160;layer.</span></p><div><div><img src="main-187.jpg" alt=""/></div></div><div>
<p><span class="font64">a</span></p></div><div><div><img src="main-188.jpg" alt=""/>
<p><span class="font64">b</span></p></div></div><div><div><img src="main-189.jpg" alt=""/></div></div>
<p><span class="font64">c</span></p>
<p><span class="font64">Figure 20.1: Examples of models that may be built with restricted Boltzmann machines. </span><span class="font64" style="font-weight:bold;font-style:italic;">(a)</span><span class="font64"> The restricted Boltzmann machine itself is an undirected graphical model based on&#160;a bipartite graph, with visible units in one part of the graph and hidden units in the&#160;other part. There are no connections among the visible units, nor any connections among&#160;the hidden units. Typically every visible unit is connected to every hidden unit but it&#160;is possible to construct sparsely connected RBMs such as convolutional RBMs. </span><span class="font64" style="font-weight:bold;font-style:italic;">(b)</span><span class="font64"> A&#160;deep belief network is a hybrid graphical model involving both directed and undirected&#160;connections. Like an RBM, it has no intra-layer connections. However, a DBN has&#160;multiple hidden layers, and thus there are connections between hidden units that are in&#160;separate layers. All of the local conditional probability distributions needed by the deep&#160;belief network are copied directly from the local conditional probability distributions of&#160;its constituent RBMs. Alternatively, we could also represent the deep belief network with&#160;a completely undirected graph, but it would need intra-layer connections to capture the&#160;dependencies between parents. </span><span class="font64" style="font-weight:bold;font-style:italic;">(c)</span><span class="font64"> A deep Boltzmann machine is an undirected graphical&#160;model with several layers of latent variables. Like RBMs and DBNs, DBMs lack intra-layer&#160;connections. DBMs are less closely tied to RBMs than DBNs are. When initializing a&#160;DBM from a stack of RBMs, it is necessary to modify the RBM parameters slightly. Some&#160;kinds of DBMs may be trained without first training a set of RBMs.</span></p>
<p><span class="font64">We begin with the binary version of the restricted Boltzmann machine, but as we see later there are extensions to other types of visible and hidden units.</span></p>
<p><span class="font64">More formally, let the observed layer consist of a set of </span><span class="font64" style="font-weight:bold;font-style:italic;">n <sub>v</sub></span><span class="font64"> binary random variables which we refer to collectively with the vector </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">. We refer to the latent or&#160;hidden layer of n^ binary random variables as </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">.</span></p>
<p><span class="font64">Like the general Boltzmann machine, the restricted Boltzmann machine is an energy-based model with the joint probability distribution specified by its energy&#160;function:</span></p><div>
<p><span class="font64">1</span></p>
<p><span class="font64">P(</span><span class="font64" style="font-weight:bold;">v </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">h </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) = — exp </span><span class="font64" style="font-weight:bold;font-style:italic;">(-E</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">v</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">)).</span></p>
<p><span class="font64">The energy function for an RBM is given by</span></p>
<p><span class="font64">E(</span><span class="font64" style="font-weight:bold;">v</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) = </span><span class="font64" style="font-weight:bold;font-style:italic;">-b v - c<sup>T</sup>h - v Wh, </span><span class="font64">and </span><span class="font64" style="font-weight:bold;font-style:italic;">Z</span><span class="font64"> is the normalizing constant known as the partition function:</span></p>
<p><span class="font64">Z = &#160;&#160;&#160;<sup>ex</sup>P {<sup>-E(</sup>^ </span><span class="font64" style="font-weight:bold;"><sup>h</sup></span><span class="font64"><sup>)</sup>} .</span></p>
<p><span class="font46" style="font-weight:bold;font-style:italic;">v h</span></p></div>
<p><span class="font64">(20.4)</span></p>
<p><span class="font64">(20.5) (20.6)</span></p>
<p><span class="font64">It is apparent from the definition of the partition function Z that the naive method of computing Z (exhaustively summing over all states) could be computationally&#160;intractable, unless a cleverly designed algorithm could exploit regularities in the&#160;probability distribution to compute Z faster. In the case of restricted Boltzmann&#160;machines, Long and Servedio (2010) formally proved that the partition function Z&#160;is intractable. The intractable partition function Z implies that the normalized&#160;joint probability distribution P(</span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) is also intractable to evaluate.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">20.2.1 Conditional Distributions</span></h5>
<p><span class="font64">Though P (v) is intractable, the bipartite graph structure of the RBM has the very special property that its conditional distributions P(</span><span class="font64" style="font-weight:bold;">h </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) and P(</span><span class="font64" style="font-weight:bold;">v </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) are&#160;factorial and relatively simple to compute and to sample from.</span></p>
<p><span class="font64">Deriving the conditional distributions from the joint distribution is straightforward:</span></p><div>
<p><span class="font64" style="font-weight:bold;">P(h | v) =</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">P(h, v) P(v)&#160;11</span></p></div><div>
<p><span class="font64">exp</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">|b<sup>T</sup>v + c h + v<sup>T</sup>Wh|</span></p></div><div>
<p><span class="font64">P(</span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) Z 1</span></p>
<p><span class="font64">- exp j </span><span class="font64" style="font-weight:bold;">ch </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;">v<sup>T</sup> Wh|</span></p></div><div>
<p><span class="font64">Z</span></p></div><div>
<p><span class="font64">(20.7)</span></p>
<p><span class="font64">(20.8) (20.9)</span></p></div><div>
<p><span class="font45" style="font-weight:bold;font-style:italic;"><sup>n</sup>h</span></p></div><div>
<p><span class="font45" style="font-weight:bold;font-style:italic;"><sup>n</sup>h</span></p></div><div>
<p><span class="font64" style="font-weight:bold;"><sub>z</sub></span><span class="font64">, </span><span class="font64" style="font-weight:bold;"><sup>ex</sup>p &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">c <sup>h</sup>3 +</span><span class="font64" style="font-style:italic;">J2</span><span class="font64" style="font-weight:bold;"><sup>v</sup></span><span class="font64"><sup>T </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>W</sup>-J <sup>h</sup></span></p></div><div>
<p><span class="font64">Z</span></p></div><div>
<p><span class="font64">(20.10)</span></p></div><div>
<p><span class="font64"><sup>j</sup>=1</span></p></div><div>
<p><span class="font19" style="font-style:italic;"><sup>j</sup></span><span class="font64" style="font-style:italic;">=<sup>1</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;">1 </span><span class="font64">nh</span></p>
<p><span class="font64" style="font-weight:bold;">= z &#160;&#160;&#160;<sup>ex</sup>P </span><span class="font64">{</span><span class="font64" style="font-weight:bold;"><sup>c</sup> </span><span class="font64">A׳ </span><span class="font64" style="font-weight:bold;">+ </span><span class="font64"><sup>v T</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">'<sup>W</sup>:,j</span><span class="font64"> <sup>h</sup>j}</span></p></div><div>
<p><span class="font64">(20.11)</span></p></div><div>
<p><span class="font64"><sup>j</sup>=l</span></p></div>
<p><span class="font64">Since we are conditioning on the visible units v, we can treat these as constant with respect to the distribution P (h | v). The factorial nature of the conditional&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(h | v) follows immediately from our ability to write the joint probability over&#160;the vector h as the product of (unnormalized) distributions over the individual&#160;elements, h</span><span class="font64" style="font-weight:bold;font-style:italic;">j</span><span class="font64">. It is now a simple matter of normalizing the distributions over the&#160;individual binary hj.</span></p><div>
<p><span class="font64" style="font-weight:bold;">P(h</span><span class="font64">j </span><span class="font64" style="font-weight:bold;">= 1 | v) =</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">P<sup>(h</sup>j</span><span class="font64"> = 1 <sup>1 v)</sup></span></p></div><div>
<p><span class="font64">/<sup>5</sup>(hj = 0 | v) + </span><span class="font64" style="font-weight:bold;font-style:italic;">P(hj</span><span class="font64"> = 1 | v) exp </span><span class="font64" style="font-weight:bold;font-style:italic;">\cj</span><span class="font64"> + v<sup>T</sup> Wj&#160;exp {0} + exp {cj + v<sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">W<sub>:</sub>,j</span><span class="font64"> }</span></p></div><div>
<p><span class="font64">= a </span><span class="font64" style="font-weight:bold;font-style:italic;">Cj</span><span class="font64"> + v<sup>T</sup> W<sub>:</sub></span></p></div><div>
<p dir="rtl"><span class="font64" style="font-weight:bold;font-style:italic;">נ,</span></p></div><div>
<p><span class="font64">(20.12)</span></p>
<p><span class="font64">(20.13)</span></p>
<p><span class="font64">(20.14)</span></p></div>
<p><span class="font64">We can now express the full conditional over the hidden layer as the factorial distribution:</span></p>
<p><span class="font64"><sup>n</sup>h</span></p><div>
<p><span class="font64">(20.15)</span></p></div><div>
<p><span class="font64"><sup>j</sup>=1</span></p></div>
<p><span class="font64">P(h | v) = &#160;&#160;&#160;a((2h - 1) 0 (c + W<sup>T</sup>v)&#160;&#160;&#160;&#160;.</span></p>
<p><span class="font64">A similar derivation will show that the other condition of interest to us, P(v | h), is also a factorial distribution:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">n<sub>v</sub></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(v | h) = &#160;&#160;&#160;a ((2v - 1) 0 </span><span class="font64" style="font-weight:bold;font-style:italic;">(b</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">Wh))<sub>i</sub>.</span><span class="font64">&#160;&#160;&#160;&#160;(20.16)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=</span><span class="font64"> 1</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">20.2.2 Training Restricted Boltzmann Machines</span></h5>
<p><span class="font64">Because the RBM admits efficient evaluation and differentiation of P<sup>P</sup>(v) and efficient MCMC sampling in the form of block Gibbs sampling, it can readily be&#160;trained with any of the techniques described in Chapter 18 for training models&#160;that have intractable partition functions. This includes CD, SML (PCD), ratio&#160;matching and so on. Compared to other undirected models used in deep learning,&#160;the RBM is relatively straightforward to train because we can compute P(h | v)&#160;exactly in closed form. Some other deep models, such as the deep Boltzmann&#160;machine, combine both the difficulty of an intractable partition function and the&#160;difficulty of intractable inference.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">20.3 Deep Belief Networks</span></h4>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Deep belief networks</span><span class="font64"> (DBNs) were one of the first non-convolutional models to successfully admit training of deep architectures (Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006; Hinton,&#160;2007b). The introduction of deep belief networks in 2006 began the current deep&#160;learning renaissance. Prior to the introduction of deep belief networks, deep models&#160;were considered too difficult to optimize. Kernel machines with convex objective&#160;functions dominated the research landscape. Deep belief networks demonstrated&#160;that deep architectures can be successful, by outperforming kernelized support&#160;vector machines on the MNIST dataset (Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006). Today, deep belief&#160;networks have mostly fallen out of favor and are rarely used, even compared to&#160;other unsupervised or generative learning algorithms, but they are still deservedly&#160;recognized for their important role in deep learning history.</span></p>
<p><span class="font64">Deep belief networks are generative models with several layers of latent variables. The latent variables are typically binary, while the visible units may be binary&#160;or real. There are no intra-layer connections. Usually, every unit in each layer is&#160;connected to every unit in each neighboring layer, though it is possible to construct&#160;more sparsely connected DBNs. The connections between the top two layers are&#160;undirected. The connections between all other layers are directed, with the arrows&#160;pointed toward the layer that is closest to the data. See Fig. 20.1b for an example.</span></p>
<p><span class="font64">A DBN with </span><span class="font64" style="font-weight:bold;font-style:italic;">l</span><span class="font64"> hidden layers contains </span><span class="font64" style="font-weight:bold;font-style:italic;">l</span><span class="font64"> weight matrices: W<sup>(1)</sup></span><span class="font64" style="font-weight:bold;">,..., </span><span class="font64">W<sup>(l)</sup>. It also contains </span><span class="font64" style="font-weight:bold;">l</span><span class="font64">+ 1 bias vectors: b<sup>(0)</sup></span><span class="font64" style="font-weight:bold;">,..., </span><span class="font64">b<sup>(l)</sup>, with providing the biases for the&#160;visible layer. The probability distribution represented by the DBN is given by</span></p>
<p><span class="font64" style="font-weight:bold;">P</span><span class="font64">(h<sup>(l)</sup></span><span class="font64" style="font-weight:bold;">, </span><span class="font64">h<sup>(l-1)</sup>) a exp (b<sup>(l)T</sup>h<sup>(l)</sup> + b<sup>(l-1)T</sup>h<sup>(l-1)</sup> + h<sup>(l-1)T</sup>W<sup>(l)</sup>h<sup>(l)</sup>) </span><span class="font64" style="font-weight:bold;">, &#160;&#160;&#160;</span><span class="font64">(20.17)</span></p>
<p><span class="font64" style="font-weight:bold;">P </span><span class="font64">(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64">(<sup>k)</sup> = 1 | h<sup>(k</sup>+<sup>1)</sup>) = &#160;&#160;&#160;+ w(<sup>k</sup>+<sup>1)T</sup> h<sup>(k</sup>+<sup>1</sup>^ V</span><span class="font64" style="font-weight:bold;">i, </span><span class="font64" style="font-weight:bold;font-style:italic;">Vk</span><span class="font64"> e 1</span><span class="font64" style="font-weight:bold;">,...,l </span><span class="font64">- 2</span><span class="font64" style="font-weight:bold;">,&#160;&#160;&#160;&#160;</span><span class="font64">(20.18)</span></p>
<p><span class="font64" style="font-weight:bold;">P</span><span class="font64" style="font-weight:bold;font-style:italic;">(v<sub>{</sub> = 1</span><span class="font64"> | h<sup>(1)</sup>) = </span><span class="font64" style="font-weight:bold;">a </span><span class="font64">(\<sup>(0)</sup> + W<sub>;</sub>(<sub>i</sub><sup>L)T</sup>h<sup>(1</sup>^ V</span><span class="font64" style="font-weight:bold;">i. &#160;&#160;&#160;</span><span class="font64">(20.19)</span></p>
<p><span class="font64">In the case of real-valued visible units, substitute</span></p>
<p><span class="font64">v &#160;&#160;&#160;(v; b<sup>(0)</sup> + W<sup>(1)T</sup> h<sup>(1)</sup></span><span class="font64" style="font-weight:bold;">, </span><span class="font64">0<sup>-1</sup>)&#160;&#160;&#160;&#160;(20.20)</span></p>
<p><span class="font64">with diagonal for tractability. Generalizations to other exponential family visible units are straightforward, at least in theory. A DBN with only one hidden layer is&#160;just an RBM.</span></p>
<p><span class="font64">To generate a sample from a DBN, we first run several steps of Gibbs sampling on the top two hidden layers. This stage is essentially drawing a sample from&#160;the RBM defined by the top two hidden layers. We can then use a single pass of&#160;ancestral sampling through the rest of the model to draw a sample from the visible&#160;units.</span></p>
<p><span class="font64">Deep belief networks incur many of the problems associated with both directed models and undirected models.</span></p>
<p><span class="font64">Inference in a deep belief network is intractable due to the explaining away effect within each directed layer, and due to the interaction between the two hidden&#160;layers that have undirected connections. Evaluating or maximizing the standard&#160;evidence lower bound on the log-likelihood is also intractable, because the evidence&#160;lower bound takes the expectation of cliques whose size is equal to the network&#160;width.</span></p>
<p><span class="font64">Evaluating or maximizing the log-likelihood requires not just confronting the problem of intractable inference to marginalize out the latent variables, but also&#160;the problem of an intractable partition function within the undirected model of&#160;the top two layers.</span></p>
<p><span class="font64">To train a deep belief network, one begins by training an RBM to maximize Ev~p<sub>data</sub> log</span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">p</span><span class="font64">(<sup>v</sup>) using contrastive divergence or stochastic maximum likelihood.&#160;The parameters of the RBM then define the parameters of the first layer of the&#160;DBN. Next, a second RBM is trained to approximately maximize</span></p><div>
<p><span class="font64" style="font-weight:bold;">E</span><span class="font64">-v^pdata</span><span class="font64" style="font-weight:bold;"><sup>E</sup></span><span class="font64">h<sup>(1</sup>)~p<sup>(1</sup>)(h<sup>(1</sup>) |v)</span></p></div><div>
<p><span class="font64">log p<sup>(2)</sup>(h<sup>(1)</sup>)</span></p></div><div>
<p><span class="font64">(20.21)</span></p></div>
<p><span class="font64">where p<sup>(1)</sup> is the probability distribution represented by the first RBM and p<sup>(2) </sup>is the probability distribution represented by the second RBM. In other words,&#160;the second RBM is trained to model the distribution defined by sampling the&#160;hidden units of the first RBM, when the first RBM is driven by the data. This&#160;procedure can be repeated indefinitely, to add as many layers to the DBN as&#160;desired, with each new RBM modeling the samples of the previous one. Each RBM&#160;defines another layer of the DBN. This procedure can be justified as increasing a&#160;variational lower bound on the log-likelihood of the data under the DBN (Hinton&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006).</span></p>
<p><span class="font64">In most applications, no effort is made to jointly train the DBN after the greedy layer-wise procedure is complete. However, it is possible to perform generative&#160;fine-tuning using the wake-sleep algorithm.</span></p>
<p><span class="font64">The trained DBN may be used directly as a generative model, but most of the interest in DBNs arose from their ability to improve classification models. We can&#160;take the weights from the DBN and use them to define an MLP:</span></p>
<p><span class="font64" style="font-weight:bold;">h<sup>(1)</sup> = </span><span class="font64" style="font-weight:bold;font-style:italic;">a(b</span><span class="font64" style="font-weight:bold;"><sup>(1)</sup> + v<sup>T</sup>W<sup>(1)</sup>) . &#160;&#160;&#160;</span><span class="font64">(20.22)</span></p>
<p><span class="font64" style="font-weight:bold;">h<sup>(1)</sup> = a (b(° + h<sup>(1-1)T</sup>W<sup>(1)</sup>) </span><span class="font64">V</span><span class="font64" style="font-weight:bold;">/ </span><span class="font64">e </span><span class="font64" style="font-weight:bold;">2,..., m, &#160;&#160;&#160;</span><span class="font64">(20.23)</span></p>
<p><span class="font64">After initializing this MLP with the weights and biases learned via generative training of the DBN, we may train the MLP to perform a classification task. This&#160;additional training of the MLP is an example of discriminative fine-tuning.</span></p>
<p><span class="font64">This specific choice of MLP is somewhat arbitrary, compared to many of the inference equations in Chapter 19 that are derived from first principles. This MLP&#160;is a heuristic choice that seems to work well in practice and is used consistently&#160;in the literature. Many approximate inference techniques are motivated by their&#160;ability to find a maximally </span><span class="font64" style="font-weight:bold;font-style:italic;">tight</span><span class="font64"> variational lower bound on the log-likelihood&#160;under some set of constraints. One can construct a variational lower bound on the&#160;log-likelihood using the hidden unit expectations defined by the DBN’s MLP, but&#160;this is true of </span><span class="font64" style="font-weight:bold;font-style:italic;">any</span><span class="font64"> probability distribution over the hidden units, and there is no&#160;reason to believe that this MLP provides a particularly tight bound. In particular,&#160;the MLP ignores many important interactions in the DBN graphical model. The&#160;MLP propagates information upward from the visible units to the deepest hidden&#160;units, but does not propagate any information downward or sideways. The DBN&#160;graphical model has explaining away interactions between all of the hidden units&#160;within the same layer as well as top-down interactions between layers.</span></p>
<p><span class="font64">While the log-likelihood of a DBN is intractable, it may be approximated with AIS (Salakhutdinov and Murray, 2008). This permits evaluating its quality as a&#160;generative model.</span></p>
<p><span class="font64">The term “deep belief network” is commonly used incorrectly to refer to any kind of deep neural network, even networks without latent variable semantics.&#160;The term “deep belief network” should refer specifically to models with undirected&#160;connections in the deepest layer and directed connections pointing downward&#160;between all other pairs of consecutive layers.</span></p>
<p><span class="font64">The term “deep belief network” may also cause some confusion because the term “belief network” is sometimes used to refer to purely directed models, while&#160;deep belief networks contain an undirected layer. Deep belief networks also share&#160;the acronym DBN with dynamic Bayesian networks (Dean and Kanazawa, 1989),&#160;which are Bayesian networks for representing Markov chains.</span></p><div><img src="main-190.jpg" alt=""/>
<p><span class="font64">Figure 20.2: The graphical model for a deep Boltzmann machine with one visible layer (bottom) and two hidden layers. Connections are only between units in neighboring layers.&#160;There are no intra-layer layer connections.</span></p></div><h4><a id="bookmark6"></a><span class="font65" style="font-weight:bold;">20.4 Deep Boltzmann Machines</span></h4>
<p><span class="font64">A </span><span class="font64" style="font-weight:bold;font-style:italic;">deep Boltzmann machine</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">DBM</span><span class="font64"> (Salakhutdinov and Hinton, 2009a) is another kind of deep, generative model. Unlike the deep belief network (DBN), it is an&#160;entirely undirected model. Unlike the RBM, the DBM has several layers of latent&#160;variables (RBMs have just one). But like the RBM, within each layer, each of the&#160;variables are mutually independent, conditioned on the variables in the neighboring&#160;layers. See Fig. 20.2 for the graph structure. Deep Boltzmann machines have been&#160;applied to a variety of tasks including document modeling (Srivastava </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013).</span></p>
<p><span class="font64">Like RBMs and DBNs, DBMs typically contain only binary units—as we assume for simplicity of our presentation of the model—but it is straightforward&#160;to include real-valued visible units.</span></p>
<p><span class="font64">A DBM is an energy-based model, meaning that the the joint probability distribution over the model variables is parametrized by an energy function </span><span class="font64" style="font-weight:bold;">E</span><span class="font64">. In&#160;the case of a deep Boltzmann machine with one visible layer, </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">, and three hidden&#160;layers, </span><span class="font64" style="font-weight:bold;">h<sup>(1)</sup></span><span class="font64">, </span><span class="font64" style="font-weight:bold;">h<sup>(2)</sup> </span><span class="font64">and </span><span class="font64" style="font-weight:bold;">h<sup>(3)</sup></span><span class="font64">, the joint probability is given by:</span></p>
<p><span class="font64" style="font-weight:bold;">P (v, h<sup>(1)</sup>, h<sup>(2)</sup>, h<sup>(3)</sup>) = </span><span class="font64" style="font-weight:bold;font-style:italic;">Z^exp </span><span class="font64" style="font-style:italic;">(</span><span class="font64" style="font-weight:bold;font-style:italic;">-E</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">(v, h<sup>(1)</sup>, h<sup>(2)</sup>, h<sup>(3)</sup>; 0}) . &#160;&#160;&#160;</span><span class="font64">(20.24)</span></p>
<p><span class="font64">To simplify our presentation, we omit the bias parameters below. The DBM energy function is then defined as follows:</span></p>
<p><span class="font64" style="font-weight:bold;">E(v, h<sup>(1)</sup>, h<sup>(2)</sup>, h<sup>(3)</sup>; 0} = -v<sup>T</sup> W<sup>(1)</sup>h<sup>(1)</sup> - h<sup>(1)T</sup>W<sup>(2)</sup>h<sup>(2)</sup> - h<sup>(2)T</sup>W<sup>(3)</sup>h<sup>(3)</sup>.</span></p>
<p><span class="font64">(20.25)</span></p><div><div><img src="main-191.jpg" alt=""/>
<p><span class="font64">Figure 20.3: A deep Boltzmann machine, re-arranged to reveal its bipartite graph structure.</span></p></div></div><div><div><img src="main-192.jpg" alt=""/></div></div>
<p><span class="font64">In comparison to the RBM energy function (Eq. 20.5), the DBM energy function includes connections between the hidden units (latent variables) in the&#160;form of the weight matrices (W<sup>(2)</sup> and W<sup>(3)</sup>). As we will see, these connections&#160;have significant consequences for both the model behavior as well as how we go&#160;about performing inference in the model.</span></p>
<p><span class="font64">In comparison to fully connected Boltzmann machines (with every unit connected to every other unit), the DBM offers some advantages that are similar to those offered by the RBM. Specifically, as illustrated in Fig. 20.3, the DBM layers&#160;can be organized into a bipartite graph, with odd layers on one side and even layers&#160;on the other. This immediately implies that when we condition on the variables in&#160;the even layer, the variables in the odd layers become conditionally independent.&#160;Of course, when we condition on the variables in the odd layers, the variables in&#160;the even layers also become conditionally independent.</span></p>
<p><span class="font64">The bipartite structure of the DBM means that we can apply the same equations we have previously used for the conditional distributions of an RBM to determine the conditional distributions in a DBM. The units within a layer are&#160;conditionally independent from each other given the values of the neighboring&#160;layers, so the distributions over binary variables can be fully described by the&#160;Bernoulli parameters giving the probability of each unit being active. In our&#160;example with two hidden layers, the activation probabilities are given by:</span></p>
<p><span class="font64">P</span><span class="font64" style="font-weight:bold;font-style:italic;">(vi =</span><span class="font64"> 1 | h<sup>(1)</sup>) = a (W<sup>1</sup>^) ’ &#160;&#160;&#160;(20.26)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(1)</sup> = 1 | </span><span class="font64" style="font-weight:bold;">v,h</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(2)</sup>)</span><span class="font64"> = </span><span class="font64" style="font-weight:bold;">a</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">v<sup>T</sup>W</span><span class="font64">^ + </span><span class="font64" style="font-weight:bold;font-style:italic;">WV?</span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(2)</sup>) &#160;&#160;&#160;(20.27)</span></p>
<p><span class="font64">and</span></p>
<p><span class="font64">P(hk<sup>2)</sup> = 1 | </span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(1)</sup>) = a </span><span class="font64" style="font-weight:bold;font-style:italic;">(h<sup>(1)T</sup>W</span><span class="font64" style="font-style:italic;">™)</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">. &#160;&#160;&#160;(20.28)</span></p>
<p><span class="font64">The bipartite structure makes Gibbs sampling in a deep Boltzmann machine efficient. The naive approach to Gibbs sampling is to update only one variable&#160;at a time. RBMs allow all of the visible units to be updated in one block and all&#160;of the hidden units to be updated in a second block. One might naively assume&#160;that a DBM with </span><span class="font64" style="font-weight:bold;">l </span><span class="font64">layers requires </span><span class="font64" style="font-weight:bold;">l </span><span class="font64">+ 1 updates, with each iteration updating a&#160;block consisting of one layer of units. Instead, it is possible to update all of the&#160;units in only two iterations. Gibbs sampling can be divided into two blocks of&#160;updates, one including all even layers (including the visible layer) and the other&#160;including all odd layers. Due to the bipartite DBM connection pattern, given&#160;the even layers, the distribution over the odd layers is factorial and thus can be&#160;sampled simultaneously and independently as a block. Likewise, given the odd&#160;layers, the even layers can be sampled simultaneously and independently as a&#160;block. Efficient sampling is especially important for training with the stochastic&#160;maximum likelihood algorithm.</span></p><h5><a id="bookmark7"></a><span class="font64" style="font-weight:bold;">20.4.1 Interesting Properties</span></h5>
<p><span class="font64">Deep Boltzmann machines have many interesting properties.</span></p>
<p><span class="font64">DBMs were developed after DBNs. Compared to DBNs, the posterior distribution </span><span class="font64" style="font-weight:bold;">P(h </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">v) </span><span class="font64">is simpler for DBMs. Somewhat counterintuitively, the simplicity of this posterior distribution allows richer approximations of the posterior. In the case&#160;of the DBN, we perform classification using a heuristically motivated approximate&#160;inference procedure, in which we guess that a reasonable value for the mean field&#160;expectation of the hidden units can be provided by an upward pass through the&#160;network in an MLP that uses sigmoid activation functions and the same weights&#160;as the original DBN. Any distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">Q(h)</span><span class="font64"> may be used to obtain a variational&#160;lower bound on the log-likelihood. This heuristic procedure therefore allows us to&#160;obtain such a bound. However, the bound is not explicitly optimized in any way, so&#160;the bound may be far from tight. In particular, the heuristic estimate of </span><span class="font64" style="font-weight:bold;">Q </span><span class="font64">ignores&#160;interactions between hidden units within the same layer as well as the top-down&#160;feedback influence of hidden units in deeper layers on hidden units that are closer&#160;to the input. Because the heuristic MLP-based inference procedure in the DBN&#160;is not able to account for these interactions, the resulting </span><span class="font64" style="font-weight:bold;">Q </span><span class="font64">is presumably far&#160;from optimal. In DBMs, all of the hidden units within a layer are conditionally&#160;independent given the other layers. This lack of intra-layer interaction makes it&#160;possible to use fixed point equations to actually optimize the variational lower&#160;bound and find the true optimal mean field expectations (to within some numerical&#160;tolerance).</span></p>
<p><span class="font64">The use of proper mean field allows the approximate inference procedure for DBMs to capture the influence of top-down feedback interactions. This makes&#160;DBMs interesting from the point of view of neuroscience, because the human brain&#160;is known to use many top-down feedback connections. Because of this property,&#160;DBMs have been used as computational models of real neuroscientific phenomena&#160;(Series </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010; Reichert </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011).</span></p>
<p><span class="font64">One unfortunate property of DBMs is that sampling from them is relatively difficult. DBNs only need to use MCMC sampling in their top pair of layers. The&#160;other layers are used only at the end of the sampling process, in one efficient&#160;ancestral sampling pass. To generate a sample from a DBM, it is necessary to&#160;use MCMC across all layers, with every layer of the model participating in every&#160;Markov chain transition.</span></p><h5><a id="bookmark8"></a><span class="font64" style="font-weight:bold;">20.4.2 DBM Mean Field Inference</span></h5>
<p><span class="font64">The conditional distribution over one DBM layer given the neighboring layers is factorial. In the example of the DBM with two hidden layers, these distributions&#160;are </span><span class="font64" style="font-style:italic;">P</span><span class="font64" style="font-weight:bold;font-style:italic;">(</span><span class="font64" style="font-style:italic;">v</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">h<sup>(1)</sup></span><span class="font64">), </span><span class="font64" style="font-style:italic;">P</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">h<sup>(1)</sup> </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">v, h<sup>(2)</sup></span><span class="font64">) and </span><span class="font64" style="font-style:italic;">P</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">h<sup>(2)</sup> </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">h<sup>(1)</sup></span><span class="font64">). The distribution over </span><span class="font64" style="font-weight:bold;">all&#160;</span><span class="font64">hidden layers generally does not factorize because of interactions between layers.&#160;In the example with two hidden layers, </span><span class="font64" style="font-weight:bold;">P</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">h<sup>(1)</sup>, h<sup>(2)</sup> </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) does not factorize due due&#160;to the interaction weights </span><span class="font64" style="font-weight:bold;">W<sup>(2)</sup> </span><span class="font64">between </span><span class="font64" style="font-weight:bold;">h<sup>(1)</sup> </span><span class="font64">and </span><span class="font64" style="font-weight:bold;">h<sup>(2)</sup> </span><span class="font64">which render these variables&#160;mutually dependent.</span></p>
<p><span class="font64">As was the case with the DBN, we are left to seek out methods to approximate the DBM posterior distribution. However, unlike the DBN, the DBM posterior&#160;distribution over their hidden units—while complicated—is easy to approximate&#160;with a </span><span class="font64" style="font-weight:bold;font-style:italic;">variational</span><span class="font64"> approximation (as discussed in Sec. 19.4), specifically a mean&#160;field approximation. The mean field approximation is a simple form of variational&#160;inference, where we restrict the approximating distribution to fully factorial distributions. In the context of DBMs, the mean field equations capture the bidirectional&#160;interactions between layers. In this section we derive the iterative approximate&#160;inference procedure originally introduced in Salakhutdinov and Hinton (2009a).</span></p>
<p><span class="font64">In variational approximations to inference, we approach the task of approximating a particular target distribution—in our case, the posterior distribution over the hidden units given the visible units—by some reasonably simple family of distributions. In the case of the mean field approximation, the approximating family&#160;is the set of distributions where the hidden units are conditionally independent.</span></p>
<p><span class="font64">We now develop the mean field approach for the example with two hidden layers. Let Q (</span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(1)</sup>, </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">&lt;<sup>2)</sup> | </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) be the approximation of P(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(1)</sup>, </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">&lt;<sup>2)</sup> | </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">). The mean&#160;field assumption implies that</span></p>
<p><span class="font64">Q(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(1)</sup>, </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">&lt;<sup>2)</sup> | </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) =</span><span class="font20" style="font-weight:bold;">JJ </span><span class="font64">Q(hj<sup>1)</sup> | </span><span class="font64" style="font-weight:bold;">v </span><span class="font64">)</span><span class="font20" style="font-weight:bold;">JJ </span><span class="font64" style="font-weight:bold;font-style:italic;">Q(hf<sup>2</sup></span><span class="font64"> | </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">). &#160;&#160;&#160;(20.29)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">j &#160;&#160;&#160;k</span></p>
<p><span class="font64">The mean field approximation attempts to find a member of this family of distributions that best fits the true posterior P(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(1)</sup>, </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">&lt;<sup>2)</sup> | </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">). Importantly, the&#160;inference process must be run again to find a different distribution Q every time&#160;we use a new value of </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">.</span></p>
<p><span class="font64">One can conceive of many ways of measuring how well Q(</span><span class="font64" style="font-weight:bold;">h </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) fits P(</span><span class="font64" style="font-weight:bold;">h </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">). The mean field approach is to minimize</span></p>
<p><span class="font64"><sup>KL</sup>(QI</span><span class="font20">I</span><span class="font64"><sup>P</sup>) = </span><span class="font64" style="font-weight:bold;">E</span><span class="font64">Q(</span><span class="font64" style="font-weight:bold;"><sup>h</sup></span><span class="font64"><sup>&lt;1)</sup>'</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>h</sup></span><span class="font64"><sup>(2)</sup>ו </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">)<sup>1</sup>״g (</span><span class="font64" style="text-decoration:line-through;">^&lt;1(</span><span class="font64" style="font-weight:bold;text-decoration:line-through;"><sub>h</sub></span><span class="font64" style="text-decoration:line-through;">&lt;2) |</span><span class="font64" style="font-weight:bold;text-decoration:line-through;">v</span><span class="font64" style="text-decoration:line-through;">j</span><span class="font64">) . &#160;&#160;&#160;(<sup>203</sup>°׳)</span></p>
<p><span class="font64">In general, we do not have to provide a parametric form of the approximating distribution beyond enforcing the independence assumptions. The variational&#160;approximation procedure is generally able to recover a functional form of the&#160;approximate distribution. However, in the case of a mean field assumption on&#160;binary hidden units (the case we are developing here) there is no loss of generality&#160;resulting from fixing a parametrization of the model in advance.</span></p>
<p><span class="font64">We parametrize Q as a product of Bernoulli distributions, that is we associate the probability of each element of </span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(1)</sup> with a parameter. Specifically, for each j,</span></p>
<p><span class="font64">hj<sup>1</sup> = Q(hj<sup>1)</sup> = 1 | </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">), where hj<sup>1</sup> £ [0,1] and for each k, hk<sup>2)</sup> = Q(h׳k<sup>2)</sup> = 1 | </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">), (2)</span></p>
<p><span class="font64">where hk ' £ [0,1]. Thus we have the following approximation to the posterior:</span></p><div>
<p><span class="font64">(20.31)</span></p></div>
<p><span class="font64" style="font-weight:bold;"><sup>h</sup> &lt;<sup>2) 1</sup> v) &#160;&#160;&#160;<sup>Q(h</sup> j<sup>1) 1</sup> v) jj <sup>Q</sup>(<sup>h</sup> <sub>k</sub><sup>2) 1</sup> </span><span class="font64" style="font-style:italic;">v</span><span class="font64" style="font-weight:bold;">)</span></p>
<p><span class="font64">(1 - hj<sup>1)</sup> )&lt;<sup>I-h</sup>j’<sup>)</sup> x &#160;&#160;&#160;(hk<sup>2)</sup>)<sup>h</sup>k<sup>2)</sup> (1 - hk<sup>2)</sup>)&lt;<sup>1</sup>-<sup>h</sup>k<sup>2</sup>').</span></p>
<p><span class="font64">jk</span></p>
<p><span class="font64">(20.32)</span></p>
<p><span class="font64">Of course, for DBMs with more layers the approximate posterior parametrization can be extended in the obvious way, exploiting the bipartite structure of the graph&#160;to update all of the even layers simultaneously and then to update all of the odd&#160;layers simultaneously, following the same schedule as Gibbs sampling.</span></p>
<p><span class="font64">Now that we have specified our family of approximating distributions Q, it remains to specify a procedure for choosing the member of this family that best&#160;fits P. The most straightforward way to do this is to use the mean field equations&#160;specified by Eq. 19.56. These equations were derived by solving for where the&#160;derivatives of the variational lower bound are zero. They describe in an abstract&#160;manner how to optimize the variational lower bound for any model, simply by&#160;taking expectations with respect to Q.</span></p>
<p><span class="font64">Applying these general equations, we obtain the update rules (again, ignoring bias terms):</span></p><div><div><img src="main-193.png" alt=""/>
<p><span class="font64" style="font-weight:bold;font-style:italic;">a</span></p></div></div><div>
<p><span class="font64" style="font-weight:bold;">fe ״ *T,j + E W2 ׳T)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i &#160;&#160;&#160;k</span><span class="font64">׳</span></p></div><div>
<p><span class="font64"><sup>V</sup></span><span class="font64" style="font-weight:bold;"><sup>j</sup></span></p></div><div>
<p><span class="font64">(20.33)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">hk = a</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">(E </span><span class="font64">jk</span><span class="font64" style="font-weight:bold;">׳</span><span class="font64">j<sup>1</sup> </span><span class="font64" style="font-weight:bold;">I , </span><span class="font64">V</span><span class="font64" style="font-weight:bold;">k. &#160;&#160;&#160;</span><span class="font64">(20.34)</span></p>
<p><span class="font64">At a fixed point of this system of equations, we have a local maximum of the variational lower bound L (</span><span class="font64" style="font-weight:bold;">Q</span><span class="font64">). Thus these fixed point update equations define&#160;an iterative algorithm where we alternate updates of </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">j<sup>1</sup> (using Eq. 20.33) and</span></p>
<p><span class="font64">updates of </span><span class="font64" style="font-weight:bold;">׳׳</span><span class="font64">k<sup>2)</sup> (using Eq. 20.34). On small problems such as MNIST, as few as ten iterations can be sufficient to find an approximate positive phase gradient&#160;for learning, and fifty usually suffice to obtain a high quality representation of&#160;a single specific example to be used for high-accuracy classification. Extending&#160;approximate variational inference to deeper DBMs is straightforward.</span></p><h5><a id="bookmark9"></a><span class="font64" style="font-weight:bold;">20.4.3 DBM Parameter Learning</span></h5>
<p><span class="font64">Learning in the DBM must confront both the challenge of an intractable partition function, using the techniques from Chapter 18, and the challenge of an&#160;intractable posterior distribution, using the techniques from Chapter 19.</span></p>
<p><span class="font64">As described in Sec. 20.4.2, variational inference allows the construction of a distribution Q( </span><span class="font64" style="font-weight:bold;">h </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) that approximates the intractable </span><span class="font64" style="font-weight:bold;font-style:italic;">P(h</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">). Learning then&#160;proceeds by maximizing L (</span><span class="font64" style="font-weight:bold;">v</span><span class="font64">, Q, </span><span class="font64" style="font-weight:bold;">6</span><span class="font64">), the variational lower bound on the intractable&#160;log-likelihood, log P(</span><span class="font64" style="font-weight:bold;">v</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">6</span><span class="font64">).</span></p>
<p><span class="font64">For a deep Boltzmann machine with two hidden layers, L is given by</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">L(Q,</span><span class="font64"> 6) </span><span class="font64" style="font-weight:bold;font-style:italic;">= </span><span class="font66" style="font-style:italic;">EE</span><span class="font67"> v W; j &gt; + E E h(<sup>1</sup> ’wfk׳ h<sup>21</sup> - log Z (6) </span><span class="font64" style="font-weight:bold;font-style:italic;">+</span><span class="font64"> H(Q). (20.35)</span></p>
<p><span class="font64"><sup>i</sup> j׳ &#160;&#160;&#160;j׳ k׳</span></p>
<p><span class="font64">This expression still contains the log partition function, log </span><span class="font64" style="font-weight:bold;font-style:italic;">Z( 6).</span><span class="font64"> Because a deep Boltzmann machine contains restricted Boltzmann machines as components, the&#160;hardness results for computing the partition function and sampling that apply to&#160;restricted Boltzmann machines also apply to deep Boltzmann machines. This means&#160;that evaluating the probability mass function of a Boltzmann machine requires&#160;approximate methods such as annealed importance sampling. Likewise, training&#160;the model requires approximations to the gradient of the log partition function. See&#160;Chapter 18 for a general description of these methods. DBMs are typically trained&#160;using stochastic maximum likelihood. Many of the other techniques described in&#160;Chapter 18 are not applicable. Techniques such as pseudolikelihood require the&#160;ability to evaluate the unnormalized probabilities, rather than merely obtain a&#160;variational lower bound on them. Contrastive divergence is slow for deep Boltzmann&#160;machines because they do not allow efficient sampling of the hidden units given the&#160;visible units—instead, contrastive divergence would require burning in a Markov&#160;chain every time a new negative phase sample is needed.</span></p>
<p><span class="font64">The non-variational version of stochastic maximum likelihood algorithm was discussed earlier, in Sec. 18.2. Variational stochastic maximum likelihood as applied&#160;to the DBM is given in Algorithm 20.1. Recall that we describe a simplified varient&#160;of the DBM that lacks bias parameters; including them is trivial.</span></p><h5><a id="bookmark10"></a><span class="font64" style="font-weight:bold;">20.4.4 Layer-Wise Pretraining</span></h5>
<p><span class="font64">Unfortunately, training a DBM using stochastic maximum likelihood (as described above) from a random initialization usually results in failure. In some cases, the&#160;model fails to learn to represent the distribution adequately. In other cases, the&#160;DBM may represent the distribution well, but with no higher likelihood than could&#160;be obtained with just an RBM. A DBM with very small weights in all but the first&#160;layer represents approximately the same distribution as an RBM.</span></p>
<p><span class="font64">Various techniques that permit joint training have been developed and are described in Sec. 20.4.5. However, the original and most popular method for&#160;overcoming the joint training problem of DBMs is greedy layer-wise pretraining.&#160;In this method, each layer of the DBM is trained in isolation as an RBM. The&#160;first layer is trained to model the input data. Each subsequent RBM is trained to&#160;model samples from the previous RBM’s posterior distribution. After all of the</span></p>
<p><span class="font64">Algorithm 20.1 The variational stochastic maximum likelihood algorithm for training a DBM with two hidden layers.</span></p>
<p><span class="font64">Set e, the step size, to a small positive number</span></p>
<p><span class="font64">Set k, the number of Gibbs steps, high enough to allow a Markov chain of p( v, h<sup>(1)</sup>, h<sup>(2)</sup>; </span><span class="font18">0</span><span class="font64"> + eA</span><span class="font18">0</span><span class="font64">) to burn in, starting from samples from</span><span class="font64" style="font-weight:bold;font-style:italic;">p(v,</span><span class="font64"> h<sup>(1)</sup>, h<sup>(2)</sup>; </span><span class="font64" style="font-weight:bold;font-style:italic;">0).&#160;</span><span class="font64">Initialize three matrices, </span><span class="font64" style="font-weight:bold;font-style:italic;">V</span><span class="font64">, H<sup>(1)</sup> and H<sup>(2)</sup> each with m rows set to random&#160;values (e.g., from Bernoulli distributions, possibly with marginals matched to&#160;the model’s marginals).&#160;while not converged (learning loop) do</span></p>
<p><span class="font64">Sample a minibatch of m examples from the training data and arrange them as the rows of a design matrix </span><span class="font64" style="font-weight:bold;font-style:italic;">V</span><span class="font64">.</span></p>
<p><span class="font64">Initialize matrices </span><span class="font64" style="font-weight:bold;font-style:italic;">H</span><span class="font64"><sup>(1)</sup> and </span><span class="font64" style="font-weight:bold;font-style:italic;">H</span><span class="font64"><sup>(2)</sup>, possibly to the model’s marginals. while not converged (mean field inference loop) do</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">H</span><span class="font64"> <sup>(1)</sup> a (VW<sup>(1)</sup> + H<sup>(2)</sup> W<sup>(2)T</sup></span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">(h</span><span class="font64" style="font-variant:small-caps;"> <sup>(1)</sup>w <sup>(2)</sup>)</span></p></div><div>
<p><span class="font64">H<sup>(2)</sup> ^ </span><span class="font64" style="font-weight:bold;">a</span></p>
<p><span class="font64">end while</span></p>
<p><span class="font64" style="font-variant:small-caps;"><sup>a</sup>w o ^ m <sup>v TH (1)</sup></span></p>
<p><span class="font64" style="font-variant:small-caps;">Aw (2) ^ </span><span class="font64" style="font-weight:bold;font-style:italic;">m H<sup>{1)</sup></span><span class="font64"><sup> T</sup> H<sup>2)</sup>־<sup>)</sup></span></p>
<p><span class="font64">for </span><span class="font64" style="font-weight:bold;">l </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> 1 to </span><span class="font64" style="font-weight:bold;">k </span><span class="font64">(Gibbs sampling) do Gibbs block 1:</span></p>
<p><span class="font64">V</span><span class="font64" style="font-weight:bold;">i, j, V</span><span class="font64">ij sampled from </span><span class="font64" style="font-weight:bold;">P</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">V</span><span class="font64">ij = 1) = </span><span class="font64" style="font-weight:bold;">a &#160;&#160;&#160;</span><span class="font64">.</span></p>
<p><span class="font64">V</span><span class="font64" style="font-weight:bold;">i, j, &#160;&#160;&#160;</span><span class="font64">sampled from </span><span class="font64" style="font-weight:bold;">P</span><span class="font64" style="font-variant:small-caps;">( <sup>h(2)</sup></span></p>
<p><span class="font64">Gibbs block 2:</span></p></div><div>
<p><span class="font64"><sup>i,</sup>j</span></p></div><div>
<p><span class="font64">Vi, j, pj sampled from P(.P<sup>(1)</sup></span></p></div><div>
<p><span class="font64">•<sup>,</sup>j</span></p></div><div>
<p><span class="font64">end for</span></p></div><div>
<p><span class="font64">j,</span></p>
<p><span class="font64" style="font-variant:small-caps;">H<sup>(1)</sup>w ־j</span><span class="font64"><sup>2)</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;">= 1)</span></p>
<p><span class="font64" style="font-weight:bold;">= 1) = a[ V•־W.</span></p></div><div>
<p><span class="font64">a</span></p></div><div>
<p><span class="font64">J </span><span class="font64" style="font-weight:bold;">+ i^</span><span class="font64"><sub>i</sub><sup>(2)</sup></span><span class="font64" style="font-weight:bold;">W</span><span class="font64"><sub>J</sub><sup>(2)T</sup></span></p></div>
<p><span class="font64" style="font-variant:small-caps;">Aw (!) ^ A w(!) - m V</span><span class="font64"><sup>T</sup>H<sup>(1)</sup></span></p>
<p><span class="font64" style="font-variant:small-caps;">Aw (2) ^ a w(2) - m H</span><span class="font64"><sup>(1)t</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">Hi</span><span class="font64"><sup>(2)</sup></span></p>
<p><span class="font64">W<sup>(1)</sup> ^ W<sup>(1)</sup> + e Aw(</span><span class="font18">1</span><span class="font64">) (this is a cartoon illustration, in practice use a more effective algorithm, such as momentum with a decaying learning rate)</span></p>
<p><span class="font64">W<sup>(2)</sup> ^ W<sup>(2)</sup> + eA<sub>W</sub>(</span><span class="font18">2</span><span class="font64">) end while</span></p>
<p><span class="font64">RBMs have been trained in this way, they can be combined to form a DBM. The DBM may then be trained with PCD. Typically PCD training will make only a&#160;small change in the model’s parameters and its performance as measured by the&#160;log-likelihood it assigns to the data, or its ability to classify inputs. See Fig. 20.4&#160;for an illustration of the training procedure.</span></p>
<p><span class="font64">This greedy layer-wise training procedure is not just coordinate ascent. It bears some passing resemblance to coordinate ascent because we optimize one subset of&#160;the parameters at each step. However, in the case of the greedy layer-wise training&#160;procedure, we actually use a different objective function at each step.</span></p>
<p><span class="font64">Greedy layer-wise pretraining of a DBM differs from greedy layer-wise pretraining of a DBN. The parameters of each individual RBM may be copied to the corresponding DBN directly. In the case of the DBM, the RBM parameters&#160;must be modified before inclusion in the DBM. A layer in the middle of the stack&#160;of RBMs is trained with only bottom-up input, but after the stack is combined&#160;to form the DBM, the layer will have both bottom-up and top-down input. To&#160;account for this effect, Salakhutdinov and Hinton (2009a) advocate dividing the&#160;weights of all but the top and bottom RBM in half before inserting them into the&#160;DBM. Additionally, the bottom RBM must be trained using two “copies” of each&#160;visible unit and the weights tied to be equal between the two copies. This means&#160;that the weights are effectively doubled during the upward pass. Similarly, the top&#160;RBM should be trained with two copies of the topmost layer.</span></p>
<p><span class="font64">Obtaining the state of the art results with the deep Boltzmann machine requires a modification of the standard SML algorithm, which is to use a small amount of&#160;mean field during the negative phase of the joint PCD training step (Salakhutdinov&#160;and Hinton, 2009a). Specifically, the expectation of the energy gradient should&#160;be computed with respect to the mean field distribution in which all of the units&#160;are independent from each other. The parameters of this mean field distribution&#160;should be obtained by running the mean field fixed point equations for just one&#160;step. See Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013b) for a comparison of the performance of centered&#160;DBMs with and without the use of partial mean field in the negative phase.</span></p><h5><a id="bookmark11"></a><span class="font64" style="font-weight:bold;">20.4.5 Jointly Training Deep Boltzmann Machines</span></h5>
<p><span class="font64">Classic DBMs require greedy unsupervised pretraining, and to perform classification well, require a separate MLP-based classifier on top of the hidden features they&#160;extract. This has some undesirable properties. It is hard to track performance&#160;during training because we cannot evaluate properties of the full DBM while&#160;training the first RBM. Thus, it is hard to tell how well our hyperparameters</span></p><div><div><img src="main-194.jpg" alt=""/>
<p><span class="font64">Figure 20.4: The deep Boltzmann machine training procedure used to classify the MNIST dataset (Salakhutdinov and Hinton, 2009a; Srivastava </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2014). </span><span class="font64" style="font-style:italic;">(a)</span><span class="font64"> Train an RBM&#160;by using CD to approximately maximize log </span><span class="font64" style="font-style:italic;">P</span><span class="font64"> (v). </span><span class="font64" style="font-style:italic;">(b)</span><span class="font64"> Train a second RBM that models&#160;h<sup>(1)</sup> and target class y by using CD-k to approximately maximize log P(h<sup>(1)</sup>, y) where&#160;is drawn from the first RBM’s posterior conditioned on the data. Increasek from 1&#160;to 20 during learning. </span><span class="font64" style="font-style:italic;">(c)</span><span class="font64"> Combine the two RBMs into a DBM. Train it to approximately&#160;maximize log P( v, y) using stochastic maximum likelihood withk = 5. </span><span class="font64" style="font-style:italic;">(d)</span><span class="font64"> Delete y from&#160;the model. Define a new set of features h<sup>(1)</sup> and h<sup>(2)</sup> that are obtained by running mean&#160;field inference in the model lacking y. Use these features as input to an MLP whose&#160;structure is the same as an additional pass of mean field, with an additional output layer&#160;for the estimate of y. Initialize the MLP’s weights to be the same as the DBM’s weights.&#160;Train the MLP to approximately maximize log P(y | v) using stochastic gradient descent&#160;and dropout. Figure reprinted from (Goodfellow </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2013b).</span></p></div></div>
<p><span class="font64">are working until quite late in the training process. Software implementations of DBMs need to have many different components for CD training of individual&#160;RBMs, PCD training of the full DBM, and training based on back-propagation&#160;through the MLP. Finally, the MLP on top of the Boltzmann machine loses many&#160;of the advantages of the Boltzmann machine probabilistic model, such as being&#160;able to perform inference when some input values are missing.</span></p>
<p><span class="font64">There are two main ways to resolve the joint training problem of the deep Boltzmann machine. The first is the </span><span class="font64" style="font-weight:bold;font-style:italic;">centered deep Boltzmann machine</span><span class="font64"> (Montavon&#160;and Muller, 2012), which reparametrizes the model in order to make the Hessian of&#160;the cost function better-conditioned at the beginning of the learning process. This&#160;yields a model that can be trained without a greedy layer-wise pretraining stage.&#160;The resulting model obtains excellent test set log-likelihood and produces high&#160;quality samples. Unfortunately, it remains unable to compete with appropriately&#160;regularized MLPs as a classifier. The second way to jointly train a deep Boltzmann&#160;machine is to use a </span><span class="font64" style="font-weight:bold;font-style:italic;">multi-prediction deep Boltzmann machine</span><span class="font64"> (Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2013b). This model uses an alternative training criterion that allows the use&#160;of the back-propagation algorithm in order to avoid the problems with MCMC&#160;estimates of the gradient. Unfortunately, the new criterion does not lead to good&#160;likelihood or samples, but, compared to the MCMC approach, it does lead to&#160;superior classification performance and ability to reason well about missing inputs.</span></p>
<p><span class="font64">The centering trick for the Boltzmann machine is easiest to describe if we return to the general view of a Boltzmann machine as consisting of a set of units </span><span class="font64" style="font-weight:bold;">x&#160;</span><span class="font64">with a weight matrix </span><span class="font64" style="font-weight:bold;">U </span><span class="font64">and biases </span><span class="font64" style="font-weight:bold;font-style:italic;">b</span><span class="font64">. Recall from Eq. 20.2 that he energy function&#160;is given by</span></p>
<p><span class="font64" style="font-weight:bold;">E(x) = -x<sup>T</sup>Ux - b x. &#160;&#160;&#160;</span><span class="font64">(20.36)</span></p>
<p><span class="font64">Using different sparsity patterns in the weight matrix </span><span class="font64" style="font-weight:bold;">U</span><span class="font64">, we can implement structures of Boltzmann machines, such as RBMs, or DBMs with different numbers&#160;of layers. This is accomplished by partitioning </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">into visible and hidden units and&#160;zeroing out elements of </span><span class="font64" style="font-weight:bold;">U </span><span class="font64">for units that do not interact. The centered Boltzmann&#160;machine introduces a vector </span><span class="font64" style="font-weight:bold;font-style:italic;">fi</span><span class="font64"> that is subtracted from all of the states:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">E</span><span class="font64" style="font-weight:bold;">(x; U, b) = —(x — </span><span class="font20">1</span><span class="font64" style="font-weight:bold;">)<sup>T</sup>U(x — i) — (x — </span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;">) b.</span><span class="font64"> &#160;&#160;&#160;(20.37)</span></p>
<p><span class="font64">Typically </span><span class="font64" style="font-weight:bold;">i </span><span class="font64">is a hyperparameter fixed at the beginning of training. It is usually chosen to make sure that </span><span class="font64" style="font-weight:bold;">x — i « 0 </span><span class="font64">when the model is initialized. This reparametrization does not change the set of probability distributions that the&#160;model can represent, but it does change the dynamics of stochastic gradient descent&#160;applied to the likelihood. Specifically, in many cases, this reparametrization results&#160;in a Hessian matrix that is better conditioned. Melchior </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013) experimentally&#160;confirmed that the conditioning of the Hessian matrix improves, and observed&#160;that the centering trick is equivalent to another Boltzmann machine learning&#160;technique, the </span><span class="font64" style="font-weight:bold;font-style:italic;">enhanced gradient</span><span class="font64"> (Cho </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011). The improved conditioning of&#160;the Hessian matrix allows learning to succeed, even in difficult cases like training a&#160;deep Boltzmann machine with multiple layers.</span></p>
<p><span class="font64">The other approach to jointly training deep Boltzmann machines is the multiprediction deep Boltzmann machine (MP-DBM) which works by viewing the mean field equations as defining a family of recurrent networks for approximately solving&#160;every possible inference problem (Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013b). Rather than training&#160;the model to maximize the likelihood, the model is trained to make each recurrent&#160;network obtain an accurate answer to the corresponding inference problem. The&#160;training process is illustrated in Fig. 20.5. It consists of randomly sampling a&#160;training example, randomly sampling a subset of inputs to the inference network,&#160;and then training the inference network to predict the values of the remaining&#160;units.</span></p>
<p><span class="font64">This general principle of back-propagating through the computational graph for approximate inference has been applied to other models (Stoyanov </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011;&#160;Brakel </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013). In these models and in the MP-DBM, the final loss is not&#160;the lower bound on the likelihood. Instead, the final loss is typically based on&#160;the approximate conditional distribution that the approximate inference network&#160;imposes over the missing values. This means that the training of these models&#160;is somewhat heuristically motivated. If we inspect the </span><span class="font64" style="font-weight:bold;font-style:italic;">p(</span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) represented by the&#160;Boltzmann machine learned by the MP-DBM, it tends to be somewhat defective,&#160;in the sense that Gibbs sampling yields poor samples.</span></p>
<p><span class="font64">Back-propagation through the inference graph has two main advantages. First, it trains the model as it is really used—with approximate inference. This means&#160;that approximate inference, for example, to fill in missing inputs, or to perform&#160;classification despite the presence of missing inputs, is more accurate in the MP-DBM than in the original DBM. The original DBM does not make an accurate&#160;classifier on its own; the best classification results with the original DBM were&#160;based on training a separate classifier to use features extracted by the DBM,&#160;rather than by using inference in the DBM to compute the distribution over the&#160;class labels. Mean field inference in the MP-DBM performs well as a classifier&#160;without special modifications. The other advantage of back-propagating through&#160;approximate inference is that back-propagation computes the exact gradient of&#160;the loss. This is better for optimization than the approximate gradients of SML&#160;training, which suffer from both bias and variance. This probably explains why MP-DBMs may be trained jointly while DBMs require a greedy layer-wise pretraining.</span></p><div><div><img src="main-195.jpg" alt=""/>
<p><span class="font64">Figure 20.5: An illustration of the multi-prediction training process for a deep Boltzmann machine. Each row indicates a different example within a minibatch for the same training&#160;step. Each column represents a time step within the mean field inference process. For&#160;each example, we sample a subset of the data variables to serve as inputs to the inference&#160;process. These variables are shaded black to indicate conditioning. We then run the&#160;mean field inference process, with arrows indicating which variables influence which other&#160;variables in the process. In practical applications, we unroll mean field for several steps.&#160;In this illustration, we unroll for only two steps. Dashed arrows indicate how the process&#160;could be unrolled for more steps. The data variables that were not used as inputs to the&#160;inference process become targets, shaded in gray. We can view the inference process for&#160;each example as a recurrent network. We use gradient descent and back-propagation to&#160;train these recurrent networks to produce the correct targets given their inputs. This&#160;trains the mean field process for the MP-DBM to produce accurate estimates. Figure&#160;adapted from Goodfellow </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2013b).</span></p></div></div>
<p><span class="font64">The disadvantage of back-propagating through the approximate inference graph is that it does not provide a way to optimize the log-likelihood, but rather a heuristic&#160;approximation of the generalized pseudolikelihood.</span></p>
<p><span class="font64">The MP-DBM inspired the NADE-k (Raiko </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014) extension to the NADE framework, which is described in Sec. 20.10.10.</span></p>
<p><span class="font64">The MP-DBM has some connections to dropout. Dropout shares the same parameters among many different computational graphs, with the difference between each graph being whether it includes or excludes each unit. The MP-DBM also&#160;shares parameters across many computational graphs. In the case of the MP-DBM,&#160;the difference between the graphs is whether each input unit is observed or not.&#160;When a unit is not observed, the MP-DBM does not delete it entirely as in the&#160;case of dropout. Instead, the MP-DBM treats it as a latent variable to be inferred.&#160;One could imagine applying dropout to the MP-DBM by additionally removing&#160;some units rather than making them latent.</span></p>
</body>
</html>