<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h4><a id="bookmark0"></a><span class="font65" style="font-weight:bold;">20.10 Directed Generative Nets</span></h4>
<p><span class="font64">As discussed in Chapter 16, directed graphical models make up a prominent class of graphical models. While directed graphical models have been very popular&#160;within the greater machine learning community, within the smaller deep learning&#160;community they have until roughly 2013 been overshadowed by undirected models&#160;such as the RBM.</span></p>
<p><span class="font64">In this section we review some of the standard directed graphical models that have traditionally been associated with the deep learning community.</span></p>
<p><span class="font64">We have already described deep belief networks, which are a partially directed model. We have also already described sparse coding models, which can be thought&#160;of as shallow directed generative models. They are often used as feature learners&#160;in the context of deep learning, though they tend to perform poorly at sample&#160;generation and density estimation. We now describe a variety of deep, fully directed&#160;models.</span></p><h5><a id="bookmark1"></a><span class="font64" style="font-weight:bold;">20.10.1 Sigmoid Belief Nets</span></h5>
<p><span class="font64">Sigmoid belief networks (Neal, 1990) are a simple form of directed graphical model with a specific kind of conditional probability distribution. In general, we can&#160;think of a sigmoid belief network as having a vector of binary states </span><span class="font64" style="font-weight:bold;">s</span><span class="font64">, with each&#160;element of the state influenced by its ancestors:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(si</span><span class="font64">) = a &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">Wj,isj</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">bi J</span><span class="font64"> .&#160;&#160;&#160;&#160;(20.70)</span></p>
<p><span class="font64">The most common structure of sigmoid belief network is one that is divided into many layers, with ancestral sampling proceeding through a series of many&#160;hidden layers and then ultimately generating the visible layer. This structure is&#160;very similar to the deep belief network, except that the units at the beginning of&#160;the sampling process are independent from each other, rather than sampled from&#160;a restricted Boltzmann machine. Such a structure is interesting for a variety of&#160;reasons. One reason is that the structure is a universal approximator of probability&#160;distributions over the visible units, in the sense that it can approximate any&#160;probability distribution over binary variables arbitrarily well, given enough depth,&#160;even if the width of the individual layers is restricted to the dimensionality of the&#160;visible layer (Sutskever and Hinton, 2008).</span></p>
<p><span class="font64">While generating a sample of the visible units is very efficient in a sigmoid belief network, most other operations are not. Inference over the hidden units given&#160;the visible units is intractable. Mean field inference is also intractable because the&#160;variational lower bound involves taking expectations of cliques that encompass&#160;entire layers. This problem has remained difficult enough to restrict the popularity&#160;of directed discrete networks.</span></p>
<p><span class="font64">One approach for performing inference in a sigmoid belief network is to construct a different lower bound that is specialized for sigmoid belief networks (Saul </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">1996). This approach has only been applied to very small networks. Another&#160;approach is to use learned inference mechanisms as described in Sec. 19.5. The&#160;Helmholtz machine (Dayan </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1995; Dayan and Hinton, 1996) is a sigmoid belief&#160;network combined with an inference network that predicts the parameters of the&#160;mean field distribution over the hidden units. Modern approaches (Gregor </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2014; Mnih and Gregor, 2014) to sigmoid belief networks still use this inference&#160;network approach. These techniques remain difficult due to the discrete nature of&#160;the latent variables. One cannot simply back-propagate through the output of the&#160;inference network, but instead must use the relatively unreliable machinery for back-propagating through discrete sampling processes, described in Sec. 20.9.1. Recent&#160;approaches based on importance sampling, reweighted wake-sleep (Bornschein&#160;and Bengio, 2015) and bidirectional Helmholtz machines (Bornschein </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015)&#160;make it possible to quickly train sigmoid belief networks and reach state-of-the-art&#160;performance on benchmark tasks.</span></p>
<p><span class="font64">A special case of sigmoid belief networks is the case where there are no latent variables. Learning in this case is efficient, because there is no need to marginalize&#160;latent variables out of the likelihood. A family of models called auto-regressive&#160;networks generalize this fully visible belief network to other kinds of variables&#160;besides binary variables and other structures of conditional distributions besides log-linear relationships. Auto-regressive networks are described later, in Sec. 20.10.7.</span></p><h5><a id="bookmark2"></a><span class="font64" style="font-weight:bold;">20.10.2 Differentiable Generator Nets</span></h5>
<p><span class="font64">Many generative models are based on the idea of using a differentiable </span><span class="font64" style="font-weight:bold;font-style:italic;">generator network.</span><span class="font64"> The model transforms samples of latent variables </span><span class="font64" style="font-weight:bold;">z </span><span class="font64">to samples </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">or&#160;to distributions over samples </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">using a differentiable function g(</span><span class="font64" style="font-weight:bold;">z</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">0</span><span class="font64"><sup>(g)</sup>) which is&#160;typically represented by a neural network. This model class includes variational&#160;autoencoders, which pair the generator net with an inference net, generative&#160;adversarial networks, which pair the generator network with a discriminator&#160;network, and techniques that train generator networks in isolation.</span></p>
<p><span class="font64">Generator networks are essentially just parametrized computational procedures for generating samples, where the architecture provides the family of possible&#160;distributions to sample from and the parameters select a distribution from within&#160;that family.</span></p>
<p><span class="font64">As an example, the standard procedure for drawing samples from a normal distribution with mean </span><span class="font64" style="font-weight:bold;font-style:italic;">fi</span><span class="font64"> and covariance </span><span class="font64" style="font-weight:bold;font-style:italic;">£</span><span class="font64"> is to feed samples </span><span class="font64" style="font-weight:bold;">z </span><span class="font64">from a normal&#160;distribution with zero mean and identity covariance into a very simple generator&#160;network. This generator network contains just one affine layer:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x</span></p></div><div>
<p><span class="font64">g(z) = p + </span><span class="font64" style="font-weight:bold;font-style:italic;">Lz</span></p></div><div>
<p><span class="font64">(20.71)</span></p></div>
<p><span class="font64">where L is given by the Cholesky decomposition of </span><span class="font64" style="font-weight:bold;font-style:italic;">£.</span></p>
<p><span class="font64">Pseudorandom number generators can also use nonlinear transformations of simple distributions. For example, </span><span class="font64" style="font-weight:bold;font-style:italic;">inverse transform sampling</span><span class="font64"> (Devroye, 2013)&#160;draws a scalar z from U(0, 1) and applies a nonlinear transformation to a scalar&#160;x. In this case </span><span class="font64" style="font-weight:bold;font-style:italic;">g(z</span><span class="font64">) is given by the inverse of the cumulative distribution function&#160;F(x) = /* </span><span class="font64" style="font-weight:bold;font-style:italic;">p(v)dv</span><span class="font64">. If we are able to specify p(x), integrate over x, and invert the&#160;resulting function, we can sample from p(x) without using machine learning.</span></p>
<p><span class="font64">To generate samples from more complicated distributions that are difficult to specify directly, difficult to integrate over, or whose resulting integrals are&#160;difficult to invert, we use a feedforward network to represent a parametric family&#160;of nonlinear functions g, and use training data to infer the parameters selecting&#160;the desired function.</span></p>
<p><span class="font64">We can think of g as providing a nonlinear change of variables that transforms the distribution over </span><span class="font64" style="font-weight:bold;">z </span><span class="font64">into the desired distribution over </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">.</span></p>
<p><span class="font64">Recall from Eq. 3.47 that, for invertible, differentiable, continuous g,</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Pz<sup>(z</sup>) = Px<sup>(</sup>g<sup>(z))</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;"><sup>det(</sup> dZ י</span></p></div><div>
<p><span class="font64">(20.72)</span></p></div>
<p><span class="font64">This implicitly imposes a probability distribution over </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">:</span></p><div>
<p><span class="font64">(20.73)</span></p></div>
<p><span class="font64" style="font-weight:bold;">Px(x) = <sup>PZ -I(X))</sup></span></p>
<p><span class="font64">Of course, this formula may be difficult to evaluate, depending on the choice of g, so we often use indirect means of learning g, rather than trying to maximize&#160;logp(x) directly.</span></p>
<p><span class="font64">In some cases, rather than using g to provide a sample of </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> directly, we use </span><span class="font64" style="font-weight:bold;font-style:italic;">g </span><span class="font64">to define a conditional distribution over x. For example, we could use a generator&#160;net whose final layer consists of sigmoid outputs to provide the mean parameters&#160;of Bernoulli distributions:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(xi</span><span class="font64"> = 1 | z) = </span><span class="font64" style="font-weight:bold;font-style:italic;">g(z)i.</span><span class="font64"> &#160;&#160;&#160;(20.74)</span></p>
<p><span class="font64">In this case, when we use g to define p(x | z), we impose a distribution over x by marginalizing z:</span></p>
<p><span class="font64">p(x) = E<sub>z</sub>p(x | z). &#160;&#160;&#160;(20.75)</span></p>
<p><span class="font64">Both approaches define a distribution p<sub>g</sub> (x) and allow us to train various criteria of p<sub>g</sub> using the reparametrization trick of Sec. 20.9.</span></p>
<p><span class="font64">The two different approaches to formulating generator nets—emitting the parameters of a conditional distribution versus directly emitting samples—have&#160;complementary strengths and weaknesses. When the generator net defines a&#160;conditional distribution over x, it is capable of generating discrete data as well&#160;as continuous data. When the generator net provides samples directly, it is&#160;capable of generating only continuous data (we could introduce discretization in&#160;the forward propagation, but this would lose the ability to learn the model using&#160;back-propagation). The advantage to direct sampling is that we are no longer&#160;forced to use conditional distributions whose form can be easily written down and&#160;algebraically manipulated by a human designer.</span></p>
<p><span class="font64">Approaches based on differentiable generator networks are motivated by the success of gradient descent applied to differentiable feedforward networks for&#160;classification. In the context of supervised learning, deep feedforward networks&#160;trained with gradient-based learning seem practically guaranteed to succeed given&#160;enough hidden units and enough training data. Can this same recipe for success&#160;transfer to generative modeling?</span></p>
<p><span class="font64">Generative modeling seems to be more difficult than classification or regression because the learning process requires optimizing intractable criteria. In the context&#160;of differentiable generator nets, the criteria are intractable because the data does&#160;not specify both the inputs z and the outputs x of the generator net. In the case&#160;of supervised learning, both the inputs x and the outputs y were given, and the&#160;optimization procedure needs only to learn how to produce the specified mapping.&#160;In the case of generative modeling, the learning procedure needs to determine how&#160;to arrange z space in a useful way and additionally how to map from z to x.</span></p>
<p><span class="font64">Dosovitskiy </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) studied a simplified problem, where the correspondence between z and x is given. Specifically, the training data is computer-rendered&#160;imagery of chairs. The latent variables z are parameters given to the rendering&#160;engine describing the choice of which chair model to use, the position of the chair,&#160;and other configuration details that affect the rendering of the image. Using this&#160;synthetically generated data, a convolutional network is able to learn to map z&#160;descriptions of the content of an image to x approximations of rendered images.&#160;This suggests that contemporary differentiable generator networks have sufficient&#160;model capacity to be good generative models, and that contemporary optimization&#160;algorithms have the ability to fit them. The difficulty lies in determining how to&#160;train generator networks when the value of z for each x is not fixed and known&#160;ahead of each time.</span></p>
<p><span class="font64">The following sections describe several approaches to training differentiable generator nets given only training samples of x.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">20.10.3 Variational Autoencoders</span></h5>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">variational autoencoder</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">VAE</span><span class="font64"> (Kingma, 2013; Rezende </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014) is a directed model that uses learned approximate inference and can be trained purely&#160;with gradient-based methods.</span></p>
<p><span class="font64">To generate a sample from the model, the VAE first draws a sample z from the code distribution p<sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64">( z). The sample is then run through a differentiable&#160;generator networkg(z). Finally, x is sampled from a distribution p<sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64">(x; g(z)) =&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">p<sub>mo</sub>d</span><span class="font62" style="font-style:italic;"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;font-style:italic;"> (x |</span><span class="font64"> z). However, during training, the approximate inference network (or&#160;encoder) q(z | x) is used to obtain z and p<sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (x | z) is then viewed as a decoder&#160;network.</span></p>
<p><span class="font64">The key insight behind variational autoencoders is that they may be trained by maximizing the variational lower bound L(q) associated with data point x:</span></p>
<p><span class="font64"><sup>L(</sup>q<sup>)</sup> = E־~q(z|*) <sup>10</sup>gPmodel<sup>(z</sup>ל <sup>x)</sup> + <sup>H(</sup>q<sup>(z 1 x)) &#160;&#160;&#160;(</sup>20.7<sup>6)</sup></span></p>
<p><span class="font64"><sup>E</sup>z~q(z|®) <sup>10gp</sup>mode1<sup>(x 1 z) &#160;&#160;&#160;D</sup>KL<sup>(q(z 1 x) 1|p</sup>mode1<sup>(z))&#160;&#160;&#160;&#160;(2</sup>°.<sup>77)</sup></span></p>
<p><span class="font64">&lt; logPmode1 (x). &#160;&#160;&#160;(20.78)</span></p>
<p><span class="font64">In Eq. 20.76, we recognize the first term as the joint log-likelihood of the visible and hidden variables under the approximate posterior over the latent variables (just&#160;like with EM, except that we use an approximate rather than the exact posterior).&#160;We recognize also a second term, the entropy of the approximate posterior. When&#160;q is chosen to be a Gaussian distribution, with noise added to a predicted mean&#160;value, maximizing this entropy term encourages increasing the standard deviation&#160;of this noise. More generally, this entropy term encourages the variational posterior&#160;to place high probability mass on many z values that could have generated x,&#160;rather than collapsing to a single point estimate of the most likely value. In Eq.&#160;20.77, we recognize the first term as the reconstruction log-likelihood found in&#160;other autoencoders. The second term tries to make the approximate posterior&#160;distribution q(</span><span class="font64" style="font-weight:bold;">z | </span><span class="font64">x) and the model prior p</span><span class="font64" style="font-weight:bold;">m<sub>o</sub>d<sub>e</sub>1 </span><span class="font64">(z) approach each other.</span></p>
<p><span class="font64">Traditional approaches to variational inference and learning infer q via an optimization algorithm, typically iterated fixed point equations (Sec. 19.4). These&#160;approaches are slow and often require the ability to compute </span><span class="font64" style="font-weight:bold;font-style:italic;">E<sub>z</sub>^<sub>q</sub></span><span class="font64"> logp</span><span class="font64" style="font-weight:bold;"><sub>mo</sub>d</span><span class="font20"><sub>e</sub>1</span><span class="font64">(z, x)&#160;in closed form. The main idea behind the variational autoencoder is to train a&#160;parametric encoder (also sometimes called an inference network or recognition&#160;model) that produces the parameters of q. So long as z is a continuous variable, we&#160;can then back-propagate through samples of z drawn from q(z </span><span class="font64" style="font-weight:bold;">| </span><span class="font64">x) = q(z; f (x; </span><span class="font64" style="font-style:italic;">6</span><span class="font64" style="font-weight:bold;font-style:italic;">))&#160;</span><span class="font64">in order to obtain a gradient with respect to </span><span class="font64" style="font-style:italic;">d</span><span class="font64">. Learning then consists solely of&#160;maximizing </span><span class="font64" style="font-weight:bold;">L </span><span class="font64">with respect to the parameters of the encoder and decoder. All of&#160;the expectations in </span><span class="font64" style="font-weight:bold;">L </span><span class="font64">may be approximated by Monte Carlo sampling.</span></p>
<p><span class="font64">The variational autoencoder approach is elegant, theoretically pleasing, and simple to implement. It also obtains excellent results and is among the state of&#160;the art approaches to generative modeling. Its main drawback is that samples&#160;from variational autoencoders trained on images tend to be somewhat blurry. The&#160;causes of this phenomenon are not yet known. One possibility is that the blurriness&#160;is an intrinsic effect of maximum likelihood, which minimizes </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">Dkl</span><span class="font64"> (p</span><span class="font64" style="font-weight:bold;">d<sub>ata</sub></span><span class="font64">||p</span><span class="font64" style="font-weight:bold;"><sub>mo</sub>d<sub>e</sub>i</span><span class="font64">).&#160;As illustrated in Fig. 3.6, this means that the model will assign high probability to&#160;points that occur in the training set, but may also assign high probability to other&#160;points. These other points may include blurry images. Part of the reason that the&#160;model would choose to put probability mass on blurry images rather than some&#160;other part of the space is that the variational autoencoders used in practice usually&#160;have a Gaussian distribution for p</span><span class="font64" style="font-weight:bold;"><sub>mo</sub>d</span><span class="font20"><sub>e</sub>1</span><span class="font64">(x; g(z)). Maximizing a lower bound on&#160;the likelihood of such a distribution is similar to training a traditional autoencoder&#160;with mean squared error, in the sense that it has a tendency to ignore features&#160;of the input that occupy few pixels or that cause only a small change in the&#160;brightness of the pixels that they occupy. This issue is not specific to VAEs and&#160;is shared with generative models that optimize a log-likelihood, or equivalently,&#160;Dkl (p</span><span class="font64" style="font-weight:bold;">d<sub>ata</sub></span><span class="font64">||p</span><span class="font64" style="font-weight:bold;"><sub>mo</sub>d<sub>e</sub>i</span><span class="font64">), as argued by Theis </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) and by Huszar (2015). Another&#160;troubling issue with contemporary VAE models is that they tend to use only a small&#160;subset of the dimensions of z, as if the encoder was not able to transform enough&#160;of the local directions in input space to a space where the marginal distribution&#160;matches the factorized prior.</span></p>
<p><span class="font64">The VAE framework is very straightforward to extend to a wide range of model architectures. This is a key advantage over Boltzmann machines, which require&#160;extremely careful model design to maintain tractability. VAEs work very well&#160;with a diverse family of differentiable operators. One particularly sophisticated&#160;VAE is the </span><span class="font64" style="font-style:italic;">deep recurrent attention writer</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">or </span><span class="font64" style="font-style:italic;">DRAW</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">model (Gregor </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> 2015).&#160;DRAW uses a recurrent encoder and recurrent decoder combined with an attention&#160;mechanism. The generation process for the DRAW model consists of sequentially&#160;visiting different small image patches and drawing the values of the pixels at those&#160;points. VAEs can also be extended to generate sequences by defining variational&#160;RNNs (Chung </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> 2015b) by using a recurrent encoder and decoder within&#160;the VAE framework. Generating a sample from a traditional RNN involves only&#160;non-deterministic operations at the output space. Variational RNNs also have&#160;random variability at the potentially more abstract level captured by the VAE&#160;latent variables.</span></p>
<p><span class="font64">The VAE framework has been extended to maximize not just the traditional variational lower bound, but instead the </span><span class="font64" style="font-style:italic;">importance weighted autoencoder</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">(Burda&#160;</span><span class="font64" style="font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> 2015) objective:</span></p><div>
<p><span class="font64" style="font-style:italic;"><sup>L</sup></span><span class="font64">k</span><span class="font64" style="font-weight:bold;"><sup>(x</sup>, q<sup>)</sup> = <sup>E</sup>z</span><span class="font64">(</span><span class="font18">1</span><span class="font64">) &#160;&#160;&#160;~g(z|x) </span><span class="font64" style="font-weight:bold;">l°g k</span></p></div><div>
<p><span class="font64">Pmodel </span><span class="font64" style="font-weight:bold;font-style:italic;">(x,</span><span class="font64"> Z<sup>(i)</sup> )</span></p></div><div>
<p><span class="font64" style="font-style:italic;">k &#160;&#160;&#160;q</span><span class="font64" style="font-weight:bold;font-style:italic;">(z</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">W </span><span class="font64">| x)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p></div><div>
<p><span class="font64">(20.79)</span></p></div>
<p><span class="font64">This new objective is equivalent to the traditional lower bound L when k = 1. However, it may also be interpreted as forming an estimate of the true logp<sub>mo</sub>d<sub>e</sub>i(x)&#160;using importance sampling of z from proposal distribution q(z | x). The importance&#160;weighted autoencoder objective is also a lower bound on logp<sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (x) and becomes&#160;tighter as k increases.</span></p>
<p><span class="font64">Variational autoencoders have some interesting connections to the MP-DBM and other approaches that involve back-propagation through the approximate&#160;inference graph (Goodfellow </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> 2013b; Stoyanov </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> 2011; Brakel </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> 2013).&#160;These previous approaches required an inference procedure such as mean field fixed&#160;point equations to provide the computational graph. The variational autoencoder&#160;is defined for arbitrary computational graphs, which makes it applicable to a wider&#160;range of probabilistic model families because there is no need to restrict the choice&#160;of models to those with tractable mean field fixed point equations. The variational&#160;autoencoder also has the advantage that it increases a bound on the log-likelihood&#160;of the model, while the criteria for the MP-DBM and related models are more&#160;heuristic and have little probabilistic interpretation beyond making the results of&#160;approximate inference accurate. One disadvantage of the variational autoencoder&#160;is that it learns an inference network for only one problem, inferring z given x.</span></p>
<p><span class="font64">The older methods are able to perform approximate inference over any subset of variables given any other subset of variables, because the mean field fixed point&#160;equations specify how to share parameters between the computational graphs for&#160;all of these different problems.</span></p>
<p><span class="font64">One very nice property of the variational autoencoder is that simultaneously training a parametric encoder in combination with the generator network forces&#160;the model to learn a predictable coordinate system that the encoder can capture.&#160;This makes it an excellent manifold learning algorithm. See Fig. 20.6 for examples&#160;of low-dimensional manifolds learned by the variational autoencoder. In one of the&#160;cases demonstrated in the figure, the algorithm discovered two independent factors&#160;of variation present in images of faces: angle of rotation and emotional expression.</span></p><div>
<p><span class="font64">** *«•I«•I*•1• •1• &#160;&#160;&#160;•ן• % U </span><span class="font58" style="font-weight:bold;font-style:italic;">U 6 6</span><span class="font64"> 2</span></p>
<p><span class="font42">i J i i</span></p>
<p><span class="font62" style="font-style:italic;">*</span><span class="font64"> •4* •l« &#160;&#160;&#160;M 2 2 2 2</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">it m m m</span><span class="font64"> &#160;&#160;&#160;*גגגייי</span></p>
<p><span class="font64">.»«4**1**1**1«*1*»4**1*<sup>%</sup>(*<sup>%</sup>1■* ‘I 3 3 2 2</span></p>
<p><span class="font64">ii‘<sup>1</sup>‘<sup>1</sup>‘<sup>ג</sup>י ׳</span></p>
<p><span class="font65">ף״(* !J ^ ^ ף</span></p>
<p><span class="font64">ד H ר וי י</span></p>
<p><span class="font64" style="font-weight:bold;">**f**f**f<sup>a</sup>*f**V**V*M**f**l&gt;* ף ף ף </span><span class="font56" style="font-style:italic;">ף ף</span></p><h5><a id="bookmark4"></a><span class="font64">* &#160;&#160;&#160;-&#160;&#160;&#160;&#160;* * * * j ף </span><span class="font64" style="font-weight:bold;font-style:italic;">ף ף ף</span><span class="font64"> ד</span></h5>
<p><span class="font62" style="font-style:italic;">•</span><span class="font64"> &#160;&#160;&#160;*1# </span><span class="font63" style="font-style:italic;">*1</span><span class="font64">&#160;&#160;&#160;&#160;•1* «ו*«ן</span><span class="font62" style="font-style:italic;"> </span><span class="font51" style="font-style:italic;font-variant:small-caps;">h</span><span class="font62" style="font-style:italic;"> </span><span class="font51" style="font-style:italic;font-variant:small-caps;">h</span><span class="font64"> 9 &lt;?</span></p>
<p><span class="font64">ד</span></p>
<p><span class="font64" style="font-weight:bold;">7 .»•ן*•1**1* •1•»ו* •1• </span><span class="font65" style="font-weight:bold;font-style:italic;">•</span><span class="font64" style="font-weight:bold;">ו* </span><span class="font65" style="font-weight:bold;font-style:italic;">•</span><span class="font64" style="font-weight:bold;"> v v </span><span class="font65" style="font-weight:bold;font-style:italic;">H 9</span></p>
<p><span class="font64">^ &gt; w.</span></p>
<p><span class="font64">• &#160;&#160;&#160;V y V « 9</span></p>
<p><span class="font64">י־ y y y 9 7</span></p>
<p><span class="font63">U1w*v««f*»b ‘1• *I* - {••C•(** </span><span class="font65" style="font-style:italic;">7 9 7 7 1</span></p>
<p><span class="font65" style="font-weight:bold;font-style:italic;">7 7 7 7 7</span></p></div><div>
<p><span class="font64">i&amp;OOO </span><span class="font58" style="font-weight:bold;">2</span><span class="font64"> </span><span class="font58" style="font-weight:bold;">2</span><span class="font64"> </span><span class="font58" style="font-weight:bold;">2</span><span class="font64"> </span><span class="font58" style="font-weight:bold;">0</span><span class="font64"> </span><span class="font58" style="font-weight:bold;">0&#160;</span><span class="font70" style="font-weight:bold;">2</span><span class="font64"> </span><span class="font70" style="font-weight:bold;">2</span><span class="font64"> </span><span class="font70" style="font-weight:bold;">2</span><span class="font64"> </span><span class="font70" style="font-weight:bold;">2</span><span class="font64"> </span><span class="font70" style="font-weight:bold;">5&#160;</span><span class="font64" style="font-weight:bold;">2 2 2 2 3&#160;2 2 2 2 3&#160;22233</span></p>
<p><span class="font64" style="font-weight:bold;">2 &#160;&#160;&#160;2 2 3 3</span></p>
<p><span class="font64" style="font-weight:bold;">3 &#160;&#160;&#160;2 3 3 3&#160;3 3 3 3 3</span></p>
<p><span class="font64">2 </span><span class="font1" style="font-weight:bold;">8</span></p>
<p dir="rtl"><span class="font64">ן </span><span class="font64" style="font-weight:bold;font-style:italic;">9&#160;9&#160;9&#160;1</span></p></div><div>
<p><span class="font64">8 </span><span class="font64" style="font-weight:bold;">8&#160;</span><span class="font65" style="font-weight:bold;font-style:italic;">9&#160;9&#160;8&#160;9&#160;9&#160;9</span></p>
<p dir="rtl"><span class="font64" style="font-weight:bold;font-style:italic;">ו</span></p>
<p dir="rtl"><span class="font64" style="font-weight:bold;font-style:italic;">1 ו</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">3 3 </span><span class="font58" style="font-weight:bold;">8</span><span class="font64"> </span><span class="font58" style="font-weight:bold;">8&#160;</span><span class="font64">8 S&#160;</span><span class="font57">8</span><span class="font64"> </span><span class="font57">8</span></p></div><div>
<p><span class="font64">OOO OOO&#160;5 5 0&#160;</span><span class="font64" style="font-weight:bold;">5 5 5&#160;3 3 5&#160;3 3 3&#160;3 3 3&#160;3 3 3&#160;3 3 3&#160;3 3 3&#160;8 3 3&#160;S S 6&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">6&#160;6&#160;6&#160;</span><span class="font64" style="font-weight:bold;">4</span></p></div><div>
<p dir="rtl"><span class="font64" style="font-weight:bold;font-style:italic;">1 1 </span><span class="font64">ו </span><span class="font64" style="font-weight:bold;font-style:italic;">1&#160;</span><span class="font64">ו</span></p></div><div>
<p><span class="font41" style="font-style:italic;">1</span><span class="font0" style="font-weight:bold;font-style:italic;"> \ \</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">4 4 4 I&#160;</span><span class="font64">t I</span></p>
<p><span class="font64">I I</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">OOO</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">OOO</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">000 </span><span class="font58" style="font-weight:bold;font-style:italic;">6</span><span class="font64" style="font-style:italic;"> </span><span class="font58" style="font-weight:bold;font-style:italic;">0</span><span class="font64" style="font-style:italic;"> </span><span class="font58" style="font-weight:bold;font-style:italic;">0&#160;</span><span class="font65" style="font-weight:bold;font-style:italic;">3 3 0&#160;3 3S&#160;</span><span class="font58" style="font-weight:bold;font-style:italic;">3 5S&#160;3 3S&#160;3 3 8</span></p>
<p><span class="font58" style="font-weight:bold;font-style:italic;">3 &#160;&#160;&#160;8 8&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">8</span><span class="font64" style="font-style:italic;"> </span><span class="font64" style="font-weight:bold;font-style:italic;">8</span><span class="font64" style="font-style:italic;"> </span><span class="font64" style="font-weight:bold;font-style:italic;">8&#160;</span><span class="font41" style="font-style:italic;">8</span><span class="font64" style="font-style:italic;"> </span><span class="font41" style="font-style:italic;">8</span><span class="font64" style="font-style:italic;"> </span><span class="font41" style="font-style:italic;">6&#160;6</span><span class="font64" style="font-style:italic;"> </span><span class="font41" style="font-style:italic;">6</span><span class="font64" style="font-style:italic;"> </span><span class="font41" style="font-style:italic;">6&#160;6</span><span class="font64" style="font-style:italic;"> </span><span class="font41" style="font-style:italic;">6</span><span class="font64" style="font-style:italic;"> </span><span class="font41" style="font-style:italic;">6&#160;6</span><span class="font64" style="font-style:italic;"> </span><span class="font41" style="font-style:italic;">6</span><span class="font64" style="font-style:italic;"> </span><span class="font41" style="font-style:italic;">6</span></p>
<p><span class="font64">1 &#160;&#160;&#160;6 6</span></p>
<p><span class="font64" style="font-weight:bold;">4 &#160;&#160;&#160;</span><span class="font65" style="font-weight:bold;font-style:italic;">l i&#160;</span><span class="font64">I I </span><span class="font64" style="font-weight:bold;font-style:italic;">l&#160;</span><span class="font64">I i I&#160;I I I</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">0 003 0 0 0 3&#160;</span><span class="font69" style="font-weight:bold;font-style:italic;">0</span><span class="font64" style="font-style:italic;"> </span><span class="font69" style="font-weight:bold;font-style:italic;">0</span><span class="font64" style="font-style:italic;"> </span><span class="font69" style="font-weight:bold;font-style:italic;">0</span><span class="font64" style="font-style:italic;"> </span><span class="font69" style="font-weight:bold;font-style:italic;">3&#160;</span><span class="font65" style="font-weight:bold;font-style:italic;">0 6 3 3&#160;S 3 3 3&#160;S S J J&#160;S S S</span><span class="font64" style="font-weight:bold;"> 7&#160;</span><span class="font65" style="font-weight:bold;font-style:italic;">S S f /</span></p>
<p><span class="font58" style="font-weight:bold;font-style:italic;">8 r r ?</span></p>
<p><span class="font65" style="font-weight:bold;font-style:italic;">8 ? r 7</span></p>
<p><span class="font58" style="font-weight:bold;font-style:italic;">8 f r r</span></p>
<p><span class="font58" style="font-weight:bold;font-style:italic;">s </span><span class="font65" style="font-weight:bold;font-style:italic;">s r s s r&#160;6 s f&#160;i</span></p></div>
<p><span class="font64">Figure 20.6: Examples of two-dimensional coordinate systems for high-dimensional manifolds, learned by a variational autoencoder (Kingma and Welling, 2014a). Two dimensions may be plotted directly on the page for visualization, so we can gain an understanding of&#160;how the model works by training a model with a 2-D latent code, even if we believe the&#160;intrinsic dimensionality of the data manifold is much higher. The images shown are not&#160;examples from the training set but images x actually generated by the model </span><span class="font64" style="font-style:italic;">p(x</span><span class="font64"> | z),&#160;simply by changing the 2-D “code” z (each image corresponds to a different choice of “code”&#160;z on a 2-D uniform grid). </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> The two-dimensional map of the Frey faces manifold.&#160;One dimension that has been discovered (horizontal) mostly corresponds to a rotation of&#160;the face, while the other (vertical) corresponds to the emotional expression. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> The&#160;two-dimensional map of the MNIST manifold.</span></p><h5><a id="bookmark5"></a><span class="font64" style="font-weight:bold;">20.10.4 Generative Adversarial Networks</span></h5>
<p><span class="font64">Generative adversarial networks or GANs (Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al</span><span class="font64">2014c) are another generative modeling approach based on differentiable generator networks.</span></p>
<p><span class="font64">Generative adversarial networks are based on a game theoretic scenario in which the generator network must compete against an adversary. The generator&#160;network directly produces samples </span><span class="font64" style="font-weight:bold;font-style:italic;">x = g</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">z</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">0</span><span class="font64"><sup>(g)</sup>). Its adversary, the </span><span class="font64" style="font-weight:bold;font-style:italic;">discriminator&#160;network</span><span class="font64">, attempts to distinguish between samples drawn from the training data&#160;and samples drawn from the generator. The discriminator emits a probability value&#160;given by </span><span class="font64" style="font-weight:bold;font-style:italic;">d(x;</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">0</span><span class="font64"><sup>(d)</sup>), indicating the probability that </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">is a real training example&#160;rather than a fake sample drawn from the model.</span></p>
<p><span class="font64">The simplest way to formulate learning in generative adversarial networks is as a zero-sum game, in which a function </span><span class="font64" style="font-weight:bold;font-style:italic;">v</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">0</span><span class="font64"><sup>(g)</sup>, </span><span class="font64" style="font-weight:bold;">0</span><span class="font64"><sup>(d)</sup>) determines the payoff of the&#160;discriminator. The generator receives — </span><span class="font64" style="font-weight:bold;font-style:italic;">v(d</span><span class="font64"><sup>(g)</sup>, </span><span class="font64" style="font-weight:bold;">0</span><span class="font64"><sup>(d)</sup>) as its own payoff. During&#160;learning, each player attempts to maximize its own payoff, so that at convergence</span></p>
<p><span class="font64">g* = argminmax v(g,d). &#160;&#160;&#160;(20.80)</span></p>
<p><span class="font64">g &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">d</span></p>
<p><span class="font64">The default choice for v is</span></p>
<p><span class="font64"><sup>v(</sup></span><span class="font64" style="font-weight:bold;">0 </span><span class="font64"><sup>(g)</sup> </span><span class="font64" style="font-weight:bold;">0</span><span class="font64"><sup>(d) )</sup> = </span><span class="font64" style="font-weight:bold;">E</span><span class="font64">x^p</span><span class="font64" style="font-weight:bold;">data </span><span class="font64"><sup>10</sup>g <sup>d(</sup></span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>)</sup> + </span><span class="font64" style="font-weight:bold;"><sup>E</sup> &#160;&#160;&#160;odel </span><span class="font64"><sup>10</sup>g<sup>(</sup>l <sup>— d(</sup></span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>))</sup> .&#160;&#160;&#160;&#160;<sup>(2</sup>°.<sup>81)</sup></span></p>
<p><span class="font64">This drives the discriminator to attempt to learn to correctly classify samples as real or fake. Simultaneously, the generator attempts to fool the classifier into believing&#160;its samples are real. At convergence, the generator’s samples are indistinguishable&#160;from real data, and the discriminator outputs | everywhere. The discriminator&#160;may then be discarded.</span></p>
<p><span class="font64">The main motivation for the design of GANs is that the learning process requires neither approximate inference nor approximation of a partition function&#160;gradient. In the case where maxd v(g, d) is convex in </span><span class="font64" style="font-weight:bold;">0</span><span class="font64"><sup>(g)</sup> (such as the case where&#160;optimization is performed directly in the space of probability density functions)&#160;then the procedure is guaranteed to converge and is asymptotically consistent.</span></p>
<p><span class="font64">Unfortunately, learning in GANs can be difficult in practice when g and d are represented by neural networks and maxd v(g,d) is not convex. Goodfellow</span></p>
<p><span class="font64">(2014) identified non-convergence as an issue that may cause GANs to underfit.&#160;In general, simultaneous gradient descent on two players’ costs is not guaranteed&#160;to reach an equilibrium. Consider for example the value function v(a, </span><span class="font64" style="font-weight:bold;font-style:italic;">b)</span><span class="font64"> = </span><span class="font64" style="font-weight:bold;font-style:italic;">ab</span><span class="font64">,&#160;where one player controls a and incurs cost </span><span class="font64" style="font-weight:bold;font-style:italic;">ab</span><span class="font64">, while the other player controls b&#160;and receives a cost </span><span class="font64" style="font-weight:bold;font-style:italic;">—ab.</span><span class="font64"> If we model each player as making infinitesimally small&#160;gradient steps, each player reducing their own cost at the expense of the other&#160;player, then </span><span class="font64" style="font-weight:bold;">a </span><span class="font64">and </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">go into a stable, circular orbit, rather than arriving at the&#160;equilibrium point at the origin. Note that the equilibria for a minimax game are&#160;not local minima of </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">. Instead, they are points that are simultaneously minima&#160;for both players’ costs. This means that they are saddle points of </span><span class="font64" style="font-weight:bold;">v </span><span class="font64">that are local&#160;minima with respect to the first player’s parameters and local maxima with respect&#160;to the second player’s parameters. It is possible for the two players to take turns&#160;increasing then decreasing </span><span class="font64" style="font-weight:bold;">v </span><span class="font64">forever, rather than landing exactly on the saddle&#160;point where neither player is capable of reducing their cost. It is not known to&#160;what extent this non-convergence problem affects GANs.</span></p>
<p><span class="font64">Goodfellow (2014) identified an alternative formulation of the payoffs, in which the game is no longer zero-sum, that has the same expected gradient&#160;as maximum likelihood learning whenever the discriminator is optimal. Because&#160;maximum likelihood training converges, this reformulation of the GAN game should&#160;also converge, given enough samples. Unfortunatley, this alternative formulation&#160;does not seem to perform well in practice, possibly due to suboptimality of the&#160;discriminator, or possibly due to high variance around the expected gradient.</span></p>
<p><span class="font64">In practice, the best-performing formulation of the GAN game is a different formulation that is neither zero-sum nor equivalent to maximum likelihood, introduced by Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014c) with a heuristic motivation. In this best-performing&#160;formulation, the generator aims to increase the log probability that the discriminator makes a mistake, rather than aiming to decrease the log probability that the&#160;discriminator makes the correct prediction. This reformulation is motivated solely&#160;by the observation that it causes the derivative of the generator’s cost function&#160;with respect to the discriminator’s logits to remain large even in the situation&#160;where the discriminator confidently rejects all generator samples.</span></p>
<p><span class="font64">Stabilization of GAN learning remains an open problem. Fortunately, GAN learning performs well when the model architecture and hyperparameters are carefully selected. Radford </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) crafted a deep convolutional GAN (DCGAN)&#160;that performs very well for image synthesis tasks, and showed that its latent&#160;representation space captures important factors of variation, as shown in Fig. 15.9.&#160;See Fig. 20.7 for examples of images generated by a DCGAN generator.</span></p>
<p><span class="font64">The GAN learning problem can also be simplified by breaking the generation process into many levels of detail. It is possible to train conditional GANs (Mirza&#160;and Osindero, 2014) that learn to sample from a distribution </span><span class="font64" style="font-weight:bold;">p</span><span class="font64">(x | y) rather&#160;than simply sampling from a marginal distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p(</span><span class="font64">x). Denton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015)&#160;showed that a series of conditional GANs can be trained to first generate a very&#160;low-resolution version of an image, then incrementally add details to the image.</span></p><div><img src="main-196.jpg" alt=""/>
<p><span class="font64">Figure 20.7: Images generated by GANs trained on the LSUN dataset. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> Images of bedrooms generated by a DCGAN model, reproduced with permission from Radford&#160;</span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2015). </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> Images of churches generated by a LAPGAN model, reproduced&#160;with permission from Denton </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2015).</span></p></div>
<p><span class="font64">This technique is called the LAPGAN model, due to the use of a Laplacian pyramid to generate the images containing varying levels of detail. LAPGAN generators&#160;are able to fool not only discriminator networks but also human observers, with&#160;experimental subjects identifying up to 40% of the outputs of the network as being&#160;real data. See Fig. 20.7 for examples of images generated by a LAPGAN generator.</span></p>
<p><span class="font64">One unusual capability of the GAN training procedure is that it can fit probability distributions that assign zero probability to the training points. Rather than maximizing the log probability of specific points, the generator net learns to trace&#160;out a manifold whose points resemble training points in some way. Somewhat paradoxically, this means that the model may assign a log-likelihood of negative infinity&#160;to the test set, while still representing a manifold that a human observer judges&#160;to capture the essence of the generation task. This is not clearly an advantage or&#160;a disadvantage, and one may also guarantee that the generator network assigns&#160;non-zero probability to all points simply by making the last layer of the generator&#160;network add Gaussian noise to all of the generated values. Generator networks&#160;that add Gaussian noise in this manner sample from the same distribution that one&#160;obtains by using the generator network to parametrize the mean of a conditional&#160;Gaussian distribution.</span></p>
<p><span class="font64">Dropout seems to be important in the discriminator network. In particular, units should be stochastically dropped while computing the gradient for the&#160;generator network to follow. Following the gradient of the deterministic version of&#160;the discriminator with its weights divided by two does not seem to be as effective.</span></p>
<p><span class="font64">Likewise, never using dropout seems to yield poor results.</span></p>
<p><span class="font64">While the GAN framework is designed for differentiable generator networks, similar principles can be used to train other kinds of models. For example, </span><span class="font64" style="font-weight:bold;font-style:italic;">self-supervised boosting</span><span class="font64"> can be used to train an RBM generator to fool a logistic&#160;regression discriminator (Welling </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2002).</span></p><h5><a id="bookmark6"></a><span class="font64" style="font-weight:bold;">20.10.5 Generative Moment Matching Networks</span></h5>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Generative moment matching networks</span><span class="font64"> (Li </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015; Dziugaite </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015) are another form of generative model based on differentiable generator networks.&#160;Unlike VAEs and GANs, they do not need to pair the generator network with any&#160;other network—neither an inference network as used with VAEs nor a discriminator&#160;network as used with GANs.</span></p>
<p><span class="font64">These networks are trained with a technique called </span><span class="font64" style="font-weight:bold;font-style:italic;">moment matching.</span><span class="font64"> The basic idea behind moment matching is to train the generator in such a way that&#160;many of the statistics of samples generated by the model are as similar as possible&#160;to those of the statistics of the examples in the training set. In this context, a&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">moment</span><span class="font64"> is an expectation of different powers of a random variable. For example,&#160;the first moment is the mean, the second moment is the mean of the squared&#160;values, and so on. In multiple dimensions, each element of the random vector may&#160;be raised to different powers, so that a moment may be any quantity of the form</span></p>
<p><span class="font64">Ex n x&quot;■ &#160;&#160;&#160;(20.82)</span></p>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;font-style:italic;">n =</span><span class="font64"> [n </span><span class="font18">1</span><span class="font64">, n</span><span class="font18">2</span><span class="font64">,..., nd]<sup>T</sup> is a vector of non-negative integers.</span></p>
<p><span class="font64">Upon first examination, this approach seems to be computationally infeasible. For example, if we want to match all the moments of the form x^x </span><span class="font64" style="font-weight:bold;font-style:italic;">j</span><span class="font64">, then we need&#160;to minimize the difference between a number of values that is quadratic in the&#160;dimension of x. Moreover, even matching all of the first and second moments&#160;would only be sufficient to fit a multivariate Gaussian distribution, which captures&#160;only linear relationships between values. Our ambitions for neural networks are to&#160;capture complex nonlinear relationships, which would require far more moments.&#160;GANs avoid this problem of exhaustively enumerating all moments by using a&#160;dynamically updated discriminator, that automatically focuses its attention on&#160;whichever statistic the generator network is matching the least effectively.</span></p>
<p><span class="font64">Instead, generative moment matching networks can be trained by minimizing a cost function called </span><span class="font64" style="font-weight:bold;font-style:italic;">maximum mean discrepancy</span><span class="font64"> (Scholkopf and Smola, 2002;&#160;Gretton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012) or MMD. This cost function measures the error in the first&#160;moments in an infinite-dimensional space, using an implicit mapping to feature&#160;space defined by a kernel function in order to make computations on infinitedimensional vectors tractable. The MMD cost is zero if and only if the two&#160;distributions being compared are equal.</span></p>
<p><span class="font64">Visually, the samples from generative moment matching networks are somewhat disappointing. Fortunately, they can be improved by combining the generator&#160;network with an autoencoder. First, an autoencoder is trained to reconstruct the&#160;training set. Next, the encoder of the autoencoder is used to transform the entire&#160;training set into code space. The generator network is then trained to generate&#160;code samples, which may be mapped to visually pleasing samples via the decoder.</span></p>
<p><span class="font64">Unlike GANs, the cost function is defined only with respect to a batch of examples from both the training set and the generator network. It is not possible&#160;to make a training update as a function of only one training example or only&#160;one sample from the generator network. This is because the moments must be&#160;computed as an empirical average across many samples. When the batch size is too&#160;small, MMD can underestimate the true amount of variation in the distributions&#160;being sampled. No finite batch size is sufficiently large to eliminate this problem&#160;entirely, but larger batches reduce the amount of underestimation. When the batch&#160;size is too large, the training procedure becomes infeasibly slow, because many&#160;examples must be processed in order to compute a single small gradient step.</span></p>
<p><span class="font64">As with GANs, it is possible to train a generator net using MMD even if that generator net assigns zero probability to the training points.</span></p><h5><a id="bookmark7"></a><span class="font64" style="font-weight:bold;">20.10.6 Convolutional Generative Networks</span></h5>
<p><span class="font64">When generating images, it is often useful to use a generator network that includes a convolutional structure (see for example Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014c) or Dosovitskiy&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015)). To do so, we use the “transpose” of the convolution operator,&#160;described in Sec. 9.5. This approach often yields more realistic images and does&#160;so using fewer parameters than using fully connected layers without parameter&#160;sharing.</span></p>
<p><span class="font64">Convolutional networks for recognition tasks have information flow from the image to some summarization layer at the top of the network, often a class label.&#160;As this image flows upward through the network, information is discarded as the&#160;representation of the image becomes more invariant to nuisance transformations.&#160;In a generator network, the opposite is true. Rich details must be added as&#160;the representation of the image to be generated propagates through the network,&#160;culminating in the final representation of the image, which is of course the image&#160;itself, in all of its detailed glory, with object positions and poses and textures and&#160;lighting. The primary mechanism for discarding information in a convolutional&#160;recognition network is the pooling layer. The generator network seems to need to&#160;add information. We cannot put the inverse of a pooling layer into the generator&#160;network because most pooling functions are not invertible. A simpler operation is&#160;to merely increase the spatial size of the representation. An approach that seems&#160;to perform acceptably is to use an “un-pooling” as introduced by Dosovitskiy </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span></p>
<p><span class="font64">(2015). This layer corresponds to the inverse of the max-pooling operation under certain simplifying conditions. First, the stride of the max-pooling operation is&#160;constrained to be equal to the width of the pooling region. Second, the maximum&#160;input within each pooling region is assumed to be the input in the upper-left&#160;corner. Finally, all non-maximal inputs within each pooling region are assumed to&#160;be zero. These are very strong and unrealistic assumptions, but they do allow the&#160;max-pooling operator to be inverted. The inverse un-pooling operation allocates&#160;a tensor of zeros, then copies each value from spatial coordinate i of the input&#160;to spatial coordinate i x k of the output. The integer value k defines the size&#160;of the pooling region. Even though the assumptions motivating the definition of&#160;the un-pooling operator are unrealistic, the subsequent layers are able to learn to&#160;compensate for its unusual output, so the samples generated by the model as a&#160;whole are visually pleasing.</span></p><h5><a id="bookmark8"></a><span class="font64" style="font-weight:bold;">20.10.7 Auto-Regressive Networks</span></h5>
<p><span class="font64">Auto-regressive networks are directed probabilistic models with no latent random variables. The conditional probability distributions in these models are represented&#160;by neural networks (sometimes extremely simple neural networks such as logistic&#160;regression). The graph structure of these models is the complete graph. They&#160;decompose a joint probability over the observed variables using the chain rule of&#160;probability to obtain a product of conditionals of the form </span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(x </span><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"> | <sup>a</sup>d<sub>-</sub></span><span class="font18">1</span><span class="font64">,... , x </span><span class="font18">1</span><span class="font64">).&#160;Such models have been called </span><span class="font64" style="font-weight:bold;font-style:italic;">fully-visible Bayes networks</span><span class="font64"> (FVBNs) and used&#160;successfully in many forms, first with logistic regression for each conditional&#160;distribution (Frey, 1998) and then with neural networks with hidden units (Bengio&#160;and Bengio, 2000b; Larochelle and Murray, 2011). In some forms of autoregressive networks, such as NADE (Larochelle and Murray, 2011), described in&#160;Sec. 20.10.10 below, we can introduce a form of parameter sharing that brings both&#160;a statistical advantage (fewer unique parameters) and a computational advantage&#160;(less computation). This is one more instance of the recurring deep learning motif&#160;of </span><span class="font64" style="font-weight:bold;font-style:italic;">reuse of features.</span></p><div><img src="main-197.jpg" alt=""/></div><div><img src="main-198.jpg" alt=""/>
<p><span class="font64">Figure 20.8: A fully visible belief network predicts the i-th variable from the </span><span class="font64" style="font-style:italic;">i — 1 </span><span class="font64">previous ones. </span><span class="font64" style="font-style:italic;">(Top)</span><span class="font64"> The directed graphical model for an FVBN. </span><span class="font64" style="font-style:italic;">(Bottom)</span><span class="font64"> Corresponding&#160;computational graph, in the case of the logistic FVBN, where each prediction is made by&#160;a linear predictor.</span></p></div><h5><a id="bookmark9"></a><span class="font64" style="font-weight:bold;">20.10.8 Linear Auto-Regressive Networks</span></h5>
<p><span class="font64">The simplest form of auto-regressive network has no hidden units and no sharing of parameters or features. Each P (x<sub>i</sub> | x<sub>i-1</sub>,..., x</span><span class="font18">1</span><span class="font64">) is parametrized as a linear&#160;model (linear regression for real-valued data, logistic regression for binary data,&#160;softmax regression for discrete data). This model was introduced by Frey (1998)&#160;and has O(d<sup>2</sup>) parameters when there are d variables to model. It is illustrated in&#160;Fig. 20.8.</span></p>
<p><span class="font64">If the variables are continuous, a linear auto-regressive model is merely another way to formulate a multivariate Gaussian distribution, capturing linear pairwise&#160;interactions between the observed variables.</span></p>
<p><span class="font64">Linear auto-regressive networks are essentially the generalization of linear classification methods to generative modeling. They therefore have the same&#160;advantages and disadvantages as linear classifiers. Like linear classifiers, they may&#160;be trained with convex loss functions, and sometimes admit closed form solutions&#160;(as in the Gaussian case). Like linear classifiers, the model itself does not offer&#160;a way of increasing its capacity, so capacity must be raised using techniques like&#160;basis expansions of the input or the kernel trick.</span></p><div><img src="main-199.jpg" alt=""/>
<p><span class="font64">Figure 20.9: A neural auto-regressive network predicts thei-th variable x<sub>i</sub> from the </span><span class="font64" style="font-weight:bold;font-style:italic;">i — 1 </span><span class="font64">previous ones, but is parametrized so that features (groups of hidden units denotedh<sub>i</sub>)&#160;that are functions of x<sub>1</sub>,..., x<sub>i</sub> can be reused in predicting all of the subsequent variables</span></p>
<p><span class="font64"><sup>x</sup> i+1, <sup>x</sup>i+2 </span><span class="font64" style="font-weight:bold;font-style:italic;">, • • • , <sup>x</sup>d•</span></p></div><h5><a id="bookmark10"></a><span class="font64" style="font-weight:bold;">20.10.9 Neural Auto-Regressive Networks</span></h5>
<p><span class="font64">Neural auto-regressive networks (Bengio and Bengio, 2000a,b) have the same left-to-right graphical model as logistic auto-regressive networks (Fig. 20.8) but&#160;employ a different parametrization of the conditional distributions within that&#160;graphical model structure. The new parametrization is more powerful in the sense&#160;that its capacity can be increased as much as needed, allowing approximation of&#160;any joint distribution. The new parametrization can also improve generalization&#160;by introducing a parameter sharing and feature sharing principle common to deep&#160;learning in general. The models were motivated by the objective of avoiding the&#160;curse of dimensionality arising out of traditional tabular graphical models, sharing&#160;the same structure as Fig. 20.8. In tabular discrete probabilistic models, each&#160;conditional distribution is represented by a table of probabilities, with one entry&#160;and one parameter for each possible configuration of the variables involved. By&#160;using a neural network instead, two advantages are obtained:</span></p>
<p><span class="font64">1. The parametrization of each P</span><span class="font64" style="font-weight:bold;font-style:italic;">(x^</span><span class="font64"> | x<sub>i-</sub></span><span class="font18">1</span><span class="font64">,..., xi) by a neural network with (i — 1) x k inputs and </span><span class="font64" style="font-weight:bold;font-style:italic;">k</span><span class="font64"> outputs (if the variables are discrete and take </span><span class="font64" style="font-weight:bold;font-style:italic;">k&#160;</span><span class="font64">values, encoded one-hot) allows one to estimate the conditional probability&#160;without requiring an exponential number of parameters (and examples), yet&#160;still is able to capture high-order dependencies between the random variables.</span></p>
<p><span class="font64">2. &#160;&#160;&#160;Instead of having a different neural network for the prediction of each x^,&#160;a </span><span class="font64" style="font-weight:bold;font-style:italic;">left-to-right</span><span class="font64"> connectivity illustrated in Fig. 20.9 allows one to merge all&#160;the neural networks into one. Equivalently, it means that the hidden layer&#160;features computed for predicting x<sub>i</sub> can be reused for predicting </span><span class="font64" style="font-weight:bold;font-style:italic;">xi+k</span><span class="font64"> (k &gt; 0).&#160;The hidden units are thus organized in </span><span class="font64" style="font-weight:bold;font-style:italic;">groups</span><span class="font64"> that have the particularity&#160;that all the units in the i-th group only depend on the input values x </span><span class="font18">1</span><span class="font64">,..., x<sub>i</sub>.&#160;The parameters used to compute these hidden units are jointly optimized&#160;to improve the prediction of all the variables in the sequence. This is&#160;an instance of the </span><span class="font64" style="font-weight:bold;font-style:italic;">reuse principle</span><span class="font64"> that recurs throughout deep learning in&#160;scenarios ranging from recurrent and convolutional network architectures to&#160;multi-task and transfer learning.</span></p>
<p><span class="font64">Each P( x<sub>i</sub> | x<sub>i-</sub></span><span class="font18">1</span><span class="font64">,..., x</span><span class="font18">1</span><span class="font64">) can represent a conditional distribution by having outputs of the neural network predict </span><span class="font64" style="font-weight:bold;font-style:italic;">parameters</span><span class="font64"> of the conditional distribution&#160;of x<sub>i</sub>, as discussed in Sec. 6.2.1.1. Although the original neural auto-regressive&#160;networks were initially evaluated in the context of purely discrete multivariate&#160;data (with a sigmoid output for a Bernoulli variable or softmax output for a&#160;multinoulli variable) it is natural to extend such models to continuous variables or&#160;joint distributions involving both discrete and continuous variables.</span></p><h5><a id="bookmark11"></a><span class="font64" style="font-weight:bold;">20.10.10 NADE</span></h5>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">neural autoregressive density estimator</span><span class="font64"> (NADE) is a very successful recent form of neural auto-regressive network (Larochelle and Murray, 2011). The connectivity&#160;is the same as for the original neural auto-regressive network of Bengio and&#160;Bengio (2000b) but NADE introduces an additional parameter sharing scheme, as&#160;illustrated in Fig. 20.10. The parameters of the hidden units of different groups j&#160;are shared.</span></p>
<p><span class="font64">The weights </span><span class="font64" style="font-weight:bold;font-style:italic;">W' <sub>k</sub></span><span class="font64"> • from the i-th input x<sub>i</sub> to the k-th element of the j-th group</span></p>
<p><span class="font18"><sup>3</sup></span><span class="font64"> ,<sup>k</sup>,<sup>i</sup></span></p>
<p><span class="font64">of hidden unit hk<sup>3)</sup> (j &gt; i) are shared among the groups:</span></p>
<p><span class="font64">Wj , k ,i = Wk ,i. &#160;&#160;&#160;(20.83)</span></p>
<p><span class="font64">The remaining weights, where j &lt; i, are zero.</span></p>
<p><span class="font64">Larochelle and Murray (2011) chose this sharing scheme so that forward propagation in a NADE model loosely resembles the computations performed in&#160;mean field inference to fill in missing inputs in an RBM. This mean field inference&#160;corresponds to running a recurrent network with shared weights and the first step&#160;of that inference is the same as in NADE. The only difference is that with NADE,&#160;the output weights connecting the hidden units to the output are parametrized</span></p><div><div><img src="main-200.jpg" alt=""/>
<p><span class="font64">Figure 20.10: An illustration of the neural autoregressive density estimator (NADE). The hidden units are organized in groups h<sup>(j)</sup> so that only the inputs x</span><span class="font18">1</span><span class="font64">,... , x</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>i</sub></span><span class="font64"> participate&#160;in computing h<sup>(i)</sup> and predicting </span><span class="font64" style="font-weight:bold;font-style:italic;">P (xj</span><span class="font64"> | x<sub>j-1</sub>,..., x<sub>1</sub>), for </span><span class="font64" style="font-weight:bold;font-style:italic;">j &gt; i.</span><span class="font64"> NADE is differentiated&#160;from earlier neural auto-regressive networks by the use of a particular weight sharing&#160;pattern: </span><span class="font64" style="font-weight:bold;font-style:italic;">Wj<sub>k</sub></span><span class="font64"><sub> i</sub> = </span><span class="font64" style="font-weight:bold;font-style:italic;">Wk,<sub>i</sub></span><span class="font64"> is shared (indicated in the figure by the use of the same line pattern&#160;for every instance of a replicated weight) for all the weights going out fromx <sub>i</sub> to the k-th&#160;unit of any group j &gt; i. Recall that the vector (W <sub>i</sub>, W<sub>2 i5</sub>. .., </span><span class="font64" style="font-weight:bold;font-style:italic;">W<sub>n</sub></span><span class="font64"><sub> i</sub>) is denoted </span><span class="font64" style="font-weight:bold;font-style:italic;">W<sub>:</sub></span><span class="font64"><sub> i</sub>.</span></p></div></div><div><div><img src="main-201.jpg" alt=""/></div></div>
<p><span class="font64">independently from the weights connecting the input units to the hidden units. In the RBM, the hidden-to-output weights are the transpose of the input-to-hidden&#160;weights. The NADE architecture can be extended to mimic not just one time step&#160;of the mean field recurrent inference but to mimic k steps. This approach is called&#160;NADE-k (Raiko </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014).</span></p>
<p><span class="font64">As mentioned previously, auto-regressive networks may be extend to process continuous-valued data. A particularly powerful and generic way of parametrizing a&#160;continuous density is as a Gaussian mixture (introduced in Sec. 3.9.6) with mixture&#160;weights a (the coefficient or prior probability for component i), per-component&#160;conditional mean </span><span class="font64" style="font-weight:bold;font-style:italic;">^</span><span class="font64"> and per-component conditional variance a?. A model called&#160;RNADE (Uria </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013) uses this parametrization to extend NADE to real&#160;values. As with other mixture density networks, the parameters of this distribution&#160;are outputs of the network, with the mixture weight probabilities produced by a&#160;softmax unit, and the variances parametrized so that they are positive. Stochastic&#160;gradient descent can be numerically ill-behaved due to the interactions between the&#160;conditional means ^ and the conditional variances </span><span class="font64" style="font-weight:bold;font-style:italic;">a<sup>?</sup>.</span><span class="font64"> To reduce this difficulty,&#160;Uria </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013) use a pseudo-gradient that replaces the gradient on the mean, in&#160;the back-propagation phase.</span></p>
<p><span class="font64">Another very interesting extension of the neural auto-regressive architectures gets rid of the need to choose an arbitrary </span><span class="font64" style="font-weight:bold;font-style:italic;">order</span><span class="font64"> for the observed variables (Murray&#160;and Larochelle, 2014). In auto-regressive networks, the idea is to train the network&#160;to be able to cope with any order by randomly sampling orders and providing the&#160;information to hidden units specifying which of the inputs are observed (on the&#160;right side of the conditioning bar) and which are to be predicted and are thus&#160;considered missing (on the left side of the conditioning bar). This is nice because&#160;it allows one to use a trained auto-regressive network to </span><span class="font64" style="font-weight:bold;font-style:italic;">perform any inference&#160;problem</span><span class="font64"> (i.e. predict or sample from the probability distribution over any subset&#160;of variables given any subset) extremely efficiently. Finally, since many orders of&#160;variables are possible (n! for n variables) and each order o of variables yields a&#160;different p</span><span class="font64" style="font-weight:bold;">(x </span><span class="font64">| o</span><span class="font64" style="font-weight:bold;">)</span><span class="font64">, we can form an ensemble of models for many values of </span><span class="font64" style="font-weight:bold;font-style:italic;">o:</span></p>
<p><span class="font64" style="font-weight:bold;">1 k</span></p>
<p><span class="font64">P3nsemble</span><span class="font64" style="font-weight:bold;">(x) = </span><span class="font64">־ &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">p(x</span><span class="font64"> | 0<a id="footnote1"></a><sup><a href="#bookmark12">1</a></sup><sup><a id="footnote1"></a><a href="#bookmark12">1</a></sup></span><span class="font64" style="font-weight:bold;"><sup><a id="footnote1"></a><a href="#bookmark12">1</a></sup></span><span class="font64"><sup></sup></span><span class="font64" style="font-weight:bold;">)</span><span class="font64">.&#160;&#160;&#160;&#160;(20.84)</span></p>
<p><span class="font64" style="font-weight:bold;">i</span><span class="font64">= 1</span></p>
<p><span class="font64">This ensemble model usually generalizes better and assigns higher probability to the test set than does an individual model defined by a single ordering.</span></p>
<p><span class="font64">In the same paper, the authors propose deep versions of the architecture, but unfortunately that immediately makes computation as expensive as in the original&#160;neural auto-regressive neural network (Bengio and Bengio, 2000b). The first layer&#160;and the output layer can still be computed in O(nh) multiply-add operations,&#160;as in the regular NADE, where h is the number of hidden units (the size of the&#160;groups </span><span class="font64" style="font-weight:bold;font-style:italic;">h<sub>i</sub>,</span><span class="font64"> in Fig. 20.10 and Fig. 20.9), whereas it is O(n<sup>2</sup>h) in Bengio and Bengio&#160;(2000b). However, for the other hidden layers, the computation is O(n<sup>2</sup>h<sup>2</sup>) if every&#160;“previous” group at layer l participates in predicting the “next” group at layer l + 1,&#160;assuming n groups of h hidden units at each layer. Making the </span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64">-th group at layer&#160;l + 1 only depend on the i-th group, as in Murray and Larochelle (2014) at layer l&#160;reduces it to </span><span class="font64" style="font-weight:bold;font-style:italic;">O(nh<sup>2</sup>),</span><span class="font64"> which is still h times worse than the regular NADE.</span></p>
<p><a id="bookmark12"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font64"> &#160;&#160;&#160;Starting from the previous state x, inject corruption noise, sampling X from&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">C(X |</span><span class="font64"> x).</span></p>
</body>
</html>