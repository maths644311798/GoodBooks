<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 14</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Autoencoders</span></h2>
<p><span class="font64">An </span><span class="font64" style="font-weight:bold;font-style:italic;">autoencoder</span><span class="font64"> is a neural network that is trained to attempt to copy its input to its output. Internally, it has a hidden layer h that describes a </span><span class="font64" style="font-weight:bold;font-style:italic;">code</span><span class="font64"> used to&#160;represent the input. The network may be viewed as consisting of two parts: an&#160;encoder function h = f (x) and a decoder that produces a reconstruction r = g(h).&#160;This architecture is presented in Fig. 14.1. If an autoencoder succeeds in simply&#160;learning to set </span><span class="font64" style="font-weight:bold;font-style:italic;">g(f</span><span class="font64"> (x)) = </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> everywhere, then it is not especially useful. Instead,&#160;autoencoders are designed to be unable to learn to copy perfectly. Usually they are&#160;restricted in ways that allow them to copy only approximately, and to copy only&#160;input that resembles the training data. Because the model is forced to prioritize&#160;which aspects of the input should be copied, it often learns useful properties of the&#160;data.</span></p>
<p><span class="font64">Modern autoencoders have generalized the idea of an encoder and a decoder beyond deterministic functions to stochastic mappings </span><span class="font64" style="font-weight:bold;font-style:italic;">p<sub>enco</sub></span><span class="font63">d<sub>er</sub> (h | x) and</span></p>
<p><span class="font64"><sup>p</sup>decoder <sup>(x </sup></span><span class="font18"><sup>1</sup></span><span class="font64"><sup> h)</sup>.</span></p>
<p><span class="font64">The idea of autoencoders has been part of the historical landscape of neural networks for decades (LeCun, 1987; Bourlard and Kamp, 1988; Hinton and Zemel,&#160;1994). Traditionally, autoencoders were used for dimensionality reduction or&#160;feature learning. Recently, theoretical connections between autoencoders and&#160;latent variable models have brought autoencoders to the forefront of generative&#160;modeling, as we will see in Chapter 20. Autoencoders may be thought of as being&#160;a special case of feedforward networks, and may be trained with all of the same&#160;techniques, typically minibatch gradient descent following gradients computed&#160;by back-propagation. Unlike general feedforward networks, autoencoders may&#160;also be trained using </span><span class="font64" style="font-weight:bold;font-style:italic;">recirculation</span><span class="font64"> (Hinton and McClelland, 1988), a learning&#160;algorithm based on comparing the activations of the network on the original input&#160;to the activations on the reconstructed input. Recirculation is regarded as more&#160;biologically plausible than back-propagation, but is rarely used for machine learning&#160;applications.</span></p><div><img src="main-147.jpg" alt=""/></div>
<p><span class="font64">Figure 14.1: The general structure of an autoencoder, mapping an input x to an output (called reconstruction) r through an internal representation or code h. The autoencoder&#160;has two components: the encoder f (mapping x to h) and the decoder g (mapping h to</span></p>
<p><span class="font64"><sup>r)</sup>.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">14.1 Undercomplete Autoencoders</span></h4>
<p><span class="font64">Copying the input to the output may sound useless, but we are typically not interested in the output of the decoder. Instead, we hope that training the&#160;autoencoder to perform the input copying task will result in h taking on useful&#160;properties.</span></p>
<p><span class="font64">One way to obtain useful features from the autoencoder is to constrain h to have smaller dimension than x. An autoencoder whose code dimension is less&#160;than the input dimension is called </span><span class="font64" style="font-weight:bold;font-style:italic;">undercomplete.</span><span class="font64"> Learning an undercomplete&#160;representation forces the autoencoder to capture the most salient features of the&#160;training data.</span></p>
<p><span class="font64">The learning process is described simply as minimizing a loss function</span></p>
<p><span class="font64"><sup>L(x,</sup>g<sup>(/(x)))</sup> &#160;&#160;&#160;(</span><span class="font18"><sup>14</sup></span><span class="font64">A)</span></p>
<p><span class="font64">where L is a loss function penalizing g(/(x)) for being dissimilar from x, such as the mean squared error.</span></p>
<p><span class="font64">When the decoder is linear and L is the mean squared error, an undercomplete autoencoder learns to span the same subspace as PCA. In this case, an autoencoder&#160;trained to perform the copying task has learned the principal subspace of the&#160;training data as a side-effect.</span></p>
<p><span class="font64">Autoencoders with nonlinear encoder functions / and nonlinear decoder functions g can thus learn a more powerful nonlinear generalization of PCA. Unfortunately, if the encoder and decoder are allowed too much capacity, the autoencoder can learn to perform the copying task without extracting useful information about&#160;the distribution of the data. Theoretically, one could imagine that an autoencoder&#160;with a one-dimensional code but a very powerful nonlinear encoder could learn to&#160;represent each training example with the code i. The decoder could learn to&#160;map these integer indices back to the values of specific training examples. This&#160;specific scenario does not occur in practice, but it illustrates clearly that an autoencoder trained to perform the copying task can fail to learn anything useful about&#160;the dataset if the capacity of the autoencoder is allowed to become too great.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">14.2 Regularized Autoencoders</span></h4>
<p><span class="font64">Undercomplete autoencoders, with code dimension less than the input dimension, can learn the most salient features of the data distribution. We have seen that&#160;these autoencoders fail to learn anything useful if the encoder and decoder are&#160;given too much capacity.</span></p>
<p><span class="font64">A similar problem occurs if the hidden code is allowed to have dimension equal to the input, and in the </span><span class="font64" style="font-weight:bold;font-style:italic;">overcomplete</span><span class="font64"> case in which the hidden code has dimension&#160;greater than the input. In these cases, even a linear encoder and linear decoder&#160;can learn to copy the input to the output without learning anything useful about&#160;the data distribution.</span></p>
<p><span class="font64">Ideally, one could train any architecture of autoencoder successfully, choosing the code dimension and the capacity of the encoder and decoder based on the&#160;complexity of distribution to be modeled. Regularized autoencoders provide the&#160;ability to do so. Rather than limiting the model capacity by keeping the encoder&#160;and decoder shallow and the code size small, regularized autoencoders use a loss&#160;function that encourages the model to have other properties besides the ability&#160;to copy its input to its output. These other properties include sparsity of the&#160;representation, smallness of the derivative of the representation, and robustness&#160;to noise or to missing inputs. A regularized autoencoder can be nonlinear and&#160;overcomplete but still learn something useful about the data distribution even if&#160;the model capacity is great enough to learn a trivial identity function.</span></p>
<p><span class="font64">In addition to the methods described here which are most naturally interpreted as regularized autoencoders, nearly any generative model with latent variables&#160;and equipped with an inference procedure (for computing latent representations&#160;given input) may be viewed as a particular form of autoencoder. Two generative&#160;modeling approaches that emphasize this connection with autoencoders are the&#160;descendants of the Helmholtz machine (Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1995b), such as the variational&#160;autoencoder (Sec. 20.10.3) and the generative stochastic networks (Sec. 20.12).&#160;These models naturally learn high-capacity, overcomplete encodings of the input&#160;and do not require regularization for these encodings to be useful. Their encodings&#160;are naturally useful because the models were trained to approximately maximize&#160;the probability of the training data rather than to copy the input to the output.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">14.2.1 Sparse Autoencoders</span></h5>
<p><span class="font64">A sparse autoencoder is simply an autoencoder whose training criterion involves a sparsity penalty </span><span class="font64" style="font-weight:bold;font-style:italic;">Q(h)</span><span class="font64"> on the code layer h, in addition to the reconstruction error:</span></p>
<p><span class="font64">L(x,g(f (x))) + ft(h) &#160;&#160;&#160;(14.2)</span></p>
<p><span class="font64">where g (h) is the decoder output and typically we have h = f (x), the encoder output.</span></p>
<p><span class="font64">Sparse autoencoders are typically used to learn features for another task such as classification. An autoencoder that has been regularized to be sparse must&#160;respond to unique statistical features of the dataset it has been trained on, rather&#160;than simply acting as an identity function. In this way, training to perform the&#160;copying task with a sparsity penalty can yield a model that has learned useful&#160;features as a byproduct.</span></p>
<p><span class="font64">We can think of the penalty Q(h) simply as a regularizer term added to a feedforward network whose primary task is to copy the input to the output&#160;(unsupervised learning objective) and possibly also perform some supervised task&#160;(with a supervised learning objective) that depends on these sparse features.&#160;Unlike other regularizers such as weight decay, there is not a straightforward&#160;Bayesian interpretation to this regularizer. As described in Sec. 5.6.1, training&#160;with weight decay and other regularization penalties can be interpreted as a&#160;MAP approximation to Bayesian inference, with the added regularizing penalty&#160;corresponding to a prior probability distribution over the model parameters. In&#160;this view, regularized maximum likelihood corresponds to maximizing </span><span class="font64" style="font-weight:bold;font-style:italic;">p (6</span><span class="font64"> | x),&#160;which is equivalent to maximizing logp(x | </span><span class="font18">6</span><span class="font64">) + logp(</span><span class="font18">6</span><span class="font64">). The logp (x | </span><span class="font18">6</span><span class="font64">) term&#160;is the usual data log-likelihood term and the logp(</span><span class="font18">6</span><span class="font64">) term, the log-prior over&#160;parameters, incorporates the preference over particular values of </span><span class="font18">6</span><span class="font64">. This view&#160;was described in Sec. 5.6. Regularized autoencoders defy such an interpretation&#160;because the regularizer depends on the data and is therefore by definition not a&#160;prior in the formal sense of the word. We can still think of these regularization&#160;terms as implicitly expressing a preference over functions.</span></p>
<p><span class="font64">Rather than thinking of the sparsity penalty as a regularizer for the copying task, we can think of the entire sparse autoencoder framework as approximating&#160;maximum likelihood training of a generative model that has latent variables.&#160;Suppose we have a model with visible variables </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">and latent variables </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">, with&#160;an explicit joint distribution p<sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) = </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"><sub>mo</sub>d<sub>e</sub>l(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64">)p<sub>mo</sub>d<sub>e</sub></span><span class="font18">1</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">). We refer to&#160;p<sub>mo</sub>d<sub>e</sub>i(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) as the model’s prior distribution over the latent variables, representing&#160;the model’s beliefs prior to seeing </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">. This is different from the way we have&#160;previously used the word “prior,” to refer to the distribution p(</span><span class="font64" style="font-weight:bold;">6</span><span class="font64" style="font-weight:bold;font-style:italic;">)</span><span class="font64"> encoding our&#160;beliefs about the model’s parameters before we have seen the training data. The&#160;log-likelihood can be decomposed as</span></p><div>
<p><span class="font64">(14.3)</span></p></div>
<p><span class="font64" style="font-weight:bold;">log</span><span class="font63">Pmodel</span><span class="font64" style="font-weight:bold;">(x) = !0g2J </span><span class="font63">Pmodel </span><span class="font64" style="font-weight:bold;">(h</span><span class="font63">, </span><span class="font64" style="font-weight:bold;">x)</span><span class="font63">.</span></p>
<p><span class="font63">h</span></p>
<p><span class="font64">We can think of the autoencoder as approximating this sum with a point estimate for just one highly likely value for </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">. This is similar to the sparse coding generative&#160;model (Sec. 13.4), but with </span><span class="font64" style="font-weight:bold;">h </span><span class="font64">being the output of the parametric encoder rather&#160;than the result of an optimization that infers the most likely </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">. From this point of&#160;view, with this chosen </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">, we are maximizing</span></p>
<p><span class="font18"><sup>10</sup></span><span class="font64">gP</span><span class="font64" style="font-weight:bold;">model</span><span class="font64"><sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>h</sup></span><span class="font64">, </span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>)</sup> = </span><span class="font18"><sup>10</sup></span><span class="font64">gP</span><span class="font64" style="font-weight:bold;">model</span><span class="font64"><sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>h</sup></span><span class="font64"><sup>)</sup> + </span><span class="font18"><sup>10</sup></span><span class="font64">gP</span><span class="font64" style="font-weight:bold;">model </span><span class="font64"><sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>x 1 h</sup></span><span class="font64"><sup>)</sup>• &#160;&#160;&#160;<sup>(14</sup>.<sup>4)</sup></span></p>
<p><span class="font64">The logp</span><span class="font64" style="font-weight:bold;"><sub>mo</sub>d</span><span class="font19"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) term can be sparsity-inducing. For example, the Laplace prior,</span></p><div>
<p><span class="font64" style="font-weight:bold;">Pmodel(hi) = <sup>A</sup>e <sup>A|־׳i 1</sup>.</span></p></div>
<p><span class="font64">(14.5)</span></p>
<p><span class="font64">corresponds to an absolute value sparsity penalty. Expressing the log-prior as an absolute value penalty, we obtain</span></p><div>
<p><span class="font64">(14.6)</span></p>
<p><span class="font64">(14.7)</span></p></div>
<p><span class="font64">fi(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) = &#160;&#160;&#160;|</span><span class="font64" style="font-weight:bold;">hi </span><span class="font64">|</span></p>
<p><span class="font64" style="font-weight:bold;">i</span></p>
<p><span class="font64">log </span><span class="font64" style="font-weight:bold;">Pmodel </span><span class="font64">(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) = </span><span class="font64" style="font-weight:bold;font-style:italic;">^2</span><span class="font64"> (</span><span class="font64" style="font-weight:bold;">A</span><span class="font64">|</span><span class="font64" style="font-weight:bold;">h i </span><span class="font64">| - log ^ = 0(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) + const</span></p>
<p><span class="font64">where the constant term depends only on A and not </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">. We typically treat A as a hyperparameter and discard the constant term since it does not affect the parameter&#160;learning. Other priors such as the Student-t prior can also induce sparsity. From&#160;this point of view of sparsity as resulting from the effect of P</span><span class="font64" style="font-weight:bold;"><sub>mo</sub>d<sub>e</sub>l</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) on approximate&#160;maximum likelihood learning, the sparsity penalty is not a regularization term at&#160;all. It is just a consequence of the model’s distribution over its latent variables.&#160;This view provides a different motivation for training an autoencoder: it is a way&#160;of approximately training a generative model. It also provides a different reason for&#160;why the features learned by the autoencoder are useful: they describe the latent&#160;variables that explain the input.</span></p>
<p><span class="font64">Early work on sparse autoencoders (Ranzato </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2007a, 2008) explored various forms of sparsity and proposed a connection between the sparsity penalty&#160;and the log </span><span class="font64" style="font-weight:bold;">Z </span><span class="font64">term that arises when applying maximum likelihood to an undirected&#160;probabilistic model </span><span class="font64" style="font-weight:bold;">p</span><span class="font64">(x) </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">Zp</span><span class="font64">(x). The idea is that minimizing log</span><span class="font64" style="font-weight:bold;">Z </span><span class="font64">prevents a&#160;probabilistic model from having high probability everywhere, and imposing sparsity&#160;on an autoencoder prevents the autoencoder from having low reconstruction&#160;error everywhere. In this case, the connection is on the level of an intuitive&#160;understanding of a general mechanism rather than a mathematical correspondence.&#160;The interpretation of the sparsity penalty as corresponding to log</span><span class="font64" style="font-weight:bold;">p</span><span class="font64"><sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (h</span><span class="font64" style="font-weight:bold;font-style:italic;">)</span><span class="font64"> in a&#160;directed model </span><span class="font64" style="font-weight:bold;">p</span><span class="font64"><sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (h)</span><span class="font64" style="font-weight:bold;">p</span><span class="font64"><sub>mo</sub>d<sub>e</sub></span><span class="font18">1</span><span class="font64">(x | h) is more mathematically straightforward.</span></p>
<p><span class="font64">One way to achieve </span><span class="font64" style="font-weight:bold;font-style:italic;">actual zeros</span><span class="font64"> in h for sparse (and denoising) autoencoders was introduced in Glorot </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011b). The idea is to use rectified linear units to&#160;produce the code layer. With a prior that actually pushes the representations to&#160;zero (like the absolute value penalty), one can thus indirectly control the average&#160;number of zeros in the representation.</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">14.2.2 Denoising Autoencoders</span></h5>
<p><span class="font64">Rather than adding a penalty Q to the cost function, we can obtain an autoencoder that learns something useful by changing the reconstruction error term of the cost&#160;function.</span></p>
<p><span class="font64">Traditionally, autoencoders minimize some function</span></p>
<p><span class="font64" style="font-weight:bold;"><sup>L</sup></span><span class="font64"><sup>(x</sup></span><span class="font64" style="font-weight:bold;"><sup>,</sup>g</span><span class="font64"><sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>f </sup></span><span class="font64"><sup>(x))) &#160;&#160;&#160;(</sup></span><span class="font18"><sup>14</sup></span><span class="font64">.</span><span class="font18">8</span><span class="font64">)</span></p>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;">L </span><span class="font64">is a loss function penalizing </span><span class="font64" style="font-weight:bold;font-style:italic;">g(f (x))</span><span class="font64"> for being dissimilar from x, such as the </span><span class="font64" style="font-weight:bold;">L</span><span class="font18"><sup>2</sup></span><span class="font64"> norm of their difference. This encourages </span><span class="font64" style="font-weight:bold;">g </span><span class="font64">o </span><span class="font64" style="font-weight:bold;">f </span><span class="font64">to learn to be merely an&#160;identity function if they have the capacity to do so.</span></p>
<p><span class="font64">A </span><span class="font64" style="font-weight:bold;font-style:italic;">denoising autoencoder</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">DAE</span><span class="font64"> instead minimizes</span></p>
<p><span class="font64" style="font-weight:bold;"><sup>L</sup></span><span class="font64"><sup>(x</sup></span><span class="font64" style="font-weight:bold;"><sup>,</sup>g</span><span class="font64"><sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>f</sup></span><span class="font64"><sup>(x)))</sup></span><span class="font64" style="font-weight:bold;">, &#160;&#160;&#160;</span><span class="font64"><sup>(14</sup>.<sup>9)</sup></span></p>
<p><span class="font64">where x is a copy of x that has been corrupted by some form of noise. Denoising autoencoders must therefore undo this corruption rather than simply copying their&#160;input.</span></p>
<p><span class="font64">Denoising training forces </span><span class="font64" style="font-weight:bold;">f </span><span class="font64">and </span><span class="font64" style="font-weight:bold;">g </span><span class="font64">to implicitly learn the structure of </span><span class="font64" style="font-weight:bold;">p</span><span class="font64">d<sub>ata</sub> (x), as shown by Alain and Bengio (2013) and Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013c). Denoising&#160;autoencoders thus provide yet another example of how useful properties can emerge&#160;as a byproduct of minimizing reconstruction error. They are also an example of&#160;how overcomplete, high-capacity models may be used as autoencoders so long&#160;as care is taken to prevent them from learning the identity function. Denoising&#160;autoencoders are presented in more detail in Sec. 14.5.</span></p><h5><a id="bookmark5"></a><span class="font64" style="font-weight:bold;">14.2.3 Regularizing by Penalizing Derivatives</span></h5>
<p><span class="font64">Another strategy for regularizing an autoencoder is to use a penalty Q as in sparse autoencoders,</span></p>
<p><span class="font64">L(x,g(f (x))) + Q(h, x), &#160;&#160;&#160;(14.10)</span></p>
<p><span class="font64">but with a different form of Q:</span></p><div>
<p><span class="font64">(14.11)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Q(h,x) =</span><span class="font64" style="font-weight:bold;"> A^ ||V*hi||<sup>2</sup>.</span></p>
<p><span class="font64">This forces the model to learn a function that does not change much when x changes slightly. Because this penalty is applied only at training examples, it forces&#160;the autoencoder to learn features that capture information about the training&#160;distribution.</span></p>
<p><span class="font64">An autoencoder regularized in this way is called a </span><span class="font64" style="font-weight:bold;font-style:italic;">contractive autoencoder </span><span class="font64">or </span><span class="font64" style="font-weight:bold;font-style:italic;">CAE.</span><span class="font64"> This approach has theoretical connections to denoising autoencoders,&#160;manifold learning and probabilistic modeling. The CAE is described in more detail&#160;in Sec. 14.7.</span></p><h4><a id="bookmark6"></a><span class="font65" style="font-weight:bold;">14.3 Representational Power, Layer Size and Depth</span></h4>
<p><span class="font64">Autoencoders are often trained with only a single layer encoder and a single layer decoder. However, this is not a requirement. In fact, using deep encoders and&#160;decoders offers many advantages.</span></p>
<p><span class="font64">Recall from Sec. 6.4.1 that there are many advantages to depth in a feedforward network. Because autoencoders are feedforward networks, these advantages also&#160;apply to autoencoders. Moreover, the encoder is itself a feedforward network as&#160;is the decoder, so each of these components of the autoencoder can individually&#160;benefit from depth.</span></p>
<p><span class="font64">One major advantage of non-trivial depth is that the universal approximator theorem guarantees that a feedforward neural network with at least one hidden&#160;layer can represent an approximation of any function (within a broad class) to an&#160;arbitrary degree of accuracy, provided that it has enough hidden units. This means&#160;that an autoencoder with a single hidden layer is able to represent the identity&#160;function along the domain of the data arbitrarily well. However, the mapping from&#160;input to code is shallow. This means that we are not able to enforce arbitrary&#160;constraints, such as that the code should be sparse. A deep autoencoder, with at&#160;least one additional hidden layer inside the encoder itself, can approximate any&#160;mapping from input to code arbitrarily well, given enough hidden units.</span></p>
<p><span class="font64">Depth can exponentially reduce the computational cost of representing some functions. Depth can also exponentially decrease the amount of training data&#160;needed to learn some functions. See Sec. 6.4.1 for a review of the advantages of&#160;depth in feedforward networks.</span></p>
<p><span class="font64">Experimentally, deep autoencoders yield much better compression than corresponding shallow or linear autoencoders (Hinton and Salakhutdinov, 2006).</span></p>
<p><span class="font64">A common strategy for training a deep autoencoder is to greedily pretrain the deep architecture by training a stack of shallow autoencoders, so we often&#160;encounter shallow autoencoders, even when the ultimate goal is to train a deep&#160;autoencoder.</span></p><h4><a id="bookmark7"></a><span class="font65" style="font-weight:bold;">14.4 Stochastic Encoders and Decoders</span></h4>
<p><span class="font64">Autoencoders are just feedforward networks. The same loss functions and output unit types that can be used for traditional feedforward networks are also used for&#160;autoencoders.</span></p>
<p><span class="font64">As described in Sec. 6.2.2.4, a general strategy for designing the output units and the loss function of a feedforward network is to define an output distribution&#160;p(y </span><span class="font64" style="font-weight:bold;font-style:italic;">|</span><span class="font64"> x) and minimize the negative log-likelihood — log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(y | x).</span><span class="font64"> In that setting, </span><span class="font64" style="font-weight:bold;font-style:italic;">y&#160;</span><span class="font64">was a vector of targets, such as class labels.</span></p>
<p><span class="font64">In the case of an autoencoder, x is now the target as well as the input. However, we can still apply the same machinery as before. Given a hidden code h, we may&#160;think of the decoder as providing a conditional distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> d<sub>eco</sub>d<sub>er</sub>(x | h). We&#160;may then train the autoencoder by minimizing — log pd<sub>eco</sub>d<sub>er</sub>(x | h). The exact&#160;form of this loss function will change depending on the form of Pd<sub>eco</sub>d<sub>er</sub>. As with&#160;traditional feedforward networks, we usually use linear output units to parametrize&#160;the mean of a Gaussian distribution if x is real-valued. In that case, the negative&#160;log-likelihood yields a mean squared error criterion. Similarly, binary x values&#160;correspond to a Bernoulli distribution whose parameters are given by a sigmoid&#160;output unit, discrete x values correspond to a softmax distribution, and so on.</span></p>
<p><span class="font64">Typically, the output variables are treated as being conditionally independent given </span><span class="font65" style="font-weight:bold;">h </span><span class="font64">so that this probability distribution is inexpensive to evaluate, but some&#160;techniques such as mixture density outputs allow tractable modeling of outputs&#160;with correlations.</span></p><div><img src="main-148.jpg" alt=""/>
<p><span class="font64">Figure 14.2: The structure of a stochastic autoencoder, in which both the encoder and the decoder are not simple functions but instead involve some noise injection, meaning that&#160;their output can be seen as sampled from a distribution, p<sub>enco</sub>d<sub>er</sub> (h | x) for the encoder&#160;and pd<sub>eco</sub>d<sub>er</sub> (x | h) for the decoder.</span></p></div>
<p><span class="font64">To make a more radical departure from the feedforward networks we have seen previously, we can also generalize the notion of an </span><span class="font64" style="font-weight:bold;font-style:italic;">encoding function f</span><span class="font64"> (x) to an&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">encoding distribution p<sub>enco</sub>d<sub>er</sub>(h</span><span class="font64"> | x), as illustrated in Fig. 14.2.</span></p>
<p><span class="font64">Any latent variable model p<sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (h, x) defines a stochastic encoder</span></p>
<p><span class="font63">Pencoder<sup>(h 1 x)</sup> &#160;&#160;&#160;Pmodel<sup>(h 1 x)</sup>&#160;&#160;&#160;&#160;(<sup>14</sup>.<sup>12</sup>)</span></p>
<p><span class="font64">and a stochastic decoder</span></p>
<p><span class="font63"><sup>p</sup>decoder<sup>(x 1 h) &#160;&#160;&#160;p</sup>mode1<sup>(x 1 h)</sup> •&#160;&#160;&#160;&#160;<sup>(14</sup>.<sup>13)</sup></span></p>
<p><span class="font64">In general, the encoder and decoder distributions are not necessarily conditional distributions compatible with a unique joint distribution p<sub>mo</sub>d<sub>e</sub>1(x, </span><span class="font64" style="font-weight:bold;font-style:italic;">h).</span><span class="font64"> Alain </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(2015) showed that training the encoder and decoder as a denoising autoencoder&#160;will tend to make them compatible asymptotically (with enough capacity and&#160;examples).</span></p><h4><a id="bookmark8"></a><span class="font65" style="font-weight:bold;">14.5 Denoising Autoencoders</span></h4>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">denoising autoencoder</span><span class="font64"> (DAE) is an autoencoder that receives a corrupted data point as input and is trained to predict the original, uncorrupted data point as its&#160;output.</span></p>
<p><span class="font64">The DAE training procedure is illustrated in Fig. 14.3. We introduce a corruption process C(X | x) which represents a conditional distribution over</span></p><div><img src="main-149.jpg" alt=""/>
<p><span class="font64">Figure 14.3: The computational graph of the cost function for a denoising autoencoder, which is trained to reconstruct the clean data point x from its corrupted version </span><span class="font64" style="font-style:italic;">x.&#160;</span><span class="font64">This is accomplished by minimizing the loss </span><span class="font64" style="font-style:italic;">L = —</span><span class="font64"> logpd<sub>eco</sub>d<sub>e</sub>r (x | </span><span class="font64" style="font-style:italic;">h = f</span><span class="font64"> (X)), where&#160;X is a corrupted version of the data example x, obtained through a given corruption&#160;process </span><span class="font64" style="font-style:italic;">C(x</span><span class="font64"> | x). Typically the distribution Pd<sub>eco</sub>d<sub>er</sub> is a factorial distribution whose mean&#160;parameters are emitted by a feedforward network g.</span></p></div>
<p><span class="font64">corrupted samples </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">, given a data sample </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">. The autoencoder then learns a </span><span class="font64" style="font-style:italic;">reconstruction distribution</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">p<sub>reco</sub>nstru<sub>c</sub>t </span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) estimated from training pairs (</span><span class="font64" style="font-weight:bold;">x, x</span><span class="font64">),&#160;as follows:</span></p>
<p><span class="font64">1. &#160;&#160;&#160;Sample a training example </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">from the training data.</span></p>
<p><span class="font64">2. &#160;&#160;&#160;Sample a corrupted version </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">from C(</span><span class="font64" style="font-weight:bold;">x </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">).</span></p>
<p><span class="font64">3. &#160;&#160;&#160;Use (</span><span class="font64" style="font-weight:bold;">x, x</span><span class="font64">) as a training example for estimating the autoencoder reconstruction&#160;distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64" style="font-weight:bold;"><sub>reconstruct</sub></span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) = </span><span class="font64" style="font-weight:bold;">Pd<sub>eco</sub>d<sub>er</sub> </span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) with </span><span class="font64" style="font-weight:bold;">h </span><span class="font64">the output of encoder&#160;</span><span class="font64" style="font-weight:bold;">f </span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) and </span><span class="font64" style="font-weight:bold;">Pd<sub>eco</sub>d<sub>er</sub> </span><span class="font64">typically defined by a decoder </span><span class="font64" style="font-weight:bold;font-style:italic;">g</span><span class="font64" style="font-style:italic;">(h).</span></p>
<p><span class="font64">Typically we can simply perform gradient-based approximate minimization (such as minibatch gradient descent) on the negative log-likelihood — log</span><span class="font64" style="font-weight:bold;">pd<sub>eco</sub>d<sub>er</sub>(x </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">h)</span><span class="font64">.&#160;So long as the encoder is deterministic, the denoising autoencoder is a feedforward&#160;network and may be trained with exactly the same techniques as any other&#160;feedforward network.</span></p>
<p><span class="font64">We can therefore view the DAE as performing stochastic gradient descent on the following expectation:</span></p>
<p><span class="font64" style="font-weight:bold;"><sup>E</sup>x~p)<sub>data</sub> ^)®x^C(x|x) </span><span class="font64"><sup>1og</sup></span><span class="font64" style="font-weight:bold;"><sup>p</sup>decoder </span><span class="font64"><sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>x 1</sup></span><span class="font64"><sup> </sup></span><span class="font64" style="font-weight:bold;"><sup>h &#160;&#160;&#160;f </sup></span><span class="font64"><sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>))</sup>&#160;&#160;&#160;&#160;(<sup>1</sup>4</span><span class="font64" style="font-weight:bold;">T</span><span class="font64">4)</span></p>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;">pd<sub>ata</sub> </span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) is the training distribution.</span></p><div><div><img src="main-150.jpg" alt=""/>
<p><span class="font64">Figure 14.4: A denoising autoencoder is trained to map a corrupted data point x back to the original data point x. We illustrate training examples x as red crosses lying near a&#160;low-dimensional manifold illustrated with the bold black line. We illustrate the corruption&#160;process C (</span><span class="font64" style="font-style:italic;">X</span><span class="font64"> | x) with a gray circle of equiprobable corruptions. A gray arrow demonstrates&#160;how one training example is transformed into one sample from this corruption process.&#160;When the denoising autoencoder is trained to minimize the average of squared errors&#160;</span><span class="font64" style="font-style:italic;">\\g(f</span><span class="font64"> (x)) -x||<sup>2</sup>, the reconstruction</span><span class="font64" style="font-style:italic;">g</span><span class="font64">(f (x)) estimates E</span><span class="font35" style="font-weight:bold;">xx</span><span class="font64">-p<sub>data</sub> (</span><span class="font35" style="font-weight:bold;">x</span><span class="font64">)C(</span><span class="font35" style="font-weight:bold;">X</span><span class="font64">|</span><span class="font35" style="font-weight:bold;">x</span><span class="font64">)[x I x]. The vector&#160;g(f(x)) — </span><span class="font64" style="font-style:italic;">x</span><span class="font64"> points approximately towards the nearest point on the manifold, sinceg(f(x))&#160;estimates the center of mass of the clean points x which could have given rise to x. The&#160;autoencoder thus learns a vector field </span><span class="font64" style="font-style:italic;">g(f</span><span class="font64"> (x)) — x indicated by the green arrows. This&#160;vector field estimates the score V <sub>x</sub>log pd<sub>ata</sub> (x) up to a multiplicative factor that is the&#160;average root mean square reconstruction error.</span></p></div></div><h5><a id="bookmark9"></a><span class="font64" style="font-weight:bold;">14.5.1 Estimating the Score</span></h5>
<p><span class="font64">Score matching (Hyvarinen, 2005) is an alternative to maximum likelihood. It provides a consistent estimator of probability distributions based on encouraging&#160;the model to have the same </span><span class="font64" style="font-weight:bold;font-style:italic;">score</span><span class="font64"> as the data distribution at every training point&#160;x. In this context, the score is a particular gradient field:</span></p>
<p><span class="font64">Vx log p(x). &#160;&#160;&#160;(14.15)</span></p>
<p><span class="font64">Score matching is discussed further in Sec. 18.4. For the present discussion regarding autoencoders, it is sufficient to understand that learning the gradient&#160;field of log Pd<sub>ata</sub> is one way to learn the structure of pd<sub>ata</sub> itself.</span></p>
<p><span class="font64">A very important property of DAEs is that their training criterion (with conditionally Gaussian p( </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> | h)) makes the autoencoder learn a vector field&#160;(g(f(x)) — x) that estimates the score of the data distribution. This is illustrated&#160;in Fig. 14.4.</span></p>
<p><span class="font64">Denoising training of a specific kind of autoencoder (sigmoidal hidden units, linear reconstruction units) using Gaussian noise and mean squared error as the&#160;reconstruction cost is equivalent (Vincent, 2011) to training a specific kind of&#160;undirected probabilistic model called an RBM with Gaussian visible units. This&#160;kind of model will be described in detail in Sec. 20.5.1; for the present discussion&#160;it suffices to know that it is a model that provides an explicit p<sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (x; </span><span class="font18">6</span><span class="font64">). When&#160;the RBM is trained using </span><span class="font64" style="font-weight:bold;font-style:italic;">denoising score matching</span><span class="font64"> (Kingma and LeCun, 2010),&#160;its learning algorithm is equivalent to denoising training in the corresponding&#160;autoencoder. With a fixed noise level, regularized score matching is not a consistent&#160;estimator; it instead recovers a blurred version of the distribution. However, if&#160;the noise level is chosen to approach </span><span class="font18">0</span><span class="font64"> when the number of examples approaches&#160;infinity, then consistency is recovered. Denoising score matching is discussed in&#160;more detail in Sec. 18.5.</span></p>
<p><span class="font64">Other connections between autoencoders and RBMs exist. Score matching applied to RBMs yields a cost function that is identical to reconstruction error&#160;combined with a regularization term similar to the contractive penalty of the&#160;CAE (Swersky </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011). Bengio and Delalleau (2009) showed that an autoencoder gradient provides an approximation to contrastive divergence training of&#160;RBMs.</span></p>
<p><span class="font64">For continuous-valued x, the denoising criterion with Gaussian corruption and reconstruction distribution yields an estimator of the score that is applicable to&#160;general encoder and decoder parametrizations (Alain and Bengio, 2013). This&#160;means a generic encoder-decoder architecture may be made to estimate the score</span></p>
<p><span class="font64">by training with the squared error criterion</span></p>
<p><span class="font64">\\g<sup>(f (x)) - </sup></span><span class="font18"><sup>x</sup>\\<sup>2</sup></span><span class="font64"> &#160;&#160;&#160;(<sup>14</sup>•<sup>16</sup>)</span></p>
<p><span class="font64">and corruption</span></p>
<p><span class="font64">C(x </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> x\x) </span><span class="font64" style="font-weight:bold;font-style:italic;">= N(x; ^ =</span><span class="font64"> x, E = a</span><span class="font18"><sup>2</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">1)</span><span class="font64"> &#160;&#160;&#160;(14.17)</span></p>
<p><span class="font64">with noise variance a<sup>2</sup>. See Fig. 14.5 for an illustration of how this works.</span></p><div><img src="main-151.jpg" alt=""/>
<p><span class="font64">Figure 14.5: Vector field learned by a denoising autoencoder around a 1-D curved manifold near which the data concentrates in a 2-D space. Each arrow is proportional to the&#160;reconstruction minus input vector of the autoencoder and points towards higher probability&#160;according to the implicitly estimated probability distribution. The vector field has zeros&#160;at both maxima of the estimated density function (on the data manifolds) and at minima&#160;of that density function. For example, the spiral arm forms a one-dimensional manifold of&#160;local maxima that are connected to each other. Local minima appear near the middle of&#160;the gap between two arms. When the norm of reconstruction error (shown by the length&#160;of the arrows) is large, it means that probability can be significantly increased by moving&#160;in the direction of the arrow, and that is mostly the case in places of low probability.&#160;The autoencoder maps these low probability points to higher probability reconstructions.&#160;Where probability is maximal, the arrows shrink because the reconstruction becomes more&#160;accurate.</span></p></div>
<p><span class="font64">In general, there is no guarantee that the reconstruction g(f (x)) minus the input x corresponds to the gradient of any function, let alone to the score. That is</span></p>
<p><span class="font64">why the early results (Vincent, 2011) are specialized to particular parametrizations where g (f (x)) — x may be obtained by taking the derivative of another function.&#160;Kamyshanska and Memisevic (2015) generalized the results of Vincent (2011) by&#160;identifying a family of shallow autoencoders such that g(f (x)) — x corresponds to&#160;a score for all members of the family.</span></p>
<p><span class="font64">So far we have described only how the denoising autoencoder learns to represent a probability distribution. More generally, one may want to use the autoencoder as&#160;a generative model and draw samples from this distribution. This will be described&#160;later, in Sec. 20.11.</span></p>
<p><span class="font64" style="font-weight:bold;">14.5.1.1 Historical Perspective</span></p>
<p><span class="font64">The idea of using MLPs for denoising dates back to the work of LeCun (1987) and Gallinari </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (1987). Behnke (2001) also used recurrent networks to denoise&#160;images. Denoising autoencoders are, in some sense, just MLPs trained to denoise.&#160;However, the name “denoising autoencoder” refers to a model that is intended not&#160;merely to learn to denoise its input but to learn a good internal representation&#160;as a side effect of learning to denoise. This idea came much later (Vincent&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2008, 2010). The learned representation may then be used to pretrain a&#160;deeper unsupervised network or a supervised network. Like sparse autoencoders,&#160;sparse coding, contractive autoencoders and other regularized autoencoders, the&#160;motivation for DAEs was to allow the learning of a very high-capacity encoder&#160;while preventing the encoder and decoder from learning a useless identity function.</span></p>
<p><span class="font64">Prior to the introduction of the modern DAE, Inayoshi and Kurita (2005) explored some of the same goals with some of the same methods. Their approach&#160;minimizes reconstruction error in addition to a supervised objective while injecting&#160;noise in the hidden layer of a supervised MLP, with the objective to improve&#160;generalization by introducing the reconstruction error and the injected noise.&#160;However, their method was based on a linear encoder and could not learn function&#160;families as powerful as can the modern DAE.</span></p><h4><a id="bookmark10"></a><span class="font65" style="font-weight:bold;">14.6 Learning Manifolds with Autoencoders</span></h4>
<p><span class="font64">Like many other machine learning algorithms, autoencoders exploit the idea that data concentrates around a low-dimensional manifold or a small set of such&#160;manifolds, as described in Sec. 5.11.3. Some machine learning algorithms exploit&#160;this idea only insofar as that they learn a function that behaves correctly on the&#160;manifold but may have unusual behavior if given an input that is off the manifold.</span></p>
<p><span class="font64">Autoencoders take this idea further and aim to learn the structure of the manifold.</span></p>
<p><span class="font64">To understand how autoencoders do this, we must present some important characteristics of manifolds.</span></p>
<p><span class="font64">An important characterization of a manifold is the set of its </span><span class="font64" style="font-weight:bold;font-style:italic;">tangent planes.</span><span class="font64"> At a point x on a d-dimensional manifold, the tangent plane is given by d basis vectors&#160;that span the local directions of variation allowed on the manifold. As illustrated&#160;in Fig. 14.6, these local directions specify how one can change x infinitesimally&#160;while staying on the manifold.</span></p>
<p><span class="font64">All autoencoder training procedures involve a compromise between two forces:</span></p>
<p><span class="font64">1. &#160;&#160;&#160;Learning a representation h of a training example x such that x can be&#160;approximately recovered from h through a decoder. The fact that x is drawn&#160;from the training data is crucial, because it means the autoencoder need&#160;not successfully reconstruct inputs that are not probable under the data&#160;generating distribution.</span></p>
<p><span class="font64">2. &#160;&#160;&#160;Satisfying the constraint or regularization penalty. This can be an architectural constraint that limits the capacity of the autoencoder, or it can be&#160;a regularization term added to the reconstruction cost. These techniques&#160;generally prefer solutions that are less sensitive to the input.</span></p>
<p><span class="font64">Clearly, neither force alone would be useful—copying the input to the output is not useful on its own, nor is ignoring the input. Instead, the two forces together&#160;are useful because they force the hidden representation to capture information&#160;about the structure of the data generating distribution. The important principle&#160;is that the autoencoder can afford to represent </span><span class="font64" style="font-weight:bold;font-style:italic;">only the variations that are needed&#160;to reconstruct training examples.</span><span class="font64"> If the data generating distribution concentrates&#160;near a low-dimensional manifold, this yields representations that implicitly capture&#160;a local coordinate system for this manifold: only the variations tangent to the&#160;manifold around x need to correspond to changes in h = f (x). Hence the encoder&#160;learns a mapping from the input space x to a representation space, a mapping that&#160;is only sensitive to changes along the manifold directions, but that is insensitive to&#160;changes orthogonal to the manifold.</span></p>
<p><span class="font64">A one-dimensional example is illustrated in Fig. 14.7, showing that by making the reconstruction function insensitive to perturbations of the input around the&#160;data points we recover the manifold structure.</span></p>
<p><span class="font64">To understand why autoencoders are useful for manifold learning, it is instructive to compare them to other approaches. What is most commonly learned to characterize a manifold is a </span><span class="font64" style="font-weight:bold;font-style:italic;">representation</span><span class="font64"> of the data points on (or near) the</span></p><div><img src="main-152.jpg" alt=""/></div>
<p><span class="font64">Figure 14.6: An illustration of the concept of a tangent hyperplane. Here we create a one-dimensional manifold in 784-dimensional space. We take an MNIST image with 784&#160;pixels and transform it by translating it vertically. The amount of vertical translation&#160;defines a coordinate along a one-dimensional manifold that traces out a curved path&#160;through image space. This plot shows a few points along this manifold. For visualization,&#160;we have projected the manifold into two dimensional space using PCA. An n-dimensional&#160;manifold has an n-dimensional tangent plane at every point. This tangent plane touches&#160;the manifold exactly at that point and is oriented parallel to the surface at that point.&#160;It defines the space of directions in which it is possible to move while remaining on&#160;the manifold. This one-dimensional manifold has a single tangent line. We indicate an&#160;example tangent line at one point, with an image showing how this tangent direction&#160;appears in image space. Gray pixels indicate pixels that do not change as we move along&#160;the tangent line, white pixels indicate pixels that brighten, and black pixels indicate pixels&#160;that darken.</span></p><div>
<p><span class="font63">1.0</span></p>
<p><span class="font63">0.8</span></p>
<p><span class="font63">0.6</span></p>
<p><span class="font63">0.4</span></p>
<p><span class="font63">0.2</span></p>
<p><span class="font63">0.0</span></p></div><div>
<p><span class="font64">Identity</span></p>
<p><span class="font64">Optimal reconstruction</span></p></div><div>
<p><span class="font64">xo</span></p></div><div>
<p><span class="font63" style="font-style:italic;">X 1</span></p>
<p><span class="font63" style="font-style:italic;">X</span></p></div><div>
<p><span class="font63" style="font-style:italic;">X2</span></p></div>
<p><span class="font64">Figure 14.7: If the autoencoder learns a reconstruction function that is invariant to small perturbations near the data points, it captures the manifold structure of the data. Here&#160;the manifold structure is a collection of 0-dimensional manifolds. The dashed diagonal&#160;line indicates the identity function target for reconstruction. The optimal reconstruction&#160;function crosses the identity function wherever there is a data point. The horizontal&#160;arrows at the bottom of the plot indicate the r (x) </span><span class="font64" style="font-style:italic;">— x</span><span class="font64"> reconstruction direction vector&#160;at the base of the arrow, in input space, always pointing towards the nearest “manifold”&#160;(a single datapoint, in the 1-D case). The denoising autoencoder explicitly tries to make&#160;the derivative of the reconstruction function r(x) small around the data points. The&#160;contractive autoencoder does the same for the encoder. Although the derivative ofr(x) is&#160;asked to be small around the data points, it can be large between the data points. The&#160;space between the data points corresponds to the region between the manifolds, where&#160;the reconstruction function must have a large derivative in order to map corrupted points&#160;back onto the manifold.</span></p>
<p><span class="font64">manifold. Such a representation for a particular example is also called its embedding. It is typically given by a low-dimensional vector, with less dimensions than the “ambient” space of which the manifold is a low-dimensional subset. Some&#160;algorithms (non-parametric manifold learning algorithms, discussed below) directly&#160;learn an embedding for each training example, while others learn a more general&#160;mapping, sometimes called an encoder, or representation function, that maps any&#160;point in the ambient space (the input space) to its embedding.</span></p>
<p><span class="font64">Manifold learning has mostly focused on unsupervised learning procedures that attempt to capture these manifolds. Most of the initial machine learning research&#160;on learning nonlinear manifolds has focused on </span><span class="font64" style="font-weight:bold;font-style:italic;">non-parametric</span><span class="font64"> methods based on&#160;the </span><span class="font64" style="font-weight:bold;font-style:italic;">nearest-neighbor graph.</span><span class="font64"> This graph has one node per training example and&#160;edges connecting near neighbors to each other. These methods (SchOlkopf </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">1998; Roweis and Saul, 2000; Tenenbaum </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2000; Brand, 2003; Belkin and</span></p><div><img src="main-153.jpg" alt=""/>
<p><span class="font64">Figure 14.8: Non-parametric manifold learning procedures build a nearest neighbor graph whose nodes are training examples and arcs connect nearest neighbors. Various procedures&#160;can thus obtain the tangent plane associated with a neighborhood of the graph as well&#160;as a coordinate system that associates each training example with a real-valued vector&#160;position, or </span><span class="font64" style="font-style:italic;">embedding.</span><span class="font64"> It is possible to generalize such a representation to new examples&#160;by a form of interpolation. So long as the number of examples is large enough to cover&#160;the curvature and twists of the manifold, these approaches work well. Images from the&#160;QMUL Multiview Face Dataset (Gong </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2000).</span></p></div>
<p><span class="font64">Niyogi, 2003; Donoho and Grimes, 2003; Weinberger and Saul, 2004; Hinton and Roweis, 2003; van der Maaten and Hinton, 2008) associate each of nodes with a&#160;tangent plane that spans the directions of variations associated with the difference&#160;vectors between the example and its neighbors, as illustrated in Fig. 14.8.</span></p>
<p><span class="font64">A global coordinate system can then be obtained through an optimization or solving a linear system. Fig. 14.9 illustrates how a manifold can be tiled by a&#160;large number of locally linear Gaussian-like patches (or “pancakes,” because the&#160;Gaussians are flat in the tangent directions).</span></p>
<p><span class="font64">However, there is a fundamental difficulty with such local non-parametric approaches to manifold learning, raised in Bengio and Monperrus (2005): if the&#160;manifolds are not very smooth (they have many peaks and troughs and twists),&#160;one may need a very large number of training examples to cover each one of these&#160;variations, with no chance to generalize to unseen variations. Indeed, these methods</span></p><div><img src="main-154.jpg" alt=""/>
<p><span class="font64">Figure 14.9: If the tangent planes (see Fig. 14.6) at each location are known, then they can be tiled to form a global coordinate system or a density function. Each local patch&#160;can be thought of as a local Euclidean coordinate system or as a locally flat Gaussian, or&#160;</span><span class="font64" style="font-style:italic;">“pancake’’,</span><span class="font64"> with a very small variance in the directions orthogonal to the pancake and a&#160;very large variance in the directions defining the coordinate system on the pancake. A&#160;mixture of these Gaussians provides an estimated density function, as in the manifold&#160;Parzen window algorithm (Vincent and Bengio, 2003) or its non-local neural-net based&#160;variant (Bengio </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2006c).</span></p></div>
<p><span class="font64">can only generalize the shape of the manifold by interpolating between neighboring examples. Unfortunately, the manifolds involved in AI problems can have very&#160;complicated structure that can be difficult to capture from only local interpolation.&#160;Consider for example the manifold resulting from translation shown in Fig. 14.6. If&#160;we watch just one coordinate within the input vector, x<sub>i</sub>, as the image is translated,&#160;we will observe that one coordinate encounters a peak or a trough in its value&#160;once for every peak or trough in brightness in the image. In other words, the&#160;complexity of the patterns of brightness in an underlying image template drives&#160;the complexity of the manifolds that are generated by performing simple image&#160;transformations. This motivates the use of distributed representations and deep&#160;learning for capturing manifold structure.</span></p>
<table border="1">
<tr><td colspan="2" style="vertical-align:bottom;">
<p><span class="font65" style="font-weight:bold;">14.7 Contractive Autoencoders</span></p></td><td>
<p></p></td><td>
<p></p></td></tr>
<tr><td colspan="4" style="vertical-align:middle;">
<p><span class="font64">The contractive autoencoder (Rifai </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011a,b) introduces an explicit regularizer on the code h = f (x), encouraging the derivatives of f to be as small as possible</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font64">Q(h) = A</span></p></td><td style="vertical-align:middle;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">d<sup>f</sup></span><span class="font64"><sup> (x)</sup></span></p></td><td style="vertical-align:middle;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">2</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font64">(14.18)</span></p></td></tr>
<tr><td>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d x</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;">F</span></p></td></tr>
</table>
<p><span class="font64">The penalty Q(h) is the squared Frobenius norm (sum of squared elements) of the Jacobian matrix of partial derivatives associated with the encoder function.</span></p>
<p><span class="font64">There is a connection between the denoising autoencoder and the contractive autoencoder: Alain and Bengio (2013) showed that in the limit of small Gaussian&#160;input noise, the denoising reconstruction error is equivalent to a contractive&#160;penalty on the reconstruction function that maps x to r = g(f(x)). In other&#160;words, denoising autoencoders make the reconstruction function resist small but&#160;finite-sized perturbations of the input, while contractive autoencoders make the&#160;feature extraction function resist infinitesimal perturbations of the input. When&#160;using the Jacobian-based contractive penalty to pretrain features f (x) for use&#160;with a classifier, the best classification accuracy usually results from applying the&#160;contractive penalty to f (x) rather than to </span><span class="font64" style="font-weight:bold;font-style:italic;">g(f</span><span class="font64"> (x)). A contractive penalty on f (x)&#160;also has close connections to score matching, as discussed in Sec. 14.5.1.</span></p>
<p><span class="font64">The name </span><span class="font64" style="font-weight:bold;font-style:italic;">contractive</span><span class="font64"> arises from the way that the CAE warps space. Specifically, because the CAE is trained to resist perturbations of its input, it is encouraged to map a neighborhood of input points to a smaller neighborhood of output points.&#160;We can think of this as contracting the input neighborhood to a smaller output&#160;neighborhood.</span></p>
<p><span class="font64">To clarify, the CAE is contractive only locally—all perturbations of a training point x are mapped near to f (x). Globally, two different points x and </span><span class="font64" style="font-weight:bold;font-style:italic;">x'</span><span class="font64"> may be&#160;mapped to f (x) and f(x') points that are farther apart than the original points.&#160;It is plausible that f be expanding in-between or far from the data manifolds (see&#160;for example what happens in the 1-D toy example of Fig. 14.7). When the </span><span class="font64" style="font-weight:bold;font-style:italic;">Q(h)&#160;</span><span class="font64">penalty is applied to sigmoidal units, one easy way to shrink the Jacobian is to&#160;make the sigmoid units saturate to 0 or 1. This encourages the CAE to encode&#160;input points with extreme values of the sigmoid that may be interpreted as a&#160;binary code. It also ensures that the CAE will spread its code values throughout&#160;most of the hypercube that its sigmoidal hidden units can span.</span></p>
<p><span class="font64">We can think of the Jacobian matrix J at a point x as approximating the nonlinear encoder f (x) as being a linear operator. This allows us to use the word&#160;“contractive” more formally. In the theory of linear operators, a linear operator&#160;is said to be contractive if the norm of </span><span class="font64" style="font-weight:bold;font-style:italic;">Jx</span><span class="font64"> remains less than or equal to 1 for&#160;all unit-norm x. In other words, J is contractive if it shrinks the unit sphere.&#160;We can think of the CAE as penalizing the Frobenius norm of the local linear&#160;approximation of f (x) at every training point x in order to encourage each of&#160;these local linear operator to become a contraction.</span></p>
<p><span class="font64">As described in Sec. 14.6, regularized autoencoders learn manifolds by balancing two opposing forces. In the case of the CAE, these two forces are reconstruction&#160;error and the contractive penalty </span><span class="font64" style="font-weight:bold;font-style:italic;">Q(h).</span><span class="font64"> Reconstruction error alone would encourage&#160;the CAE to learn an identity function. The contractive penalty alone would&#160;encourage the CAE to learn features that are constant with respect to x. The&#160;compromise between these two forces yields an autoencoder whose derivatives&#160;are mostly tiny. Only a small number of hidden units, corresponding to a&#160;small number of directions in the input, may have significant derivatives.</span></p>
<p><span class="font64">The goal of the CAE is to learn the manifold structure of the data. Directions x with large </span><span class="font64" style="font-weight:bold;font-style:italic;">Jx</span><span class="font64"> rapidly change h, so these are likely to be directions which&#160;approximate the tangent planes of the manifold. Experiments by Rifai </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011a)&#160;and Rifai </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011b) show that training the CAE results in most singular values&#160;of J dropping below 1 in magnitude and therefore becoming contractive. However,&#160;some singular values remain above </span><span class="font18">1</span><span class="font64">, because the reconstruction error penalty&#160;encourages the CAE to encode the directions with the most local variance. The&#160;directions corresponding to the largest singular values are interpreted as the tangent&#160;directions that the contractive autoencoder has learned. Ideally, these tangent&#160;directions should correspond to real variations in the data. For example, a CAE&#160;applied to images should learn tangent vectors that show how the image changes as&#160;objects in the image gradually change pose, as shown in Fig. 14.6. Visualizations of&#160;the experimentally obtained singular vectors do seem to correspond to meaningful&#160;transformations of the input image, as shown in Fig. 14.10.</span></p>
<p><span class="font64">One practical issue with the CAE regularization criterion is that although it is cheap to compute in the case of a single hidden layer autoencoder, it becomes&#160;much more expensive in the case of deeper autoencoders. The strategy followed by&#160;Rifai </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011a) is to separately train a series of single-layer autoencoders, each&#160;trained to reconstruct the previous autoencoder’s hidden layer. The composition&#160;of these autoencoders then forms a deep autoencoder. Because each layer was&#160;separately trained to be locally contractive, the deep autoencoder is contractive&#160;as well. The result is not the same as what would be obtained by jointly training&#160;the entire architecture with a penalty on the Jacobian of the deep model, but it&#160;captures many of the desirable qualitative characteristics.</span></p>
<p><span class="font64">Another practical issue is that the contraction penalty can obtain useless results</span></p>
<p><span class="font64">Tangent vectors</span></p><div>
<p><span class="font64">Input</span></p>
<p><span class="font64">point</span></p></div>
<p><span class="font64">Local PCA (no sharing across regions)</span></p><div>
<p><span class="font44">BS</span></p></div>
<p><span class="font64">Contractive autoencoder</span></p>
<p><span class="font64">Figure 14.10: Illustration of tangent vectors of the manifold estimated by local PCA and by a contractive autoencoder. The location on the manifold is defined by the input&#160;image of a dog drawn from the CIFAR-10 dataset. The tangent vectors are estimated&#160;by the leading singular vectors of the Jacobian matrix of the input-to-code mapping.&#160;Although both local PCA and the CAE can capture local tangents, the CAE is able to&#160;form more accurate estimates from limited training data because it exploits parameter&#160;sharing across different locations that share a subset of active hidden units. The CAE&#160;tangent directions typically correspond to moving or changing parts of the object (such as&#160;the head or legs).</span></p>
<p><span class="font64">if we do not impose some sort of scale on the decoder. For example, the encoder could consist of multiplying the input by a small constant e and the decoder&#160;could consist of dividing the code by e. As e approaches 0, the encoder drives the&#160;contractive penalty </span><span class="font64" style="font-weight:bold;font-style:italic;">Q(h</span><span class="font64">) to approach </span><span class="font18">0</span><span class="font64"> without having learned anything about the&#160;distribution. Meanwhile, the decoder maintains perfect reconstruction. In Rifai&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011a), this is prevented by tying the weights of f and g. Both f and g are&#160;standard neural network layers consisting of an affine transformation followed by&#160;an element-wise nonlinearity, so it is straightforward to set the weight matrix of g&#160;to be the transpose of the weight matrix of f.</span></p><h4><a id="bookmark11"></a><span class="font65" style="font-weight:bold;">14.8 Predictive Sparse Decomposition</span></h4>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Predictive sparse decomposition</span><span class="font64"> (PSD) is a model that is a hybrid of sparse coding and parametric autoencoders (Kavukcuoglu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2008). A parametric&#160;encoder is trained to predict the output of iterative inference. PSD has been&#160;applied to unsupervised feature learning for object recognition in images and video&#160;(Kavukcuoglu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009, 2010; Jarrett </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009; Farabet </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011), as well&#160;as for audio (Henaff </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011). The model consists of an encoder f (x) and a&#160;decoder g(h) that are both parametric. During training, h is controlled by the&#160;optimization algorithm. Training proceeds by minimizing</span></p>
<p><span class="font64">||x - g(h)||<sup>2</sup> + A|h|1 + Y||</span><span class="font65" style="font-style:italic;">h </span><span class="font64" style="font-weight:bold;font-style:italic;">- f</span><span class="font64"> (x)||<sup>2</sup>. &#160;&#160;&#160;(14.19)</span></p>
<p><span class="font64">Like in sparse coding, the training algorithm alternates between minimization with respect to h and minimization with respect to the model parameters. Minimization&#160;with respect to h is fast because f (x) provides a good initial value of h and the&#160;cost function constrains h to remain near f (x) anyway. Simple gradient descent&#160;can obtain reasonable values of h in as few as ten steps.</span></p>
<p><span class="font64">The training procedure used by PSD is different from first training a sparse coding model and then training f (x) to predict the values of the sparse coding&#160;features. The PSD training procedure regularizes the decoder to use parameters&#160;for which f (x) can infer good code values.</span></p>
<p><span class="font64">Predictive sparse coding is an example of </span><span class="font64" style="font-weight:bold;font-style:italic;">learned approximate inference.</span><span class="font64"> In Sec. 19.5, this topic is developed further. The tools presented in Chapter 19 make it&#160;clear that PSD can be interpreted as training a directed sparse coding probabilistic&#160;model by maximizing a lower bound on the log-likelihood of the model.</span></p>
<p><span class="font64">In practical applications of PSD, the iterative optimization is only used during training. The parametric encoder f is used to compute the learned features when&#160;the model is deployed. Evaluating f is computationally inexpensive compared to&#160;inferring h via gradient descent. Because f is a differentiable parametric function,&#160;PSD models may be stacked and used to initialize a deep network to be trained&#160;with another criterion.</span></p><h4><a id="bookmark12"></a><span class="font65" style="font-weight:bold;">14.9 Applications of Autoencoders</span></h4>
<p><span class="font64">Autoencoders have been successfully applied to dimensionality reduction and information retrieval tasks. Dimensionality reduction was one of the first applications of representation learning and deep learning. It was one of the early motivations&#160;for studying autoencoders. For example, Hinton and Salakhutdinov (2006) trained&#160;a stack of RBMs and then used their weights to initialize a deep autoencoder&#160;with gradually smaller hidden layers, culminating in a bottleneck of 30 units. The&#160;resulting code yielded less reconstruction error than PCA into 30 dimensions and&#160;the learned representation was qualitatively easier to interpret and relate to the&#160;underlying categories, with these categories manifesting as well-separated clusters.</span></p>
<p><span class="font64">Lower-dimensional representations can improve performance on many tasks, such as classification. Models of smaller spaces consume less memory and runtime.&#160;Many forms of dimensionality reduction place semantically related examples near&#160;each other, as observed by Salakhutdinov and Hinton (2007b) and Torralba </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(2008). The hints provided by the mapping to the lower-dimensional space aid&#160;generalization.</span></p>
<p><span class="font64">One task that benefits even more than usual from dimensionality reduction is </span><span class="font64" style="font-weight:bold;font-style:italic;">information retrieval</span><span class="font64">, the task of finding entries in a database that resemble a&#160;query entry. This task derives the usual benefits from dimensionality reduction&#160;that other tasks do, but also derives the additional benefit that search can become&#160;extremely efficient in certain kinds of low dimensional spaces. Specifically, if&#160;we train the dimensionality reduction algorithm to produce a code that is lowdimensional and </span><span class="font64" style="font-weight:bold;">binary</span><span class="font64">, then we can store all database entries in a hash table&#160;mapping binary code vectors to entries. This hash table allows us to perform&#160;information retrieval by returning all database entries that have the same binary&#160;code as the query. We can also search over slightly less similar entries very&#160;efficiently, just by flipping individual bits from the encoding of the query. This&#160;approach to information retrieval via dimensionality reduction and binarization&#160;is called </span><span class="font64" style="font-weight:bold;font-style:italic;">semantic hashing</span><span class="font64"> (Salakhutdinov and Hinton, 2007b, 2009b), and has&#160;been applied to both textual input (Salakhutdinov and Hinton, 2007b, 2009b) and&#160;images (Torralba </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2008; Weiss </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2008; Krizhevsky and Hinton, 2011).</span></p>
<p><span class="font64">To produce binary codes for semantic hashing, one typically uses an encoding function with sigmoids on the final layer. The sigmoid units must be trained to be&#160;saturated to nearly 0 or nearly 1 for all input values. One trick that can accomplish&#160;this is simply to inject additive noise just before the sigmoid nonlinearity during&#160;training. The magnitude of the noise should increase over time. To fight that&#160;noise and preserve as much information as possible, the network must increase the&#160;magnitude of the inputs to the sigmoid function, until saturation occurs.</span></p>
<p><span class="font64">The idea of learning a hashing function has been further explored in several directions, including the idea of training the representations so as to optimize&#160;a loss more directly linked to the task of finding nearby examples in the hash&#160;table (Norouzi and Fleet, 2011).</span></p>
</body>
</html>