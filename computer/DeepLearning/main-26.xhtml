<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 13</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Linear Factor Models</span></h2>
<p><span class="font64">Many of the research frontiers in deep learning involve building a probabilistic model of the input, p<sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (x). Such a model can, in principle, use probabilistic inference to&#160;predict any of the variables in its environment given any of the other variables. Many&#160;of these models also have latent variables h, with p<sub>mo</sub>d<sub>e</sub></span><span class="font18">1</span><span class="font64">(x) = Ehp<sub>mo</sub>d<sub>e</sub>i(x | h).&#160;These latent variables provide another means of representing the data. Distributed&#160;representations based on latent variables can obtain all of the advantages of&#160;representation learning that we have seen with deep feedforward and recurrent&#160;networks.</span></p>
<p><span class="font64">In this chapter, we describe some of the simplest probabilistic models with latent variables: linear factor models. These models are sometimes used as building&#160;blocks of mixture models (Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1995a; Ghahramani and Hinton, 1996;&#160;Roweis </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2002) or larger, deep probabilistic models (Tang </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012). They&#160;also show many of the basic approaches necessary to build generative models that&#160;the more advanced deep models will extend further.</span></p>
<p><span class="font64">A linear factor model is defined by the use of a stochastic, linear decoder function that generates x by adding noise to a linear transformation of h.</span></p>
<p><span class="font64">These models are interesting because they allow us to discover explanatory factors that have a simple joint distribution. The simplicity of using a linear decoder&#160;made these models some of the first latent variable models to be extensively studied.</span></p>
<p><span class="font64">A linear factor model describes the data generation process as follows. First, we sample the explanatory factors h from a distribution</span></p>
<p><span class="font64">h - p(h), &#160;&#160;&#160;(13.1)</span></p>
<p><span class="font64">where p(h) is a factorial distribution, with p(h) = <sub>i</sub>p(h), so that it is easy to</span></p><div><img src="main-144.jpg" alt=""/>
<p><span class="font64">Figure 13.1: The directed graphical model describing the linear factor model family, in which we assume that an observed data vector x is obtained by a linear combination of&#160;independent latent factors h, plus some noise. Different models, such as probabilistic&#160;PCA, factor analysis or ICA, make different choices about the form of the noise and of&#160;the prior </span><span class="font64" style="font-style:italic;">p(h).</span></p></div><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">13.1 Probabilistic PCA and Factor Analysis</span></h4>
<p><span class="font64">Probabilistic PCA (principal components analysis), factor analysis and other linear factor models are special cases of the above equations (13.1 and 13.2) and only&#160;differ in the choices made for the noise distribution and the model’s prior over&#160;latent variables h before observing x.</span></p>
<p><span class="font64">In </span><span class="font64" style="font-weight:bold;font-style:italic;">factor analysis</span><span class="font64"> (Bartholomew, 1987; Basilevsky, 1994), the latent variable prior is just the unit variance Gaussian</span></p>
<p><span class="font64">h (h; 0,</span><span class="font64" style="font-weight:bold;font-style:italic;">1</span><span class="font64">) &#160;&#160;&#160;(13.3)</span></p>
<p><span class="font64">while the observed variables </span><span class="font64" style="font-weight:bold;font-style:italic;">xi</span><span class="font64"> are assumed to be </span><span class="font64" style="font-weight:bold;font-style:italic;">conditionally independent,</span><span class="font64"> givenh. Specifically, the noise is assumed to be drawn from a diagonal covariance Gaussian&#160;distribution, with covariance matrix 0 = diag(a<sup>2</sup>), with </span><span class="font64" style="font-weight:bold;font-style:italic;">a<sup>2</sup> =</span><span class="font64"> [a|, 00 ,...,2־</span><span class="font64" style="font-weight:bold;font-style:italic;">n]</span><span class="font64"> a&#160;vector of per-variable variances.</span></p>
<p><span class="font64">The role of the latent variables is thus to </span><span class="font64" style="font-weight:bold;font-style:italic;">capture the dependencies</span><span class="font64"> between the different observed variables x<sub>i</sub>. Indeed, it can easily be shown that x is just a&#160;multivariate normal random variable, with</span></p>
<p><span class="font64">x ~ N(x; </span><span class="font64" style="font-weight:bold;font-style:italic;">b, WW</span><span class="font64"><sup>T</sup> + 0). &#160;&#160;&#160;(13.4)</span></p>
<p><span class="font64">In order to cast PCA in a probabilistic framework, we can make a slight modification to the factor analysis model, making the conditional variances a</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>2</sup></span></p>
<p><span class="font5">־־r &#160;&#160;&#160;</span><span class="font9" style="font-style:italic;">&lt;r\</span></p>
<p><span class="font64">equal to each other. In that case the covariance of x is just WW <sup>1</sup> + </span><span class="font64" style="font-weight:bold;font-style:italic;">a<sup>2</sup>1</span><span class="font64">, where</span></p>
<p><span class="font6" style="font-weight:bold;">0</span><span class="font49">♦</span></p>
<p><span class="font64">a<sup>2</sup> is now a scalar. This yields the conditional distribution</span></p><div>
<p><span class="font64">(13.5)</span></p></div>
<p><span class="font64">N(x; </span><span class="font64" style="font-weight:bold;font-style:italic;">b</span><span class="font64">, WW<sup>T</sup> + a <sup>2</sup>I)</span></p><div>
<p><span class="font64">or equivalently</span></p></div><div>
<p><span class="font64">x = W h + b + az</span></p></div><div>
<p><span class="font64">(13.6)</span></p></div>
<p><span class="font64">where z </span><span class="font64" style="font-weight:bold;font-style:italic;">N(z;</span><span class="font64"> 0,</span><span class="font64" style="font-weight:bold;font-style:italic;">1</span><span class="font64"> ) is Gaussian noise. Tipping and Bishop (1999) then show an iterative EM algorithm for estimating the parameters W and a<sup>2</sup>.</span></p>
<p><span class="font64">This </span><span class="font64" style="font-weight:bold;font-style:italic;">probabilistic PCA</span><span class="font64"> model takes advantage of the observation that most variations in the data can be captured by the latent variables h, up to some&#160;small residual </span><span class="font64" style="font-weight:bold;font-style:italic;">reconstruction error a</span><span class="font64"><sup>2</sup>. As shown by Tipping and Bishop (1999),&#160;probabilistic PCA becomes PCA as a ^ 0. In that case, the conditional expected&#160;value of h given x becomes an orthogonal projection of x — b onto the space&#160;spanned by the d columns of W, like in PCA.</span></p>
<p><span class="font64">As a ^ 0, the density model defined by probabilistic PCA becomes very sharp around these d dimensions spanned by the columns of W. This can make the&#160;model assign very low likelihood to the data if the data does not actually cluster&#160;near a hyperplane.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">13.2 Independent Component Analysis (ICA)</span></h4>
<p><span class="font64">Independent component analysis (ICA) is among the oldest representation learning algorithms (Herault and Ans, 1984; Jutten and Herault, 1991; Comon, 1994;&#160;Hyvarinen, 1999; Hyvarinen </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2001a; Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2001; Teh </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2003).&#160;It is an approach to modeling linear factors that seeks to separate an observed&#160;signal into many underlying signals that are scaled and added together to form&#160;the observed data. These signals are intended to be fully independent, rather than&#160;merely decorrelated from each other.<a id="footnote1"></a><sup><a href="#bookmark3">1</a></sup><sup></sup></span></p>
<p><span class="font64">Many different specific methodologies are referred to as ICA. The variant that is most similar to the other generative models we have described here is a&#160;variant (Pham </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1992) that trains a fully parametric generative model. The&#160;prior distribution over the underlying factors, </span><span class="font64" style="font-weight:bold;font-style:italic;">p(h</span><span class="font64">), must be fixed ahead of time by&#160;the user. The model then deterministically generates x = Wh. We can perform&#160;a nonlinear change of variables (using Eq. 3.47) to determine </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x).</span><span class="font64"> Learning the&#160;model then proceeds as usual, using maximum likelihood.</span></p>
<p><span class="font64">The motivation for this approach is that by choosingp (h) to be independent, we can recover underlying factors that are as close as possible to independent.&#160;This is commonly used, not to capture high-level abstract causal factors, but to&#160;recover low-level signals that have been mixed together. In this setting, each&#160;training example is one moment in time, each </span><span class="font64" style="font-weight:bold;font-style:italic;">Xi</span><span class="font64"> is one sensor’s observation of&#160;the mixed signals, and each </span><span class="font64" style="font-weight:bold;font-style:italic;">h<sub>i</sub></span><span class="font64"> is one estimate of one of the original signals. For&#160;example, we might have n people speaking simultaneously. If we have n different&#160;microphones placed in different locations, ICA can detect the changes in the volume&#160;between each speaker as heard by each microphone, and separate the signals so&#160;that each h <sub>i</sub> contains only one person speaking clearly. This is commonly used&#160;in neuroscience for electroencephalography, a technology for recording electrical&#160;signals originating in the brain. Many electrode sensors placed on the subject’s&#160;head are used to measure many electrical signals coming from the body. The&#160;experimenter is typically only interested in signals from the brain, but signals from&#160;the subject’s heart and eyes are strong enough to confound measurements taken&#160;at the subject’s scalp. The signals arrive at the electrodes mixed together, so&#160;ICA is necessary to separate the electrical signature of the heart from the signals&#160;originating in the brain, and to separate signals in different brain regions from&#160;each other.</span></p>
<p><span class="font64">As mentioned before, many variants of ICA are possible. Some add some noise in the generation of </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> rather than using a deterministic decoder. Most do not&#160;use the maximum likelihood criterion, but instead aim to make the elements of&#160;h = W</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>-1</sup></span><span class="font64"> x independent from each other. Many criteria that accomplish this goal&#160;are possible. Eq. 3.47 requires taking the determinant of W, which can be an&#160;expensive and numerically unstable operation. Some variants of ICA avoid this&#160;problematic operation by constraining W to be orthonormal.</span></p>
<p><span class="font64">All variants of ICA require that p( h) be non-Gaussian. This is because if p(h) is an independent prior with Gaussian components, then W is not identifiable.&#160;We can obtain the same distribution over p(x) for many values of W. This is very&#160;different from other linear factor models like probabilistic PCA and factor analysis,&#160;that often require p(h) to be Gaussian in order to make many operations on the&#160;model have closed form solutions. In the maximum likelihood approach where the&#160;user explicitly specifies the distribution, a typical choice is to use p (h) = df. <sup>a</sup>(h<sub>i</sub>).&#160;Typical choices of these non-Gaussian distributions have larger peaks near 0 than&#160;does the Gaussian distribution, so we can also see most implementations of ICA&#160;as learning sparse features.</span></p>
<p><span class="font64">Many variants of ICA are not generative models in the sense that we use the phrase. In this book, a generative model either representsp(x) or can draw samples&#160;from it. Many variants of ICA only know how to transform between x and h, but&#160;do not have any way of representingp(h), and thus do not impose a distribution&#160;over p(x). For example, many ICA variants aim to increase the sample kurtosis of&#160;h = W<sup>-1</sup> x, because high kurtosis indicates thatp(h) is non-Gaussian, but this is&#160;accomplished without explicitly representing p (h). This is because ICA is more&#160;often used as an analysis tool for separating signals, rather than for generating&#160;data or estimating its density.</span></p>
<p><span class="font64">Just as PCA can be generalized to the nonlinear autoencoders described in Chapter 14, ICA can be generalized to a nonlinear generative model, in which&#160;we use a nonlinear function f to generate the observed data. See Hyvarinen&#160;and Pajunen (1999) for the initial work on nonlinear ICA and its successful&#160;use with ensemble learning by Roberts and Everson (2001) and Lappalainen&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2000). Another nonlinear extension of ICA is the approach of </span><span class="font64" style="font-weight:bold;font-style:italic;">nonlinear&#160;independent components estimation,</span><span class="font64"> or NICE (Dinh </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014), which stacks a&#160;series of invertible transformations (encoder stages) that have the property that&#160;the determinant of the Jacobian of each transformation can be computed efficiently.&#160;This makes it possible to compute the likelihood exactly and, like ICA, attempts&#160;to transform the data into a space where it has a factorized marginal distribution,&#160;but is more likely to succeed thanks to the nonlinear encoder. Because the encoder&#160;is associated with a decoder that is its perfect inverse, it is straightforward to&#160;generate samples from the model (by first sampling from p(h) and then applying&#160;the decoder).</span></p>
<p><span class="font64">Another generalization of ICA is to learn groups of features, with statistical dependence allowed within a group but discouraged between groups (Hyvarinen&#160;and Hoyer, 1999; Hyvarinen </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2001b). When the groups of related units are&#160;chosen to be non-overlapping, this is called </span><span class="font64" style="font-weight:bold;font-style:italic;">independent subspace analysis.</span><span class="font64"> It is also&#160;possible to assign spatial coordinates to each hidden unit and form overlapping&#160;groups of spatially neighboring units. This encourages nearby units to learn similar&#160;features. When applied to natural images, this </span><span class="font64" style="font-weight:bold;font-style:italic;">topographic ICA</span><span class="font64"> approach learns&#160;Gabor filters, such that neighboring features have similar orientation, location or&#160;frequency. Many different phase offsets of similar Gabor functions occur within&#160;each region, so that pooling over small regions yields translation invariance.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">13.3 Slow Feature Analysis</span></h4>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Slow feature analysis</span><span class="font64"> (SFA) is a linear factor model that uses information from time signals to learn invariant features (Wiskott and Sejnowski, 2002).</span></p>
<p><span class="font64">Slow feature analysis is motivated by a general principle called the slowness principle. The idea is that the important characteristics of scenes change very&#160;slowly compared to the individual measurements that make up a description of a&#160;scene. For example, in computer vision, individual pixel values can change very&#160;rapidly. If a zebra moves from left to right across the image, an individual pixel&#160;will rapidly change from black to white and back again as the zebra’s stripes pass&#160;over the pixel. By comparison, the feature indicating whether a zebra is in the&#160;image will not change at all, and the feature describing the zebra’s position will&#160;change slowly. We therefore may wish to regularize our model to learn features&#160;that change slowly over time.</span></p>
<p><span class="font64">The slowness principle predates slow feature analysis and has been applied to a wide variety of models (Hinton, 1989; Foldiak, 1989; Mobahi </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009;&#160;Bergstra and Bengio, 2009). In general, we can apply the slowness principle to any&#160;differentiable model trained with gradient descent. The slowness principle may be&#160;introduced by adding a term to the cost function of the form</span></p>
<p><span class="font64">A £</span><span class="font64" style="font-weight:bold;font-style:italic;">L(f (x<sup>(t</sup>+<sup>1)</sup>),f</span><span class="font64">(x</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(t)</sup>))</span><span class="font64"> &#160;&#160;&#160;(13.7)</span></p>
<p><span class="font64">t</span></p>
<p><span class="font64">where A is a hyperparameter determining the strength of the slowness regularization term, t is the index into a time sequence of examples, f is the feature extractor&#160;to be regularized, and L is a loss function measuring the distance between f (x<sup>(t)</sup>)&#160;and f (x<sup>(t+1)</sup>). A common choice for L is the mean squared difference.</span></p>
<p><span class="font64">Slow feature analysis is a particularly efficient application of the slowness principle. It is efficient because it is applied to a linear feature extractor, and can&#160;thus be trained in closed form. Like some variants of ICA, SFA is not quite a&#160;generative model per se, in the sense that it defines a linear map between input&#160;space and feature space but does not define a prior over feature space and thus&#160;does not impose a distribution p(x) on input space.</span></p>
<p><span class="font64">The SFA algorithm (Wiskott and Sejnowski, 2002) consists of defining f (x; 6) to be a linear transformation, and solving the optimization problem</span></p>
<table border="1">
<tr><td>
<p><span class="font64">minEt(f (x<sup>(t+1)</sup>) - f (x<sup>(t)</sup>) ;)<sup>2</sup></span></p></td><td>
<p><span class="font64">(13.8)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">subject to the constraints</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font64">Ef (x<sup>(t)</sup> )i = 0</span></p></td><td>
<p><span class="font64">(13.9)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">and</span></p></td><td>
<p></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">E<sub>t</sub> [f (x<sup>(t)</sup>)2 ] = 1.</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(13.10)</span></p></td></tr>
</table>
<p><span class="font64">The constraint that the learned feature have zero mean is necessary to make the problem have a unique solution; otherwise we could add a constant to all feature&#160;values and obtain a different solution with equal value of the slowness objective.&#160;The constraint that the features have unit variance is necessary to prevent the&#160;pathological solution where all features collapse to 0. Like PCA, the SFA features&#160;are ordered, with the first feature being the slowest. To learn multiple features, we&#160;must also add the constraint</span></p>
<p><span class="font64">Vi &lt; j, E [f (x<sup>(t)</sup>)i f (x<sup>(t)</sup> )j] = 0. &#160;&#160;&#160;(13.11)</span></p>
<p><span class="font64">This specifies that the learned features must be linearly decorrelated from each other. Without this constraint, all of the learned features would simply capture the&#160;one slowest signal. One could imagine using other mechanisms, such as minimizing&#160;reconstruction error, to force the features to diversify, but this decorrelation&#160;mechanism admits a simple solution due to the linearity of SFA features. The SFA&#160;problem may be solved in closed form by a linear algebra package.</span></p>
<p><span class="font64">SFA is typically used to learn nonlinear features by applying a nonlinear basis expansion to x before running SFA. For example, it is common to replace x by the&#160;quadratic basis expansion, a vector containing elements </span><span class="font64" style="font-weight:bold;font-style:italic;">x<sub>i</sub>Xj</span><span class="font64"> for all i and j. Linear&#160;SFA modules may then be composed to learn deep nonlinear slow feature extractors&#160;by repeatedly learning a linear SFA feature extractor, applying a nonlinear basis&#160;expansion to its output, and then learning another linear SFA feature extractor on&#160;top of that expansion.</span></p>
<p><span class="font64">When trained on small spatial patches of videos of natural scenes, SFA with quadratic basis expansions learns features that share many characteristics with&#160;those of complex cells in V1 cortex (Berkes and Wiskott, 2005). When trained&#160;on videos of random motion within 3-D computer rendered environments, deep&#160;SFA learns features that share many characteristics with the features represented&#160;by neurons in rat brains that are used for navigation (Franzius </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2007). SFA&#160;thus seems to be a reasonably biologically plausible model.</span></p>
<p><span class="font64">A major advantage of SFA is that it is possibly to theoretically predict which features SFA will learn, even in the deep, nonlinear setting. To make such theoretical&#160;predictions, one must know about the dynamics of the environment in terms of&#160;configuration space (e.g., in the case of random motion in the 3-D rendered&#160;environment, the theoretical analysis proceeds from knowledge of the probability&#160;distribution over position and velocity of the camera). Given the knowledge of how&#160;the underlying factors actually change, it is possible to analytically solve for the&#160;optimal functions expressing these factors. In practice, experiments with deep SFA&#160;applied to simulated data seem to recover the theoretically predicted functions.</span></p>
<p><span class="font64">This is in comparison to other learning algorithms where the cost function depends highly on specific pixel values, making it much more difficult to determine what&#160;features the model will learn.</span></p>
<p><span class="font64">Deep SFA has also been used to learn features for object recognition and pose estimation (Franzius </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2008). So far, the slowness principle has not become&#160;the basis for any state of the art applications. It is unclear what factor has limited&#160;its performance. We speculate that perhaps the slowness prior is too strong, and&#160;that, rather than imposing a prior that features should be approximately constant,&#160;it would be better to impose a prior that features should be easy to predict from&#160;one time step to the next. The position of an object is a useful feature regardless of&#160;whether the object’s velocity is high or low, but the slowness principle encourages&#160;the model to ignore the position of objects that have high velocity.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">13.4 Sparse Coding</span></h4>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Sparse coding</span><span class="font64"> (Olshausen and Field, 1996) is a linear factor model that has been heavily studied as an unsupervised feature learning and feature extraction&#160;mechanism. Strictly speaking, the term “sparse coding” refers to the process of&#160;inferring the value of h in this model, while “sparse modeling” refers to the process&#160;of designing and learning the model, but the term “sparse coding” is often used to&#160;refer to both.</span></p>
<p><span class="font64">Like most other linear factor models, it uses a linear decoder plus noise to obtain reconstructions of x, as specified in Eq. 13.2. More specifically, sparse&#160;coding models typically assume that the linear factors have Gaussian noise with&#160;isotropic precision </span><span class="font64" style="font-weight:bold;font-style:italic;">(3</span><span class="font64">:</span></p><div>
<p><span class="font18">1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(x</span><span class="font64"> | h) = </span><span class="font64" style="font-weight:bold;font-style:italic;">N(x; Wh + b, </span><span class="font62" style="font-style:italic;">3</span><span class="font64" style="font-weight:bold;font-style:italic;">1</span><span class="font64">).</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>3</sup></span></p></div><div>
<p><span class="font64">(13.12)</span></p></div>
<p><span class="font64">The distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p(h</span><span class="font64">) is chosen to be one with sharp peaks near 0 (Olshausen and Field, 1996). Common choices include factorized Laplace, Cauchy or factorized&#160;Student-t distributions. For example, the Laplace prior parametrized in terms of&#160;the sparsity penalty coefficient A is given by</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(hi)</span><span class="font64"> = Laplace(hi;0, -) = -e </span><span class="font18"><sup>2</sup></span><span class="font64"><sup> A|h </sup></span><span class="font18"><sup>1</sup></span></p>
<p><span class="font64">A 4</span></p></div><div>
<p><span class="font64">(13.13)</span></p></div>
<p><span class="font64">2. A</span></p>
<p><span class="font64">and the Student-t prior by</span></p><div>
<p><span class="font64">p(h i<sup>)</sup> «</span></p></div><div>
<p><span class="font18">1</span></p></div><div>
<p><span class="font64">. &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font18"><sup>2</sup></span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:underline;">v+1</span></p>
<p><span class="font18">(1</span><span class="font64"> + <sup>h</sup></span></p></div><div>
<p><span class="font64">(13.14)</span></p></div>
<p><span class="font64">Training sparse coding with maximum likelihood is intractable. Instead, the training alternates between encoding the data and training the decoder to better&#160;reconstruct the data given the encoding. This approach will be justified further as&#160;a principled approximation to maximum likelihood later, in Sec. 19.3.</span></p>
<p><span class="font64">For models such as PCA, we have seen the use of a parametric encoder function that predicts h and consists only of multiplication by a weight matrix. The encoder&#160;that we use with sparse coding is not a parametric encoder. Instead, the encoder&#160;is an optimization algorithm, that solves an optimization problem in which we seek&#160;the single most likely code value:</span></p>
<p><span class="font64">h* = f (x) </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> argmaxp(h | </span><span class="font64" style="font-weight:bold;font-style:italic;">x).</span><span class="font64"> &#160;&#160;&#160;(13.15)</span></p>
<p><span class="font64">h</span></p>
<p><span class="font64">When combined with Eq. 13.13 and Eq. 13.12, this yields the following optimization problem:</span></p>
<p><span class="font64">argmaxp(h | x) &#160;&#160;&#160;(13.16)</span></p>
<p><span class="font64">h</span></p>
<p><span class="font64">= argmaxlogp(h | x) &#160;&#160;&#160;(13.17)</span></p>
<p><span class="font64">h</span></p>
<p><span class="font64">= argmin A||h</span><span class="font18">||<sub>1</sub></span><span class="font64"> + fl||x — Wh|||, &#160;&#160;&#160;(13.18)</span></p>
<p><span class="font64">h</span></p>
<p><span class="font64">where we have dropped terms not depending on h and divided by positive scaling factors to simplify the equation.</span></p>
<p><span class="font64">Due to the imposition of an L</span><span class="font18"><sup>1</sup></span><span class="font64"> norm on h, that this procedure will yield a</span></p>
<p><span class="font64">sparse h* (See Sec. 7.1.2).</span></p>
<p><span class="font64">To train the model rather than just perform inference, we alternate between minimization with respect to h and minimization with respect to W. In this&#160;presentation, we treat fl as a hyperparameter. Typically it is set to 1 because its&#160;role in this optimization problem is shared with A and there is no need for both&#160;hyperparameters. In principle, we could also treat </span><span class="font64" style="font-weight:bold;font-style:italic;">(3</span><span class="font64"> as a parameter of the model&#160;and learn it. Our presentation here has discarded some terms that do not depend&#160;on h but do depend on </span><span class="font64" style="font-weight:bold;font-style:italic;">(3</span><span class="font64">. To learn fl, these terms must be included, or </span><span class="font64" style="font-weight:bold;font-style:italic;">(3</span><span class="font64"> will&#160;collapse to </span><span class="font18">0</span><span class="font64">.</span></p>
<p><span class="font64">Not all approaches to sparse coding explicitly build a p(h) and a p(x | h). Often we are just interested in learning a dictionary of features with activation&#160;values that will often be zero when extracted using this inference procedure.</span></p>
<p><span class="font64">If we sample h from a Laplace prior, it is in fact a zero probability event for an element of h to actually be zero. The generative model itself is not especially&#160;sparse, only the feature extractor is. Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013d) describe approximate&#160;inference in a different model family, the spike and slab sparse coding model, for&#160;which samples from the prior usually contain true zeros.</span></p>
<p><span class="font64">The sparse coding approach combined with the use of the non-parametric encoder can in principle minimize the combination of reconstruction error and&#160;log-prior better than any specific parametric encoder. Another advantage is that&#160;there is no generalization error to the encoder. A parametric encoder must learn&#160;how to map x to h in a way that generalizes. For unusual </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> that do not resemble&#160;the training data, a learned, parametric encoder may fail to find an h that results&#160;in accurate reconstruction or a sparse code. For the vast majority of formulations&#160;of sparse coding models, where the inference problem is convex, the optimization&#160;procedure will always find the optimal code (unless degenerate cases such as&#160;replicated weight vectors occur). Obviously, the sparsity and reconstruction costs&#160;can still rise on unfamiliar points, but this is due to generalization error in the&#160;decoder weights, rather than generalization error in the encoder. The lack of&#160;generalization error in sparse coding’s optimization-based encoding process may&#160;result in better generalization when sparse coding is used as a feature extractor for&#160;a classifier than when a parametric function is used to predict the code. Coates&#160;and Ng (2011) demonstrated that sparse coding features generalize better for&#160;object recognition tasks than the features of a related model based on a parametric&#160;encoder, the linear-sigmoid autoencoder. Inspired by their work, Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(2013d) showed that a variant of sparse coding generalizes better than other feature&#160;extractors in the regime where extremely few labels are available (twenty or fewer&#160;labels per class).</span></p>
<p><span class="font64">The primary disadvantage of the non-parametric encoder is that it requires greater time to compute h given x because the non-parametric approach requires&#160;running an iterative algorithm. The parametric autoencoder approach, developed&#160;in Chapter 14, uses only a fixed number of layers, often only one. Another&#160;disadvantage is that it is not straight-forward to back-propagate through the&#160;non-parametric encoder, which makes it difficult to pretrain a sparse coding model&#160;with an unsupervised criterion and then fine-tune it using a supervised criterion.&#160;Modified versions of sparse coding that permit approximate derivatives do exist&#160;but are not widely used (Bagnell and Bradley, 2009).</span></p>
<p><span class="font64">Sparse coding, like other linear factor models, often produces poor samples, as shown in Fig. 13.2. This happens even when the model is able to reconstruct&#160;the data well and provide useful features for a classifier. The reason is that each&#160;individual feature may be learned well, but the factorial prior on the hidden code&#160;results in the model including random subsets of all of the features in each generated&#160;sample. This motivates the development of deeper models that can impose a non-</span></p><div><img src="main-145.jpg" alt=""/>
<p><span class="font64">Figure 13.2: Example samples and weights from a spike and slab sparse coding model trained on the MNIST dataset. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> The samples from the model do not resemble the&#160;training examples. At first glance, one might assume the model is poorly fit. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> The&#160;weight vectors of the model have learned to represent penstrokes and sometimes complete&#160;digits. The model has thus learned useful features. The problem is that the factorial prior&#160;over features results in random subsets of features being combined. Few such subsets&#160;are appropriate to form a recognizable MNIST digit. This motivates the development of&#160;generative models that have more powerful distributions over their latent codes. Figure&#160;reproduced with permission from Goodfellow </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2013d).</span></p></div>
<p><span class="font64">factorial distribution on the deepest code layer, as well as the development of more sophisticated shallow models.</span></p><h4><a id="bookmark6"></a><span class="font65" style="font-weight:bold;">13.5 Manifold Interpretation of PCA</span></h4>
<p><span class="font64">Linear factor models including PCA and factor analysis can be interpreted as learning a manifold (Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1997). We can view probabilistic PCA as&#160;defining a thin pancake-shaped region of high probability—a Gaussian distribution&#160;that is very narrow along some axes, just as a pancake is very flat along its vertical&#160;axis, but is elongated along other axes, just as a pancake is wide along its horizontal&#160;axes. This is illustrated in Fig. 13.3. PCA can be interpreted as aligning this&#160;pancake with a linear manifold in a higher-dimensional space. This interpretation&#160;applies not just to traditional PCA but also to any linear autoencoder that learns&#160;matrices W and V with the goal of making the reconstruction of x lie as close to&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> as possible,</span></p>
<p><span class="font64">Let the encoder be</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">h = f</span><span class="font64"> (x) = W<sup>T</sup>(x - ^). &#160;&#160;&#160;(13.19)</span></p>
<p><span class="font64">The encoder computes a low-dimensional representation of </span><span class="font64" style="font-weight:bold;font-style:italic;">h.</span><span class="font64"> With the autoencoder view, we have a decoder computing the reconstruction</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x =</span><span class="font64"> g(h) = b + </span><span class="font64" style="font-weight:bold;font-style:italic;">Vh.</span><span class="font64"> &#160;&#160;&#160;(13.20)</span></p><div><div><img src="main-146.jpg" alt=""/>
<p><span class="font64">Figure 13.3: Flat Gaussian capturing probability concentration near a low-dimensional manifold. The figure shows the upper half of the “pancake” above the “manifold plane”&#160;which goes through its middle. The variance in the direction orthogonal to the manifold is&#160;very small (arrow pointing out of plane) and can be considered like “noise,” while the other&#160;variances are large (arrows in the plane) and correspond to “signal,” and a coordinate&#160;system for the reduced-dimension data.</span></p></div></div>
<p><span class="font64">The choices of linear encoder and decoder that minimize reconstruction error</span></p><div>
<p><span class="font64">(13.21)</span></p></div>
<p><span class="font64">E[||x — x|| </span><span class="font18"><sup>2</sup></span><span class="font64">]</span></p>
<p><span class="font64">correspond to V = W, </span><span class="font64" style="font-weight:bold;font-style:italic;">^ = b =</span><span class="font64"> E[</span><span class="font64" style="font-weight:bold;font-style:italic;">x]</span><span class="font64"> and the columns of W form an orthonormal basis which spans the same subspace as the principal eigenvectors of the covariance&#160;matrix</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">C =</span><span class="font64"> E[(x — ^)(x — ^)<sup>T</sup> ]. &#160;&#160;&#160;(13.22)</span></p>
<p><span class="font64">In the case of PCA, the columns of W are these eigenvectors, ordered by the magnitude of the corresponding eigenvalues (which are all real and non-negative).</span></p>
<p><span class="font64">One can also show that eigenvalue A<sub>i</sub> of C corresponds to the variance of x in the direction of eigenvector v<sup>(i)</sup>. If x G R<sup>D</sup> and h G with d &lt; D, then the</span></p>
<p><span class="font64">optimal reconstruction error (choosing i, b, V and </span><span class="font64" style="font-weight:bold;font-style:italic;">W as</span><span class="font64"> above) is</span></p>
<p><span class="font64">D</span></p>
<p><span class="font64">minE[||x — x||<sup>2</sup>] = &#160;&#160;&#160;X<sub>i</sub>.&#160;&#160;&#160;&#160;(13.23)</span></p>
<p><span class="font64">i=d</span><span class="font18">+1</span></p>
<p><span class="font64">Hence, if the covariance has rank d, the eigenvalues Xd</span><span class="font18">+1</span><span class="font64"> to Xd are 0 and reconstruction error is </span><span class="font18">0</span><span class="font64">.</span></p>
<p><span class="font64">Furthermore, one can also show that the above solution can be obtained by maximizing the variances of the elements of h, under orthonormal W, instead of&#160;minimizing reconstruction error.</span></p>
<p><span class="font64">Linear factor models are some of the simplest generative models and some of the simplest models that learn a representation of data. Much as linear classifiers and&#160;linear regression models may be extended to deep feedforward networks, these linear&#160;factor models may be extended to autoencoder networks and deep probabilistic&#160;models that perform the same tasks but with a much more powerful and flexible&#160;model family.</span></p>
<p><a id="bookmark3"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font64"><sup></sup> See Sec. 3.8 for a discussion of the difference between uncorrelated variables and independent variables.</span></p>
</body>
</html>