<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h5><a id="bookmark0"></a><span class="font64" style="font-weight:bold;">5.7.3 Other Simple Supervised Learning Algorithms</span></h5>
<p><span class="font64">We have already briefly encountered another non-probabilistic supervised learning algorithm, nearest neighbor regression. More generally, k-nearest neighbors is&#160;a family of techniques that can be used for classification or regression. As a&#160;non-parametric learning algorithm, k-nearest neighbors is not restricted to a fixed&#160;number of parameters. We usually think of the k-nearest neighbors algorithm&#160;as not having any parameters, but rather implementing a simple function of the&#160;training data. In fact, there is not even really a training stage or learning process.&#160;Instead, at test time, when we want to produce an output y for a new test input x,&#160;we find the k-nearest neighbors to x in the training data X. We then return the&#160;average of the corresponding y values in the training set. This works for essentially&#160;any kind of supervised learning where we can define an average over y values. In&#160;the case of classification, we can average over one-hot code vectors c with c<sub>y</sub> = 1&#160;and c</span><span class="font64" style="font-weight:bold;font-style:italic;">i =</span><span class="font64"> 0 for all other values of i. We can then interpret the average over these&#160;one-hot codes as giving a probability distribution over classes. As a non-parametric&#160;learning algorithm, k-nearest neighbor can achieve very high capacity. For example,&#160;suppose we have a multiclass classification task and measure performance with 0-1&#160;loss. In this setting, 1-nearest neighbor converges to double the Bayes error as the&#160;number of training examples approaches infinity. The error in excess of the Bayes&#160;error results from choosing a single neighbor by breaking ties between equally&#160;distant neighbors randomly. When there is infinite training data, all test points x&#160;will have infinitely many training set neighbors at distance zero. If we allow the&#160;algorithm to use all of these neighbors to vote, rather than randomly choosing one&#160;of them, the procedure converges to the Bayes error rate. The high capacity of&#160;k-nearest neighbors allows it to obtain high accuracy given a large training set.&#160;However, it does so at high computational cost, and it may generalize very badly&#160;given a small, finite training set. One weakness of k-nearest neighbors is that it&#160;cannot learn that one feature is more discriminative than another. For example,&#160;imagine we have a regression task with x G R<sup>100</sup> drawn from an isotropic Gaussian&#160;distribution, but only a single variable x! is relevant to the output. Suppose&#160;further that this feature simply encodes the output directly, i.e. that y = x! in all&#160;cases. Nearest neighbor regression will not be able to detect this simple pattern.&#160;The nearest neighbor of most points x will be determined by the large number of&#160;features x</span><span class="font18">2</span><span class="font64"> through x </span><span class="font18">!00</span><span class="font64">, not by the lone feature x!. Thus the output on small&#160;training sets will essentially be random.</span></p><div><div><img src="main-46.jpg" alt=""/>
<p><span class="font64">Figure 5.7: Diagrams describing how a decision tree works. </span><span class="font64" style="font-style:italic;">(Top)</span><span class="font64"> Each node of the tree chooses to send the input example to the child node on the left (0) or or the child node on&#160;the right (1). Internal nodes are drawn as circles and leaf nodes as squares. Each node is&#160;displayed with a binary string identifier corresponding to its position in the tree, obtained&#160;by appending a bit to its parent identifier (0=choose left or top, 1=choose right or bottom).&#160;</span><span class="font64" style="font-style:italic;">(Bottom)</span><span class="font64"> The tree divides space into regions. The 2D plane shows how a decision tree&#160;might divide R<sup>2</sup>. The nodes of the tree are plotted in this plane, with each internal node&#160;drawn along the dividing line it uses to categorize examples, and leaf nodes drawn in the&#160;center of the region of examples they receive. The result is a piecewise-constant function,&#160;with one piece per leaf. Each leaf requires at least one training example to define, so it is&#160;not possible for the decision tree to learn a function that has more local maxima than the&#160;number of training examples.</span></p></div></div>
<p><span class="font64">Another type of learning algorithm that also breaks the input space into regions and has separate parameters for each region is the </span><span class="font64" style="font-weight:bold;font-style:italic;">decision tree</span><span class="font64"> (Breiman </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">1984) and its many variants. As shown in Fig. 5.7, each node of the decision tree&#160;is associated with a region in the input space, and internal nodes break that region&#160;into one sub-region for each child of the node (typically using an axis-aligned&#160;cut). Space is thus sub-divided into non-overlapping regions, with a one-to-one&#160;correspondence between leaf nodes and input regions. Each leaf node usually maps&#160;every point in its input region to the same output. Decision trees are usually&#160;trained with specialized algorithms that are beyond the scope of this book. The&#160;learning algorithm can be considered non-parametric if it is allowed to learn a tree&#160;of arbitrary size, though decision trees are usually regularized with size constraints&#160;that turn them into parametric models in practice. Decision trees as they are&#160;typically used, with axis-aligned splits and constant outputs within each node,&#160;struggle to solve some problems that are easy even for logistic regression. For&#160;example, if we have a two-class problem and the positive class occurs wherever&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font62" style="font-style:italic;">2</span><span class="font64"> &gt; x!, the decision boundary is not axis-aligned. The decision tree will thus&#160;need to approximate the decision boundary with many nodes, implementing a step&#160;function that constantly walks back and forth across the true decision function&#160;with axis-aligned steps.</span></p>
<p><span class="font64">As we have seen, nearest neighbor predictors and decision trees have many limitations. Nonetheless, they are useful learning algorithms when computational&#160;resources are constrained. We can also build intuition for more sophisticated&#160;learning algorithms by thinking about the similarities and differences between&#160;sophisticated algorithms and k-NN or decision tree baselines.</span></p>
<p><span class="font64">See Murphy (2012), Bishop (2006), Hastie </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2001) or other machine learning textbooks for more material on traditional supervised learning algorithms.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">5.8 Unsupervised Learning Algorithms</span></h4>
<p><span class="font64">Recall from Sec. 5.1.3 that unsupervised algorithms are those that experience only “features” but not a supervision signal. The distinction between supervised and&#160;unsupervised algorithms is not formally and rigidly defined because there is no&#160;objective test for distinguishing whether a value is a feature or a target provided by&#160;a supervisor. Informally, unsupervised learning refers to most attempts to extract&#160;information from a distribution that do not require human labor to annotate&#160;examples. The term is usually associated with density estimation, learning to&#160;draw samples from a distribution, learning to denoise data from some distribution,&#160;finding a manifold that the data lies near, or clustering the data into groups of</span></p>
<p><span class="font64">related examples.</span></p>
<p><span class="font64">A classic unsupervised learning task is to find the “best” representation of the data. By ‘best’ we can mean different things, but generally speaking we are looking&#160;for a representation that preserves as much information about x as possible while&#160;obeying some penalty or constraint aimed at keeping the representation </span><span class="font64" style="font-weight:bold;font-style:italic;">simpler</span><span class="font64"> or&#160;more accessible than x itself.</span></p>
<p><span class="font64">There are multiple ways of defining a </span><span class="font64" style="font-weight:bold;font-style:italic;">simpler</span><span class="font64"> representation. Three of the most common include lower dimensional representations, sparse representations&#160;and independent representations. Low-dimensional representations attempt to&#160;compress as much information about </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> as possible in a smaller representation.&#160;Sparse representations (Barlow, 1989; Olshausen and Field, 1996; Hinton and&#160;Ghahramani, 1997) embed the dataset into a representation whose entries are&#160;mostly zeroes for most inputs. The use of sparse representations typically requires&#160;increasing the dimensionality of the representation, so that the representation&#160;becoming mostly zeroes does not discard too much information. This results in an&#160;overall structure of the representation that tends to distribute data along the axes&#160;of the representation space. Independent representations attempt to </span><span class="font64" style="font-weight:bold;font-style:italic;">disentangle&#160;</span><span class="font64">the sources of variation underlying the data distribution such that the dimensions&#160;of the representation are statistically independent.</span></p>
<p><span class="font64">Of course these three criteria are certainly not mutually exclusive. Lowdimensional representations often yield elements that have fewer or weaker dependencies than the original high-dimensional data. This is because one way to reduce the size of a representation is to find and remove redundancies. Identifying&#160;and removing more redundancy allows the dimensionality reduction algorithm to&#160;achieve more compression while discarding less information.</span></p>
<p><span class="font64">The notion of representation is one of the central themes of deep learning and therefore one of the central themes in this book. In this section, we develop some&#160;simple examples of representation learning algorithms. Together, these example&#160;algorithms show how to operationalize all three of the criteria above. Most of the&#160;remaining chapters introduce additional representation learning algorithms that&#160;develop these criteria in different ways or introduce other criteria.</span></p><h5><a id="bookmark2"></a><span class="font64">5.8.1 Principal Components Analysis</span></h5>
<p><span class="font64">In Sec. 2.12, we saw that the principal components analysis algorithm provides a means of compressing data. We can also view PCA as an unsupervised learning&#160;algorithm that learns a representation of data. This representation is based on&#160;two of the criteria for a simple representation described above. PCA learns a</span></p>
<p><span class="font69">20</span></p>
<p><span class="font69">10</span></p>
<p><span class="font69"><sub>0</sub></span></p>
<p><span class="font69">-10</span></p>
<p><span class="font69">20</span></p>
<table border="1">
<tr><td>
<p><span class="font62">—1</span><span class="font29">-r</span></p></td><td style="vertical-align:bottom;">
<p><span class="font29">-</span><span class="font69">1</span><span class="font29">-</span><span class="font69">1</span><span class="font29">-</span><span class="font69">1</span><span class="font29">-</span></p>
<p><span class="font29">•</span></p></td><td style="vertical-align:middle;">
<p><span class="font69">20</span></p></td><td>
<p><span class="font69">—1</span><span class="font29">-</span><span class="font69">1</span><span class="font29">-</span><span class="font69">1</span><span class="font29">-</span><span class="font69">1</span><span class="font29">-</span><span class="font69">1</span><span class="font29">-</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p><span class="font29">•</span></p></td><td style="vertical-align:bottom;">
<p><span class="font69">10</span></p></td><td style="vertical-align:middle;">
<p><span class="font62">-</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font65" style="font-weight:bold;font-style:italic;">J</span></p></td><td style="vertical-align:bottom;">
<p><span class="font15" style="font-weight:bold;font-style:italic;">f</span></p></td><td style="vertical-align:middle;">
<p><span class="font69">0</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font15" style="font-weight:bold;font-style:italic;">t'</span></p></td><td>
<p><span class="font38" style="font-weight:bold;">r</span></p></td><td style="vertical-align:middle;">
<p><span class="font29">-</span><span class="font69">10</span></p></td><td style="vertical-align:middle;">
<p><span class="font29">-</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font29">_</span><span class="font69">1</span><span class="font29">_</span><span class="font62">l</span><span class="font29">_</span></p></td><td style="vertical-align:bottom;">
<p><span class="font29">_</span><span class="font69">1</span><span class="font29">_</span><span class="font69">1</span><span class="font29">_</span><span class="font69">1</span><span class="font29">_</span></p></td><td style="vertical-align:middle;">
<p><span class="font29">-</span><span class="font69">20</span></p></td><td style="vertical-align:bottom;">
<p><span class="font29">_</span><span class="font69">1</span><span class="font29">_</span><span class="font69">1</span><span class="font29">_</span><span class="font69">1</span><span class="font29">_</span><span class="font69">1</span><span class="font29">_</span><span class="font69">1</span><span class="font29">_</span></p></td></tr>
</table>
<p><span class="font69">10</span><span class="font29"> </span><span class="font69">20</span></p>
<p><span class="font69">-20</span><span class="font29"> </span><span class="font69">-10</span><span class="font29"> </span><span class="font69">0</span></p>
<p><span class="font62" style="font-style:italic;"><sup>z</sup>1</span></p>
<p><span class="font69">10</span><span class="font29"> </span><span class="font69">20</span></p>
<p><span class="font69">-20</span><span class="font29"> </span><span class="font69">-10</span><span class="font29"> </span><span class="font69">0 </span><span class="font62"><sup>x</sup>1</span></p>
<p><span class="font64">Figure 5.8: PCA learns a linear projection that aligns the direction of greatest variance with the axes of the new space. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> The original data consists of samples of x. In this&#160;space, the variance might occur along directions that are not axis-aligned. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> The&#160;transformed data z = x<sup>T</sup> W now varies most along the axis z!. The direction of second&#160;most variance is now along </span><span class="font64" style="font-style:italic;">z</span><span class="font19" style="font-style:italic;">2</span><span class="font64">.</span></p>
<p><span class="font64">representation that has lower dimensionality than the original input. It also learns a representation whose elements have no linear correlation with each other. This&#160;is a first step toward the criterion of learning representations whose elements are&#160;statistically independent. To achieve full independence, a representation learning&#160;algorithm must also remove the nonlinear relationships between variables.</span></p>
<p><span class="font64">PCA learns an orthogonal, linear transformation of the data that projects an input x to a representation z as shown in Fig. 5.8. In Sec. 2.12, we saw that we&#160;could learn a one-dimensional representation that best reconstructs the original&#160;data (in the sense of mean squared error) and that this representation actually&#160;corresponds to the first principal component of the data. Thus we can use PCA&#160;as a simple and effective dimensionality reduction method that preserves as much&#160;of the information in the data as possible (again, as measured by least-squares&#160;reconstruction error). In the following, we will study how the PCA representation&#160;decorrelates the original data representation X.</span></p>
<p><span class="font64">Let us consider the m </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> n-dimensional design matrix X. We will assume that the data has a mean of zero, E [x] = </span><span class="font64" style="font-weight:bold;">0</span><span class="font64">. If this is not the case, the data can easily&#160;be centered by subtracting the mean from all examples in a preprocessing step.</span></p>
<p><span class="font64">The unbiased sample covariance matrix associated with X is given by:</span></p><div>
<p><span class="font64">Var[x]</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div>
<p><span class="font64">X <sup>T</sup>X.</span></p></div><div>
<p><span class="font64">(5.85)</span></p></div>
<p><span class="font64">PCA finds a representation (through linear transformation) </span><span class="font64" style="font-weight:bold;font-style:italic;">z =</span><span class="font64"> x<sup>T</sup>W where Var[z] is diagonal.</span></p>
<p><span class="font64">In Sec. 2.12, we saw that the principal components of a design matrix X are given by the eigenvectors of X<sup>T</sup>X. From this view,</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sub>X</sub></span><span class="font64"><sup>T</sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>X</sub></span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">W</span><span class="font64"> AW</span></p></div><div>
<p><span class="font64">■T</span></p></div><div>
<p><span class="font64">(5.86)</span></p></div>
<p><span class="font64">In this section, we exploit an alternative derivation of the principal components. The principal components may also be obtained via the singular value decomposition.&#160;Specifically, they are the right singular vectors of X. To see this, let W be the&#160;right singular vectors in the decomposition X = USW<sup>T</sup>. We then recover the&#160;original eigenvector equation with W as the eigenvector basis:</span></p><div>
<p><span class="font64">(5.87)</span></p></div>
<p><span class="font64" style="font-variant:small-caps;">X <sup>T</sup>X = (u SW <sup>T</sup>J USW<sup>T</sup> = W S<sup>2</sup> W<sup>T</sup>.</span></p>
<p><span class="font64">The SVD is helpful to show that PCA results in a diagonal Var[z]. Using the SVD of X, we can express the variance of X as:</span></p><div>
<p><span class="font64">Var[x] =</span></p>
<table border="1">
<tr><td colspan="2" style="vertical-align:middle;">
<p><span class="font64">1</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font64">—</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td><td>
<p></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font64">—</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td><td>
<p></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font64">—</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">1</span></p></td><td>
<p></p></td></tr>
</table>
<p><span class="font64">X<sup>T</sup> X</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">m —</span><span class="font64"> 1</span></p></div><div>
<p><span class="font64">(U SW <sup>T</sup>)<sup>T</sup> U SW W S<sup>T</sup>U<sup>T</sup>U SW <sup>T&#160;</sup>W S<sup>2</sup>W<sup>T</sup>,</span></p></div><div>
<p><span class="font64">■T</span></p></div><div>
<p><span class="font64">(5.88)</span></p>
<p><span class="font64">(5.89)</span></p>
<p><span class="font64">(5.90)</span></p>
<p><span class="font64">(5.91)</span></p></div>
<p><span class="font64">where we use the fact that U<sup>T</sup> U = I because the U matrix of the singular value definition is defined to be orthonormal. This shows that if we take z = x<sup>T</sup>W, we&#160;can ensure that the covariance of z is diagonal as required:</span></p><div>
<table border="1">
<tr><td>
<p><span class="font64">Z <sup>T</sup>Z</span></p></td><td>
<p><span class="font64">(5.92)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">W <sup>T</sup>X <sup>T</sup> XW</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(5.93)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">W<sup>T</sup>WS<sup>2</sup>W<sup>T</sup>W</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(5.94)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">S <sup>2</sup>,</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(5.95)</span></p></td></tr>
</table></div>
<p><span class="font64">1</span></p><div>
<p><span class="font64">m — 1 1</span></p>
<p><span class="font64">m — 1 1</span></p>
<p><span class="font64">m — 1 1</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">m — 1</span></p></div>
<p><span class="font64">Var[z ]</span></p>
<p><span class="font64">where this time we use the fact that W<sup>T</sup>W = I, again from the definition of the SVD.</span></p>
<p><span class="font64">The above analysis shows that when we project the data </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">to </span><span class="font64" style="font-weight:bold;">z</span><span class="font64">, via the linear transformation </span><span class="font64" style="font-weight:bold;">W</span><span class="font64">, the resulting representation has a diagonal covariance matrix&#160;(as given by £<sup>2</sup>) which immediately implies that the individual elements of </span><span class="font64" style="font-weight:bold;">z </span><span class="font64">are&#160;mutually uncorrelated.</span></p>
<p><span class="font64">This ability of PCA to transform data into a representation where the elements are mutually uncorrelated is a very important property of PCA. It is a simple&#160;example of a representation that attempt to </span><span class="font64" style="font-weight:bold;">disentangle the unknown factors&#160;of variation </span><span class="font64">underlying the data. In the case of PCA, this disentangling takes&#160;the form of finding a rotation of the input space (described by </span><span class="font64" style="font-weight:bold;">W</span><span class="font64">) that aligns the&#160;principal axes of variance with the basis of the new representation space associated&#160;with </span><span class="font64" style="font-weight:bold;">z</span><span class="font64">.</span></p>
<p><span class="font64">While correlation is an important category of dependency between elements of the data, we are also interested in learning representations that disentangle more&#160;complicated forms of feature dependencies. For this, we will need more than what&#160;can be done with a simple linear transformation.</span></p><h5><a id="bookmark3"></a><span class="font64">5.8.2 &#160;&#160;&#160;k-means Clustering</span></h5>
<p><span class="font64">Another example of a simple representation learning algorithm is </span><span class="font64" style="font-weight:bold;font-style:italic;">k</span><span class="font64">-means clustering. The k-means clustering algorithm divides the training set into k different clusters&#160;of examples that are near each other. We can thus think of the algorithm as&#160;providing a k-dimensional one-hot code vector </span><span class="font64" style="font-weight:bold;">h </span><span class="font64">representing an input </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">. If </span><span class="font64" style="font-weight:bold;">x&#160;</span><span class="font64">belongs to cluster i, then </span><span class="font64" style="font-weight:bold;font-style:italic;">hi</span><span class="font64"> = 1 and all other entries of the representation </span><span class="font64" style="font-weight:bold;">h </span><span class="font64">are&#160;zero.</span></p>
<p><span class="font64">The one-hot code provided by k-means clustering is an example of a sparse representation, because the majority of its entries are zero for every input. Later,&#160;we will develop other algorithms that learn more flexible sparse representations,&#160;where more than one entry can be non-zero for each input </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">. One-hot codes&#160;are an extreme example of sparse representations that lose many of the benefits&#160;of a distributed representation. The one-hot code still confers some statistical&#160;advantages (it naturally conveys the idea that all examples in the same cluster are&#160;similar to each other) and it confers the computational advantage that the entire&#160;representation may be captured by a single integer.</span></p>
<p><span class="font64">The k-means algorithm works by initializing k different centroids </span><span class="font64" style="font-weight:bold;font-style:italic;">{^</span><span class="font64"><sup>(1)</sup>,..., to different values, then alternating between two different steps until convergence.&#160;In one step, each training example is assigned to cluster i, where i is the index of&#160;the nearest centroid </span><span class="font64" style="font-weight:bold;">^</span><span class="font64"><sup>(i)</sup>. In the other step, each centroid </span><span class="font64" style="font-weight:bold;">^</span><span class="font64"><sup>(i)</sup> is updated to the&#160;mean of all training examples </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(j)</sup> assigned to cluster i.</span></p>
<p><span class="font64">One difficulty pertaining to clustering is that the clustering problem is inherently ill-posed, in the sense that there is no single criterion that measures how well a&#160;clustering of the data corresponds to the real world. We can measure properties of&#160;the clustering such as the average Euclidean distance from a cluster centroid to the&#160;members of the cluster. This allows us to tell how well we are able to reconstruct&#160;the training data from the cluster assignments. We do not know how well the&#160;cluster assignments correspond to properties of the real world. Moreover, there&#160;may be many different clusterings that all correspond well to some property of&#160;the real world. We may hope to find a clustering that relates to one feature but&#160;obtain a different, equally valid clustering that is not relevant to our task. For&#160;example, suppose that we run two clustering algorithms on a dataset consisting of&#160;images of red trucks, images of red cars, images of gray trucks, and images of gray&#160;cars. If we ask each clustering algorithm to find two clusters, one algorithm may&#160;find a cluster of cars and a cluster of trucks, while another may find a cluster of&#160;red vehicles and a cluster of gray vehicles. Suppose we also run a third clustering&#160;algorithm, which is allowed to determine the number of clusters. This may assign&#160;the examples to four clusters, red cars, red trucks, gray cars, and gray trucks. This&#160;new clustering now at least captures information about both attributes, but it has&#160;lost information about similarity. Red cars are in a different cluster from gray&#160;cars, just as they are in a different cluster from gray trucks. The output of the&#160;clustering algorithm does not tell us that red cars are more similar to gray cars&#160;than they are to gray trucks. They are different from both things, and that is all&#160;we know.</span></p>
<p><span class="font64">These issues illustrate some of the reasons that we may prefer a distributed representation to a one-hot representation. A distributed representation could have&#160;two attributes for each vehicle—one representing its color and one representing&#160;whether it is a car or a truck. It is still not entirely clear what the optimal&#160;distributed representation is (how can the learning algorithm know whether the&#160;two attributes we are interested in are color and car-versus-truck rather than&#160;manufacturer and age?) but having many attributes reduces the burden on the&#160;algorithm to guess which single attribute we care about, and allows us to measure&#160;similarity between objects in a fine-grained way by comparing many attributes&#160;instead of just testing whether one attribute matches.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">5.9 Stochastic Gradient Descent</span></h4>
<p><span class="font64">Nearly all of deep learning is powered by one very important algorithm: </span><span class="font64" style="font-weight:bold;font-style:italic;">stochastic gradient descent</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">SGD.</span><span class="font64"> Stochastic gradient descent is an extension of the gradient</span></p>
<p><span class="font64">descent algorithm introduced in Sec. 4.3.</span></p>
<p><span class="font64">A recurring problem in machine learning is that large training sets are necessary for good generalization, but large training sets are also more computationally&#160;expensive.</span></p>
<p><span class="font64">The cost function used by a machine learning algorithm often decomposes as a sum over training examples of some per-example loss function. For example, the&#160;negative conditional log-likelihood of the training data can be written as</span></p><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">J</span><span class="font64">(6) </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> E*,^,״</span><span class="font64" style="font-weight:bold;font-style:italic;">L(x, y, 6) =</span><span class="font64"> </span><span class="font64" style="font-weight:bold;"><sub>m</sub> </span><span class="font64">£</span><span class="font64" style="font-weight:bold;font-variant:small-caps;">l</span><span class="font64" style="font-variant:small-caps;">(x« </span><span class="font64" style="font-weight:bold;">, y<sup>(</sup></span><span class="font64"><sup>i</sup></span><span class="font64" style="font-weight:bold;"><sup>)</sup>, </span><span class="font64">6)</span></p>
<p><span class="font10" style="font-style:italic;">lit</span></p></div><div>
<p><span class="font64">(5.96)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64">=1</span></p></div>
<p><span class="font64">where L is the per-example loss L(x, y, 6) = — logp(y | x; 6).</span></p>
<p><span class="font64">For these additive cost functions, gradient descent requires computing</span></p><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div>
<p><span class="font64">V« J(6) = &#160;&#160;&#160;V<sub>8</sub>L(x«y« , 6).</span></p>
<p><span class="font64">m</span></p></div><div>
<p><span class="font64">(5.97)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p></div>
<p><span class="font64">The computational cost of this operation is </span><span class="font64" style="font-weight:bold;font-style:italic;">O(m</span><span class="font64">). As the training set size grows to billions of examples, the time to take a single gradient step becomes prohibitively&#160;long.</span></p>
<p><span class="font64">The insight of stochastic gradient descent is that the gradient is an expectation. The expectation may be approximately estimated using a small set of samples.&#160;Specifically, on each step of the algorithm, we can sample a </span><span class="font64" style="font-weight:bold;font-style:italic;">minibatch</span><span class="font64"> of examples&#160;B = {x<sup>(1)</sup>,..., x<sup>(m)</sup>} drawn uniformly from the training set. The minibatch size&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">m<sup>1</sup></span><span class="font64"> is typically chosen to be a relatively small number of examples, ranging from&#160;1 to a few hundred. Crucially, </span><span class="font64" style="font-weight:bold;font-style:italic;">ml</span><span class="font64"> is usually held fixed as the training set size m&#160;grows. We may fit a training set with billions of examples using updates computed&#160;on only a hundred examples.</span></p>
<p><span class="font64">The estimate of the gradient is formed as</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">g</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div>
<p><span class="font64">— V</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>e</sub>T</span><span class="font64"> L(x<sup>(i)</sup>,y<sup>(i)</sup>,6). </span><span class="font31" style="font-style:italic;">m</span></p></div><div>
<p><span class="font64">(5.98)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p></div>
<p><span class="font64">using examples from the minibatch B. The stochastic gradient descent algorithm then follows the estimated gradient downhill:</span></p><div>
<p><span class="font64">(5.99)</span></p></div>
<p><span class="font64" style="font-weight:bold;">6 ^ 6 — eg,</span></p>
<p><span class="font64">where e is the learning rate.</span></p>
<p><span class="font64">Gradient descent in general has often been regarded as slow or unreliable. In the past, the application of gradient descent to non-convex optimization problems&#160;was regarded as foolhardy or unprincipled. Today, we know that the machine&#160;learning models described in Part II work very well when trained with gradient&#160;descent. The optimization algorithm may not be guaranteed to arrive at even a&#160;local minimum in a reasonable amount of time, but it often finds a very low value&#160;of the cost function quickly enough to be useful.</span></p>
<p><span class="font64">Stochastic gradient descent has many important uses outside the context of deep learning. It is the main way to train large linear models on very large&#160;datasets. For a fixed model size, the cost per SGD update does not depend on the&#160;training set size m. In practice, we often use a larger model as the training set size&#160;increases, but we are not forced to do so. The number of updates required to reach&#160;convergence usually increases with training set size. However, as m approaches&#160;infinity, the model will eventually converge to its best possible test error before&#160;SGD has sampled every example in the training set. Increasing m further will not&#160;extend the amount of training time needed to reach the model’s best possible test&#160;error. From this point of view, one can argue that the asymptotic cost of training&#160;a model with SGD is O( 1) as a function of m.</span></p>
<p><span class="font64">Prior to the advent of deep learning, the main way to learn nonlinear models was to use the kernel trick in combination with a linear model. Many kernel learning&#160;algorithms require constructing an m x m matrix </span><span class="font64" style="font-weight:bold;font-style:italic;">G<sub>i</sub>,j =</span><span class="font64"> k(x<sup>(i)</sup>, x<sup>(j)</sup>). Constructing&#160;this matrix has computational cost O (m<sup>2</sup>), which is clearly undesirable for datasets&#160;with billions of examples. In academia, starting in 2006, deep learning was&#160;initially interesting because it was able to generalize to new examples better&#160;than competing algorithms when trained on medium-sized datasets with tens of&#160;thousands of examples. Soon after, deep learning garnered additional interest in&#160;industry, because it provided a scalable way of training nonlinear models on large&#160;datasets.</span></p>
<p><span class="font64">Stochastic gradient descent and many enhancements to it are described further in Chapter 8.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">5.10 Building a Machine Learning Algorithm</span></h4>
<p><span class="font64">Nearly all deep learning algorithms can be described as particular instances of a fairly simple recipe: combine a specification of a dataset, a cost function, an&#160;optimization procedure and a model.</span></p>
<p><span class="font64">For example, the linear regression algorithm combines a dataset consisting of</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">X</span><span class="font64"> and </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">, the cost function</span></p>
<p><span class="font64"><sup>J</sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(w</sup>,b<sup>)</sup></span><span class="font64"><sup> &#160;&#160;&#160;E</sup>x,y~pd<sub>ata</sub>, <sup>10gp</sup>mode1 (<sup>y 1 </sup></span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>)</sup>,&#160;&#160;&#160;&#160;(5.<sup>10</sup>°)</span></p>
<p><span class="font64">the model specification</span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"><sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (</span><span class="font64" style="font-weight:bold;">y </span><span class="font64">| x) = </span><span class="font64" style="font-weight:bold;font-style:italic;">N(y; x<sup>T</sup>w</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">b,</span><span class="font64">1), and, in most cases, the optimization algorithm defined by solving for where the gradient of the cost is zero&#160;using the normal equations.</span></p>
<p><span class="font64">By realizing that we can replace any of these components mostly independently from the others, we can obtain a very wide variety of algorithms.</span></p>
<p><span class="font64">The cost function typically includes at least one term that causes the learning process to perform statistical estimation. The most common cost function is the&#160;negative log-likelihood, so that minimizing the cost function causes maximum&#160;likelihood estimation.</span></p>
<p><span class="font64">The cost function may also include additional terms, such as regularization terms. For example, we can add weight decay to the linear regression cost function&#160;to obtain</span></p>
<p><span class="font64"><sup>J (</sup></span><span class="font64" style="font-weight:bold;"><sup>w</sup></span><span class="font64"><sup>,6)</sup> = A<sup>||</sup></span><span class="font64" style="font-weight:bold;"><sup>w</sup></span><span class="font64"><sup>|12 -</sup> Ex,y~pdata <sup>10</sup>g Pmodel </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(</sup>V</span><span class="font64"> <sup>| </sup></span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>)</sup> • &#160;&#160;&#160;<sup>(5</sup>.<sup>101</sup>)</span></p>
<p><span class="font64">This still allows closed-form optimization.</span></p>
<p><span class="font64">If we change the model to be nonlinear, then most cost functions can no longer be optimized in closed form. This requires us to choose an iterative numerical&#160;optimization procedure, such as gradient descent.</span></p>
<p><span class="font64">The recipe for constructing a learning algorithm by combining models, costs, and optimization algorithms supports both supervised and unsupervised learning. The&#160;linear regression example shows how to support supervised learning. Unsupervised&#160;learning can be supported by defining a dataset that contains only </span><span class="font64" style="font-weight:bold;">X </span><span class="font64">and providing&#160;an appropriate unsupervised cost and model. For example, we can obtain the first&#160;PCA vector by specifying that our loss function is</span></p>
<p><span class="font64" style="font-weight:bold;"><sup>J(w)</sup> = </span><span class="font64">Ex</span><span class="font64" style="font-weight:bold;">^</span><span class="font64">pdata \\</span><span class="font64" style="font-weight:bold;"><sup>X </sup></span><span class="font64"><sup>- </sup></span><span class="font64" style="font-weight:bold;"><sup>r(X</sup>; <sup>W)</sup></span><span class="font64"><sup>||</sup>2 &#160;&#160;&#160;<sup>(5</sup>.<sup>102)</sup></span></p>
<p><span class="font64">while our model is defined to have </span><span class="font64" style="font-weight:bold;">w </span><span class="font64">with norm one and reconstruction function </span><span class="font64" style="font-weight:bold;font-style:italic;">r(x) = w</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">xw.</span></p>
<p><span class="font64">In some cases, the cost function may be a function that we cannot actually evaluate, for computational reasons. In these cases, we can still approximately&#160;minimize it using iterative numerical optimization so long as we have some way of&#160;approximating its gradients.</span></p>
<p><span class="font64">Most machine learning algorithms make use of this recipe, though it may not immediately be obvious. If a machine learning algorithm seems especially unique or&#160;hand-designed, it can usually be understood as using a special-case optimizer. Some&#160;models such as decision trees or k-means require special-case optimizers because&#160;their cost functions have flat regions that make them inappropriate for minimization&#160;by gradient-based optimizers. Recognizing that most machine learning algorithms&#160;can be described using this recipe helps to see the different algorithms as part of a&#160;taxonomy of methods for doing related tasks that work for similar reasons, rather&#160;than as a long list of algorithms that each have separate justifications.</span></p><h4><a id="bookmark6"></a><span class="font65" style="font-weight:bold;">5.11 Challenges Motivating Deep Learning</span></h4>
<p><span class="font64">The simple machine learning algorithms described in this chapter work very well on a wide variety of important problems. However, they have not succeeded in solving&#160;the central problems in AI, such as recognizing speech or recognizing objects.</span></p>
<p><span class="font64">The development of deep learning was motivated in part by the failure of traditional algorithms to generalize well on such AI tasks.</span></p>
<p><span class="font64">This section is about how the challenge of generalizing to new examples becomes exponentially more difficult when working with high-dimensional data, and how&#160;the mechanisms used to achieve generalization in traditional machine learning&#160;are insufficient to learn complicated functions in high-dimensional spaces. Such&#160;spaces also often impose high computational costs. Deep learning was designed to&#160;overcome these and other obstacles.</span></p><h5><a id="bookmark7"></a><span class="font64">5.11.1 The Curse of Dimensionality</span></h5>
<p><span class="font64">Many machine learning problems become exceedingly difficult when the number of dimensions in the data is high. This phenomenon is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">curse&#160;of dimensionality.</span><span class="font64"> Of particular concern is that the number of possible distinct&#160;configurations of a set of variables increases exponentially as the number of variables&#160;increases.</span></p><div><img src="main-47.jpg" alt=""/>
<p><span class="font64">Figure 5.9: As the number of relevant dimensions of the data increases (from left to right), the number of configurations of interest may grow exponentially. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> In this&#160;one-dimensional example, we have one variable for which we only care to distinguish 10&#160;regions of interest. With enough examples falling within each of these regions (each region&#160;corresponds to a cell in the illustration), learning algorithms can easily generalize correctly.&#160;A straightforward way to generalize is to estimate the value of the target function within&#160;each region (and possibly interpolate between neighboring regions). </span><span class="font64" style="font-style:italic;">(Center)</span><span class="font64"> With 2&#160;dimensions (center) it is more difficult to distinguish 10 different values of each variable.&#160;We need to keep track of up to 10x10=100 regions, and we need at least that many&#160;examples to cover all those regions. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> With 3 dimensions this grows to 10 = 1000&#160;regions and at least that many examples. For d dimensions and v values to be distinguished&#160;along each axis, we seem to need </span><span class="font64" style="font-style:italic;">O (v</span><span class="font64"><sup>d</sup>) regions and examples. This is an instance of the&#160;curse of dimensionality. Figure graciously provided by Nicolas Chapados.</span></p></div>
<p><span class="font64">The curse of dimensionality arises in many places in computer science, and especially so in machine learning.</span></p>
<p><span class="font64">One challenge posed by the curse of dimensionality is a statistical challenge. As illustrated in Fig. 5.9, a statistical challenge arises because the number of&#160;possible configurations of x is much larger than the number of training examples.&#160;To understand the issue, let us consider that the input space is organized into a&#160;grid, like in the figure. In low dimensions we can describe this space with a low&#160;number of grid cells that are mostly occupied by the data. When generalizing to a&#160;new data point, we can usually tell what to do simply by inspecting the training&#160;examples that lie in the same cell as the new input. For example, if estimating&#160;the probability density at some point x, we can just return the number of training&#160;examples in the same unit volume cell as x, divided by the total number of training&#160;examples. If we wish to classify an example, we can return the most common class&#160;of training examples in the same cell. If we are doing regression we can average&#160;the target values observed over the examples in that cell. But what about the&#160;cells for which we have seen no example? Because in high-dimensional spaces the&#160;number of configurations is going to be huge, much larger than our number of&#160;examples, most configurations will have no training example associated with it.</span></p>
<p><span class="font64">How could we possibly say something meaningful about these new configurations? Many traditional machine learning algorithms simply assume that the output at a&#160;new point should be approximately the same as the output at the nearest training&#160;point.</span></p><h5><a id="bookmark8"></a><span class="font64">5.11.2 Local Constancy and Smoothness Regularization</span></h5>
<p><span class="font64">In order to generalize well, machine learning algorithms need to be guided by prior beliefs about what kind of function they should learn. Previously, we have seen&#160;these priors incorporated as explicit beliefs in the form of probability distributions&#160;over parameters of the model. More informally, we may also discuss prior beliefs as&#160;directly influencing the </span><span class="font64" style="font-weight:bold;font-style:italic;">function</span><span class="font64"> itself and only indirectly acting on the parameters&#160;via their effect on the function. Additionally, we informally discuss prior beliefs as&#160;being expressed implicitly, by choosing algorithms that are biased toward choosing&#160;some class of functions over another, even though these biases may not be expressed&#160;(or even possible to express) in terms of a probability distribution representing our&#160;degree of belief in various functions.</span></p>
<p><span class="font64">Among the most widely used of these implicit “priors” is the </span><span class="font64" style="font-weight:bold;font-style:italic;">smoothness prior </span><span class="font64">or </span><span class="font64" style="font-weight:bold;font-style:italic;">local constancy prior.</span><span class="font64"> This prior states that the function we learn should not&#160;change very much within a small region.</span></p>
<p><span class="font64">Many simpler algorithms rely exclusively on this prior to generalize well, and as a result they fail to scale to the statistical challenges involved in solving AI-level tasks. Throughout this book, we will describe how deep learning introduces&#160;additional (explicit and implicit) priors in order to reduce the generalization&#160;error on sophisticated tasks. Here, we explain why the smoothness prior alone is&#160;insufficient for these tasks.</span></p>
<p><span class="font64">There are many different ways to implicitly or explicitly express a prior belief that the learned function should be smooth or locally constant. All of these different&#160;methods are designed to encourage the learning process to learn a function </span><span class="font64" style="font-weight:bold;font-style:italic;">f</span><span class="font64"> * that&#160;satisfies the condition</span></p>
<p><span class="font64">f* (x) - f *(x + e) &#160;&#160;&#160;(5.103)</span></p>
<p><span class="font64">for most configurations x and small change </span><span class="font64" style="font-weight:bold;font-style:italic;">e</span><span class="font64"> In other words, if we know a good answer for an input x (for example, if x is a labeled training example) then that&#160;answer is probably good in the neighborhood of x. If we have several good answers&#160;in some neighborhood we would combine them (by some form of averaging or&#160;interpolation) to produce an answer that agrees with as many of them as much as&#160;possible.</span></p>
<p><span class="font64">An extreme example of the local constancy approach is the k-nearest neighbors family of learning algorithms. These predictors are literally constant over each&#160;region containing all the points </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">that have the same set of k nearest neighbors in&#160;the training set. For k = 1, the number of distinguishable regions cannot be more&#160;than the number of training examples.</span></p>
<p><span class="font64">While the k-nearest neighbors algorithm copies the output from nearby training examples, most kernel machines interpolate between training set outputs associated&#160;with nearby training examples. An important class of kernels is the family of </span><span class="font64" style="font-weight:bold;font-style:italic;">local&#160;kernels</span><span class="font64"> where k(</span><span class="font64" style="font-weight:bold;">u</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) is large when </span><span class="font64" style="font-weight:bold;font-style:italic;">u = v</span><span class="font64"> and decreases as </span><span class="font64" style="font-weight:bold;">u </span><span class="font64">and </span><span class="font64" style="font-weight:bold;">v </span><span class="font64">grow farther&#160;apart from each other. A local kernel can be thought of as a similarity function&#160;that performs template matching, by measuring how closely a test example </span><span class="font64" style="font-weight:bold;">x&#160;</span><span class="font64">resembles each training example </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(i)</sup>. Much of the modern motivation for deep&#160;learning is derived from studying the limitations of local template matching and&#160;how deep models are able to succeed in cases where local template matching fails&#160;(Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006b).</span></p>
<p><span class="font64">Decision trees also suffer from the limitations of exclusively smoothness-based learning because they break the input space into as many regions as there are&#160;leaves and use a separate parameter (or sometimes many parameters for extensions&#160;of decision trees) in each region. If the target function requires a tree with at&#160;least n leaves to be represented accurately, then at least n training examples are&#160;required to fit the tree. A multiple of n is needed to achieve some level of statistical&#160;confidence in the predicted output.</span></p>
<p><span class="font64">In general, to distinguish O (k) regions in input space, all of these methods require O(k) examples. Typically there are O (k) parameters, with O (1) parameters&#160;associated with each of the O(k) regions. The case of a nearest neighbor scenario,&#160;where each training example can be used to define at most one region, is illustrated&#160;in Fig. 5.10.</span></p>
<p><span class="font64">Is there a way to represent a complex function that has many more regions to be distinguished than the number of training examples? Clearly, assuming&#160;only smoothness of the underlying function will not allow a learner to do that.&#160;For example, imagine that the target function is a kind of checkerboard. A&#160;checkerboard contains many variations but there is a simple structure to them.&#160;Imagine what happens when the number of training examples is substantially&#160;smaller than the number of black and white squares on the checkerboard. Based&#160;on only local generalization and the smoothness or local constancy prior, we would&#160;be guaranteed to correctly guess the color of a new point if it lies within the same&#160;checkerboard square as a training example. There is no guarantee that the learner&#160;could correctly extend the checkerboard pattern to points lying in squares that do&#160;not contain training examples. With this prior alone, the only information that an</span></p><div>
<p><span class="font14">o</span></p></div><div><div><img src="main-48.jpg" alt=""/>
<p><span class="font64">Figure 5.10: Illustration of how the nearest neighbor algorithm breaks up the input space into regions. An example (represented here by a circle) within each region defines the&#160;region boundary (represented here by the lines). They value associated with each example&#160;defines what the output should be for all points within the corresponding region. The&#160;regions defined by nearest neighbor matching form a geometric pattern called a Voronoi&#160;diagram. The number of these contiguous regions cannot grow faster than the number&#160;of training examples. While this figure illustrates the behavior of the nearest neighbor&#160;algorithm specifically, other machine learning algorithms that rely exclusively on the&#160;local smoothness prior for generalization exhibit similar behaviors: each training example&#160;only informs the learner about how to generalize in some neighborhood immediately&#160;surrounding that example.</span></p></div></div>
<p><span class="font64">example tells us is the color of its square, and the only way to get the colors of the entire checkerboard right is to cover each of its cells with at least one example.</span></p>
<p><span class="font64">The smoothness assumption and the associated non-parametric learning algorithms work extremely well so long as there are enough examples for the learning algorithm to observe high points on most peaks and low points on most valleys&#160;of the true underlying function to be learned. This is generally true when the&#160;function to be learned is smooth enough and varies in few enough dimensions.&#160;In high dimensions, even a very smooth function can change smoothly but in a&#160;different way along each dimension. If the function additionally behaves differently&#160;in different regions, it can become extremely complicated to describe with a set of&#160;training examples. If the function is complicated (we want to distinguish a huge&#160;number of regions compared to the number of examples), is there any hope to&#160;generalize well?</span></p>
<p><span class="font64">The answer to both of these questions is yes. The key insight is that a very large number of regions, e.g., </span><span class="font64" style="font-weight:bold;font-style:italic;">O(2</span><span class="font64"><sup>k</sup>), can be defined with </span><span class="font64" style="font-weight:bold;font-style:italic;">O(k)</span><span class="font64"> examples, so long&#160;as we introduce some dependencies between the regions via additional assumptions&#160;about the underlying data generating distribution. In this way, we can actually&#160;generalize non-locally (Bengio and Monperrus, 2005; Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006c). Many&#160;different deep learning algorithms provide implicit or explicit assumptions that are&#160;reasonable for a broad range of AI tasks in order to capture these advantages.</span></p>
<p><span class="font64">Other approaches to machine learning often make stronger, task-specific assumptions. For example, we could easily solve the checkerboard task by providing the assumption that the target function is periodic. Usually we do not include such&#160;strong, task-specific assumptions into neural networks so that they can generalize&#160;to a much wider variety of structures. AI tasks have structure that is much too&#160;complex to be limited to simple, manually specified properties such as periodicity,&#160;so we want learning algorithms that embody more general-purpose assumptions.&#160;The core idea in deep learning is that we assume that the data was generated&#160;by the </span><span class="font64" style="font-weight:bold;">composition of factors </span><span class="font64">or features, potentially at multiple levels in a&#160;hierarchy. Many other similarly generic assumptions can further improve deep&#160;learning algorithms. These apparently mild assumptions allow an exponential gain&#160;in the relationship between the number of examples and the number of regions&#160;that can be distinguished. These exponential gains are described more precisely in&#160;Sec. 6.4.1, Sec. 15.4, and Sec. 15.5. The exponential advantages conferred by the&#160;use of deep, distributed representations counter the exponential challenges posed&#160;by the curse of dimensionality.</span></p><h5><a id="bookmark9"></a><span class="font64">5.11.3 Manifold Learning</span></h5>
<p><span class="font64">An important concept underlying many ideas in machine learning is that of a manifold.</span></p>
<p><span class="font64">A </span><span class="font64" style="font-weight:bold;font-style:italic;">manifold</span><span class="font64"> is a connected region. Mathematically, it is a set of points, associated with a neighborhood around each point. From any given point, the manifold locally&#160;appears to be a Euclidean space. In everyday life, we experience the surface of the&#160;world as a 2-D plane, but it is in fact a spherical manifold in 3-D space.</span></p>
<p><span class="font64">The definition of a neighborhood surrounding each point implies the existence of transformations that can be applied to move on the manifold from one position&#160;to a neighboring one. In the example of the world’s surface as a manifold, one can&#160;walk north, south, east, or west.</span></p>
<p><span class="font64">Although there is a formal mathematical meaning to the term “manifold,” in machine learning it tends to be used more loosely to designate a connected&#160;set of points that can be approximated well by considering only a small number&#160;of degrees of freedom, or dimensions, embedded in a higher-dimensional space.&#160;Each dimension corresponds to a local direction of variation. See Fig. 5.11 for an&#160;example of training data lying near a one-dimensional manifold embedded in twodimensional space. In the context of machine learning, we allow the dimensionality&#160;of the manifold to vary from one point to another. This often happens when a&#160;manifold intersects itself. For example, a figure eight is a manifold that has a single&#160;dimension in most places but two dimensions at the intersection at the center.</span></p><div><img src="main-49.jpg" alt=""/>
<p><span class="font64">Figure 5.11: Data sampled from a distribution in a two-dimensional space that is actually concentrated near a one-dimensional manifold, like a twisted string. The solid line indicates&#160;the underlying manifold that the learner should infer.</span></p></div>
<p><span class="font64">Many machine learning problems seem hopeless if we expect the machine learning algorithm to learn functions with interesting variations across all of&#160;R<sup>n</sup>. </span><span class="font64" style="font-weight:bold;font-style:italic;">Manifold learning</span><span class="font64"> algorithms surmount this obstacle by assuming that most&#160;of R<sup>n</sup> consists of invalid inputs, and that interesting inputs occur only along&#160;a collection of manifolds containing a small subset of points, with interesting&#160;variations in the output of the learned function occurring only along directions&#160;that lie on the manifold, or with interesting variations happening only when we&#160;move from one manifold to another. Manifold learning was introduced in the case&#160;of continuous-valued data and the unsupervised learning setting, although this&#160;probability concentration idea can be generalized to both discrete data and the&#160;supervised learning setting: the key assumption remains that probability mass is&#160;highly concentrated.</span></p>
<p><span class="font64">The assumption that the data lies along a low-dimensional manifold may not always be correct or useful. We argue that in the context of AI tasks, such as&#160;those that involve processing images, sounds, or text, the manifold assumption is&#160;at least approximately correct. The evidence in favor of this assumption consists&#160;of two categories of observations.</span></p>
<p><span class="font64">The first observation in favor of the </span><span class="font64" style="font-weight:bold;font-style:italic;">manifold hypothesis</span><span class="font64"> is that the probability distribution over images, text strings, and sounds that occur in real life is highly&#160;concentrated. Uniform noise essentially never resembles structured inputs from&#160;these domains. Fig. 5.12 shows how, instead, uniformly sampled points look like the&#160;patterns of static that appear on analog television sets when no signal is available.&#160;Similarly, if you generate a document by picking letters uniformly at random, what&#160;is the probability that you will get a meaningful English-language text? Almost&#160;zero, again, because most of the long sequences of letters do not correspond to a&#160;natural language sequence: the distribution of natural language sequences occupies&#160;a very small volume in the total space of sequences of letters.</span></p><div><img src="main-50.jpg" alt=""/>
<p><span class="font64">Figure 5.12: Sampling images uniformly at random (by randomly picking each pixel according to a uniform distribution) gives rise to noisy images. Although there is a nonzero probability to generate an image of a face or any other object frequently encountered&#160;in AI applications, we never actually observe this happening in practice. This suggests&#160;that the images encountered in AI applications occupy a negligible proportion of the&#160;volume of image space.</span></p></div>
<p><span class="font64">Of course, concentrated probability distributions are not sufficient to show that the data lies on a reasonably small number of manifolds. We must also&#160;establish that the examples we encounter are connected to each other by other&#160;examples, with each example surrounded by other highly similar examples that&#160;may be reached by applying transformations to traverse the manifold. The second&#160;argument in favor of the manifold hypothesis is that we can also imagine such&#160;neighborhoods and transformations, at least informally. In the case of images, we&#160;can certainly think of many possible transformations that allow us to trace out a&#160;manifold in image space: we can gradually dim or brighten the lights, gradually&#160;move or rotate objects in the image, gradually alter the colors on the surfaces of&#160;objects, etc. It remains likely that there are multiple manifolds involved in most&#160;applications. For example, the manifold of images of human faces may not be&#160;connected to the manifold of images of cat faces.</span></p>
<p><span class="font64">These thought experiments supporting the manifold hypotheses convey some intuitive reasons supporting it. More rigorous experiments (Cayton, 2005; Narayanan and Mitter, 2010; Scholkopf </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1998; Roweis and Saul, 2000; Tenenbaum </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2000; Brand, 2003; Belkin and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger&#160;and Saul, 2004) clearly support the hypothesis for a large class of datasets of&#160;interest in AI.</span></p>
<p><span class="font64">When the data lies on a low-dimensional manifold, it can be most natural for machine learning algorithms to represent the data in terms of coordinates on&#160;the manifold, rather than in terms of coordinates in R<sup>n</sup>. In everyday life, we can&#160;think of roads as 1-D manifolds embedded in 3-D space. We give directions to&#160;specific addresses in terms of address numbers along these 1-D roads, not in terms&#160;of coordinates in 3-D space. Extracting these manifold coordinates is challenging,&#160;but holds the promise to improve many machine learning algorithms. This general&#160;principle is applied in many contexts. Fig. 5.13 shows the manifold structure of a&#160;dataset consisting of faces. By the end of this book, we will have developed the&#160;methods necessary to learn such a manifold structure. In Fig. 20.6, we will see&#160;how a machine learning algorithm can successfully accomplish this goal.</span></p>
<p><span class="font64">This concludes Part I, which has provided the basic concepts in mathematics and machine learning which are employed throughout the remaining parts of the&#160;book. You are now prepared to embark upon your study of deep learning.</span></p>
<p><span class="font67" style="font-weight:bold;font-variant:small-caps;">ex rrr r &#160;&#160;&#160;ר</span></p>
<p><span class="font67" style="font-weight:bold;">E’rrr r r</span></p>
<p><span class="font66" style="font-weight:bold;">cfc’fE־ t' t r fctE3ssaaa;2.:2.3:*1 </span><span class="font67">cttxtrttttBsaaaaajjaa</span></p>
<p><span class="font37" style="font-weight:bold;">cfKKKaftH</span><span class="font37" style="font-weight:bold;text-decoration:underline;">MMMaaa</span><span class="font37" style="font-weight:bold;">aaaMa</span></p>
<p><span class="font66" style="font-weight:bold;">KKKKKh &#160;&#160;&#160;:■■■BIKiliiiii</span></p>
<p><span class="font64">Figure 5.13: Training examples from the QMUL Multiview Face Dataset (Gong </span><span class="font64" style="font-style:italic;">et al</span><span class="font64">2000) for which the subjects were asked to move in such a way as to cover the two-dimensional&#160;manifold corresponding to two angles of rotation. We would like learning algorithms to&#160;be able to discover and disentangle such manifold coordinates. Fig. 20.6 illustrates such a&#160;feat.</span></p>
<p><span class="font66" style="font-weight:bold;">Part II</span></p>
<p><span class="font67" style="font-weight:bold;">Deep Networks: Modern Practices</span></p>
<p><span class="font64">This part of the book summarizes the state of modern deep learning as it is used to solve practical applications.</span></p>
<p><span class="font64">Deep learning has a long history and many aspirations. Several approaches have been proposed that have yet to entirely bear fruit. Several ambitious goals&#160;have yet to be realized. These less-developed branches of deep learning appear in&#160;the final part of the book.</span></p>
<p><span class="font64">This part focuses only on those approaches that are essentially working technologies that are already used heavily in industry.</span></p>
<p><span class="font64">Modern deep learning provides a very powerful framework for supervised learning. By adding more layers and more units within a layer, a deep network can&#160;represent functions of increasing complexity. Most tasks that consist of mapping an&#160;input vector to an output vector, and that are easy for a person to do rapidly, can&#160;be accomplished via deep learning, given sufficiently large models and sufficiently&#160;large datasets of labeled training examples. Other tasks, that can not be described&#160;as associating one vector to another, or that are difficult enough that a person&#160;would require time to think and reflect in order to accomplish the task, remain&#160;beyond the scope of deep learning for now.</span></p>
<p><span class="font64">This part of the book describes the core parametric function approximation technology that is behind nearly all modern practical applications of deep learning.&#160;We begin by describing the feedforward deep network model that is used to&#160;represent these functions. Next, we present advanced techniques for regularization&#160;and optimization of such models. Scaling these models to large inputs such as high&#160;resolution images or long temporal sequences requires specialization. We introduce&#160;the convolutional network for scaling to large images and the recurrent neural&#160;network for processing temporal sequences. Finally, we present general guidelines&#160;for the practical methodology involved in designing, building, and configuring an&#160;application involving deep learning, and review some of the applications of deep&#160;learning.</span></p>
<p><span class="font64">These chapters are the most important for a practitioner—someone who wants to begin implementing and using deep learning algorithms to solve real-world&#160;problems today.</span></p>
</body>
</html>