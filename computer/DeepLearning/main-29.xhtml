<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h4><a id="bookmark0"></a><span class="font65" style="font-weight:bold;">15.5 Exponential Gains from Depth</span></h4>
<p><span class="font64">We have seen in Sec. 6.4.1 that multilayer perceptrons are universal approximators, and that some functions can be represented by exponentially smaller deep networks&#160;compared to shallow networks. This decrease in model size leads to improved&#160;statistical efficiency. In this section, we describe how similar results apply more&#160;generally to other kinds of models with distributed hidden representations.</span></p>
<p><span class="font64">In Sec. 15.4, we saw an example of a generative model that learned about the explanatory factors underlying images of faces, including the person’s gender and&#160;whether they are wearing glasses. The generative model that accomplished this&#160;task was based on a deep neural network. It would not be reasonable to expect a&#160;shallow network, such as a linear network, to learn the complicated relationship&#160;between these abstract explanatory factors and the pixels in the image. In this and&#160;other AI tasks, the factors that can be chosen almost independently in order to&#160;generate data are more likely to be very high-level and related in highly nonlinear&#160;ways to the input. We argue that this demands </span><span class="font64" style="font-weight:bold;">deep </span><span class="font64">distributed representations,&#160;where the higher level features (seen as functions of the input) or factors (seen as&#160;generative causes) are obtained through the composition of many nonlinearities.</span></p>
<p><span class="font64">It has been proven in many different settings that organizing computation through the composition of many nonlinearities and a hierarchy of reused features&#160;can give an exponential boost to statistical efficiency, on top of the exponential&#160;boost given by using a distributed representation. Many kinds of networks (e.g.,&#160;with saturating nonlinearities, Boolean gates, sum/products, or RBF units) with&#160;a single hidden layer can be shown to be universal approximators. A model&#160;family that is a universal approximator can approximate a large class of functions&#160;(including all continuous functions) up to any non-zero tolerance level, given enough&#160;hidden units. However, the required number of hidden units may be very large.&#160;Theoretical results concerning the expressive power of deep architectures state that&#160;there are families of functions that can be represented efficiently by an architecture&#160;of depth k, but would require an exponential number of hidden units (with respect&#160;to the input size) with insufficient depth (depth </span><span class="font18">2</span><span class="font64"> or depth k — </span><span class="font18">1</span><span class="font64">).</span></p>
<p><span class="font64">In Sec. 6.4.1, we saw that deterministic feedforward networks are universal approximators of functions. Many structured probabilistic models with a single&#160;hidden layer of latent variables, including restricted Boltzmann machines and deep&#160;belief networks, are universal approximators of probability distributions (Le Roux&#160;and Bengio, 2008, 2010; Montufar and Ay, 2011; Montufar, 2014; Krause </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2013).</span></p>
<p><span class="font64">In Sec. 6.4.1, we saw that a sufficiently deep feedforward network can have an exponential advantage over a network that is too shallow. Such results can also&#160;be obtained for other models such as probabilistic models. One such probabilistic&#160;model is the </span><span class="font64" style="font-weight:bold;font-style:italic;">sum-product network</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">SPN</span><span class="font64"> (Poon and Domingos, 2011). These&#160;models use polynomial circuits to compute the probability distribution over a&#160;set of random variables. Delalleau and Bengio (2011) showed that there exist&#160;probability distributions for which a minimum depth of SPN is required to avoid&#160;needing an exponentially large model. Later, Martens and Medabalimi (2014)&#160;showed that there are significant differences between every two finite depths of&#160;SPN, and that some of the constraints used to make SPNs tractable may limit&#160;their representational power.</span></p>
<p><span class="font64">Another interesting development is a set of theoretical results for the expressive power of families of deep circuits related to convolutional nets, highlighting an&#160;exponential advantage for the deep circuit even when the shallow circuit is allowed&#160;to only approximate the function computed by the deep circuit (Cohen </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span></p>
<p><span class="font64">2015). By comparison, previous theoretical work made claims regarding only the case where the shallow circuit must exactly replicate particular functions.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">15.6 Providing Clues to Discover Underlying Causes</span></h4>
<p><span class="font64">To close this chapter, we come back to one of our original questions: what makes one representation better than another? One answer, first introduced in Sec. 15.3,&#160;is that an ideal representation is one that disentangles the underlying causal factors&#160;of variation that generated the data, especially those factors that are relevant to our&#160;applications. Most strategies for representation learning are based on introducing&#160;clues that help the learning to find these underlying factors of variations. The clues&#160;can help the learner separate these observed factors from the others. Supervised&#160;learning provides a very strong clue: a label y, presented with each x, that usually&#160;specifies the value of at least one of the factors of variation directly. More generally,&#160;to make use of abundant unlabeled data, representation learning makes use of&#160;other, less direct, hints about the underlying factors. These hints take the form of&#160;implicit prior beliefs that we, the designers of the learning algorithm, impose in&#160;order to guide the learner. Results such as the no free lunch theorem show that&#160;regularization strategies are necessary to obtain good generalization. While it is&#160;impossible to find a universally superior regularization strategy, one goal of deep&#160;learning is to find a set of fairly generic regularization strategies that are applicable&#160;to a wide variety of AI tasks, similar to the tasks that people and animals are able&#160;to solve.</span></p>
<p><span class="font64">We provide here a list of these generic regularization strategies. The list is clearly not exhaustive, but gives some concrete examples of ways that learning&#160;algorithms can be encouraged to discover features that correspond to underlying&#160;factors. This list was introduced in Sec. 3.1 of Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013d) and has been&#160;partially expanded here.</span></p>
<p><span class="font64">•</span><span class="font64" style="font-weight:bold;"> Smoothness</span><span class="font64">: This is the assumption that f (</span><span class="font64" style="font-weight:bold;">x </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;font-style:italic;">ed)</span><span class="font64"> « f (</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) for unit </span><span class="font64" style="font-weight:bold;font-style:italic;">d </span><span class="font64">and small e. This assumption allows the learner to generalize from training&#160;examples to nearby points in input space. Many machine learning algorithms&#160;leverage this idea, but it is insufficient to overcome the curse of dimensionality.</span></p>
<p><span class="font64">•</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;Linearity</span><span class="font64">: Many learning algorithms assume that relationships between&#160;some variables are linear. This allows the algorithm to make predictions even&#160;very far from the observed data, but can sometimes lead to overly extreme&#160;predictions. Most simple machine learning algorithms that do not make the&#160;smoothness assumption instead make the linearity assumption. These are&#160;in fact different assumptions—linear functions with large weights applied&#160;to high-dimensional spaces may not be very smooth. See Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(2014b) for a further discussion of the limitations of the linearity assumption.</span></p>
<p><span class="font64">•</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;Multiple explanatory factors</span><span class="font64">: Many representation learning algorithms&#160;are motivated by the assumption that the data is generated by multiple&#160;underlying explanatory factors, and that most tasks can be solved easily&#160;given the state of each of these factors. Sec. 15.3 describes how this view&#160;motivates semi-supervised learning via representation learning. Learning&#160;the structure of p( </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) requires learning some of the same features that are&#160;useful for modeling p(</span><span class="font64" style="font-weight:bold;">y </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) because both refer to the same underlying&#160;explanatory factors. Sec. 15.4 describes how this view motivates the use of&#160;distributed representations, with separate directions in representation space&#160;corresponding to separate factors of variation.</span></p>
<p><span class="font64">•</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;Causal factors</span><span class="font64">: the model is constructed in such a way that it treats the&#160;factors of variation described by the learned representation </span><span class="font64" style="font-weight:bold;">h </span><span class="font64">as the causes&#160;of the observed data </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">, and not vice-versa. As discussed in Sec. 15.3, this&#160;is advantageous for semi-supervised learning and makes the learned model&#160;more robust when the distribution over the underlying causes changes or&#160;when we use the model for a new task.</span></p>
<p><span class="font64">•</span><span class="font64" style="font-weight:bold;"> Depth</span><span class="font64">, or </span><span class="font64" style="font-weight:bold;">a hierarchical organization of explanatory factors</span><span class="font64">: High-</span></p>
<p><span class="font64">level, abstract concepts can be defined in terms of simple concepts, forming a hierarchy. From another point of view, the use of a deep architecture expresses&#160;our belief that the task should be accomplished via a multi-step program,&#160;with each step referring back to the output of the processing accomplished&#160;via previous steps.</span></p>
<p><span class="font64">•</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;Shared factors across tasks</span><span class="font64">: In the context where we have many tasks,&#160;corresponding to different y^ variables sharing the same input </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">or where&#160;each task is associated with a subset or a function f<sup>(i)</sup>( </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) of a global input&#160;</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">, the assumption is that each y^ is associated with a different subset from a&#160;common pool of relevant factors </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">. Because these subsets overlap, learning&#160;all the P(y</span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) via a shared intermediate representation </span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">h </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) allows&#160;sharing of statistical strength between the tasks.</span></p>
<p><span class="font64">•</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;Manifolds</span><span class="font64">: Probability mass concentrates, and the regions in which it concentrates are locally connected and occupy a tiny volume. In the continuous&#160;case, these regions can be approximated by low-dimensional manifolds with&#160;a much smaller dimensionality than the original space where the data lives.&#160;Many machine learning algorithms behave sensibly only on this manifold&#160;(Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014b). Some machine learning algorithms, especially&#160;autoencoders, attempt to explicitly learn the structure of the manifold.</span></p>
<p><span class="font64">•</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;Natural clustering</span><span class="font64">: Many machine learning algorithms assume that each&#160;connected manifold in the input space may be assigned to a single class. The&#160;data may lie on many disconnected manifolds, but the class remains constant&#160;within each one of these. This assumption motivates a variety of learning&#160;algorithms, including tangent propagation, double backprop, the manifold&#160;tangent classifier and adversarial training.</span></p>
<p><span class="font64">•</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;Temporal and spatial coherence</span><span class="font64">: Slow feature analysis and related&#160;algorithms make the assumption that the most important explanatory factors&#160;change slowly over time, or at least that it is easier to predict the true&#160;underlying explanatory factors than to predict raw observations such as pixel&#160;values. See Sec. 13.3 for further description of this approach.</span></p>
<p><span class="font64">•</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;Sparsity</span><span class="font64">: Most features should presumably not be relevant to describing&#160;most inputs—there is no need to use a feature that detects elephant trunks&#160;when representing an image of a cat. It is therefore reasonable to impose a&#160;prior that any feature that can be interpreted as “present” or “absent” should&#160;be absent most of the time.</span></p>
<p><span class="font64">•</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;Simplicity of Factor Dependencies</span><span class="font64">: In good high-level representations,</span></p>
<p><span class="font64">the factors are related to each other through simple dependencies. The simplest possible is marginal independence, P(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64">) =&#160;&#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>i</sub> P</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sub>i</sub>), but linear&#160;dependencies or those captured by a shallow autoencoder are also reasonable&#160;assumptions. This can be seen in many laws of physics, and is assumed&#160;when plugging a linear predictor or a factorized prior on top of a learned&#160;representation.</span></p>
<p><span class="font64">The concept of representation learning ties together all of the many forms of deep learning. Feedforward and recurrent networks, autoencoders and deep&#160;probabilistic models all learn and exploit representations. Learning the best&#160;possible representation remains an exciting avenue of research.</span></p>
</body>
</html>