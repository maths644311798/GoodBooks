<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 10</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Sequence Modeling: Recurrent and Recursive Nets</span></h2>
<p><span class="font64" style="font-style:italic;">Recurrent neural networks</span><span class="font64"> or </span><span class="font64" style="font-style:italic;">RNNs</span><span class="font64"> (Rumelhart </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 1986a) are a family of neural networks for processing sequential data. Much as a convolutional network&#160;is a neural network that is specialized for processing a grid of values X such as&#160;an image, a recurrent neural network is a neural network that is specialized for&#160;processing a sequence of values x<sup>(1)</sup></span><span class="font64" style="font-variant:small-caps;">,...,x<sup>(t)</sup></span><span class="font64">. Just as convolutional networks&#160;can readily scale to images with large width and height, and some convolutional&#160;networks can process images of variable size, recurrent networks can scale to much&#160;longer sequences than would be practical for networks without sequence-based&#160;specialization. Most recurrent networks can also process sequences of variable&#160;length.</span></p>
<p><span class="font64">To go from multi-layer networks to recurrent networks, we need to take advantage of one of the early ideas found in machine learning and statistical models of the 1980s: sharing parameters across different parts of a model. Parameter sharing&#160;makes it possible to extend and apply the model to examples of different forms&#160;(different lengths, here) and generalize across them. If we had separate parameters&#160;for each value of the time index, we could not generalize to sequence lengths not&#160;seen during training, nor share statistical strength across different sequence lengths&#160;and across different positions in time. Such sharing is particularly important when&#160;a specific piece of information can occur at multiple positions within the sequence.&#160;For example, consider the two sentences “I went to Nepal in 2009” and “In 2009,&#160;I went to Nepal.” If we ask a machine learning model to read each sentence and&#160;extract the year in which the narrator went to Nepal, we would like it to recognize&#160;the year 2009 as the relevant piece of information, whether it appears in the sixth&#160;word or the second word of the sentence. Suppose that we trained a feedforward&#160;network that processes sentences of fixed length. A traditional fully connected&#160;feedforward network would have separate parameters for each input feature, so it&#160;would need to learn all of the rules of the language separately at each position in&#160;the sentence. By comparison, a recurrent neural network shares the same weights&#160;across several time steps.</span></p>
<p><span class="font64">A related idea is the use of convolution across a 1-D temporal sequence. This convolutional approach is the basis for time-delay neural networks (Lang and&#160;Hinton, 1988; Waibel </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1989; Lang </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1990). The convolution operation&#160;allows a network to share parameters across time, but is shallow. The output&#160;of convolution is a sequence where each member of the output is a function of&#160;a small number of neighboring members of the input. The idea of parameter&#160;sharing manifests in the application of the same convolution kernel at each time&#160;step. Recurrent networks share parameters in a different way. Each member of the&#160;output is a function of the previous members of the output. Each member of the&#160;output is produced using the same update rule applied to the previous outputs.&#160;This recurrent formulation results in the sharing of parameters through a very&#160;deep computational graph.</span></p>
<p><span class="font64">For the simplicity of exposition, we refer to RNNs as operating on a sequence that contains vectors with the time step index t ranging from 1 to </span><span class="font63" style="font-variant:small-caps;">t</span><span class="font64">. In&#160;practice, recurrent networks usually operate on minibatches of such sequences,&#160;with a different sequence length </span><span class="font63" style="font-variant:small-caps;">t </span><span class="font64">for each member of the minibatch. We have&#160;omitted the minibatch indices to simplify notation. Moreover, the time step index&#160;need not literally refer to the passage of time in the real world, but only to the&#160;position in the sequence. RNNs may also be applied in two dimensions across&#160;spatial data such as images, and even when applied to data involving time, the&#160;network may have connections that go backwards in time, provided that the entire&#160;sequence is observed before it is provided to the network.</span></p>
<p><span class="font64">This chapter extends the idea of a computational graph to include cycles. These cycles represent the influence of the present value of a variable on its own value&#160;at a future time step. Such computational graphs allow us to define recurrent&#160;neural networks. We then describe many different ways to construct, train, and&#160;use recurrent neural networks.</span></p>
<p><span class="font64">For more information on recurrent neural networks than is available in this chapter, we refer the reader to the textbook of Graves (2012).</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">10.1 Unfolding Computational Graphs</span></h4>
<p><span class="font64">A computational graph is a way to formalize the structure of a set of computations, such as those involved in mapping inputs and parameters to outputs and loss.&#160;Please refer to Sec. 6.5.1 for a general introduction. In this section we explain&#160;the idea of </span><span class="font64" style="font-weight:bold;font-style:italic;">unfolding</span><span class="font64"> a recursive or recurrent computation into a computational&#160;graph that has a repetitive structure, typically corresponding to a chain of events.&#160;Unfolding this graph results in the sharing of parameters across a deep network&#160;structure.</span></p>
<p><span class="font64">For example, consider the classical form of a dynamical system:</span></p>
<p><span class="font64" style="font-weight:bold;">s</span><span class="font64"><sup>(t)</sup> = f (</span><span class="font64" style="font-weight:bold;">s</span><span class="font64"><sup>(t</sup></span><span class="font64" style="font-weight:bold;"><sup>-</sup></span><span class="font64"><sup>1)</sup>; </span><span class="font64" style="font-weight:bold;">e</span><span class="font64">), &#160;&#160;&#160;(10.1)</span></p>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;">s</span><span class="font64"><sup>(t)</sup> is called the state of the system.</span></p>
<p><span class="font64">Eq. 10.1 is recurrent because the definition of </span><span class="font64" style="font-weight:bold;">s </span><span class="font64">at time t refers back to the same definition at time t — 1.</span></p>
<p><span class="font64">For a finite number of time steps </span><span class="font63" style="font-variant:small-caps;">t</span><span class="font64">, the graph can be unfolded by applying the definition </span><span class="font63" style="font-variant:small-caps;">t </span><span class="font64">— 1 times. For example, if we unfold Eq. 10.1 for </span><span class="font63" style="font-variant:small-caps;">t </span><span class="font64">= 3 time steps, we&#160;obtain</span></p><div>
<p><span class="font64" style="font-weight:bold;">s</span></p></div><div>
<p><span class="font64"><sup>(3)</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">=f</span><span class="font64"> (</span><span class="font64" style="font-weight:bold;">s</span><span class="font64"><sup>(2)</sup>; </span><span class="font64" style="font-weight:bold;">e</span><span class="font64">)</span></p>
<p><span class="font64">=f (f (</span><span class="font64" style="font-weight:bold;">s</span><span class="font64"><sup>(1)</sup>; </span><span class="font64" style="font-weight:bold;">e</span><span class="font64">); </span><span class="font64" style="font-weight:bold;">e</span><span class="font64">)</span></p></div><div>
<p><span class="font64">(10.2)</span></p>
<p><span class="font64">(10.3)</span></p></div>
<p><span class="font64">Unfolding the equation by repeatedly applying the definition in this way has yielded an expression that does not involve recurrence. Such an expression can&#160;now be represented by a traditional directed acyclic computational graph. The&#160;unfolded computational graph of Eq. 10.1 and Eq. 10.3 is illustrated in Fig. 10.1.</span></p><div><div>
<p><span class="font62">•••)</span></p><img src="main-116.jpg" alt=""/>
<p><span class="font64">f</span></p>
<p><span class="font64">Figure 10.1: The classical dynamical system described by Eq. 10.1, illustrated as an unfolded computational graph. Each node represents the state at some time t and the&#160;function </span><span class="font64" style="font-weight:bold;font-style:italic;">f</span><span class="font64"> maps the state at t to the state at t +1. The same parameters (the same value&#160;of e used to parametrize f) are used for all time steps.</span></p></div></div>
<p><span class="font64">As another example, let us consider a dynamical system driven by an external signal </span><span class="font64" style="font-weight:bold;">x<sup>(t)</sup></span><span class="font64">,</span></p>
<p><span class="font64" style="font-weight:bold;">s<sup>(t)</sup> </span><span class="font64">= f (</span><span class="font64" style="font-weight:bold;">s<sup>(t-1)</sup></span><span class="font64">, </span><span class="font64" style="font-weight:bold;">x<sup>(t)</sup></span><span class="font64">; </span><span class="font64" style="font-weight:bold;">e</span><span class="font64">), &#160;&#160;&#160;(10.4)</span></p>
<p><span class="font64">where we see that the state now contains information about the whole past sequence.</span></p>
<p><span class="font64">Recurrent neural networks can be built in many different ways. Much as almost any function can be considered a feedforward neural network, essentially&#160;any function involving recurrence can be considered a recurrent neural network.</span></p>
<p><span class="font64">Many recurrent neural networks use Eq. 10.5 or a similar equation to define the values of their hidden units. To indicate that the state is the hidden units of&#160;the network, we now rewrite Eq. 10.4 using the variable h to represent the state:</span></p><div>
<p><span class="font64">(10.5)</span></p></div>
<p><span class="font64">h<sup>(t)</sup> = f (h<sup>(t-1)</sup>, x<sup>(t)</sup>; 0),</span></p>
<p><span class="font64">illustrated in Fig. 10.2, typical RNNs will add extra architectural features such as output layers that read information out of the state h to make predictions.</span></p>
<p><span class="font64">When the recurrent network is trained to perform a task that requires predicting the future from the past, the network typically learns to use h<sup>(t)</sup> as a kind of lossy&#160;summary of the task-relevant aspects of the past sequence of inputs up to t. This&#160;summary is in general necessarily lossy, since it maps an arbitrary length sequence&#160;(x<sup>(t)</sup>, x<sup>(t-1)</sup>, x<sup>(t-2)</sup>,..., x<sup>(2)</sup>, x<sup>(1)</sup>) to a fixed length vector h<sup>(t)</sup>. Depending on the&#160;training criterion, this summary might selectively keep some aspects of the past&#160;sequence with more precision than other aspects. For example, if the RNN is used&#160;in statistical language modeling, typically to predict the next word given previous&#160;words, it may not be necessary to store all of the information in the input sequence&#160;up to time t, but rather only enough information to predict the rest of the sentence.&#160;The most demanding situation is when we ask h<sup>(t)</sup> to be rich enough to allow&#160;one to approximately recover the input sequence, as in autoencoder frameworks&#160;(Chapter 14).</span></p><div>
<p><span class="font64">h<sup>(</sup>■■■<sup>) L</sup></span></p></div><div>
<p><span class="font64">h <sup>(t</sup>+<sup>x</sup></span></p></div><div>
<p><span class="font62">I </span><span class="font62" style="font-style:italic;">h</span><span class="font62">(■■■) '</span></p></div><div>
<p><span class="font64" style="font-style:italic;">f</span></p></div>
<p><span class="font63">Unfold</span></p><div>
<p><span class="font11" style="font-weight:bold;">x</span></p></div><div>
<p><span class="font12" style="font-style:italic;">x</span></p></div><div>
<p><span class="font64">(t-1)</span></p></div><div>
<p><span class="font12" style="font-style:italic;">x</span></p></div><div>
<p><span class="font62">(t)</span></p></div><div>
<p><span class="font12" style="font-style:italic;">x</span></p></div><div>
<p><span class="font64">(t+1)</span></p></div>
<p><span class="font64">Figure 10.2: A recurrent network with no outputs. This recurrent network just processes information from the input x by incorporating it into the state h that is passed forward&#160;through time. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> Circuit diagram. The black square indicates a delay of 1 time step.&#160;</span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> The same network seen as an unfolded computational graph, where each node is&#160;now associated with one particular time instance.</span></p>
<p><span class="font64">Eq. 10.5 can be drawn in two different ways. One way to draw the RNN is with a diagram containing one node for every component that might exist in a</span></p>
<p><span class="font64">physical implementation of the model, such as a biological neural network. In this view, the network defines a circuit that operates in real time, with physical&#160;parts whose current state can influence their future state, as in the left of Fig. 10.2.&#160;Throughout this chapter, we use a black square in a circuit diagram to indicate&#160;that an interaction takes place with a delay of 1 time step, from the state at time&#160;t to the state at time t +1. The other way to draw the RNN is as an unfolded&#160;computational graph, in which each component is represented by many different&#160;variables, with one variable per time step, representing the state of the component&#160;at that point in time. Each variable for each time step is drawn as a separate node&#160;of the computational graph, as in the right of Fig. 10.2. What we call </span><span class="font64" style="font-weight:bold;font-style:italic;">unfolding</span><span class="font64"> is&#160;the operation that maps a circuit as in the left side of the figure to a computational&#160;graph with repeated pieces as in the right side. The unfolded graph now has a size&#160;that depends on the sequence length.</span></p>
<p><span class="font64">We can represent the unfolded recurrence after t steps with a function g</span><span class="font64" style="font-weight:bold;"><sup>(t)</sup></span><span class="font64">:</span></p>
<p><span class="font64">=g</span><span class="font64" style="font-weight:bold;"><sup>(t)</sup> </span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x<sup>(t)</sup></span><span class="font64">, </span><span class="font64" style="font-weight:bold;">x<sup>(t-1)</sup></span><span class="font64">, </span><span class="font64" style="font-weight:bold;">x<sup>(t-2)</sup></span><span class="font64">,..., </span><span class="font64" style="font-weight:bold;">x<sup>(2)</sup></span><span class="font64">, </span><span class="font64" style="font-weight:bold;">x<sup>(1)</sup></span><span class="font64">) &#160;&#160;&#160;(10.6)</span></p>
<p><span class="font64">=f </span><span class="font64" style="font-weight:bold;font-style:italic;">(h<sup>(t-1)</sup></span><span class="font64">, </span><span class="font64" style="font-weight:bold;">x<sup>(t)</sup></span><span class="font64">; </span><span class="font64" style="font-weight:bold;">6</span><span class="font64">) &#160;&#160;&#160;(10.7)</span></p>
<p><span class="font64">The function g</span><span class="font64" style="font-weight:bold;"><sup>(t)</sup> </span><span class="font64">takes the whole past sequence (</span><span class="font64" style="font-weight:bold;">x<sup>(t)</sup></span><span class="font64">, </span><span class="font64" style="font-weight:bold;">x<sup>(t-1)</sup></span><span class="font64">, </span><span class="font64" style="font-weight:bold;">x<sup>(t-2)</sup></span><span class="font64">,..., </span><span class="font64" style="font-weight:bold;">x<sup>(2)</sup></span><span class="font64">, </span><span class="font64" style="font-weight:bold;">x<sup>(1)</sup></span><span class="font64">) as input and produces the current state, but the unfolded recurrent structure&#160;allows us to factorize g</span><span class="font64" style="font-weight:bold;"><sup>(t)</sup> </span><span class="font64">into repeated application of a function f. The unfolding&#160;process thus introduces two major advantages:</span></p>
<p><span class="font64">1. &#160;&#160;&#160;Regardless of the sequence length, the learned model always has the same&#160;input size, because it is specified in terms of transition from one state to&#160;another state, rather than specified in terms of a variable-length history of&#160;states.</span></p>
<p><span class="font64">2. &#160;&#160;&#160;It is possible to use the </span><span class="font64" style="font-weight:bold;">same </span><span class="font64">transition function f with the same parameters&#160;at every time step.</span></p>
<p><span class="font64">These two factors make it possible to learn a single model f that operates on all time steps and all sequence lengths, rather than needing to learn a separate&#160;model g</span><span class="font64" style="font-weight:bold;"><sup>(t)</sup> </span><span class="font64">for all possible time steps. Learning a single, shared model allows&#160;generalization to sequence lengths that did not appear in the training set, and&#160;allows the model to be estimated with far fewer training examples than would be&#160;required without parameter sharing.</span></p>
<p><span class="font64">Both the recurrent graph and the unrolled graph have their uses. The recurrent graph is succinct. The unfolded graph provides an explicit description of which&#160;computations to perform. The unfolded graph also helps to illustrate the idea of&#160;information flow forward in time (computing outputs and losses) and backward&#160;in time (computing gradients) by explicitly showing the path along which this&#160;information flows.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">10.2 Recurrent Neural Networks</span></h4>
<p><span class="font64">Armed with the graph unrolling and parameter sharing ideas of Sec. 10.1, we can design a wide variety of recurrent neural networks.</span></p><div><div><img src="main-117.jpg" alt=""/></div></div><div><div><img src="main-118.jpg" alt=""/>
<p><span class="font64">Figure 10.3: The computational graph to compute the training loss of a recurrent network that maps an input sequence of x values to a corresponding sequence of output o values.&#160;A loss L measures how far each o is from the corresponding training target y. When using&#160;softmax outputs, we assume o is the unnormalized log probabilities. The loss L internally&#160;computes y = softmax(o) and compares this to the target y. The RNN has input to hidden&#160;connections parametrized by a weight matrix U, hidden-to-hidden recurrent connections&#160;parametrized by a weight matrix W, and hidden-to-output connections parametrized by&#160;a weight matrix V. Eq. 10.8 defines forward propagation in this model. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> The RNN&#160;and its loss drawn with recurrent connections. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> The same seen as an time-unfolded&#160;computational graph, where each node is now associated with one particular time instance.</span></p></div></div>
<p><span class="font64">Some examples of important design patterns for recurrent neural networks include the following:</span></p>
<p><span class="font64">• Recurrent networks that produce an output at each time step and have</span></p>
<p><span class="font64">recurrent connections between hidden units, illustrated in Fig. 10.3.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Recurrent networks that produce an output at each time step and have&#160;recurrent connections only from the output at one time step to the hidden&#160;units at the next time step, illustrated in Fig. 10.4</span></p>
<p><span class="font64">• &#160;&#160;&#160;Recurrent networks with recurrent connections between hidden units, that&#160;read an entire sequence and then produce a single output, illustrated in Fig.&#160;10.5.</span></p>
<p><span class="font64">Fig. 10.3 is a reasonably representative example that we return to throughout most of the chapter.</span></p>
<p><span class="font64">The recurrent neural network of Fig. 10.3 and Eq. 10.8 is universal in the sense that any function computable by a Turing machine can be computed by such&#160;a recurrent network of a finite size. The output can be read from the RNN after&#160;a number of time steps that is asymptotically linear in the number of time steps&#160;used by the Turing machine and asymptotically linear in the length of the input&#160;(Siegelmann and Sontag, 1991; Siegelmann, 1995; Siegelmann and Sontag, 1995;&#160;Hyotyniemi, 1996). The functions computable by a Turing machine are discrete,&#160;so these results regard exact implementation of the function, not approximations.&#160;The RNN, when used as a Turing machine, takes a binary sequence as input and its&#160;outputs must be discretized to provide a binary output. It is possible to compute all&#160;functions in this setting using a single specific RNN of finite size (Siegelmann and&#160;Sontag (1995) use 886 units). The “input” of the Turing machine is a specification&#160;of the function to be computed, so the same network that simulates this Turing&#160;machine is sufficient for all problems. The theoretical RNN used for the proof&#160;can simulate an unbounded stack by representing its activations and weights with&#160;rational numbers of unbounded precision.</span></p>
<p><span class="font64">We now develop the forward propagation equations for the RNN depicted in Fig. 10.3. The figure does not specify the choice of activation function for the&#160;hidden units. Here we assume the hyperbolic tangent activation function. Also,&#160;the figure does not specify exactly what form the output and loss function take.&#160;Here we assume that the output is discrete, as if the RNN is used to predict words&#160;or characters. A natural way to represent discrete variables is to regard the output&#160;</span><span class="font64" style="font-weight:bold;">o </span><span class="font64">as giving the unnormalized log probabilities of each possible value of the discrete&#160;variable. We can then apply the softmax operation as a post-processing step to&#160;obtain a vector </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"> of normalized probabilities over the output. Forward propagation&#160;begins with a specification of the initial state </span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(0)</sup>. Then, for each time step from&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">t =</span><span class="font64"> 1 to </span><span class="font64" style="font-weight:bold;font-style:italic;">t = t</span><span class="font64">, we apply the following update equations:</span></p>
<p><span class="font64" style="font-weight:bold;">a</span><span class="font64"><sup>(t)</sup> = </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;">Wh</span><span class="font64"><sup>(t-1)</sup> + </span><span class="font64" style="font-weight:bold;">Ux</span><span class="font64"><sup>(t)</sup> &#160;&#160;&#160;(10.8)</span></p><div><div><img src="main-119.jpg" alt=""/>
<p><span class="font64">Figure 10.4: An RNN whose only recurrence is the feedback connection from the output to the hidden layer. At each time step t, the input is x <sub>t</sub>, the hidden layer activations are&#160;h<sup>(t)</sup>, the outputs are o<sup>(t)</sup>, the targets are yand the loss is . </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> Circuit diagram.&#160;</span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> Unfolded computational graph. Such an RNN is less powerful (can express a&#160;smaller set of functions) than those in the family represented by Fig. 10.3. The RNN&#160;in Fig. 10.3 can choose to put any information it wants about the past into its hidden&#160;representation h and transmit h to the future. The RNN in this figure is trained to&#160;put a specific output value into o, and o is the only information it is allowed to send&#160;to the future. There are no direct connections from h going forward. The previous h&#160;is connected to the present only indirectly, via the predictions it was used to produce.&#160;Unless o is very high-dimensional and rich, it will usually lack important information&#160;from the past. This makes the RNN in this figure less powerful, but it may be easier to&#160;train because each time step can be trained in isolation from the others, allowing greater&#160;parallelization during training, as described in Sec. 10.2.1.</span></p></div></div>
<p><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(t)</sup> &#160;&#160;&#160;=&#160;&#160;&#160;&#160;tanh(</span><span class="font64" style="font-weight:bold;">a</span><span class="font64"><sup>(t)</sup>)&#160;&#160;&#160;&#160;(10.9)</span></p>
<p><span class="font64" style="font-weight:bold;">o</span><span class="font64"><sup>(t)</sup> &#160;&#160;&#160;=&#160;&#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;">c </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;font-style:italic;">Vh</span><span class="font64"><sup>(t)</sup>&#160;&#160;&#160;&#160;(10.10)</span></p>
<p><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(t)</sup> &#160;&#160;&#160;=&#160;&#160;&#160;&#160;s</span><span class="font18">0</span><span class="font64">ftmax(</span><span class="font64" style="font-weight:bold;">o</span><span class="font64"><sup>(t)</sup>)&#160;&#160;&#160;&#160;(10.11)</span></p>
<p><span class="font64">where the parameters are the &#160;&#160;&#160;bias vectors </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">and </span><span class="font64" style="font-weight:bold;">c </span><span class="font64">along with the weight matrices</span></p>
<p><span class="font64" style="font-weight:bold;">U</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">V </span><span class="font64">and </span><span class="font64" style="font-weight:bold;">W</span><span class="font64">, respectively for input-to-hidden, hidden-to-output and hidden-to-hidden connections. This is an example of a recurrent network that maps an input sequence to an output sequence of the same length. The total loss for a&#160;given sequence of </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">values paired with a sequence of </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"> values would then be just&#160;the sum of the losses over all the time steps. For example, if L<sup>(t)</sup> is the negative&#160;log-likelihood of y<sup>(t)</sup> given </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(t)</sup>, then</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">L</span><span class="font64"> ({</span><span class="font64" style="font-weight:bold;">x&lt;<sup>1)</sup></span><span class="font64">...., </span><span class="font64" style="font-weight:bold;">x<sup>(T)</sup></span><span class="font64">}, {</span><span class="font64" style="font-weight:bold;">y<sup>(1)</sup></span><span class="font64">,..., </span><span class="font64" style="font-weight:bold;">y<sup>(T)</sup></span><span class="font64">}</span><span class="font64" style="font-weight:bold;">)</span></p>
<p><span class="font66">E L</span><span class="font64" style="font-weight:bold;"><sup>(t) </sup><sub>t</sub></span></p></td><td>
<p><span class="font64">(10.12)</span></p>
<p><span class="font64">(10.13)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">- E <sup>10</sup>gP</span><span class="font64" style="font-weight:bold;">model </span><span class="font64">(y</span><span class="font64" style="font-weight:bold;"><sup>(t)</sup> </span><span class="font64">| {</span><span class="font64" style="font-weight:bold;">x<sup>(1</sup></span><span class="font64">\ . . . <sup>, </sup></span><span class="font64" style="font-weight:bold;"><sup>x (t)</sup></span><span class="font64">}) <sup>,</sup></span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(10.14)</span></p></td></tr>
</table>
<p><span class="font64">t</span></p>
<p><span class="font64">where pmodei (y<sup>(t)</sup> | &#160;&#160;&#160;is given by reading the entry for </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"><sup>(t)</sup> from the</span></p>
<p><span class="font64">model’s output vector y<sup>(t)</sup>. Computing the gradient of this loss function with respect to the parameters is an expensive operation. The gradient computation&#160;involves performing a forward propagation pass moving left to right through our&#160;illustration of the unrolled graph in Fig. 10.3, followed by a backward propagation&#160;pass moving right to left through the graph. The runtime is O ( </span><span class="font63" style="font-variant:small-caps;">t</span><span class="font64">) and cannot be&#160;reduced by parallelization because the forward propagation graph is inherently&#160;sequential; each time step may only be computed after the previous one. States&#160;computed in the forward pass must be stored until they are reused during the&#160;backward pass, so the memory cost is also O(</span><span class="font63" style="font-variant:small-caps;">t</span><span class="font64">). The back-propagation algorithm&#160;applied to the unrolled graph with O (</span><span class="font63" style="font-variant:small-caps;">t</span><span class="font64">) cost is called </span><span class="font64" style="font-weight:bold;font-style:italic;">back-propagation through&#160;time</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">BPTT</span><span class="font64"> and is discussed further Sec. 10.2.2. The network with recurrence&#160;between hidden units is thus very powerful but also expensive to train. Is there an&#160;alternative?</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">10.2.1 Teacher Forcing and Networks with Output Recurrence</span></h5>
<p><span class="font64">The network with recurrent connections only from the output at one time step to the hidden units at the next time step (shown in Fig. 10.4) is strictly less powerful&#160;because it lacks hidden-to-hidden recurrent connections. For example, it cannot&#160;simulate a universal Turing machine. Because this network lacks hidden-to-hidden&#160;recurrence, it requires that the output units capture all of the information about&#160;the past that the network will use to predict the future. Because the output units&#160;are explicitly trained to match the training set targets, they are unlikely to capture&#160;the necessary information about the past history of the input, unless the user&#160;knows how to describe the full state of the system and provides it as part of the&#160;training set targets. The advantage of eliminating hidden-to-hidden recurrence&#160;is that, for any loss function based on comparing the prediction at time t to the&#160;training target at time t, all the time steps are decoupled. Training can thus be&#160;parallelized, with the gradient for each step t computed in isolation. There is no&#160;need to compute the output for the previous time step first, because the training&#160;set provides the ideal value of that output.</span></p><div><img src="main-120.jpg" alt=""/>
<p><span class="font64">Figure 10.5: Time-unfolded recurrent neural network with a single output at the end of the sequence. Such a network can be used to summarize a sequence and produce a&#160;fixed-size representation used as input for further processing. There might be a target&#160;right at the end (as depicted here) or the gradient on the output can be obtained by&#160;back-propagating from further downstream modules.</span></p></div>
<p><span class="font64">Models that have recurrent connections from their outputs leading back into the model may be trained with </span><span class="font64" style="font-weight:bold;font-style:italic;">teacher forcing.</span><span class="font64"> Teacher forcing is a procedure&#160;that emerges from the maximum likelihood criterion, in which during training the&#160;model receives the ground truth output as input at time t + 1. We can see&#160;this by examining a sequence with two time steps. The conditional maximum&#160;likelihood criterion is</span></p>
<p><span class="font64">log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(^y</span><span class="font64"><sup>(1)</sup>,</span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"><sup>(2)</sup> | x<sup>(1)</sup>,x(10.15)</span></p><div><div><img src="main-121.jpg" alt=""/></div></div><div><div><img src="main-122.jpg" alt=""/></div></div>
<p><span class="font64">Figure 10.6: Illustration of teacher forcing. Teacher forcing is a training technique that is applicable to RNNs that have connections from their output to their hidden states at the&#160;next time step. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> At train time, we feed the </span><span class="font64" style="font-style:italic;">correct output</span><span class="font64">y<sup>(t)</sup> drawn from the train&#160;set as input to h<sup>(t</sup>+<sup>1)</sup>. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> When the model is deployed, the true output is generally&#160;not known. In this case, we approximate the correct output y<sup>(t)</sup> with the model’s output&#160;o<sup>(t)</sup>, and feed the output back into the model.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> log</span><span class="font64" style="font-weight:bold;font-style:italic;">p (y</span><span class="font64"><sup>(2)</sup> | </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(1)</sup>, </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(1)</sup>, </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"><sup>(2)</sup>^ + logp </span><span class="font64" style="font-weight:bold;font-style:italic;">(y</span><span class="font64"><sup>(1)</sup> | </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> <sup>(1)</sup>, </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(2)</sup>^ &#160;&#160;&#160;(10.16)</span></p>
<p><span class="font64">In this example, we see that at time t </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> 2, the model is trained to maximize the conditional probability of </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(2)</sup> given </span><span class="font64" style="font-weight:bold;">both </span><span class="font64">the </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">sequence so far and the previous </span><span class="font64" style="font-weight:bold;">y&#160;</span><span class="font64">value from the training set. Maximum likelihood thus specifies that during training,&#160;rather than feeding the model’s own output back into itself, these connections&#160;should be fed with the target values specifying what the correct output should be.&#160;This is illustrated in Fig. 10.6.</span></p>
<p><span class="font64">We originally motivated teacher forcing as allowing us to avoid back-propagation through time in models that lack hidden-to-hidden connections. Teacher forcing&#160;may still be applied to models that have hidden-to-hidden connections so long as&#160;they have connections from the output at one time step to values computed in the&#160;next time step. However, as soon as the hidden units become a function of earlier&#160;time steps, the BPTT algorithm is necessary. Some models may thus be trained&#160;with both teacher forcing and BPTT.</span></p>
<p><span class="font64">The disadvantage of strict teacher forcing arises if the network is going to be later used in an </span><span class="font64" style="font-weight:bold;font-style:italic;">open-loop</span><span class="font64"> mode, with the network outputs (or samples from the&#160;output distribution) fed back as input. In this case, the kind of inputs that the&#160;network sees during training could be quite different from the kind of inputs that&#160;it will see at test time. One way to mitigate this problem is to train with both&#160;teacher-forced inputs and with free-running inputs, for example by predicting the&#160;correct target a number of steps in the future through the unfolded recurrent&#160;output-to-input paths. In this way, the network can learn to take into account&#160;input conditions (such as those it generates itself in the free-running mode) not&#160;seen during training and how to map the state back towards one that will make&#160;the network generate proper outputs after a few steps. Another approach (Bengio&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015b) to mitigate the gap between the inputs seen at train time and the&#160;inputs seen at test time randomly chooses to use generated values or actual data&#160;values as input. This approach exploits a curriculum learning strategy to gradually&#160;use more of the generated values as input.</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">10.2.2 Computing the Gradient in a Recurrent Neural Network</span></h5>
<p><span class="font64">Computing the gradient through a recurrent neural network is straightforward. One simply applies the generalized back-propagation algorithm of Sec. 6.5.6 to the&#160;unrolled computational graph. No specialized algorithms are necessary. The use&#160;of back-propagation on the unrolled graph is called the </span><span class="font64" style="font-weight:bold;font-style:italic;">back-propagation through&#160;time</span><span class="font64"> (BPTT) algorithm. Gradients obtained by back-propagation may then be&#160;used with any general-purpose gradient-based techniques to train an RNN.</span></p>
<p><span class="font64">To gain some intuition for how the BPTT algorithm behaves, we provide an example of how to compute gradients by BPTT for the RNN equations above&#160;(Eq. 10.8 and Eq. 10.12). The nodes of our computational graph include the&#160;parameters </span><span class="font64" style="font-weight:bold;font-style:italic;">U, V</span><span class="font64">, W, b and c as well as the sequence of nodes indexed by t for&#160;x<sup>(t)</sup>, h<sup>(t)</sup>, o<sup>(t)</sup> and L<sup>(t)</sup>. For each node </span><span class="font64" style="font-weight:bold;">N </span><span class="font64">we need to compute the gradient V</span><span class="font63" style="font-variant:small-caps;">n </span><span class="font64">L&#160;recursively, based on the gradient computed at nodes that follow it in the graph.&#160;We start the recursion with the nodes immediately preceding the final loss</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dL</span></p>
<p><span class="font64">dLW</span></p></div><div>
<p><span class="font64">= 1.</span></p></div><div>
<p><span class="font64">(10.17)</span></p></div><div>
<p><span class="font64">In this derivation we assume that the outputs o<sup>(t)</sup> are used as the argument to the softmax function to obtain the vector </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"> of probabilities over the output. We also&#160;assume that the loss is the negative log-likelihood of the true target y<sup>(t)</sup> given the&#160;input so far. The gradient V</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>o</sub>(t)L</span><span class="font64"> on the outputs at time step t, for all i,t, is as&#160;follows:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dL dL dL<sup>(t)</sup></span><span class="font64"> &#160;&#160;&#160;<sub>w</sub></span></p>
<p><span class="font64"><sup>1</sup> ־ i,y<sup>(t)</sup>.</span></p></div><div>
<p><span class="font64" style="font-variant:small-caps;">(Vm L)i</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">do,</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">P dL<sup>(t)</sup> do!<sup>t)</sup></span></p></div><div>
<p><span class="font64">Vi</span></p></div><div>
<p><span class="font64">(10.18)</span></p></div>
<p><span class="font64">We work our way backwards, starting from the end of the sequence. At the final time step t, h<sup>(t)</sup> only has 0<sup>T)</sup> as a descendent, so its gradient is simple:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Vh(T) L = V</span></p></div><div>
<p><span class="font12" style="font-style:italic;">■T</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;"><sup>V</sup>0(t )L.</span></p></div><div>
<p><span class="font64">(10.19)</span></p></div>
<p><span class="font64">We can then iterate backwards in time to back-propagate gradients through time, from t = </span><span class="font63" style="font-variant:small-caps;">t </span><span class="font64">— 1 down to t = 1, noting that h<sup>(t)</sup> (for t &lt; </span><span class="font63" style="font-variant:small-caps;">t</span><span class="font64">) has as descendents both&#160;o<sup>(t)</sup> and h<sup>(t+1)</sup>. Its gradient is thus given by</span></p><div>
<p><span class="font64"><sup>v</sup>h(t)<sup>L</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"> h<sup>(t+1) </sup></span><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"> h(*)</span></p></div><div>
<p><span class="font12" style="font-style:italic;">T</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(V</sup>h<sup>(t</sup>+<sup>l)</sup></span><span class="font64"><sup> L)</sup> +</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"> h(*)</span></p>
<p><span class="font64">= W<sup>T</sup> (Vh(t+</span><span class="font18">1</span><span class="font64">)L) diag 1 —</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"> o<sup>(t)</sup></span></p>
<p><span class="font64"><sup>(Vo</sup></span></p>
<p><span class="font64">^1—(h<sup>(m)</sup></span></p></div><div>
<p><span class="font64">+ </span><span class="font64" style="font-weight:bold;font-style:italic;">V <sup>T</sup></span><span class="font64"> (V o:t) L)</span></p></div><div>
<p><span class="font64">(10.20)</span></p>
<p><span class="font64">(10.21)</span></p></div>
<p><span class="font64">where diag 1 — (h<sup>(t+1)</sup> &#160;&#160;&#160;indicates the diagonal matrix containing the elements</span></p>
<p><span class="font64">1 — (h<sup>(t+1)</sup> )<sup>2</sup>. This is the Jacobian of the hyperbolic tangent associated with the hidden unit </span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> at time </span><span class="font64" style="font-weight:bold;font-style:italic;">t</span><span class="font64"> + 1 .</span></p>
<p><span class="font64">Once the gradients on the internal nodes of the computational graph are obtained, we can obtain the gradients on the parameter nodes. Because the&#160;parameters are shared across many time steps, we must take some care when&#160;denoting calculus operations involving these variables. The equations we wish to&#160;implement use the </span><span class="font64" style="font-weight:bold;">bprop </span><span class="font64">method of Sec. 6.5.6, that computes the contribution&#160;of a single edge in the computational graph to the gradient. However, the V</span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">wf&#160;</span><span class="font64">operator used in calculus takes into account the contribution of </span><span class="font64" style="font-weight:bold;">W </span><span class="font64">to the value&#160;of f due to </span><span class="font64" style="font-weight:bold;">all </span><span class="font64">edges in the computational graph. To resolve this ambiguity, we&#160;introduce dummy variables </span><span class="font64" style="font-weight:bold;">W</span><span class="font64"><sup>(t)</sup> that are defined to be copies of </span><span class="font64" style="font-weight:bold;">W </span><span class="font64">but with each&#160;</span><span class="font64" style="font-weight:bold;">W</span><span class="font64"><sup>(t)</sup> used only at time step t. We may then use V </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">w(.)</span><span class="font64"> to denote the contribution&#160;of the weights at time step t to the gradient.</span></p>
<p><span class="font64">Using this notation, the gradient on the remaining parameters is given by:</span></p><div>
<p><span class="font64">VcL</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">VbL</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">Vv L V w L</span></p></div><div>
<p><span class="font66"><sup>E</sup> (* &#160;&#160;&#160;V״<sub>W</sub>L = £ V<sub>0W</sub>L</span></p>
<p><span class="font66">E (S) &quot;Vh(.)L = E diag (1 - (h<sup>(t)</sup>)) Vh(.)L <sup>EE</sup>(go) </span><span class="font64" style="font-variant:small-caps;">VvO7 = </span><span class="font66"><sup>E</sup> (Vo(.)L)h<sup>(t)T</sup></span></p>
<p><span class="font66">E &#160;&#160;&#160;<sup>V</sup> W - &quot;</span></p>
<p><span class="font66">E diag 1 - (h<sup>(t)</sup> )<sup>2</sup>) (Vh(.)L) h<sup>(t-1)T</sup></span></p></div><div>
<p><span class="font64">V״L =</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dL</span></p></div><div>
<p><span class="font64">V״)״ h<sup>(t)</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">״<sup>(t)</sup> 'I </span><span class="font64" style="font-weight:bold;">2'</span></p></div><div>
<p><span class="font64">dh<sup>((t)</sup></span></p>
<p><span class="font64"><sup>dia</sup>g ^1 - &#160;&#160;&#160;^ (Vh(t)<sup>L) x(t)</sup></span></p></div><div>
<p><span class="font64">T</span></p></div><div>
<p><span class="font64">(10.22)</span></p>
<p><span class="font64">(10.23)</span></p>
<p><span class="font64">(10.24)</span></p>
<p><span class="font64">(10.25)</span></p>
<p><span class="font64">(10.26)</span></p>
<p><span class="font64">(10.27)</span></p>
<p><span class="font64">(10.28)</span></p></div>
<p><span class="font64">We do not need to compute the gradient with respect to x<sup>(t)</sup> for training because it does not have any parameters as ancestors in the computational graph defining&#160;the loss.</span></p><h5><a id="bookmark5"></a><span class="font64" style="font-weight:bold;">10.2.3 Recurrent Networks as Directed Graphical Models</span></h5>
<p><span class="font64">In the example recurrent network we have developed so far, the losses L<sup>(t)</sup> were cross-entropies between training targets y<sup>(t)</sup> and outputs o<sup>(t)</sup>. As with a feedforward&#160;network, it is in principle possible to use almost any loss with a recurrent network.&#160;The loss should be chosen based on the task. As with a feedforward network, we&#160;usually wish to interpret the output of the RNN as a probability distribution, and&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">we</span><span class="font64"> usually use the cross-entropy associated with that distribution to define the loss.&#160;Mean squared error is the cross-entropy loss associated with an output distribution&#160;that is a unit Gaussian, for example, just as with a feedforward network.</span></p>
<p><span class="font64">When we use a predictive log-likelihood training objective, such as Eq. 10.12, we train the RNN to estimate the conditional distribution of the next sequence element&#160;</span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(t)</sup> given the past inputs. This may mean that we maximize the log-likelihood</span></p>
<p><span class="font64">logp(</span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(t)</sup> </span><span class="font64" style="font-weight:bold;">| x</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(t)</sup>), &#160;&#160;&#160;(10.29)</span></p>
<p><span class="font64">or, if the model includes connections from the output at one time step to the next time step,</span></p>
<p><span class="font64">log p(</span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(t)</sup> </span><span class="font64" style="font-weight:bold;">| x</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(t)</sup>, </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(t-1)</sup>). &#160;&#160;&#160;(10.30)</span></p>
<p><span class="font64">Decomposing the joint probability over the sequence of </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">values as a series of one-step probabilistic predictions is one way to capture the full joint distribution&#160;across the whole sequence. When we do not feed past </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">values as inputs that&#160;condition the next step prediction, the directed graphical model contains no edges&#160;from any </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(i)</sup> in the past to the current </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(t)</sup> . In this case, the outputs </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">are&#160;conditionally independent given the sequence of </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">values. When we do feed the&#160;actual </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">values (not their prediction, but the actual observed or generated values)&#160;back into the network, the directed graphical model contains edges from all </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(i)&#160;</sup>values in the past to the current y^ value.</span></p><div>
<p dir="rtl"><span class="font64">' ✓</span></p>
<p dir="rtl"><span class="font64">א ✓</span></p>
<p><span class="font64">✓ &#160;&#160;&#160;N</span></p><img src="main-123.jpg" alt=""/>
<p><span class="font64">Figure 10.7: Fully connected graphical model for a sequencey<sup>(1)</sup>,y<sup>(2)</sup>,... ,y<sup>(t)</sup>,...: every past observation yW may influence the conditional distribution of some y<sup>(t)</sup> (for </span><span class="font64" style="font-weight:bold;font-style:italic;">t &gt;</span><span class="font64"> i),&#160;given the previous values. Parametrizing the graphical model directly according to this&#160;graph (as in Eq. 10.6) might be very inefficient, with an ever growing number of inputs&#160;and parameters for each element of the sequence. RNNs obtain the same full connectivity&#160;but efficient parametrization, as illustrated in Fig. 10.8.</span></p></div>
<p><span class="font64">As a simple example, let us consider the case where the RNN models only a sequence of scalar random variables Y = {y<sup>(1)</sup>,..., y</span><span class="font64" style="font-variant:small-caps;"><sup>(t</sup>)}, with no additional inputs&#160;x. The input at time step t is simply the output at time step </span><span class="font64" style="font-weight:bold;font-style:italic;">t —</span><span class="font64"> 1. The RNN then&#160;defines a directed graphical model over the y variables. We parametrize the joint&#160;distribution of these observations using the chain rule (Eq. 3.6) for conditional&#160;probabilities:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(</span><span class="font64" style="font-variant:small-caps;">y) = </span><span class="font64" style="font-weight:bold;font-style:italic;">P(y</span><span class="font64"><sup>(1)</sup>,...,y<sup>T)</sup>) = n</span><span class="font64" style="font-weight:bold;font-style:italic;">P(y</span><span class="font64"><sup>(t)</sup> Iy<sup>(t-1)</sup>׳y<sup>(t-2)</sup>׳•••׳y<sup>(1)</sup>) &#160;&#160;&#160;(10-31)</span></p>
<p><span class="font64">t=1</span></p>
<p><span class="font64">where the right-hand side of the bar is empty for t = 1, of course. Hence the negative log-likelihood of a set of values {y<sup>(1)</sup>,..., y <sup>(t)</sup>} according to such a model&#160;is</span></p>
<p><span class="font64">L &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">L<sup>(t)</sup></span><span class="font64">&#160;&#160;&#160;&#160;(10.32)</span></p><div>
<p><span class="font64">where</span></p></div><div>
<p><span class="font64">L<sup>(t)</sup> = — logP(y<sup>(t)</sup> = y<sup>(t)</sup> | y<sup>(t-1)</sup>,y<sup>(t2</sup>־<sup>)</sup>,...,y<sup>(1)</sup>).</span></p></div><div>
<p><span class="font64">(10.33)</span></p></div><div><div><img src="main-124.jpg" alt=""/>
<p><span class="font64">Figure 10.8: Introducing the state variable in the graphical model of the RNN, even though it is a deterministic function of its inputs, helps to see how we can obtain a very&#160;efficient parametrization, based on Eq. 10.5. Every stage in the sequence (for and</span></p>
<p><span class="font64">) involves the same structure (the same number of inputs for each node) and can share the same parameters with the other stages.</span></p></div></div>
<p><span class="font64">The edges in a graphical model indicate which variables depend directly on other variables. Many graphical models aim to achieve statistical and computational&#160;efficiency by omitting edges that do not correspond to strong interactions. For&#160;example, it is common to make the Markov assumption that the graphical model&#160;should only contain edges from {y<sup>(t-k)</sup>,..., y<sup>(t-1)</sup>} to y<sup>(t)</sup>, rather than containing&#160;edges from the entire past history. However, in some cases, we believe that all past&#160;inputs should have an influence on the next element of the sequence. RNNs are&#160;useful when we believe that the distribution over y<sup>(t)</sup> may depend on a value of y<sup>(i)&#160;</sup>from the distant past in a way that is not captured by the effect of yW on y<sup>(t-1)</sup>.</span></p>
<p><span class="font64">One way to interpret an RNN as a graphical model is to view the RNN as defining a graphical model whose structure is the complete graph, able to represent&#160;direct dependencies between any pair of y values. The graphical model over the&#160;y values with the complete graph structure is shown in Fig. 10.7. The complete&#160;graph interpretation of the RNN is based on ignoring the hidden units </span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(t)</sup> by&#160;marginalizing them out of the model.</span></p>
<p><span class="font64">It is more interesting to consider the graphical model structure of RNNs that results from regarding the hidden units </span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(t)</sup> as random variables.<a id="footnote1"></a><sup><a href="#bookmark6">1</a></sup><sup></sup> Including the&#160;hidden units in the graphical model reveals that the RNN provides a very efficient&#160;parametrization of the joint distribution over the observations. Suppose that we&#160;represented an arbitrary joint distribution over discrete values with a tabular&#160;representation—an array containing a separate entry for each possible assignment&#160;of values, with the value of that entry giving the probability of that assignment&#160;occurring. If y can take on k different values, the tabular representation would&#160;have </span><span class="font64" style="font-weight:bold;font-style:italic;">O(k</span><span class="font64"><sup>T</sup>) parameters. By comparison, due to parameter sharing, the number&#160;of parameters in the RNN is O(1) as a function of sequence length. The number&#160;of parameters in the RNN may be adjusted to control model capacity but is not&#160;forced to scale with sequence length. Eq. 10.5 shows that the RNN parametrizes&#160;long-term relationships between variables efficiently, using recurrent applications&#160;of the same function f and same parameters </span><span class="font64" style="font-weight:bold;">6 </span><span class="font64">at each time step. Fig. 10.8&#160;illustrates the graphical model interpretation. Incorporating the </span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(t)</sup> nodes in&#160;the graphical model decouples the past and the future, acting as an intermediate&#160;quantity between them. A variable yin the distant past may influence a variable&#160;y<sup>(t)</sup> via its effect on </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">. The structure of this graph shows that the model can be&#160;efficiently parametrized by using the same conditional probability distributions at&#160;each time step, and that when the variables are all observed, the probability of the&#160;joint assignment of all variables can be evaluated efficiently.</span></p>
<p><span class="font64">Even with the efficient parametrization of the graphical model, some operations remain computationally challenging. For example, it is difficult to predict missing&#160;values in the middle of the sequence.</span></p>
<p><span class="font64">The price recurrent networks pay for their reduced number of parameters is that </span><span class="font64" style="font-weight:bold;">optimizing </span><span class="font64">the parameters may be difficult.</span></p>
<p><span class="font64">The parameter sharing used in recurrent networks relies on the assumption that the same parameters can be used for different time steps. Equivalently, the&#160;assumption is that the conditional probability distribution over the variables at&#160;time t + 1 given the variables at time t is </span><span class="font64" style="font-weight:bold;font-style:italic;">stationary</span><span class="font64">, meaning that the relationship&#160;between the previous time step and the next time step does not depend on t. In&#160;principle, it would be possible to use t as an extra input at each time step and let&#160;the learner discover any time-dependence while sharing as much as it can between&#160;different time steps. This would already be much better than using a different&#160;conditional probability distribution for each t, but the network would then have to&#160;extrapolate when faced with new values of t.</span></p>
<p><span class="font64">To complete our view of an RNN as a graphical model, we must describe how to draw samples from the model. The main operation that we need to perform is&#160;simply to sample from the conditional distribution at each time step. However,&#160;there is one additional complication. The RNN must have some mechanism for&#160;determining the length of the sequence. This can be achieved in various ways.</span></p>
<p><span class="font64">In the case when the output is a symbol taken from a vocabulary, one can add a special symbol corresponding to the end of a sequence (Schmidhuber, 2012).&#160;When that symbol is generated, the sampling process stops. In the training set,&#160;we insert this symbol as an extra member of the sequence, immediately after </span><span class="font64" style="font-variant:small-caps;">x<sup>(t)&#160;</sup></span><span class="font64">in each training example.</span></p>
<p><span class="font64">Another option is to introduce an extra Bernoulli output to the model that represents the decision to either continue generation or halt generation at each&#160;time step. This approach is more general than the approach of adding an extra&#160;symbol to the vocabulary, because it may be applied to any RNN, rather than&#160;only RNNs that output a sequence of symbols. For example, it may be applied to&#160;an RNN that emits a sequence of real numbers. The new output unit is usually a&#160;sigmoid unit trained with the cross-entropy loss. In this approach the sigmoid is&#160;trained to maximize the log-probability of the correct prediction as to whether the&#160;sequence ends or continues at each time step.</span></p>
<p><span class="font64">Another way to determine the sequence length </span><span class="font64" style="font-variant:small-caps;">t </span><span class="font64">is to add an extra output to the model that predicts the integer </span><span class="font64" style="font-variant:small-caps;">t </span><span class="font64">itself. The model can sample a value of </span><span class="font64" style="font-variant:small-caps;">t&#160;</span><span class="font64">and then sample </span><span class="font64" style="font-variant:small-caps;">t </span><span class="font64">steps worth of data. This approach requires adding an extra&#160;input to the recurrent update at each time step so that the recurrent update is&#160;aware of whether it is near the end of the generated sequence. This extra input&#160;can either consist of the value of </span><span class="font64" style="font-variant:small-caps;">t </span><span class="font64">or can consist of </span><span class="font64" style="font-variant:small-caps;">t </span><span class="font64">— t, the number of remaining&#160;time steps. Without this extra input, the RNN might generate sequences that&#160;end abruptly, such as a sentence that ends before it is complete. This approach is&#160;based on the decomposition</span></p>
<p><span class="font64">P(x<sup>(1)</sup>,...,</span><span class="font64" style="font-variant:small-caps;">x<sup>(t)</sup></span><span class="font64">) = P</span><span class="font64" style="font-variant:small-caps;">(t</span><span class="font64">)P</span><span class="font64" style="font-style:italic;">(x</span><span class="font64"><sup>(1)</sup>,...,</span><span class="font64" style="font-variant:small-caps;">x<sup>(t)</sup> | t</span><span class="font64">). &#160;&#160;&#160;(10.34)</span></p>
<p><span class="font64">The strategy of predicting </span><span class="font64" style="font-variant:small-caps;">t </span><span class="font64">directly is used for example by Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span></p>
<p><span class="font64">(2014d).</span></p><h5><a id="bookmark7"></a><span class="font64" style="font-weight:bold;">10.2.4 Modeling Sequences Conditioned on Context with RNNs</span></h5>
<p><span class="font64">In the previous section we described how an RNN could correspond to a directed graphical model over a sequence of random variables y<sup>(t)</sup> with no inputs x. Of&#160;course, our development of RNNs as in Eq. 10.8 included a sequence of inputs&#160;x<sup>(1)</sup>, x<sup>(2)</sup>,..., </span><span class="font64" style="font-variant:small-caps;">x<sup>(t)</sup></span><span class="font64"> . In general, RNNs allow the extension of the graphical model&#160;view to represent not only a joint distribution over the y variables but also a&#160;conditional distribution over y given x. As discussed in the context of feedforward&#160;networks in Sec. 6.2.1.1, any model representing a variable P (y; 6) can be reinterpreted as a model representing a conditional distribution P</span><span class="font64" style="font-weight:bold;font-style:italic;">(y \u</span><span class="font64">) with u </span><span class="font64" style="font-weight:bold;font-style:italic;">= 6</span><span class="font64">. We&#160;can extend such a model to represent a distribution P(y \ x) by using the same&#160;P(y \ u) as before, but making u a function of x. In the case of an RNN, this&#160;can be achieved in different ways. We review here the most common and obvious&#160;choices.</span></p>
<p><span class="font64">Previously, we have discussed RNNs that take a sequence of vectors x<sup>(t)</sup> for </span><span class="font64" style="font-weight:bold;font-style:italic;">t =</span><span class="font64"> 1,..., </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">t</span><span class="font64"> as input. Another option is to take only a single vector x as input.&#160;When x is a fixed-size vector, we can simply make it an extra input of the RNN&#160;that generates the y sequence. Some common ways of providing an extra input to&#160;an RNN are:</span></p>
<p><span class="font64">1. &#160;&#160;&#160;as an extra input at each time step, or</span></p>
<p><span class="font64">2. &#160;&#160;&#160;as the initial state h<sup>(0)</sup>, or</span></p>
<p><span class="font64">3. &#160;&#160;&#160;both.</span></p>
<p><span class="font64">The first and most common approach is illustrated in Fig. 10.9. The interaction between the input x and each hidden unit vector is parametrized by a newly&#160;introduced weight matrix R that was absent from the model of only the sequence&#160;of y values. The same product x<sup>T</sup>R is added as additional input to the hidden&#160;units at every time step. We can think of the choice of x as determining the value&#160;of x</span><span class="font64" style="font-variant:small-caps;"><sup>t</sup>R </span><span class="font64">that is effectively a new bias parameter used for each of the hidden units.&#160;The weights remain independent of the input. We can think of this model as taking&#160;the parameters 6 of the non-conditional model and turning them into u, where&#160;the bias parameters within u are now a function of the input.</span></p>
<p><span class="font64">Rather than receiving only a single vector x as input, the RNN may receive a sequence of vectors x<sup>(t)</sup> as input. The RNN described in Eq. 10.8 corresponds to a</span></p><div><div><img src="main-125.jpg" alt=""/>
<p><span class="font64">Figure 10.9: An RNN that maps a fixed-length vectorx into a distribution over sequences Y. This RNN is appropriate for tasks such as image captioning, where a single image is&#160;used as input to a model that then produces a sequence of words describing the image.&#160;Each element y<sup>(t)</sup> of the observed output sequence serves both as input (for the current&#160;time step) and, during training, as target (for the previous time step).</span></p></div></div><div><div><img src="main-126.jpg" alt=""/>
<p><span class="font64">Figure 10.10: A conditional recurrent neural network mapping a variable-length sequence of x values into a distribution over sequences of y values of the same length. Compared&#160;to Fig. 10.3, this RNN contains connections from the previous output to the current state.&#160;These connections allow this RNN to model an arbitrary distribution over sequences ofy&#160;given sequences of x of the same length. The RNN of Fig. 10.3 is only able to represent&#160;distributions in which the y values are conditionally independent from each other given&#160;the x values.</span></p></div></div>
<p><span class="font64">conditional distribution P(</span><span class="font64" style="font-weight:bold;">y<sup>(1)</sup></span><span class="font64">,..., </span><span class="font64" style="font-weight:bold;">y<sup>(r)</sup> | x<sup>(1)</sup></span><span class="font64">,..., </span><span class="font64" style="font-variant:small-caps;">x<sup>(t</sup></span><span class="font64" style="font-weight:bold;"><sup>)</sup></span><span class="font64">) that makes a conditional independence assumption that this distribution factorizes as</span></p>
<p><span class="font64" style="font-weight:bold;">JJ</span><span class="font64">P(</span><span class="font64" style="font-weight:bold;">y<sup>(t)</sup> | x<sup>(1)</sup>x<sup>(t)</sup></span><span class="font64">). &#160;&#160;&#160;(10.35)</span></p>
<p><span class="font64" style="font-weight:bold;">t</span></p>
<p><span class="font64">To remove the conditional independence assumption, we can add connections from the output at time t to the hidden unit at time t +1, as shown in Fig. 10.10. The&#160;model can then represent arbitrary probability distributions over the </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">sequence.&#160;This kind of model representing a distribution over a sequence given another&#160;sequence still has one restriction, which is that the length of both sequences must&#160;be the same. We describe how to remove this restriction in Sec. 10.4.</span></p><div><img src="main-127.jpg" alt=""/>
<p><span class="font64">Figure 10.11: Computation of a typical bidirectional recurrent neural network, meant to learn to map input sequences x to target sequences y, with loss at each step t.&#160;The h recurrence propagates information forward in time (towards the right) while the&#160;g recurrence propagates information backward in time (towards the left). Thus at each&#160;point t, the output units o<sup>(t)</sup> can benefit from a relevant summary of the past in its h<sup>(t)&#160;</sup>input and from a relevant summary of the future in its g<sup>(t)</sup> input.</span></p></div><h4><a id="bookmark8"></a><span class="font65" style="font-weight:bold;">10.3 Bidirectional RNNs</span></h4>
<p><span class="font64">All of the recurrent networks we have considered up to now have a “causal” structure, meaning that the state at time t only captures information from the past, </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(t-1)</sup>, and the present input </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(t)</sup>. Some of the models we have discussed&#160;also allow information from past </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">values to affect the current state when the </span><span class="font64" style="font-weight:bold;">y&#160;</span><span class="font64">values are available.</span></p>
<p><span class="font64">However, in many applications we want to output a prediction of </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(t)</sup> which may depend on </span><span class="font64" style="font-weight:bold;">the whole input sequence</span><span class="font64">. For example, in speech recognition,&#160;the correct interpretation of the current sound as a phoneme may depend on the&#160;next few phonemes because of co-articulation and potentially may even depend on&#160;the next few words because of the linguistic dependencies between nearby words: if&#160;there are two interpretations of the current word that are both acoustically plausible,&#160;we may have to look far into the future (and the past) to disambiguate them.&#160;This is also true of handwriting recognition and many other sequence-to-sequence&#160;learning tasks, described in the next section.</span></p>
<p><span class="font64">Bidirectional recurrent neural networks (or bidirectional RNNs) were invented to address that need (Schuster and Paliwal, 1997). They have been extremely successful (Graves, 2012) in applications where that need arises, such as handwriting&#160;recognition (Graves </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2008; Graves and Schmidhuber, 2009), speech recognition (Graves and Schmidhuber, 2005; Graves </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013) and bioinformatics (Baldi&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1999).</span></p>
<p><span class="font64">As the name suggests, bidirectional RNNs combine an RNN that moves forward through time beginning from the start of the sequence with another RNN that&#160;moves backward through time beginning from the end of the sequence. Fig. 10.11&#160;illustrates the typical bidirectional RNN, with standing for the state of the&#160;sub-RNN that moves forward through time and </span><span class="font64" style="font-weight:bold;">g</span><span class="font64"><sup>(t)</sup> standing for the state of the&#160;sub-RNN that moves backward through time. This allows the output units </span><span class="font64" style="font-weight:bold;">o</span><span class="font64"><sup>(t)</sup> to&#160;compute a representation that depends on </span><span class="font64" style="font-weight:bold;">both the past and the future </span><span class="font64">but&#160;is most sensitive to the input values around time t, without having to specify a&#160;fixed-size window around t (as one would have to do with a feedforward network,&#160;a convolutional network, or a regular RNN with a fixed-size look-ahead buffer).</span></p>
<p><span class="font64">This idea can be naturally extended to 2-dimensional input, such as images, by having </span><span class="font64" style="font-weight:bold;">four </span><span class="font64">RNNs, each one going in one of the four directions: up, down,&#160;left, right. At each point (i, j) of a 2-D grid, an output </span><span class="font64" style="font-weight:bold;font-style:italic;">Oi,j</span><span class="font64"> could then compute a&#160;representation that would capture mostly local information but could also depend&#160;on long-range inputs, if the RNN is able to learn to carry that information.&#160;Compared to a convolutional network, RNNs applied to images are typically more&#160;expensive but allow for long-range lateral interactions between features in the&#160;same feature map (Visin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015; Kalchbrenner </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015). Indeed, the&#160;forward propagation equations for such RNNs may be written in a form that shows&#160;they use a convolution that computes the bottom-up input to each layer, prior&#160;to the recurrent propagation across the feature map that incorporates the lateral&#160;interactions.</span></p><h4><a id="bookmark9"></a><span class="font65" style="font-weight:bold;">10.4 Encoder-Decoder Sequence-to-Sequence Architectures</span></h4>
<p><span class="font64">We have seen in Fig. 10.5 how an RNN can map an input sequence to a fixed-size vector. We have seen in Fig. 10.9 how an RNN can map a fixed-size vector to a&#160;sequence. We have seen in Fig. 10.3, Fig. 10.4, Fig. 10.10 and Fig. 10.11 how an&#160;RNN can map an input sequence to an output sequence of the same length.</span></p>
<p><span class="font64">Here we discuss how an RNN can be trained to map an input sequence to an output sequence which is not necessarily of the same length. This comes up in&#160;many applications, such as speech recognition, machine translation or question&#160;answering, where the input and output sequences in the training set are generally&#160;not of the same length (although their lengths might be related).</span></p>
<p><span class="font64">We often call the input to the RNN the “context.” We want to produce a representation of this context, C. The context C might be a vector or sequence of&#160;vectors that summarize the input sequence </span><span class="font64" style="font-weight:bold;font-style:italic;">X =</span><span class="font64"> (</span><span class="font64" style="font-weight:bold;">a</span><span class="font64">^</span><span class="font64" style="font-weight:bold;"><sup>1</sup>)</span><span class="font64">,..., </span><span class="font64" style="font-weight:bold;">x)</span><span class="font64">).</span></p>
<p><span class="font64">The simplest RNN architecture for mapping a variable-length sequence to another variable-length sequence was first proposed by Cho </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014a) and shortly after by Sutskever </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014), who independently developed that architecture and&#160;were the first to obtain state-of-the-art translation using this approach. The former&#160;system is based on scoring proposals generated by another machine translation&#160;system, while the latter uses a standalone recurrent network to generate the translations. These authors respectively called this architecture, illustrated in Fig. 10.12,&#160;the encoder-decoder or sequence-to-sequence architecture. The idea is very simple:&#160;(1) an </span><span class="font64" style="font-weight:bold;font-style:italic;">encoder</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">reader</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">input</span><span class="font64"> RNN processes the input sequence. The encoder&#160;emits the context C, usually as a simple function of its final hidden state. (2) a&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">decoder</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">writer</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">output</span><span class="font64"> RNN is conditioned on that fixed-length vector (just like&#160;in Fig. 10.9) to generate the output sequence </span><span class="font64" style="font-weight:bold;font-style:italic;">Y</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">= (y<sup>(1)</sup></span><span class="font64">,..., </span><span class="font64" style="font-weight:bold;">y))</span><span class="font64">. The innovation&#160;of this kind of architecture over those presented in earlier sections of this chapter is&#160;that the lengths </span><span class="font64" style="font-weight:bold;font-style:italic;">n<sub>x</sub></span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">n<sub>y</sub></span><span class="font64"> can vary from each other, while previous architectures&#160;constrained n<sub>x</sub> = n<sub>y</sub> = </span><span class="font63" style="font-variant:small-caps;">t</span><span class="font64">. In a sequence-to-sequence architecture, the two RNNs</span></p><div><div><img src="main-128.jpg" alt=""/>
<p><span class="font64">Figure 10.12: Example of an encoder-decoder or sequence-to-sequence RNN architecture, for learning to generate an output sequence </span><span class="font64" style="font-style:italic;">(y</span><span class="font64"><sup>(1)</sup>,...,y<sup>(ny)</sup>) given an input sequence&#160;(x<sup>(1)</sup>, x<sup>(2)</sup>,..., x<sup>(nx)</sup>). It is composed of an encoder RNN that reads the input sequence&#160;and a decoder RNN that generates the output sequence (or computes the probability of a&#160;given output sequence). The final hidden state of the encoder RNN is used to compute a&#160;generally fixed-size context variable C which represents a semantic summary of the input&#160;sequence and is given as input to the decoder RNN.</span></p></div></div>
<p><span class="font64">are trained jointly to maximize the average of log P (</span><span class="font64" style="font-weight:bold;">y<sup>(1)</sup></span><span class="font64">,..., </span><span class="font64" style="font-weight:bold;">y<sup>(ny)</sup> | x<sup>(1)</sup></span><span class="font64">,..., </span><span class="font64" style="font-weight:bold;">x<sup>(nx)</sup></span><span class="font64">) over all the pairs of </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">and </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">sequences in the training set. The last state </span><span class="font64" style="font-weight:bold;">h</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>x</sub></span><span class="font64"> of&#160;the encoder RNN is typically used as a representation C of the input sequence&#160;that is provided as input to the decoder RNN.</span></p>
<p><span class="font64">If the context C is a vector, then the decoder RNN is simply a vector-to-sequence RNN as described in Sec. 10.2.4. As we have seen, there are at least two ways for a vector-to-sequence RNN to receive input. The input can be provided as&#160;the initial state of the RNN, or the input can be connected to the hidden units at&#160;each time step. These two ways can also be combined.</span></p>
<p><span class="font64">There is no constraint that the encoder must have the same size of hidden layer as the decoder.</span></p>
<p><span class="font64">One clear limitation of this architecture is when the context C output by the encoder RNN has a dimension that is too small to properly summarize a long&#160;sequence. This phenomenon was observed by Bahdanau </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) in the context&#160;of machine translation. They proposed to make C a variable-length sequence rather&#160;than a fixed-size vector. Additionally, they introduced an </span><span class="font64" style="font-weight:bold;font-style:italic;">attention mechanism&#160;</span><span class="font64">that learns to associate elements of the sequence C to elements of the output&#160;sequence. See Sec. 12.4.5.1 for more details.</span></p>
<p><a id="bookmark6"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font64"><sup></sup> The conditional distribution over these variables given their parents is deterministic. This is perfectly legitimate, though it is somewhat rare to design a graphical model with such deterministic&#160;hidden units.</span></p>
</body>
</html>