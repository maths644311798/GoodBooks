<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 15</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Representation Learning</span></h2>
<p><span class="font64">In this chapter, we first discuss what it means to learn representations and how the notion of representation can be useful to design deep architectures. We discuss&#160;how learning algorithms share statistical strength across different tasks, including&#160;using information from unsupervised tasks to perform supervised tasks. Shared&#160;representations are useful to handle multiple modalities or domains, or to transfer&#160;learned knowledge to tasks for which few or no examples are given but a task&#160;representation exists. Finally, we step back and argue about the reasons for the&#160;success of representation learning, starting with the theoretical advantages of&#160;distributed representations (Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1986) and deep representations and&#160;ending with the more general idea of underlying assumptions about the data&#160;generating process, in particular about underlying causes of the observed data.</span></p>
<p><span class="font64">Many information processing tasks can be very easy or very difficult depending on how the information is represented. This is a general principle applicable to&#160;daily life, computer science in general, and to machine learning. For example, it&#160;is straightforward for a person to divide 210 by 6 using long division. The task&#160;becomes considerably less straightforward if it is instead posed using the Roman&#160;numeral representation of the numbers. Most modern people asked to divide CCX&#160;by VI would begin by converting the numbers to the Arabic numeral representation,&#160;permitting long division procedures that make use of the place value system. More&#160;concretely, we can quantify the asymptotic runtime of various operations using&#160;appropriate or inappropriate representations. For example, inserting a number&#160;into the correct position in a sorted list of numbers is an O(n) operation if the&#160;list is represented as a linked list, but only O(log n) if the list is represented as a&#160;red-black tree.</span></p>
<p><span class="font64">In the context of machine learning, what makes one representation better than another? Generally speaking, a good representation is one that makes a subsequent&#160;learning task easier. The choice of representation will usually depend on the choice&#160;of the subsequent learning task.</span></p>
<p><span class="font64">We can think of feedforward networks trained by supervised learning as performing a kind of representation learning. Specifically, the last layer of the network is typically a linear classifier, such as a softmax regression classifier. The rest of&#160;the network learns to provide a representation to this classifier. Training with a&#160;supervised criterion naturally leads to the representation at every hidden layer (but&#160;more so near the top hidden layer) taking on properties that make the classification&#160;task easier. For example, classes that were not linearly separable in the input&#160;features may become linearly separable in the last hidden layer. In principle, the&#160;last layer could be another kind of model, such as a nearest neighbor classifier&#160;(Salakhutdinov and Hinton, 2007a). The features in the penultimate layer should&#160;learn different properties depending on the type of the last layer.</span></p>
<p><span class="font64">Supervised training of feedforward networks does not involve explicitly imposing any condition on the learned intermediate features. Other kinds of representation&#160;learning algorithms are often explicitly designed to shape the representation in&#160;some particular way. For example, suppose we want to learn a representation that&#160;makes density estimation easier. Distributions with more independences are easier&#160;to model, so we could design an objective function that encourages the elements&#160;of the representation vector h to be independent. Just like supervised networks,&#160;unsupervised deep learning algorithms have a main training objective but also&#160;learn a representation as a side effect. Regardless of how a representation was&#160;obtained, it can can be used for another task. Alternatively, multiple tasks (some&#160;supervised, some unsupervised) can be learned together with some shared internal&#160;representation.</span></p>
<p><span class="font64">Most representation learning problems face a tradeoff between preserving as much information about the input as possible and attaining nice properties (such&#160;as independence).</span></p>
<p><span class="font64">Representation learning is particularly interesting because it provides one way to perform unsupervised and semi-supervised learning. We often have very&#160;large amounts of unlabeled training data and relatively little labeled training&#160;data. Training with supervised learning techniques on the labeled subset often&#160;results in severe overfitting. Semi-supervised learning offers the chance to resolve&#160;this overfitting problem by also learning from the unlabeled data. Specifically,&#160;we can learn good representations for the unlabeled data, and then use these&#160;representations to solve the supervised learning task.</span></p>
<p><span class="font64">Humans and animals are able to learn from very few labeled examples. We do</span></p>
<p><span class="font64">not yet know how this is possible. Many factors could explain improved human performance—for example, the brain may use very large ensembles of classifiers&#160;or Bayesian inference techniques. One popular hypothesis is that the brain is&#160;able to leverage unsupervised or semi-supervised learning. There are many ways&#160;to leverage unlabeled data. In this chapter, we focus on the hypothesis that the&#160;unlabeled data can be used to learn a good representation.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">15.1 Greedy Layer-Wise Unsupervised Pretraining</span></h4>
<p><span class="font64">Unsupervised learning played a key historical role in the revival of deep neural networks, allowing for the first time to train a deep supervised network without&#160;requiring architectural specializations like convolution or recurrence. We call this&#160;procedure </span><span class="font64" style="font-style:italic;">unsupervised pretraining</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> or more precisely, </span><span class="font64" style="font-style:italic;">greedy layer-wise unsupervised pretraining</span><span class="font64" style="font-weight:bold;font-style:italic;">.</span><span class="font64"> This procedure is a canonical example of how a representation&#160;learned for one task (unsupervised learning, trying to capture the shape of the&#160;input distribution) can sometimes be useful for another task (supervised learning&#160;with the same input domain).</span></p>
<p><span class="font64">Greedy layer-wise unsupervised pretraining relies on a single-layer representation learning algorithm such as an RBM, a single-layer autoencoder, a sparse coding model, or another model that learns latent representations. Each layer is&#160;pretrained using unsupervised learning, taking the output of the previous layer&#160;and producing as output a new representation of the data, whose distribution (or&#160;its relation to other variables such as categories to predict) is hopefully simpler.&#160;See Algorithm 15.1 for a formal description.</span></p>
<p><span class="font64">Greedy layer-wise training procedures based on unsupervised criteria have long been used to sidestep the difficulty of jointly training the layers of a deep neural net&#160;for a supervised task. This approach dates back at least as far as the Neocognitron&#160;(Fukushima, 1975). The deep learning renaissance of 2006 began with the discovery&#160;that this greedy learning procedure could be used to find a good initialization for&#160;a joint learning procedure over all the layers, and that this approach could be used&#160;to successfully train even fully connected architectures (Hinton </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> 2006; Hinton&#160;and Salakhutdinov, 2006; Hinton, 2006; Bengio </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> 2007; Ranzato </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> 2007a).&#160;Prior to this discovery, only convolutional deep networks or networks whose depth&#160;resulted from recurrence were regarded as feasible to train. Today, we now know&#160;that greedy layer-wise pretraining is not required to train fully connected deep&#160;architectures, but the unsupervised pretraining approach was the first method to&#160;succeed.</span></p>
<p><span class="font64">Greedy layer-wise pretraining is called </span><span class="font64" style="font-style:italic;">greedy</span><span class="font64"> because it is a </span><span class="font64" style="font-style:italic;">greedy algorithm, </span><span class="font64">meaning that it optimizes each piece of the solution independently, one piece at a&#160;time, rather than jointly optimizing all pieces. It is called </span><span class="font64" style="font-weight:bold;font-style:italic;">layer-wise</span><span class="font64"> because these&#160;independent pieces are the layers of the network. Specifically, greedy layer-wise&#160;pretraining proceeds one layer at a time, training the k-th layer while keeping the&#160;previous ones fixed. In particular, the lower layers (which are trained first) are not&#160;adapted after the upper layers are introduced. It is called </span><span class="font64" style="font-weight:bold;font-style:italic;">unsupervised</span><span class="font64"> because each&#160;layer is trained with an unsupervised representation learning algorithm. However&#160;it is also called </span><span class="font64" style="font-weight:bold;font-style:italic;">pretraining,</span><span class="font64"> because it is supposed to be only a first step before&#160;a joint training algorithm is applied to </span><span class="font64" style="font-weight:bold;font-style:italic;">fine-tune</span><span class="font64"> all the layers together. In the&#160;context of a supervised learning task, it can be viewed as a regularizer (in some&#160;experiments, pretraining decreases test error without decreasing training error)&#160;and a form of parameter initialization.</span></p>
<p><span class="font64">It is common to use the word “pretraining” to refer not only to the pretraining stage itself but to the entire two phase protocol that combines the pretraining&#160;phase and a supervised learning phase. The supervised learning phase may involve&#160;training a simple classifier on top of the features learned in the pretraining phase,&#160;or it may involve supervised fine-tuning of the entire network learned in the&#160;pretraining phase. No matter what kind of unsupervised learning algorithm or&#160;what model type is employed, in the vast majority of cases, the overall training&#160;scheme is nearly the same. While the choice of unsupervised learning algorithm&#160;will obviously impact the details, most applications of unsupervised pretraining&#160;follow this basic protocol.</span></p>
<p><span class="font64">Greedy layer-wise unsupervised pretraining can also be used as initialization for other unsupervised learning algorithms, such as deep autoencoders (Hinton&#160;and Salakhutdinov, 2006) and probabilistic models with many layers of latent&#160;variables. Such models include deep belief networks (Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006) and deep&#160;Boltzmann machines (Salakhutdinov and Hinton, 2009a). These deep generative&#160;models will be described in Chapter 20.</span></p>
<p><span class="font64">As discussed in Sec. 8.7.4, it is also possible to have greedy layer-wise </span><span class="font64" style="font-weight:bold;font-style:italic;">supervised</span><span class="font64"> pretraining. This builds on the premise that training a shallow network is easier than training a deep one, which seems to have been validated in several&#160;contexts (Erhan </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010).</span></p><h5><a id="bookmark2"></a><span class="font64" style="font-weight:bold;">15.1.1 When and Why Does Unsupervised Pretraining Work?</span></h5>
<p><span class="font64">On many tasks, greedy layer-wise unsupervised pretraining can yield substantial improvements in test error for classification tasks. This observation was responsible&#160;for the renewed interested in deep neural networks starting in 2006 (Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span></p>
<p><span class="font64" style="font-weight:bold;">Algorithm 15.1 Greedy layer-wise unsupervised pretraining protocol</span><span class="font64">.</span></p>
<p><span class="font64">Given the following: Unsupervised feature learning algorithm L, which takes a training set of examples and returns an encoder or feature function f. The raw&#160;input data is X, with one row per example and f <sup>(1)</sup>(X) is the output of the first&#160;stage encoder on X and the dataset used by the second level unsupervised feature&#160;learner. In the case where fine-tuning is performed, we use a learner T which takes&#160;an initial function f, input examples X (and in the supervised fine-tuning case,&#160;associated targets Y), and returns a tuned function. The number of stages is </span><span class="font64" style="font-weight:bold;font-style:italic;">m.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">f ^</span><span class="font64"> Identity function</span></p>
<p><span class="font63" style="font-style:italic;">X </span><span class="font64" style="font-weight:bold;font-style:italic;">= </span><span class="font63" style="font-style:italic;">X</span></p>
<p><span class="font64" style="font-weight:bold;">for </span><span class="font64" style="font-weight:bold;font-style:italic;">k =</span><span class="font64"> </span><span class="font18">1</span><span class="font64">,..., m </span><span class="font64" style="font-weight:bold;">do</span></p>
<p><span class="font64">f<sup>(k)</sup> = L(X)</span></p>
<p><span class="font64">f ^ f<sup>(k)</sup> ◦ f </span><span class="font63" style="font-style:italic;">X </span><span class="font64" style="font-weight:bold;font-style:italic;">^ f</span><span class="font64"> <sup>(k)</sup>(X)</span></p>
<p><span class="font64" style="font-weight:bold;">end for</span></p>
<p><span class="font64" style="font-weight:bold;">if </span><span class="font64" style="font-weight:bold;font-style:italic;">fine-tuning then f&#160;&#160;&#160;&#160;(f, </span><span class="font63" style="font-style:italic;">X</span><span class="font64" style="font-weight:bold;font-style:italic;">, </span><span class="font63" style="font-style:italic;">Y</span><span class="font64">)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">end if</span></p>
<p><span class="font64" style="font-weight:bold;">Return </span><span class="font64">f</span></p>
<p><span class="font64">2006; Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2007; Ranzato </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2007a). On many other tasks, however, unsupervised pretraining either does not confer a benefit or even causes noticeable&#160;harm. Ma </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) studied the effect of pretraining on machine learning&#160;models for chemical activity prediction and found that, on average, pretraining was&#160;slightly harmful, but for many tasks was significantly helpful. Because unsupervised&#160;pretraining is sometimes helpful but often harmful it is important to understand&#160;when and why it works in order to determine whether it is applicable to a particular&#160;task.</span></p>
<p><span class="font64">At the outset, it is important to clarify that most of this discussion is restricted to greedy unsupervised pretraining in particular. There are other, completely&#160;different paradigms for performing semi-supervised learning with neural networks,&#160;such as virtual adversarial training described in Sec. 7.13. It is also possible to&#160;train an autoencoder or generative model at the same time as the supervised model.&#160;Examples of this single-stage approach include the discriminative RBM (Larochelle&#160;and Bengio, 2008) and the ladder network (Rasmus </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015), in which the total&#160;objective is an explicit sum of the two terms (one using the labels and one only&#160;using the input).</span></p>
<p><span class="font64">Unsupervised pretraining combines two different ideas. First, it makes use of the idea that the choice of initial parameters for a deep neural network can have&#160;a significant regularizing effect on the model (and, to a lesser extent, that it can&#160;improve optimization). Second, it makes use of the more general idea that learning&#160;about the input distribution can help to learn about the mapping from inputs to&#160;outputs.</span></p>
<p><span class="font64">Both of these ideas involve many complicated interactions between several parts of the machine learning algorithm that are not entirely understood.</span></p>
<p><span class="font64">The first idea, that the choice of initial parameters for a deep neural network can have a strong regularizing effect on its performance, is the least well understood.&#160;At the time that pretraining became popular, it was understood as initializing the&#160;model in a location that would cause it to approach one local minimum rather than&#160;another. Today, local minima are no longer considered to be a serious problem&#160;for neural network optimization. We now know that our standard neural network&#160;training procedures usually do not arrive at a critical point of any kind. It remains&#160;possible that pretraining initializes the model in a location that would otherwise&#160;be inaccessible—for example, a region that is surrounded by areas where the cost&#160;function varies so much from one example to another that minibatches give only&#160;a very noisy estimate of the gradient, or a region surrounded by areas where the&#160;Hessian matrix is so poorly conditioned that gradient descent methods must use&#160;very small steps. However, our ability to characterize exactly what aspects of the&#160;pretrained parameters are retained during the supervised training stage is limited.&#160;This is one reason that modern approaches typically use simultaneous unsupervised&#160;learning and supervised learning rather than two sequential stages. One may&#160;also avoid struggling with these complicated ideas about how optimization in the&#160;supervised learning stage preserves information from the unsupervised learning&#160;stage by simply freezing the parameters for the feature extractors and using&#160;supervised learning only to add a classifier on top of the learned features.</span></p>
<p><span class="font64">The other idea, that a learning algorithm can use information learned in the unsupervised phase to perform better in the supervised learning stage, is better&#160;understood. The basic idea is that some features that are useful for the unsupervised&#160;task may also be useful for the supervised learning task. For example, if we train&#160;a generative model of images of cars and motorcycles, it will need to know about&#160;wheels, and about how many wheels should be in an image. If we are fortunate,&#160;the representation of the wheels will take on a form that is easy for the supervised&#160;learner to access. This is not yet understood at a mathematical, theoretical level,&#160;so it is not always possible to predict which tasks will benefit from unsupervised&#160;learning in this way. Many aspects of this approach are highly dependent on&#160;the specific models used. For example, if we wish to add a linear classifier on&#160;top of pretrained features, the features must make the underlying classes linearly&#160;separable. These properties often occur naturally but do not always do so. This&#160;is another reason that simultaneous supervised and unsupervised learning can be&#160;preferable—the constraints imposed by the output layer are naturally included&#160;from the start.</span></p>
<p><span class="font64">From the point of view of unsupervised pretraining as learning a representation, we can expect unsupervised pretraining to be more effective when the initial&#160;representation is poor. One key example of this is the use of word embeddings.&#160;Words represented by one-hot vectors are not very informative because every two&#160;distinct one-hot vectors are the same distance from each other (squared </span><span class="font64" style="font-weight:bold;font-style:italic;">L<sup>2</sup></span><span class="font64"> distance&#160;of 2). Learned word embeddings naturally encode similarity between words by their&#160;distance from each other. Because of this, unsupervised pretraining is especially&#160;useful when processing words. It is less useful when processing images, perhaps&#160;because images already lie in a rich vector space where distances provide a low&#160;quality similarity metric.</span></p>
<p><span class="font64">From the point of view of unsupervised pretraining as a regularizer, we can expect unsupervised pretraining to be most helpful when the number of labeled&#160;examples is very small. Because the source of information added by unsupervised&#160;pretraining is the unlabeled data, we may also expect unsupervised pretraining&#160;to perform best when the number of unlabeled examples is very large. The&#160;advantage of semi-supervised learning via unsupervised pretraining with many&#160;unlabeled examples and few labeled examples was made particularly clear in&#160;</span><span class="font18">2011</span><span class="font64"> with unsupervised pretraining winning two international transfer learning&#160;competitions (Mesnil </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011; Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011), in settings where the&#160;number of labeled examples in the target task was small (from a handful to dozens&#160;of examples per class). These effects were also documented in carefully controlled&#160;experiments by Paine </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014).</span></p>
<p><span class="font64">Other factors are likely to be involved. For example, unsupervised pretraining is likely to be most useful when the function to be learned is extremely complicated.&#160;Unsupervised learning differs from regularizers like weight decay because it does not&#160;bias the learner toward discovering a simple function but rather toward discovering&#160;feature functions that are useful for the unsupervised learning task. If the true&#160;underlying functions are complicated and shaped by regularities of the input&#160;distribution, unsupervised learning can be a more appropriate regularizer.</span></p>
<p><span class="font64">These caveats aside, we now analyze some success cases where unsupervised pretraining is known to cause an improvement, and explain what is known about&#160;why this improvement occurs. Unsupervised pretraining has usually been used&#160;to improve classifiers, and is usually most interesting from the point of view of</span></p>
<p><span class="font11" style="font-weight:bold;">1500 1000&#160;500&#160;0</span></p><div><div><img src="main-155.jpg" alt=""/>
<p><span class="font11" style="font-weight:bold;">4000 &#160;&#160;&#160;3000&#160;&#160;&#160;&#160;2000&#160;&#160;&#160;&#160;1000&#160;&#160;&#160;&#160;0&#160;&#160;&#160;&#160;1000&#160;&#160;&#160;&#160;2000&#160;&#160;&#160;&#160;3000&#160;&#160;&#160;&#160;4000</span></p></div></div>
<p><span class="font11" style="font-weight:bold;">500 1000&#160;1500</span></p>
<p><span class="font64">Figure 15.1: Visualization via nonlinear projection of the learning trajectories of different neural networks in </span><span class="font64" style="font-weight:bold;">function space </span><span class="font64">(not parameter space, to avoid the issue of many-to-one mappings from parameter vectors to functions), with different random initializations&#160;and with or without unsupervised pretraining. Each point corresponds to a different&#160;neural network, at a particular time during its training process. This figure is adapted&#160;with permission from Erhan </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2010). A coordinate in function space is an infinitedimensional vector associating every input x with an output y. Erhan </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2010) made&#160;a linear projection to high-dimensional space by concatenating the y for many specific x&#160;points. They then made a further nonlinear projection to 2-D by Isomap (Tenenbaum&#160;</span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2000). Color indicates time. All networks are initialized near the center of the plot&#160;(corresponding to the region of functions that produce approximately uniform distributions&#160;over the class </span><span class="font64" style="font-style:italic;">y</span><span class="font64"> for most inputs). Over time, learning moves the function outward, to&#160;points that make strong predictions. Training consistently terminates in one region when&#160;using pretraining and in another, non-overlapping region when not using pretraining.&#160;Isomap tries to preserve global relative distances (and hence volumes) so the small region&#160;corresponding to pretrained models may indicate that the pretraining-based estimator&#160;has reduced variance.</span></p>
<p><span class="font64">reducing test set error. However, unsupervised pretraining can help tasks other than classification, and can act to improve optimization rather than being merely&#160;a regularizer. For example, it can improve both train and test reconstruction error&#160;for deep autoencoders (Hinton and Salakhutdinov, 2006).</span></p>
<p><span class="font64">Erhan </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2010) performed many experiments to explain several successes of unsupervised pretraining. Both improvements to training error and improvements&#160;to test error may be explained in terms of unsupervised pretraining taking the&#160;parameters into a region that would otherwise be inaccessible. Neural network&#160;training is non-deterministic, and converges to a different function every time it&#160;is run. Training may halt at a point where the gradient becomes small, a point&#160;where early stopping ends training to prevent overfitting, or at a point where the&#160;gradient is large but it is difficult to find a downhill step due to problems such as&#160;stochasticity or poor conditioning of the Hessian. Neural networks that receive&#160;unsupervised pretraining consistently halt in the same region of function space,&#160;while neural networks without pretraining consistently halt in another region. See&#160;Fig. 15.1 for a visualization of this phenomenon. The region where pretrained&#160;networks arrive is smaller, suggesting that pretraining reduces the variance of the&#160;estimation process, which can in turn reduce the risk of severe over-fitting. In&#160;other words, unsupervised pretraining initializes neural network parameters into&#160;a region that they do not escape, and the results following this initialization are&#160;more consistent and less likely to be very bad than without this initialization.</span></p>
<p><span class="font64">Erhan </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2010) also provide some answers as to </span><span class="font64" style="font-weight:bold;">when </span><span class="font64">pretraining works best—the mean and variance of the test error were most reduced by pretraining for&#160;deeper networks. Keep in mind that these experiments were performed before the&#160;invention and popularization of modern techniques for training very deep networks&#160;(rectified linear units, dropout and batch normalization) so less is known about the&#160;effect of unsupervised pretraining in conjunction with contemporary approaches.</span></p>
<p><span class="font64">An important question is how unsupervised pretraining can act as a regularizer. One hypothesis is that pretraining encourages the learning algorithm to discover&#160;features that relate to the underlying causes that generate the observed data.&#160;This is an important idea motivating many other algorithms besides unsupervised&#160;pretraining, and is described further in Sec. 15.3.</span></p>
<p><span class="font64">Compared to other ways of incorporating this belief by using unsupervised learning, unsupervised pretraining has the disadvantage that it operates with&#160;two separate training phases. One reason that these two training phases are&#160;disadvantageous is that there is not a single hyperparameter that predictably&#160;reduces or increases the strength of the regularization arising from the unsupervised&#160;pretraining. Instead, there are very many hyperparameters, whose effect may be&#160;measured after the fact but is often difficult to predict ahead of time. When we&#160;perform unsupervised and supervised learning simultaneously, instead of using the&#160;pretraining strategy, there is a single hyperparameter, usually a coefficient attached&#160;to the unsupervised cost, that determines how strongly the unsupervised objective&#160;will regularize the supervised model. One can always predictably obtain less&#160;regularization by decreasing this coefficient. In the case of unsupervised pretraining,&#160;there is not a way of flexibly adapting the strength of the regularization—either&#160;the supervised model is initialized to pretrained parameters, or it is not.</span></p>
<p><span class="font64">Another disadvantage of having two separate training phases is that each phase has its own hyperparameters. The performance of the second phase usually cannot&#160;be predicted during the first phase, so there is a long delay between proposing&#160;hyperparameters for the first phase and being able to update them using feedback&#160;from the second phase. The most principled approach is to use validation set error&#160;in the supervised phase in order to select the hyperparameters of the pretraining&#160;phase, as discussed in Larochelle </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2009). In practice, some hyperparameters,&#160;like the number of pretraining iterations, are more conveniently set during the&#160;pretraining phase, using early stopping on the unsupervised objective, which is&#160;not ideal but computationally much cheaper than using the supervised objective.</span></p>
<p><span class="font64">Today, unsupervised pretraining has been largely abandoned, except in the field of natural language processing, where the natural representation of words as&#160;one-hot vectors conveys no similarity information and where very large unlabeled&#160;sets are available. In that case, the advantage of pretraining is that one can pretrain&#160;once on a huge unlabeled set (for example with a corpus containing billions of&#160;words), learn a good representation (typically of words, but also of sentences), and&#160;then use this representation or fine-tune it for a supervised task for which the&#160;training set contains substantially fewer examples. This approach was pioneered&#160;by by Collobert and Weston (2008b), Turian </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2010), and Collobert </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(</span><span class="font18">2011</span><span class="font64">a) and remains in common use today.</span></p>
<p><span class="font64">Deep learning techniques based on supervised learning, regularized with dropout or batch normalization, are able to achieve human-level performance on very many&#160;tasks, but only with extremely large labeled datasets. These same techniques&#160;outperform unsupervised pretraining on medium-sized datasets such as CIFAR-10&#160;and MNIST, which have roughly 5,000 labeled examples per class. On extremely&#160;small datasets, such as the alternative splicing dataset, Bayesian methods outperform methods based on unsupervised pretraining (Srivastava, 2013). For these&#160;reasons, the popularity of unsupervised pretraining has declined. Nevertheless,&#160;unsupervised pretraining remains an important milestone in the history of deep&#160;learning research and continues to influence contemporary approaches. The idea of&#160;pretraining has been generalized to </span><span class="font64" style="font-weight:bold;font-style:italic;">supervised pretraining</span><span class="font64"> discussed in Sec. 8.7.4,&#160;as a very common approach for transfer learning. Supervised pretraining for&#160;transfer learning is popular (Oquab </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014; Yosinski </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014) for use with&#160;convolutional networks pretrained on the ImageNet dataset. Practitioners publish&#160;the parameters of these trained networks for this purpose, just like pretrained word&#160;vectors are published for natural language tasks (Collobert </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011a; Mikolov&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013a).</span></p><h4><a id="bookmark3"></a><span class="font65" style="font-weight:bold;">15.2 Transfer Learning and Domain Adaptation</span></h4>
<p><span class="font64">Transfer learning and domain adaptation refer to the situation where what has been learned in one setting (i.e., distribution P!) is exploited to improve generalization&#160;in another setting (say distribution P</span><span class="font18">2</span><span class="font64">). This generalizes the idea presented in the&#160;previous section, where we transferred representations between an unsupervised&#160;learning task and a supervised learning task.</span></p>
<p><span class="font64">In </span><span class="font64" style="font-weight:bold;font-style:italic;">transfer learning,</span><span class="font64"> the learner must perform two or more different tasks, but we assume that many of the factors that explain the variations in P! are&#160;relevant to the variations that need to be captured for learning P</span><span class="font18">2</span><span class="font64">. This is typically&#160;understood in a supervised learning context, where the input is the same but the&#160;target may be of a different nature. For example, we may learn about one set of&#160;visual categories, such as cats and dogs, in the first setting, then learn about a&#160;different set of visual categories, such as ants and wasps, in the second setting. If&#160;there is significantly more data in the first setting (sampled from P!), then that&#160;may help to learn representations that are useful to quickly generalize from only&#160;very few examples drawn from P</span><span class="font18">2</span><span class="font64">. Many visual categories </span><span class="font64" style="font-weight:bold;">share </span><span class="font64">low-level notions&#160;of edges and visual shapes, the effects of geometric changes, changes in lighting, etc.&#160;In general, transfer learning, multi-task learning (Sec. 7.7), and domain adaptation&#160;can be achieved via representation learning when there exist features that are&#160;useful for the different settings or tasks, corresponding to underlying factors that&#160;appear in more than one setting. This is illustrated in Fig. 7.2, with shared lower&#160;layers and task-dependent upper layers.</span></p>
<p><span class="font64">However, sometimes, what is shared among the different tasks is not the semantics of the input but the semantics of the output. For example, a speech&#160;recognition system needs to produce valid sentences at the output layer, but&#160;the earlier layers near the input may need to recognize very different versions of&#160;the same phonemes or sub-phonemic vocalizations depending on which person&#160;is speaking. In cases like these, it makes more sense to share the upper layers&#160;(near the output) of the neural network, and have a task-specific preprocessing, as</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font64">illustrated in Fig. 15.2.</span></p></td><td>
<p></p></td><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font20">( </span><span class="font64" style="font-weight:bold;font-style:italic;">y </span><span class="font20" style="font-style:italic;">)</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font64">h(shared)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font63">\ Selection switch</span></p></td></tr>
<tr><td>
<p><span class="font20">\ </span><span class="font64">^<sup>(1)</sup> </span><span class="font20">)</span></p></td><td>
<p><span class="font20">( </span><span class="font64">h<sup>(2</sup></span><span class="font20">M</span></p></td><td style="vertical-align:middle;">
<p><span class="font20">( </span><span class="font64">h<sup>(3)</sup> </span><span class="font20">'</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">x<sup>(1)</sup></span></p></td><td style="vertical-align:middle;">
<p><span class="font20" style="font-style:italic;">f</span><span class="font20"> </span><span class="font64">*<sup>(2</sup></span><span class="font20">n</span></p></td><td style="vertical-align:middle;">
<p><span class="font20">i </span><span class="font63" style="font-style:italic;">x<sup>(3)</sup></span></p></td></tr>
</table>
<p><span class="font64">Figure 15.2: Example architecture for multi-task or transfer learning when the output variable y has the same semantics for all tasks while the input variable x has a different&#160;meaning (and possibly even a different dimension) for each task (or, for example, each&#160;user), called x<sup>(1)</sup>, x<sup>(2)</sup> and x<sup>(3)</sup> for three tasks. The lower levels (up to the selection&#160;switch) are task-specific, while the upper levels are shared. The lower levels learn to&#160;translate their task-specific input into a generic set of features.</span></p>
<p><span class="font64">In the related case of </span><span class="font64" style="font-weight:bold;font-style:italic;">domain adaptation</span><span class="font64">, the task (and the optimal input-to-output mapping) remains the same between each setting, but the input distribution is slightly different. For example, consider the task of sentiment analysis, which&#160;consists of determining whether a comment expresses positive or negative sentiment.&#160;Comments posted on the web come from many categories. A domain adaptation&#160;scenario can arise when a sentiment predictor trained on customer reviews of&#160;media content such as books, videos and music is later used to analyze comments&#160;about consumer electronics such as televisions or smartphones. One can imagine&#160;that there is an underlying function that tells whether any statement is positive,&#160;neutral or negative, but of course the vocabulary and style may vary from one&#160;domain to another, making it more difficult to generalize across domains. Simple&#160;unsupervised pretraining (with denoising autoencoders) has been found to be very&#160;successful for sentiment analysis with domain adaptation (Glorot </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011b).</span></p>
<p><span class="font64">A related problem is that of </span><span class="font64" style="font-weight:bold;font-style:italic;">concept drift,</span><span class="font64"> which we can view as a form of transfer learning due to gradual changes in the data distribution over time. Both concept&#160;drift and transfer learning can be viewed as particular forms of multi-task learning.</span></p>
<p><span class="font64">While the phrase “multi-task learning” typically refers to supervised learning tasks, the more general notion of transfer learning is applicable to unsupervised learning&#160;and reinforcement learning as well.</span></p>
<p><span class="font64">In all of these cases, the objective is to take advantage of data from the first setting to extract information that may be useful when learning or even when&#160;directly making predictions in the second setting. The core idea of representation&#160;learning is that the same representation may be useful in both settings. Using the&#160;same representation in both settings allows the representation to benefit from the&#160;training data that is available for both tasks.</span></p>
<p><span class="font64">As mentioned before, unsupervised deep learning for transfer learning has found success in some machine learning competitions (Mesnil </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011; Goodfellow&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011). In the first of these competitions, the experimental setup is the&#160;following. Each participant is first given a dataset from the first setting (from&#160;distribution P!), illustrating examples of some set of categories. The participants&#160;must use this to learn a good feature space (mapping the raw input to some&#160;representation), such that when we apply this learned transformation to inputs&#160;from the transfer setting (distribution P</span><span class="font18">2</span><span class="font64">), a linear classifier can be trained and&#160;generalize well from very few labeled examples. One of the most striking results&#160;found in this competition is that as an architecture makes use of deeper and&#160;deeper representations (learned in a purely unsupervised way from data collected&#160;in the first setting, P!), the learning curve on the new categories of the second&#160;(transfer) setting P</span><span class="font18">2</span><span class="font64"> becomes much better. For deep representations, fewer labeled&#160;examples of the transfer tasks are necessary to achieve the apparently asymptotic&#160;generalization performance.</span></p>
<p><span class="font64">Two extreme forms of transfer learning are </span><span class="font64" style="font-weight:bold;font-style:italic;">one-shot learning</span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">zero-shot learning,</span><span class="font64"> sometimes also called </span><span class="font64" style="font-weight:bold;font-style:italic;">zero-data learning.</span><span class="font64"> Only one labeled example of the&#160;transfer task is given for one-shot learning, while no labeled examples are given at&#160;all for the zero-shot learning task.</span></p>
<p><span class="font64">One-shot learning (Fei-Fei </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006) is possible because the representation learns to cleanly separate the underlying classes during the first stage. During the&#160;transfer learning stage, only one labeled example is needed to infer the label of many&#160;possible test examples that all cluster around the same point in representation&#160;space. This works to the extent that the factors of variation corresponding to&#160;these invariances have been cleanly separated from the other factors, in the learned&#160;representation space, and we have somehow learned which factors do and do not&#160;matter when discriminating objects of certain categories.</span></p>
<p><span class="font64">As an example of a zero-shot learning setting, consider the problem of having a learner read a large collection of text and then solve object recognition problems.</span></p>
<p><span class="font64">It may be possible to recognize a specific object class even without having seen an image of that object, if the text describes the object well enough. For example,&#160;having read that a cat has four legs and pointy ears, the learner might be able to&#160;guess that an image is a cat, without having seen a cat before.</span></p>
<p><span class="font64">Zero-data learning (Larochelle </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2008) and zero-shot learning (Palatucci </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009; Socher </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013b) are only possible because additional information&#160;has been exploited during training. We can think of the zero-data learning scenario&#160;as including three random variables: the traditional inputs x, the traditional&#160;outputs or targets y, and an additional random variable describing the task, T.&#160;The model is trained to estimate the conditional distribution p(y | x,T) where&#160;T is a description of the task we wish the model to perform. In our example of&#160;recognizing cats after having read about cats, the output is a binary variable y&#160;with y = 1 indicating “yes” and y = 0 indicating “no.” The task variable T then&#160;represents questions to be answered such as “Is there a cat in this image?” If we&#160;have a training set containing unsupervised examples of objects that live in the&#160;same space as T, we may be able to infer the meaning of unseen instances of T.&#160;In our example of recognizing cats without having seen an image of the cat, it is&#160;important that we have had unlabeled text data containing sentences such as “cats&#160;have four legs” or “cats have pointy ears.”</span></p>
<p><span class="font64">Zero-shot learning requires T to be represented in a way that allows some sort of generalization. For example, T cannot be just a one-hot code indicating an&#160;object category. Socher </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013b) provide instead a distributed representation&#160;of object categories by using a learned word embedding for the word associated&#160;with each category.</span></p>
<p><span class="font64">A similar phenomenon happens in machine translation (Klementiev </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012; Mikolov </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013b; Gouws </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014): we have words in one language, and&#160;the relationships between words can be learned from unilingual corpora; on the&#160;other hand, we have translated sentences which relate words in one language with&#160;words in the other. Even though we may not have labeled examples translating&#160;word A in language X to word B in language </span><span class="font64" style="font-weight:bold;font-style:italic;">Y</span><span class="font64">, we can generalize and guess a&#160;translation for word A because we have learned a distributed representation for&#160;words in language X, a distributed representation for words in language Y, and&#160;created a link (possibly two-way) relating the two spaces, via training examples&#160;consisting of matched pairs of sentences in both languages. This transfer will be&#160;most successful if all three ingredients (the two representations and the relations&#160;between them) are learned jointly.</span></p>
<p><span class="font64">Zero-shot learning is a particular form of transfer learning. The same principle explains how one can perform </span><span class="font64" style="font-weight:bold;font-style:italic;">multi-modal learning,</span><span class="font64"> capturing a representation in</span></p>
<p><span class="font63" style="font-style:italic;">h</span><span class="font34" style="font-style:italic;">x </span><span class="font63" style="font-style:italic;">= f</span><span class="font34" style="font-style:italic;">x</span><span class="font62"> (x)</span></p><div><img src="main-156.jpg" alt=""/>
<p><span class="font63">□ </span><span class="font16">1</span><span class="font63">=־ — ► </span><span class="font63" style="font-style:italic;">f<sub>y</sub></span><span class="font63"> : encoder function for y</span></p>
<p><span class="font63" style="font-style:italic;">+ -</span><span class="font63">.....</span><span class="font63" style="font-style:italic;">p.</span><span class="font63"> Relationship between embedded points within one of the domains</span></p></div>
<p><span class="font63">- &#160;&#160;&#160;» Maps between representation spaces</span></p>
<p><span class="font64">Figure 15.3: Transfer learning between two domains x and y enables zero-shot learning. Labeled or unlabeled examples of x allow one to learn a representation function f<sub>x</sub> and&#160;similarly with examples of y to learn f<sub>y</sub>. Each application of the f<sub>x</sub> and f <sub>y</sub> functions&#160;appears as an upward arrow, with the style of the arrows indicating which function is&#160;applied. Distance in </span><span class="font64" style="font-style:italic;">h<sub>x</sub></span><span class="font64"> space provides a similarity metric between any pair of points&#160;in x space that may be more meaningful than distance in x space. Likewise, distance&#160;in h<sub>y</sub> space provides a similarity metric between any pair of points in y space. Both&#160;of these similarity functions are indicated with dotted bidirectional arrows. Labeled&#160;examples (dashed horizontal lines) are pairs (x, y) which allow one to learn a one-way&#160;or two-way map (solid bidirectional arrow) between the representationsf<sub>x</sub> (x) and the&#160;representations f <sub>y</sub> (</span><span class="font64" style="font-style:italic;">y</span><span class="font64">) and anchor these representations to each other. Zero-data learning&#160;is then enabled as follows. One can associate an image x<sub>test</sub> to a word y<sub>test</sub>, even if no&#160;image of that word was ever presented, simply because word-representations f<sub>y</sub> (yt<sub>est</sub>)&#160;and image-representations f<sub>x</sub> (x<sub>test</sub>) can be related to each other via the maps between&#160;representation spaces. It works because, although that image and that word were never&#160;paired, their respective feature vectors f<sub>x</sub>(x<sub>test</sub>) and </span><span class="font64" style="font-style:italic;">f<sub>y</sub>(y</span><span class="font64"><sub>test</sub>) have been related to each&#160;other. Figure inspired from suggestion by Hrant Khachatrian.</span></p>
<p><span class="font64">one modality, a representation in the other, and the relationship (in general a joint distribution) between pairs (x, </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64">) consisting of one observation </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> in one modality&#160;and another observation y in the other modality (Srivastava and Salakhutdinov,&#160;2012). By learning all three sets of parameters (from x to its representation, from&#160;y to its representation, and the relationship between the two representations),&#160;concepts in one representation are anchored in the other, and vice-versa, allowing&#160;one to meaningfully generalize to new pairs. The procedure is illustrated in&#160;Fig. 15.3.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">15.3 Semi-Supervised Disentangling of Causal Factors</span></h4>
<p><span class="font64">An important question about representation learning is “what makes one representation better than another?” One hypothesis is that an ideal representation is one in which the features within the representation correspond to the underlying causes of the observed data, with separate features or directions in feature&#160;space corresponding to different causes, so that the representation disentangles the&#160;causes from one another. This hypothesis motivates approaches in which we first&#160;seek a good representation for p(x). Such a representation may also be a good&#160;representation for computing p( y | x) if y is among the most salient causes of&#160;x. This idea has guided a large amount of deep learning research since at least&#160;the 1990s (Becker and Hinton, 1992; Hinton and Sejnowski, 1999), in more detail.&#160;For other arguments about when semi-supervised learning can outperform pure&#160;supervised learning, we refer the reader to Sec. 1.2 of Chapelle </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2006).</span></p>
<p><span class="font64">In other approaches to representation learning, we have often been concerned with a representation that is easy to model—for example, one whose entries are&#160;sparse, or independent from each other. A representation that cleanly separates&#160;the underlying causal factors may not necessarily be one that is easy to model.&#160;However, a further part of the hypothesis motivating semi-supervised learning&#160;via unsupervised representation learning is that for many AI tasks, these two&#160;properties coincide: once we are able to obtain the underlying explanations for&#160;what we observe, it generally becomes easy to isolate individual attributes from&#160;the others. Specifically, if a representation h represents many of the underlying&#160;causes of the observed x, and the outputs y are among the most salient causes,&#160;then it is easy to predict y from h.</span></p>
<p><span class="font64">First, let us see how semi-supervised learning can fail because unsupervised learning of p(x) is of no help to learn p(y | x). Consider for example the case&#160;where p(x) is uniformly distributed and we want to learn f (x) = E[y | x]. Clearly,&#160;observing a training set of x values alone gives us no information about p(y | x).</span></p><div>
<p><span class="font64">Mixture model</span></p><img src="main-157.jpg" alt=""/>
<p><span class="font64">Figure 15.4: Example of a density over x that is a mixture over three components. The component identity is an underlying explanatory factor, </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64" style="font-style:italic;">.</span><span class="font64"> Because the mixture&#160;components (e.g., natural object classes in image data) are statistically salient, just&#160;modeling p(x) in an unsupervised way with no labeled example already reveals the factor</span></p>
<p><span class="font64">y.</span></p></div>
<p><span class="font64">Next, let us see a simple example of how semi-supervised learning can succeed. Consider the situation where x arises from a mixture, with one mixture component&#160;per value of y, as illustrated in Fig. 15.4. If the mixture components are well-separated, then modeling p(x) reveals precisely where each component is, and a&#160;single labeled example of each class will then be enough to perfectly learn </span><span class="font64" style="font-weight:bold;font-style:italic;">p (y</span><span class="font64"> | x).&#160;But more generally, what could make p(y | x) and p(x) be tied together?</span></p>
<p><span class="font64">If y is closely associated with one of the causal factors of x, then p(x) and p(y | x) will be strongly tied, and unsupervised representation learning that&#160;tries to disentangle the underlying factors of variation is likely to be useful as a&#160;semi-supervised learning strategy.</span></p>
<p><span class="font64">Consider the assumption that y is one of the causal factors of x, and let h represent all those factors. The true generative process can be conceived as&#160;structured according to this directed graphical model, with h as the parent of x:</span></p>
<p><span class="font64">p(h, x) = p(x | h)p(h). &#160;&#160;&#160;(15.1)</span></p>
<p><span class="font64">As a consequence, the data has marginal probability</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(x) =</span><span class="font64"> Ehp(x | h). &#160;&#160;&#160;(15.2)</span></p>
<p><span class="font64">From this straightforward observation, we conclude that the best possible model of x (from a generalization point of view) is the one that uncovers the above “true”&#160;structure, with </span><span class="font64" style="font-weight:bold;">h </span><span class="font64">as a latent variable that explains the observed variations in </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">.&#160;The “ideal” representation learning discussed above should thus recover these latent&#160;factors. If </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">is one of these (or closely related to one of them), then it will be&#160;very easy to learn to predict </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">from such a representation. We also see that the&#160;conditional distribution of </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">given </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">is tied by Bayes rule to the components in&#160;the above equation:</span></p>
<p><span class="font64">p<sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>x </sup></span><span class="font18"><sup>1</sup></span><span class="font64"> </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">)p(</span><span class="font64" style="font-weight:bold;">y</span><span class="font64">)</span></p>
<p><span class="font64"><sup>p(</sup></span><span class="font64" style="font-weight:bold;"><sup>y </sup></span><span class="font18"><sup>1</sup></span><span class="font64"><sup> </sup></span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>)</sup> = &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">p(x)</span><span class="font64">&#160;&#160;&#160;&#160;■&#160;&#160;&#160;&#160;<sup>(15</sup>'<sup>3)</sup></span></p>
<p><span class="font64">Thus the marginal p(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) is intimately tied to the conditional p(</span><span class="font64" style="font-weight:bold;">y </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) and knowledge of the structure of the former should be helpful to learn the latter. Therefore, in&#160;situations respecting these assumptions, semi-supervised learning should improve&#160;performance.</span></p>
<p><span class="font64">An important research problem regards the fact that most observations are formed by an extremely large number of underlying causes. Suppose </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">= hj, but&#160;the unsupervised learner does not know which h<sub>i</sub>. The brute force solution is for&#160;an unsupervised learner to learn a representation that captures </span><span class="font64" style="font-weight:bold;">all </span><span class="font64">the reasonably&#160;salient generative factors hj and disentangles them from each other, thus making&#160;it easy to predict </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">from </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">, regardless of which hj is associated with </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">.</span></p>
<p><span class="font64">In practice, the brute force solution is not feasible because it is not possible to capture all or most of the factors of variation that influence an observation.&#160;For example, in a visual scene, should the representation always encode all of&#160;the smallest objects in the background? It is a well-documented psychological&#160;phenomenon that human beings fail to perceive changes in their environment that&#160;are not immediately relevant to the task they are performing—see, e.g., Simons&#160;and Levin (1998). An important research frontier in semi-supervised learning is&#160;determining </span><span class="font64" style="font-weight:bold;">what </span><span class="font64">to encode in each situation. Currently, two of the main strategies&#160;for dealing with a large number of underlying causes are to use a supervised learning&#160;signal at the same time as the unsupervised learning signal so that the model will&#160;choose to capture the most relevant factors of variation, or to use much larger&#160;representations if using purely unsupervised learning.</span></p>
<p><span class="font64">An emerging strategy for unsupervised learning is to modify the definition of which underlying causes are most salient. Historically, autoencoders and generative&#160;models have been trained to optimize a fixed criterion, often similar to mean&#160;squared error. These fixed criteria determine which causes are considered salient.&#160;For example, mean squared error applied to the pixels of an image implicitly&#160;specifies that an underlying cause is only salient if it significantly changes the&#160;brightness of a large number of pixels. This can be problematic if the task we&#160;wish to solve involves interacting with small objects. See Fig. 15.5 for an example</span></p><div><img src="main-158.jpg" alt=""/>
<p><span class="font64">Figure 15.5: An autoencoder trained with mean squared error for a robotics task has failed to reconstruct a ping pong ball. The existence of the ping pong ball and all of its&#160;spatial coordinates are important underlying causal factors that generate the image and&#160;are relevant to the robotics task. Unfortunately, the autoencoder has limited capacity,&#160;and the training with mean squared error did not identify the ping pong ball as being&#160;salient enough to encode. Images graciously provided by Chelsea Finn.</span></p></div>
<p><span class="font64">of a robotics task in which an autoencoder has failed to learn to encode a small ping pong ball. This same robot is capable of successfully interacting with larger&#160;objects, such as baseballs, which are more salient according to mean squared error.</span></p>
<p><span class="font64">Other definitions of salience are possible. For example, if a group of pixels follow a highly recognizable pattern, even if that pattern does not involve extreme&#160;brightness or darkness, then that pattern could be considered extremely salient.&#160;One way to implement such a definition of salience is to use a recently developed&#160;approach called </span><span class="font64" style="font-weight:bold;font-style:italic;">generative adversarial networks</span><span class="font64"> (Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014c). In&#160;this approach, a generative model is trained to fool a feedforward classifier. The&#160;feedforward classifier attempts to recognize all samples from the generative model&#160;as being fake, and all samples from the training set as being real. In this framework,&#160;any structured pattern that the feedforward network can recognize is highly salient.&#160;The generative adversarial network will be described in more detail in Sec. 20.10.4.&#160;For the purposes of the present discussion, it is sufficient to understand that they&#160;</span><span class="font64" style="font-weight:bold;">learn </span><span class="font64">how to determine what is salient. Lotter </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) showed that models&#160;trained to generate images of human heads will often neglect to generate the ears&#160;when trained with mean squared error, but will successfully generate the ears when&#160;trained with the adversarial framework. Because the ears are not extremely bright&#160;or dark compared to the surrounding skin, they are not especially salient according&#160;to mean squared error loss, but their highly recognizable shape and consistent</span></p><div><img src="main-159.jpg" alt=""/>
<p><span class="font64">Figure 15.6: Predictive generative networks provide an example of the importance of learning which features are salient. In this example, the predictive generative network&#160;has been trained to predict the appearance of a 3-D model of a human head at a specific&#160;viewing angle. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> Ground truth. This is the correct image, that the network should&#160;emit. </span><span class="font64" style="font-style:italic;">(Center)</span><span class="font64"> Image produced by a predictive generative network trained with mean&#160;squared error alone. Because the ears do not cause an extreme difference in brightness&#160;compared to the neighboring skin, they were not sufficiently salient for the model to learn&#160;to represent them. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> Image produced by a model trained with a combination of&#160;mean squared error and adversarial loss. Using this learned cost function, the ears are&#160;salient because they follow a predictable pattern. Learning which underlying causes are&#160;important and relevant enough to model is an important active area of research. Figures&#160;graciously provided by Lotter </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2015).</span></p></div>
<p><span class="font64">position means that a feedforward network can easily learn to detect them, making them highly salient under the generative adversarial framework. See Fig. 15.6&#160;for example images. Generative adversarial networks are only one step toward&#160;determining which factors should be represented. We expect that future research&#160;will discover better ways of determining which factors to represent, and develop&#160;mechanisms for representing different factors depending on the task.</span></p>
<p><span class="font64">A benefit of learning the underlying causal factors, as pointed out by Scholkopf </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (</span><span class="font18">2012</span><span class="font64">), is that if the true generative process has </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">as an effect and </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">as&#160;a cause, then modeling p(</span><span class="font64" style="font-weight:bold;">x </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">) is robust to changes in p(</span><span class="font64" style="font-weight:bold;">y</span><span class="font64">). If the cause-effect&#160;relationship was reversed, this would not be true, since by Bayes rule, p(</span><span class="font64" style="font-weight:bold;">x </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">)&#160;would be sensitive to changes in p( </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">). Very often, when we consider changes in&#160;distribution due to different domains, temporal non-stationarity, or changes in&#160;the nature of the task, </span><span class="font64" style="font-weight:bold;">the causal mechanisms remain invariant </span><span class="font64">(“the laws&#160;of the universe are constant”) while the marginal distribution over the underlying&#160;causes can change. Hence, better generalization and robustness to all kinds of&#160;changes can be expected via learning a generative model that attempts to recover&#160;the causal factors </span><span class="font64" style="font-weight:bold;">h </span><span class="font64">and p(</span><span class="font64" style="font-weight:bold;">x | h</span><span class="font64">).</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">15.4 Distributed Representation</span></h4>
<p><span class="font64">Distributed representations of concepts—representations composed of many elements that can be set separately from each other—are one of the most important tools for representation learning. Distributed representations are powerful because&#160;they can use n features with k values to describe </span><span class="font64" style="font-weight:bold;font-style:italic;">k<sup>n</sup></span><span class="font64"> different concepts. As we&#160;have seen throughout this book, both neural networks with multiple hidden units&#160;and probabilistic models with multiple latent variables make use of the strategy of&#160;distributed representation. We now introduce an additional observation. Many&#160;deep learning algorithms are motivated by the assumption that the hidden units&#160;can learn to represent the underlying causal factors that explain the data, as&#160;discussed in Sec. 15.3. Distributed representations are natural for this approach,&#160;because each direction in representation space can correspond to the value of a&#160;different underlying configuration variable.</span></p>
<p><span class="font64">An example of a distributed representation is a vector of n binary features, which can take </span><span class="font18">2</span><span class="font64"><sup>n</sup> configurations, each potentially corresponding to a different&#160;region in input space, as illustrated in Fig. 15.7. This can be compared with&#160;a </span><span class="font64" style="font-weight:bold;font-style:italic;">symbolic representation,</span><span class="font64"> where the input is associated with a single symbol or&#160;category. If there are n symbols in the dictionary, one can imagine n feature&#160;detectors, each corresponding to the detection of the presence of the associated&#160;category. In that case only n different configurations of the representation space&#160;are possible, carving n different regions in input space, as illustrated in Fig. 15.8.&#160;Such a symbolic representation is also called a one-hot representation, since it can&#160;be captured by a binary vector with n bits that are mutually exclusive (only one&#160;of them can be active). A symbolic representation is a specific example of the&#160;broader class of non-distributed representations, which are representations that&#160;may contain many entries but without significant meaningful separate control over&#160;each entry.</span></p>
<p><span class="font64">Examples of learning algorithms based on non-distributed representations include:</span></p>
<p><span class="font64">• &#160;&#160;&#160;Clustering methods, including the k-means algorithm: each input point is&#160;assigned to exactly one cluster.</span></p>
<p><span class="font64">• &#160;&#160;&#160;k-nearest neighbors algorithms: one or a few templates or prototype examples&#160;are associated with a given input. In the case of k &gt; 1, there are multiple</span></p>
<p><span class="font41" style="font-style:italic;">h2</span><span class="font12"> &#160;&#160;&#160;h</span><span class="font35" style="font-weight:bold;">3</span></p><div><div><img src="main-160.jpg" alt=""/></div></div>
<p><span class="font64">Figure 15.7: Illustration of how a learning algorithm based on a distributed representation breaks up the input space into regions. In this example, there are three binary features&#160;hi, h<sub>2</sub>, and h</span><span class="font19">3</span><span class="font64">. Each feature is defined by thresholding the output of a learned, linear&#160;transformation. Each feature divides R<sup>2</sup> into two half-planes. Let h+ be the set of input&#160;points for which </span><span class="font64" style="font-style:italic;">hi</span><span class="font64"> = 1 and h- be the set of input points for which h = 0. In this&#160;illustration, each line represents the decision boundary for oneh<sub>i</sub>, with the corresponding&#160;arrow pointing to the h+ side of the boundary. The representation as a whole takes&#160;on a unique value at each possible intersection of these half-planes. For example, the&#160;representation value [1,1, 1]<sup>T</sup> corresponds to the region h+ ח h+ H h+. Compare this to&#160;the non-distributed representations in Fig. 15.8. In the general case ofd input dimensions,&#160;a distributed representation dividesR<sup>d</sup> by intersecting half-spaces rather than half-planes.&#160;The distributed representation with n features assigns unique codes to </span><span class="font64" style="font-style:italic;">O(n<sup>d</sup>)</span><span class="font64"> different&#160;regions, while the nearest neighbor algorithm withn examples assigns unique codes to only&#160;n regions. The distributed representation is thus able to distinguish exponentially many&#160;more regions than the non-distributed one. Keep in mind that not allh values are feasible&#160;(there is no h = 0 in this example) and that a linear classifier on top of the distributed&#160;representation is not able to assign different class identities to every neighboring region;&#160;even a deep linear-threshold network has a VC dimension of only O (w log w) where w&#160;is the number of weights (Sontag, 1998). The combination of a powerful representation&#160;layer and a weak classifier layer can be a strong regularizer; a classifier trying to learn&#160;the concept of “person” versus “not a person” does not need to assign a different class to&#160;an input represented as “woman with glasses” than it assigns to an input represented as&#160;“man without glasses.” This capacity constraint encourages each classifier to focus on few&#160;h<sub>i</sub> and encourages h to learn to represent the classes in a linearly separable way.</span></p>
<p><span class="font64">values describing each input, but they can not be controlled separately from each other, so this does not qualify as a true distributed representation.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Decision trees: only one leaf (and the nodes on the path from root to leaf) is&#160;activated when an input is given.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Gaussian mixtures and mixtures of experts: the templates (cluster centers)&#160;or experts are now associated with a </span><span class="font64" style="font-weight:bold;">degree </span><span class="font64">of activation. As with the&#160;k-nearest neighbors algorithm, each input is represented with multiple values,&#160;but those values cannot readily be controlled separately from each other.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Kernel machines with a Gaussian kernel (or other similarly local kernel):&#160;although the degree of activation of each “support vector” or template example&#160;is now continuous-valued, the same issue arises as with Gaussian mixtures.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Language or translation models based on n-grams. The set of contexts&#160;(sequences of symbols) is partitioned according to a tree structure of suffixes.&#160;A leaf may correspond to the last two words being </span><span class="font64" style="font-weight:bold;font-style:italic;">w </span><span class="font62" style="font-style:italic;">1</span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">w</span><span class="font62" style="font-style:italic;">2</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> for example.&#160;Separate parameters are estimated for each leaf of the tree (with some sharing&#160;being possible).</span></p>
<p><span class="font64">For some of these non-distributed algorithms, the output is not constant by parts but instead interpolates between neighboring regions. The relationship&#160;between the number of parameters (or examples) and the number of regions they&#160;can define remains linear.</span></p>
<p><span class="font64">An important related concept that distinguishes a distributed representation from a symbolic one is that </span><span class="font64" style="font-weight:bold;">generalization arises due to shared attributes</span></p>
<p><span class="font64">between different concepts. As pure symbols, “</span><span class="font36" style="font-weight:bold;">cat</span><span class="font64">” and “</span><span class="font36" style="font-weight:bold;">dog</span><span class="font64">” are as far from each other as any other two symbols. However, if one associates them with a meaningful&#160;distributed representation, then many of the things that can be said about cats&#160;can generalize to dogs and vice-versa. For example, our distributed representation&#160;may contain entries such as “</span><span class="font36" style="font-weight:bold;">has_fur</span><span class="font64">” or “</span><span class="font36" style="font-weight:bold;">number_of_legs</span><span class="font64">” that have the same&#160;value for the embedding of both “ </span><span class="font36" style="font-weight:bold;">cat</span><span class="font64">” and “</span><span class="font36" style="font-weight:bold;">dog</span><span class="font64">.” Neural language models that&#160;operate on distributed representations of words generalize much better than other&#160;models that operate directly on one-hot representations of words, as discussed&#160;in Sec. 12.4. Distributed representations induce a rich </span><span class="font64" style="font-weight:bold;font-style:italic;">similarity space,</span><span class="font64"> in which&#160;semantically close concepts (or inputs) are close in distance, a property that is&#160;absent from purely symbolic representations.</span></p>
<p><span class="font64">When and why can there be a statistical advantage from using a distributed representation as part of a learning algorithm? Distributed representations can</span></p><div>
<p><span class="font14">o</span></p></div><div><div><img src="main-161.jpg" alt=""/>
<p><span class="font64">Figure 15.8: Illustration of how the nearest neighbor algorithm breaks up the input space into different regions. The nearest neighbor algorithm provides an example of a learning&#160;algorithm based on a non-distributed representation. Different non-distributed algorithms&#160;may have different geometry, but they typically break the input space into regions,</span><span class="font64" style="font-weight:bold;">with&#160;a separate set of parameters for each region </span><span class="font64">The advantage of a non-distributed&#160;approach is that, given enough parameters, it can fit the training set without solving a&#160;difficult optimization algorithm, because it is straightforward to choose a different output&#160;</span><span class="font64" style="font-weight:bold;">independently </span><span class="font64">for each region. The disadvantage is that such non-distributed models&#160;generalize only locally via the smoothness prior, making it difficult to learn a complicated&#160;function with more peaks and troughs than the available number of examples. Contrast&#160;this with a distributed representation, Fig. 15.7.</span></p></div></div>
<p><span class="font64">have a statistical advantage when an apparently complicated structure can be compactly represented using a small number of parameters. Some traditional nondistributed learning algorithms generalize only due to the smoothness assumption,&#160;which states that if u ~ v, then the target function f to be learned has the&#160;property that f (u) « f(v), in general. There are many ways of formalizing such an&#160;assumption, but the end result is that if we have an example (x,y) for which we&#160;know that f (x) ~ y, then we choose an estimator </span><span class="font64" style="font-weight:bold;font-style:italic;">f</span><span class="font64"> that approximately satisfies&#160;these constraints while changing as little as possible when we move to a nearby&#160;input x + e. This assumption is clearly very useful, but it suffers from the curse of&#160;dimensionality: in order to learn a target function that increases and decreases&#160;many times in many different regions</span><span class="font18">,<a id="footnote1"></a><sup><a href="#bookmark6">1</a></sup><sup></sup></span><span class="font64"> we may need a number of examples that is&#160;at least as large as the number of distinguishable regions. One can think of each of&#160;these regions as a category or symbol: by having a separate degree of freedom for&#160;each symbol (or region), we can learn an arbitrary decoder mapping from symbol&#160;to value. However, this does not allow us to generalize to new symbols for new&#160;regions.</span></p>
<p><span class="font64">If we are lucky, there may be some regularity in the target function, besides being smooth. For example, a convolutional network with max-pooling can recognize an&#160;object regardless of its location in the image, even though spatial translation of&#160;the object may not correspond to smooth transformations in the input space.</span></p>
<p><span class="font64">Let us examine a special case of a distributed representation learning algorithm, that extracts binary features by thresholding linear functions of the input. Each&#160;binary feature in this representation divides </span><span class="font64" style="font-weight:bold;font-style:italic;">R<sup>d</sup></span><span class="font64"> into a pair of half-spaces, as&#160;illustrated in Fig. 15.7. The exponentially large number of intersections of n&#160;of the corresponding half-spaces determines how many regions this distributed&#160;representation learner can distinguish. How many regions are generated by an&#160;arrangement of n hyperplanes in </span><span class="font64" style="font-weight:bold;font-style:italic;">R<sup>d</sup></span><span class="font64"> ? By applying a general result concerning the&#160;intersection of hyperplanes (Zaslavsky, 1975), one can show (Pascanu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014b)&#160;that the number of regions this binary feature representation can distinguish is</span></p>
<p><span class="font64">E n = O(n<sup>d</sup>). &#160;&#160;&#160;(15.4)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">j</span><span class="font18">=0</span><span class="font64"> ' </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>j</sup></span></p>
<p><span class="font64">Therefore, we see a growth that is exponential in the input size and polynomial in the number of hidden units.</span></p>
<p><span class="font64">This provides a geometric argument to explain the generalization power of distributed representation: with O (nd) parameters (for n linear-threshold features&#160;in R<sup>d</sup>) we can distinctly represent O</span><span class="font64" style="font-weight:bold;font-style:italic;">(n<sup>d</sup>)</span><span class="font64"> regions in input space. If instead we made&#160;no assumption at all about the data, and used a representation with one unique&#160;symbol for each region, and separate parameters for each symbol to recognize its&#160;corresponding portion of then specifying O </span><span class="font64" style="font-weight:bold;font-style:italic;">(n<sup>d</sup>)</span><span class="font64"> regions would require </span><span class="font64" style="font-weight:bold;font-style:italic;">O(n</span><span class="font64"><sup>d</sup>)&#160;examples. More generally, the argument in favor of the distributed representation&#160;could be extended to the case where instead of using linear threshold units we&#160;use nonlinear, possibly continuous, feature extractors for each of the attributes in&#160;the distributed representation. The argument in this case is that if a parametric&#160;transformation with k parameters can learn about r regions in input space, with&#160;k ^ r, and if obtaining such a representation was useful to the task of interest, then&#160;we could potentially generalize much better in this way than in a non-distributed&#160;setting where we would need O (r) examples to obtain the same features and&#160;associated partitioning of the input space into r regions. Using fewer parameters to&#160;represent the model means that we have fewer parameters to fit, and thus require&#160;far fewer training examples to generalize well.</span></p>
<p><span class="font64">A further part of the argument for why models based on distributed representations generalize well is that their capacity remains limited despite being able to distinctly encode so many different regions. For example, the VC dimension of a&#160;neural network of linear threshold units is only O(w log w), where w is the number&#160;of weights (Sontag, 1998). This limitation arises because, while we can assign very&#160;many unique codes to representation space, we cannot use absolutely all of the code&#160;space, nor can we learn arbitrary functions mapping from the representation space&#160;h to the output y using a linear classifier. The use of a distributed representation&#160;combined with a linear classifier thus expresses a prior belief that the classes to&#160;be recognized are linearly separable as a function of the underlying causal factors&#160;captured by h. We will typically want to learn categories such as the set of all&#160;images of all green objects or the set of all images of cars, but not categories that&#160;require nonlinear, XOR logic. For example, we typically do not want to partition&#160;the data into the set of all red cars and green trucks as one class and the set of all&#160;green cars and red trucks as another class.</span></p>
<p><span class="font64">The ideas discussed so far have been abstract, but they may be experimentally validated. Zhou </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) find that hidden units in a deep convolutional&#160;network trained on the ImageNet and Places benchmark datasets learn features&#160;that are very often interpretable, corresponding to a label that humans would&#160;naturally assign. In practice it is certainly not always the case that hidden units&#160;learn something that has a simple linguistic name, but it is interesting to see this&#160;emerge near the top levels of the best computer vision deep networks. What such</span></p><div><img src="main-162.jpg" alt=""/>
<p><span class="font64">Figure 15.9: A generative model has learned a distributed representation that disentangles the concept of gender from the concept of wearing glasses. If we begin with the representation of the concept of a man with glasses, then subtract the vector representing the&#160;concept of a man without glasses, and finally add the vector representing the concept&#160;of a woman without glasses, we obtain the vector representing the concept of a woman&#160;with glasses. The generative model correctly decodes all of these representation vectors to&#160;images that may be recognized as belonging to the correct class. Images reproduced with&#160;permission from Radford </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2015).</span></p></div>
<p><span class="font64">features have in common is that one could imagine </span><span class="font64" style="font-weight:bold;">learning about each of them without having to see all the configurations of all the others</span><span class="font64">. Radford&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) demonstrated that a generative model can learn a representation of&#160;images of faces, with separate directions in representation space capturing different&#160;underlying factors of variation. Fig. 15.9 demonstrates that one direction in&#160;representation space corresponds to whether the person is male or female, while&#160;another corresponds to whether the person is wearing glasses. These features were&#160;discovered automatically, not fixed a priori. There is no need to have labels for&#160;the hidden unit classifiers: gradient descent on an objective function of interest&#160;naturally learns semantically interesting features, so long as the task requires&#160;such features. We can learn about the distinction between male and female, or&#160;about the presence or absence of glasses, without having to characterize all of&#160;the configurations of the </span><span class="font64" style="font-weight:bold;font-style:italic;">n —</span><span class="font64"> </span><span class="font18">1</span><span class="font64"> other features by examples covering all of these&#160;combinations of values. This form of statistical separability is what allows one to&#160;generalize to new configurations of a person’s features that have never been seen&#160;during training.</span></p>
<p><a id="bookmark6"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font64"><sup></sup> Potentially, we may want to learn a function whose behavior is distinct in exponentially many regions: in a d-dimensional space with at least 2 different values to distinguish per dimension, we&#160;might want f to differ in </span><span class="font64" style="font-style:italic;">2<sup>d</sup></span><span class="font64"> different regions, requiring O(2 <sup>d</sup>) training examples.</span></p>
</body>
</html>