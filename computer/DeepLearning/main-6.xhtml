<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 3</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Probability and Information Theory</span></h2>
<p><span class="font64">In this chapter, we describe probability theory and information theory.</span></p>
<p><span class="font64">Probability theory is a mathematical framework for representing uncertain statements. It provides a means of quantifying uncertainty and axioms for deriving&#160;new uncertain statements. In artificial intelligence applications, we use probability&#160;theory in two major ways. First, the laws of probability tell us how AI systems&#160;should reason, so we design our algorithms to compute or approximate various&#160;expressions derived using probability theory. Second, we can use probability and&#160;statistics to theoretically analyze the behavior of proposed AI systems.</span></p>
<p><span class="font64">Probability theory is a fundamental tool of many disciplines of science and engineering. We provide this chapter to ensure that readers whose background is&#160;primarily in software engineering with limited exposure to probability theory can&#160;understand the material in this book.</span></p>
<p><span class="font64">While probability theory allows us to make uncertain statements and reason in the presence of uncertainty, information allows us to quantify the amount of&#160;uncertainty in a probability distribution.</span></p>
<p><span class="font64">If you are already familiar with probability theory and information theory, you may wish to skip all of this chapter except for Sec. 3.14, which describes the&#160;graphs we use to describe structured probabilistic models for machine learning. If&#160;you have absolutely no prior experience with these subjects, this chapter should&#160;be sufficient to successfully carry out deep learning research projects, but we do&#160;suggest that you consult an additional resource, such as Jaynes (2003).</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">3.1 Why Probability?</span></h4>
<p><span class="font64">Many branches of computer science deal mostly with entities that are entirely deterministic and certain. A programmer can usually safely assume that a CPU will&#160;execute each machine instruction flawlessly. Errors in hardware do occur, but are&#160;rare enough that most software applications do not need to be designed to account&#160;for them. Given that many computer scientists and software engineers work in a&#160;relatively clean and certain environment, it can be surprising that machine learning&#160;makes heavy use of probability theory.</span></p>
<p><span class="font64">This is because machine learning must always deal with uncertain quantities, and sometimes may also need to deal with stochastic (non-deterministic) quantities.&#160;Uncertainty and stochasticity can arise from many sources. Researchers have made&#160;compelling arguments for quantifying uncertainty using probability since at least&#160;the 1980s. Many of the arguments presented here are summarized from or inspired&#160;by Pearl (1988).</span></p>
<p><span class="font64">Nearly all activities require some ability to reason in the presence of uncertainty. In fact, beyond mathematical statements that are true by definition, it is difficult&#160;to think of any proposition that is absolutely true or any event that is absolutely&#160;guaranteed to occur.</span></p>
<p><span class="font64">There are three possible sources of uncertainty:</span></p>
<p><span class="font64">1. &#160;&#160;&#160;Inherent stochasticity in the system being modeled. For example, most&#160;interpretations of quantum mechanics describe the dynamics of subatomic&#160;particles as being probabilistic. We can also create theoretical scenarios that&#160;we postulate to have random dynamics, such as a hypothetical card game&#160;where we assume that the cards are truly shuffled into a random order.</span></p>
<p><span class="font64">2. &#160;&#160;&#160;Incomplete observability. Even deterministic systems can appear stochastic&#160;when we cannot observe all of the variables that drive the behavior of the&#160;system. For example, in the Monty Hall problem, a game show contestant is&#160;asked to choose between three doors and wins a prize held behind the chosen&#160;door. Two doors lead to a goat while a third leads to a car. The outcome&#160;given the contestant’s choice is deterministic, but from the contestant’s point&#160;of view, the outcome is uncertain. <a id="footnote1"></a><sup><a href="#bookmark2">1</a></sup><sup></sup>&#160;robot discretizes space when predicting the future location of these objects,&#160;then the discretization makes the robot immediately become uncertain about&#160;the precise position of objects: each object could be anywhere within the&#160;discrete cell that it was observed to occupy.</span></p>
<p><span class="font64">In many cases, it is more practical to use a simple but uncertain rule rather than a complex but certain one, even if the true rule is deterministic and our&#160;modeling system has the fidelity to accommodate a complex rule. For example, the&#160;simple rule “Most birds fly” is cheap to develop and is broadly useful, while a rule&#160;of the form, “Birds fly, except for very young birds that have not yet learned to&#160;fly, sick or injured birds that have lost the ability to fly, flightless species of birds&#160;including the cassowary, ostrich and kiwi...” is expensive to develop, maintain and&#160;communicate, and after all of this effort is still very brittle and prone to failure.</span></p>
<p><span class="font64">Given that we need a means of representing and reasoning about uncertainty, it is not immediately obvious that probability theory can provide all of the tools&#160;we want for artificial intelligence applications. Probability theory was originally&#160;developed to analyze the frequencies of events. It is easy to see how probability&#160;theory can be used to study events like drawing a certain hand of cards in a&#160;game of poker. These kinds of events are often repeatable. When we say that&#160;an outcome has a probability p of occurring, it means that if we repeated the&#160;experiment (e.g., draw a hand of cards) infinitely many times, then proportion p&#160;of the repetitions would result in that outcome. This kind of reasoning does not&#160;seem immediately applicable to propositions that are not repeatable. If a doctor&#160;analyzes a patient and says that the patient has a 40% chance of having the flu,&#160;this means something very different—we can not make infinitely many replicas of&#160;the patient, nor is there any reason to believe that different replicas of the patient&#160;would present with the same symptoms yet have varying underlying conditions. In&#160;the case of the doctor diagnosing the patient, we use probability to represent a&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">degree of belief,</span><span class="font64"> with 1 indicating absolute certainty that the patient has the flu&#160;and 0 indicating absolute certainty that the patient does not have the flu. The&#160;former kind of probability, related directly to the rates at which events occur, is&#160;known as </span><span class="font64" style="font-weight:bold;font-style:italic;">frequentist probability,</span><span class="font64"> while the latter, related to qualitative levels of&#160;certainty, is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">Bayesian probability.</span></p>
<p><span class="font64">If we list several properties that we expect common sense reasoning about uncertainty to have, then the only way to satisfy those properties is to treat&#160;Bayesian probabilities as behaving exactly the same as frequentist probabilities.&#160;For example, if we want to compute the probability that a player will win a poker&#160;game given that she has a certain set of cards, we use exactly the same formulas&#160;as when we compute the probability that a patient has a disease given that she&#160;has certain symptoms. For more details about why a small set of common sense&#160;assumptions implies that the same axioms must control both kinds of probability,&#160;see Ramsey (1926).</span></p>
<p><span class="font64">Probability can be seen as the extension of logic to deal with uncertainty. Logic provides a set of formal rules for determining what propositions are implied to&#160;be true or false given the assumption that some other set of propositions is true&#160;or false. Probability theory provides a set of formal rules for determining the&#160;likelihood of a proposition being true given the likelihood of other propositions.</span></p><h4><a id="bookmark3"></a><span class="font65" style="font-weight:bold;">3.2 Random Variables</span></h4>
<p><span class="font64">A </span><span class="font64" style="font-weight:bold;font-style:italic;">random variable</span><span class="font64"> is a variable that can take on different values randomly. We typically denote the random variable itself with a lower case letter in plain typeface,&#160;and the values it can take on with lower case script letters. For example, </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font62" style="font-style:italic;">1</span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font62" style="font-style:italic;">2&#160;</span><span class="font64">are both possible values that the random variable x can take on. For vector-valued&#160;variables, we would write the random variable as x and one of its values as x. On&#160;its own, a random variable is just a description of the states that are possible; it&#160;must be coupled with a probability distribution that specifies how likely each of&#160;these states are.</span></p>
<p><span class="font64">Random variables may be discrete or continuous. A discrete random variable is one that has a finite or countably infinite number of states. Note that these&#160;states are not necessarily the integers; they can also just be named states that&#160;are not considered to have any numerical value. A continuous random variable is&#160;associated with a real value.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">3.3 Probability Distributions</span></h4>
<p><span class="font64">A </span><span class="font64" style="font-weight:bold;font-style:italic;">probability distribution</span><span class="font64"> is a description of how likely a random variable or set of random variables is to take on each of its possible states. The way we&#160;describe probability distributions depends on whether the variables are discrete or&#160;continuous.</span></p><h5><a id="bookmark5"></a><span class="font64" style="font-weight:bold;">3.3.1 Discrete Variables and Probability Mass Functions</span></h5>
<p><span class="font64">A probability distribution over discrete variables may be described using a </span><span class="font64" style="font-weight:bold;font-style:italic;">probability mass function</span><span class="font64"> (PMF). We typically denote probability mass functions with a capital P. Often we associate each random variable with a different probability&#160;mass function and the reader must infer which probability mass function to use&#160;based on the identity of the random variable, rather than the name of the function;&#160;P(x) is usually not the same as </span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(y).</span></p>
<p><span class="font64">The probability mass function maps from a state of a random variable to the probability of that random variable taking on that state. The probability&#160;that x = </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> is denoted as P(x), with a probability of 1 indicating that x = </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> is&#160;certain and a probability of 0 indicating that x = x is impossible. Sometimes&#160;to disambiguate which PMF to use, we write the name of the random variable&#160;explicitly: P(x = x). Sometimes we define a variable first, then use ~ notation to&#160;specify which distribution it follows later: x ~ P(x).</span></p>
<p><span class="font64">Probability mass functions can act on many variables at the same time. Such a probability distribution over many variables is known as a </span><span class="font64" style="font-weight:bold;font-style:italic;">joint probability&#160;distribution. P</span><span class="font64">(x = x, y = </span><span class="font64" style="font-weight:bold;font-style:italic;">y)</span><span class="font64"> denotes the probability that x = x and y = </span><span class="font64" style="font-weight:bold;font-style:italic;">y&#160;</span><span class="font64">simultaneously. We may also write P(x,y) for brevity.</span></p>
<p><span class="font64">To be a probability mass function on a random variable x, a function P must satisfy the following properties:</span></p>
<p><span class="font64">• &#160;&#160;&#160;The domain of P must be the set of all possible states of x.</span></p>
<p><span class="font64">• Vx E x,0 &lt; P(x) &lt; 1. An impossible event has probability 0 and no state can be less probable than that. Likewise, an event that is guaranteed to happen&#160;has probability 1, and no state can have a greater chance of occurring.</span></p>
<p><span class="font64">• &#160;&#160;&#160;^<sub>x</sub> P(x) = 1. We refer to this property as being </span><span class="font64" style="font-weight:bold;font-style:italic;">normalized.</span><span class="font64"> Without this&#160;property, we could obtain probabilities greater than one by computing the&#160;probability of one of many events occurring.</span></p>
<p><span class="font64">For example, consider a single discrete random variable x with k different states. We can place a </span><span class="font64" style="font-weight:bold;font-style:italic;">uniform distribution</span><span class="font64"> on x—that is, make each of its states equally&#160;likely—by setting its probability mass function to</span></p>
<p><span class="font64">1</span></p>
<p><span class="font64">P (x = x </span><span class="font64" style="font-weight:bold;font-style:italic;">i)</span><span class="font64"> = k &#160;&#160;&#160;(3.1)</span></p>
<p><span class="font64">for all </span><span class="font64" style="font-weight:bold;font-style:italic;">i.</span><span class="font64"> We can see that this fits the requirements for a probability mass function. The value | is positive because k is a positive integer. We also see that</span></p>
<p><span class="font64">^ <sup>P (x</sup> = <sup>x</sup>i<sup>)</sup> = ^ k = <sup>k 1 &#160;&#160;&#160;(3</sup>.<sup>2)</sup></span></p>
<p><span class="font64">ii</span></p>
<p><span class="font64">so the distribution is properly normalized.</span></p><h5><a id="bookmark6"></a><span class="font64" style="font-weight:bold;">3.3.2 Continuous Variables and Probability Density Functions</span></h5>
<p><span class="font64">When working with continuous random variables, we describe probability distributions using a </span><span class="font64" style="font-weight:bold;font-style:italic;">probability density function (PDF)</span><span class="font64"> rather than a probability mass function. To be a probability density function, a function p must satisfy the&#160;following properties:</span></p>
<p><span class="font64">• &#160;&#160;&#160;The domain of p must be the set of all possible states of x.</span></p>
<p><span class="font64">• Vx </span><span class="font64" style="font-weight:bold;font-style:italic;">E x,p(x)</span><span class="font64"> &gt; 0. Note that we do not require p(x) &lt; 1.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">• &#160;&#160;&#160;f p(x)dx =</span><span class="font64"> 1.</span></p>
<p><span class="font64">A probability density function </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x)</span><span class="font64"> does not give the probability of a specific state directly, instead the probability of landing inside an infinitesimal region with&#160;volume Sx is given by </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x)Sx.</span></p>
<p><span class="font64">We can integrate the density function to find the actual probability mass of a set of points. Specifically, the probability that x lies in some set S is given by the&#160;integral of p (x) over that set. In the univariate example, the probability that x&#160;lies in the interval [a, </span><span class="font64" style="font-weight:bold;font-style:italic;">b]</span><span class="font64"> is given by J[<sub>ab</sub>] </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x)dx.</span></p>
<p><span class="font64">For an example of a probability density function corresponding to a specific probability density over a continuous random variable, consider a uniform distribution on an interval of the real numbers. We can do this with a function </span><span class="font64" style="font-weight:bold;font-style:italic;">u(x; a,b</span><span class="font64">),&#160;where a and b are the endpoints of the interval, with </span><span class="font64" style="font-weight:bold;font-style:italic;">b &gt; a.</span><span class="font64"> The “;” notation means&#160;“parametrized by”; we consider x to be the argument of the function, while a and&#160;b are parameters that define the function. To ensure that there is no probability&#160;mass outside the interval, we say u(x; a, b) = 0 for all x E [a,b]. Within [a, b],&#160;u(x; a, b) = bza. We can see that this is nonnegative everywhere. Additionally, it&#160;integrates to 1. We often denote that x follows the uniform distribution on [a,b]&#160;by writing x&#160;&#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">U(a, b).</span></p><h4><a id="bookmark7"></a><span class="font65" style="font-weight:bold;">3.4 Marginal Probability</span></h4>
<p><span class="font64">Sometimes we know the probability distribution over a set of variables and we want to know the probability distribution over just a subset of them. The probability&#160;distribution over the subset is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">marginal probability</span><span class="font64"> distribution.</span></p>
<p><span class="font64">For example, suppose we have discrete random variables x and y, and we know P(x, y). We can find </span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(x) with the </span><span class="font64" style="font-weight:bold;font-style:italic;">sum rule:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Vx E x,P</span><span class="font64"> (x = x) &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64"> (x = x, y = y).&#160;&#160;&#160;&#160;(3.3)</span></p>
<p><span class="font64">y</span></p>
<p><span class="font64">The name “marginal probability” comes from the process of computing marginal probabilities on paper. When the values of P(x, y) are written in a grid with&#160;different values of x in rows and different values of y in columns, it is natural to&#160;sum across a row of the grid, then write P(x) in the margin of the paper just to&#160;the right of the row.</span></p>
<p><span class="font64">For continuous variables, we need to use integration instead of summation:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(x) = &#160;&#160;&#160;p(x,y</span><span class="font64"> )dy.</span></p></div><div>
<p><span class="font64">x</span></p></div><div>
<p><span class="font64">x</span></p></div><div>
<p><span class="font64">(3.4)</span></p></div><h4><a id="bookmark8"></a><span class="font65" style="font-weight:bold;">3.5 Conditional Probability</span></h4>
<p><span class="font64">In many cases, we are interested in the probability of some event, given that some other event has happened. This is called a </span><span class="font64" style="font-weight:bold;font-style:italic;">conditional probability.</span><span class="font64"> We denote&#160;the conditional probability that y = y given x = x as P(y = y | x = x). This&#160;conditional probability can be computed with the formula</span></p><div>
<p><span class="font64" style="font-variant:small-caps;">p (y = y | x = x) =</span></p></div><div>
<p><span class="font64" style="font-variant:small-caps;">p (</span><span class="font64" style="text-decoration:underline;">y = </span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:underline;">y,</span><span class="font64" style="text-decoration:underline;"> x =</span><span class="font64"> x)</span></p>
<p><span class="font64">P (x = x)</span></p></div><div>
<p><span class="font64">(3.5)</span></p></div>
<p><span class="font64">The conditional probability is only defined when P( x = x) &gt; 0. We cannot compute the conditional probability conditioned on an event that never happens.</span></p>
<p><span class="font64">It is important not to confuse conditional probability with computing what would happen if some action were undertaken. The conditional probability that&#160;a person is from Germany given that they speak German is quite high, but if&#160;a randomly selected person is taught to speak German, their country of origin&#160;does not change. Computing the consequences of an action is called making an&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">intervention query.</span><span class="font64"> Intervention queries are the domain of </span><span class="font64" style="font-weight:bold;font-style:italic;">causal modeling,</span><span class="font64"> which&#160;we do not explore in this book.</span></p><h4><a id="bookmark9"></a><span class="font65" style="font-weight:bold;">3.6 The Chain Rule of Conditional Probabilities</span></h4>
<p><span class="font64">Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable:</span></p>
<p><span class="font64">P(x<sup>(1)</sup>,...,x<sup>(n)</sup>) = P(x<sup>(1)</sup>)n<sup>n</sup>=<sub>2</sub>P(x<sup>(i)</sup> | x<sup>(1)</sup>,...,x<sup>(i-1)</sup>). &#160;&#160;&#160;(3.6)</span></p>
<p><span class="font64">This observation is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">chain rule</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">product rule</span><span class="font64"> of probability. It follows immediately from the definition of conditional probability in Eq. 3.5. For&#160;example, applying the definition twice, we get</span></p>
<p><span class="font64">P(a, b, c) &#160;&#160;&#160;= </span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(a | b, c)P(b, c)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(b, c) &#160;&#160;&#160;= P(b | c)P(c)</span></p>
<p><span class="font64">P(a, b, c) = P(a | b, c)P(b | c)P(c).</span></p><h4><a id="bookmark10"></a><span class="font65" style="font-weight:bold;">3.7 Independence and Conditional Independence</span></h4>
<p><span class="font64">Two random variables x and y are </span><span class="font64" style="font-weight:bold;font-style:italic;">independent</span><span class="font64"> if their probability distribution can be expressed as a product of two factors, one involving only x and one involving&#160;only y:</span></p>
<p><span class="font65">Vx e x, y e y, p(x = x, y = </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64">) </span><span class="font64" style="font-weight:bold;font-style:italic;">= p(x = x)p(y = y).</span><span class="font64"> &#160;&#160;&#160;(3.7)</span></p>
<p><span class="font64">Two random variables x and y are </span><span class="font64" style="font-weight:bold;font-style:italic;">conditionally independent</span><span class="font64"> given a random variable z if the conditional probability distribution over x and y factorizes in this&#160;way for every value of z:</span></p>
<p><span class="font65">Vx e x, y e y, z e z, p(x = x, y = y | z = z) = p(x = x | z = z)p(y = y | z = z).</span></p>
<p><span class="font64">(3.8)</span></p>
<p><span class="font64">We can denote independence and conditional independence with compact notation: xTy means that x and y are independent, while xTy | z means that x&#160;and y are conditionally independent given z.</span></p><h4><a id="bookmark11"></a><span class="font65" style="font-weight:bold;">3.8 Expectation, Variance and Covariance</span></h4>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">expectation</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">expected value</span><span class="font64"> of some function f (x) with respect to a probability distribution P(x) is the average or mean value that f takes on when x is drawn&#160;from P. For discrete variables this can be computed with a summation:</span></p>
<p><span class="font64">Ex~p<sup>[f (x)]</sup> = ^ <sup>P(x)f (x) &#160;&#160;&#160;(3</sup>-<sup>9)</sup></span></p>
<p><span class="font64">X</span></p>
<p><span class="font64">while for continuous variables, it is computed with an integral:</span></p>
<p><span class="font64">Ex~p [f (x)] &#160;&#160;&#160;p(x)f (x)dx.&#160;&#160;&#160;&#160;(3.10)</span></p>
<p><span class="font64">When the identity of the distribution is clear from the context, we may simply write the name of the random variable that the expectation is over, as in E</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>x</sub>[f</span><span class="font64"> (x)].&#160;If it is clear which random variable the expectation is over, we may omit the&#160;subscript entirely, as in E[f (x)]. By default, we can assume that E[■] averages over&#160;the values of all the random variables inside the brackets. Likewise, when there is&#160;no ambiguity, we may omit the square brackets.</span></p>
<p><span class="font64">Expectations are linear, for example,</span></p>
<p><span class="font64">Ex[af (x) + </span><span class="font64" style="font-weight:bold;font-style:italic;">Pg(x)\ =</span><span class="font64"> aEx [f (x)] + 3 E x[g<sup>(x)]</sup>, &#160;&#160;&#160;(<sup>3</sup>.U)</span></p>
<p><span class="font64">when a and 3 are not dependent on x.</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">variance</span><span class="font64"> gives a measure of how much the values of a function of a random variable x vary as we sample different values of x from its probability distribution:</span></p><div>
<p><span class="font64">Var (f (x)) = E</span></p></div><div>
<p><span class="font64">(f (x) - E[f (x)\)<sup>2</sup></span></p></div><div>
<p><span class="font64">(3.12)</span></p></div>
<p><span class="font64">When the variance is low, the values of f (x) cluster near their expected value. The square root of the variance is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">standard deviation.</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">covariance</span><span class="font64"> gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:</span></p>
<p><span class="font64"><sup>Cov(f (x)</sup>, </span><span class="font64" style="font-weight:bold;font-style:italic;">g<sup>(</sup>y<sup>))</sup></span><span class="font64"> = <sup>E [(f (x) - E [f (x)]) (</sup>g<sup>(</sup>y<sup>) - E [</sup>g<sup>(</sup>y<sup>)])</sup>\. &#160;&#160;&#160;<sup>(3</sup>.<sup>13)</sup></span></p>
<p><span class="font64">High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time. If the sign of the&#160;covariance is positive, then both variables tend to take on relatively high values&#160;simultaneously. If the sign of the covariance is negative, then one variable tends to&#160;take on a relatively high value at the times that the other takes on a relatively low&#160;value and vice versa. Other measures such as </span><span class="font64" style="font-weight:bold;font-style:italic;">correlation</span><span class="font64"> normalize the contribution&#160;of each variable in order to measure only how much the variables are related, rather&#160;than also being affected by the scale of the separate variables.</span></p>
<p><span class="font64">The notions of covariance and dependence are related, but are in fact distinct concepts. They are related because two variables that are independent have zero&#160;covariance, and two variables that have non-zero covariance are dependent. However, independence is a distinct property from covariance. For two variables to have&#160;zero covariance, there must be no linear dependence between them. Independence&#160;is a stronger requirement than zero covariance, because independence also excludes&#160;nonlinear relationships. It is possible for two variables to be dependent but have&#160;zero covariance. For example, suppose we first sample a real number x from a&#160;uniform distribution over the interval [-1, 1\. We next sample a random variable&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">s.</span><span class="font64"> With probability 1, we choose the value of s to be 1. Otherwise, we choose&#160;the value of s to be — 1. We can then generate a random variable y by assigning&#160;y = </span><span class="font64" style="font-weight:bold;font-style:italic;">sx.</span><span class="font64"> Clearly, </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> and y are not independent, because </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> completely determines&#160;the magnitude of y. However, Cov(x, y) = 0.</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">covariance matrix</span><span class="font64"> of a random vector x £ R<sup>n</sup> is an n x n matrix, such that</span></p><div>
<p><span class="font64"><sup>Cov</sup>(<sup>x</sup>kj</span></p></div><div>
<p><span class="font64">Cov(x״ x j).</span></p></div><div>
<p><span class="font64">(3.14)</span></p></div>
<p><span class="font64">The diagonal elements of the covariance give the variance:</span></p><div>
<p><span class="font64">(3.15)</span></p></div>
<p><span class="font64">Cov(x!, x!) = Var(x!).</span></p><h4><a id="bookmark12"></a><span class="font65" style="font-weight:bold;">3.9 Common Probability Distributions</span></h4>
<p><span class="font64">Several simple probability distributions are useful in many contexts in machine learning.</span></p><h5><a id="bookmark13"></a><span class="font64" style="font-weight:bold;">3.9.1 Bernoulli Distribution</span></h5>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">Bernoulli</span><span class="font64"> distribution is a distribution over a single binary random variable. It is controlled by a single parameter 0 £ [0,1], which gives the probability of the&#160;random variable being equal to 1. It has the following properties:</span></p><div>
<p><span class="font64">(3.16)</span></p>
<p><span class="font64">(3.17)</span></p>
<p><span class="font64">(3.18)</span></p>
<p><span class="font64">(3.19)</span></p>
<p><span class="font64">(3.20)</span></p></div>
<p><span class="font64">P (x = 1) = 0 P (x = 0) = 1 — 0&#160;P (x = x) = 0<sup>x</sup> (1 — 0)<sup>1-x&#160;</sup>Ex[x] = 0</span></p>
<p><span class="font64">Var x(x) = 0(1 — 0)</span></p><h5><a id="bookmark14"></a><span class="font64" style="font-weight:bold;">3.9.2 Multinoulli Distribution</span></h5>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">multinoulli</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">categorical</span><span class="font64"> distribution is a distribution over a single discrete variable with k different states, where </span><span class="font64" style="font-weight:bold;font-style:italic;">k</span><span class="font64"> is finite.<a id="footnote2"></a><sup><a href="#bookmark15">2</a></sup><sup></sup> The multinoulli distribution is&#160;parametrized by a vector </span><span class="font64" style="font-weight:bold;">p </span><span class="font64" style="font-weight:bold;font-style:italic;">G</span><span class="font64"> [0,1]<sup>k-1</sup>, where </span><span class="font64" style="font-weight:bold;font-style:italic;">pi</span><span class="font64"> gives the probability of the i-th&#160;state. The final, k-th state’s probability is given by 1 — 1<sup>T</sup></span><span class="font64" style="font-weight:bold;">p</span><span class="font64">. Note that we must&#160;constrain 1<sup>T</sup></span><span class="font64" style="font-weight:bold;">p </span><span class="font64">&lt; 1. Multinoulli distributions are often used to refer to distributions&#160;over categories of objects, so we do not usually assume that state 1 has numerical&#160;value 1, etc. For this reason, we do not usually need to compute the expectation&#160;or variance of multinoulli-distributed random variables.</span></p>
<p><span class="font64">The Bernoulli and multinoulli distributions are sufficient to describe any distribution over their domain. This is because they model discrete variables for which it is feasible to simply enumerate all of the states. When dealing with continuous&#160;variables, there are uncountably many states, so any distribution described by a&#160;small number of parameters must impose strict limits on the distribution.</span></p><h5><a id="bookmark16"></a><span class="font64" style="font-weight:bold;">3.9.3 Gaussian Distribution</span></h5>
<p><span class="font64">The most commonly used distribution over real numbers is the </span><span class="font64" style="font-weight:bold;font-style:italic;">normal distribution, </span><span class="font64">also known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">Gaussian distribution:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>N</sup></span><span class="font64"><sup>(x</sup>;p<sup>,a2)</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">=y[J±</span><span class="font64">־<sup>ex</sup>p &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">-2—</span><span class="font64"><sub>2</sub><sup>(x —</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">p)<sup>)2</sup>j .</span><span class="font64">&#160;&#160;&#160;&#160;(<sup>3</sup>.<sup>21</sup>)</span></p>
<p><span class="font64">See Fig. 3.1 for a plot of the density function.</span></p>
<p><span class="font64">The two parameters p G R and a G (0, to) control the normal distribution. The parameter p gives the coordinate of the central peak. This is also the mean of&#160;the distribution: E[x] = p. The standard deviation of the distribution is given by&#160;a, and the variance by a<sup>2</sup>.</span></p>
<p><span class="font64">When we evaluate the PDF, we need to square and invert a. When we need to frequently evaluate the PDF with different parameter values, a more efficient way&#160;of parametrizing the distribution is to use a parameter </span><span class="font64" style="font-weight:bold;font-style:italic;">(3 G</span><span class="font64" style="font-variant:small-caps;"> (0, to) to control the&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">precision</span><span class="font64"> or inverse variance of the distribution:</span></p><div>
<p><span class="font33" style="font-weight:bold;"><sup>.</sup></span></p></div><div>
<p><span class="font64">(3.22)</span></p></div>
<p><span class="font64">N(x; p, 3<sup>-1)</sup> = y—. <sup>ex</sup>P ^—<sup>1</sup>3(x — p)<sup>2</sup></span></p>
<p><span class="font64">Normal distributions are a sensible choice for many applications. In the absence of prior knowledge about what form a distribution over the real numbers should&#160;take, the normal distribution is a good default choice for two major reasons.</span></p>
<p><span class="font64">First, many distributions we wish to model are truly close to being normal distributions. The </span><span class="font64" style="font-weight:bold;font-style:italic;">central limit theorem</span><span class="font64"> shows that the sum of many independent&#160;random variables is approximately normally distributed. This means that in</span></p><div><div>
<p><span class="font64">The normal distribution</span></p>
<p><span class="font64">0.40</span></p>
<p><span class="font64">0.35</span></p>
<p><span class="font12">^ 0.20</span></p>
<p><span class="font12">0.00</span></p><img src="main-21.jpg" alt=""/>
<p><span class="font64">Figure 3.1: </span><span class="font64" style="font-style:italic;">The normal distribution:</span><span class="font64"> The normal distributionN(x; </span><span class="font64" style="font-style:italic;">^,</span><span class="font64"> a<sup>2</sup>) exhibits a classic “bell curve” shape, with the x coordinate of its central peak given by and the width&#160;of its peak controlled by a. In this example, we depict the </span><span class="font64" style="font-style:italic;">standard normal distribution</span><span class="font64">,&#160;with </span><span class="font64" style="font-style:italic;">!! =</span><span class="font64"> 0 and a = 1.</span></p></div></div>
<p><span class="font64">practice, many complicated systems can be modeled successfully as normally distributed noise, even if the system can be decomposed into parts with more&#160;structured behavior.</span></p>
<p><span class="font64">Second, out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the&#160;real numbers. We can thus think of the normal distribution as being the one that&#160;inserts the least amount of prior knowledge into a model. Fully developing and&#160;justifying this idea requires more mathematical tools, and is postponed to Sec.&#160;19.4.2.</span></p>
<p><span class="font64">The normal distribution generalizes to R<sup>n</sup>, in which case it is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">multivariate normal distribution.</span><span class="font64"> It may be parametrized with a positive definite&#160;symmetric matrix S:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">N</span><span class="font64">(X; 1, </span><span class="font64" style="font-weight:bold;font-style:italic;">S)</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">=^l</span></p></div><div>
<p><span class="font64">1</span></p>
<p><span class="font64">(2n) <sup>n</sup>det(S)</span></p></div><div>
<p><span class="font64">exp</span></p></div><div>
<p><span class="font64">(<sup>-</sup>2<sup>(</sup></span></p></div><div>
<p><span class="font64"><sup>-</sup>2<sup>(x -</sup> l<sup>)T</sup>S <sup>1</sup></span></p></div><div>
<p><span class="font64"><sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>x </sup></span><span class="font65"><sup>-</sup> </span><span class="font33" style="font-weight:bold;">.</span></p></div><div>
<p><span class="font64">(3.23)</span></p></div>
<p><span class="font64">The parameter i still gives the mean of the distribution, though now it is vector-valued. The parameter S gives the covariance matrix of the distribution.&#160;As in the univariate case, when we wish to evaluate the PDF several times for</span></p>
<p><span class="font64">many different values of the parameters, the covariance is not a computationally efficient way to parametrize the distribution, since we need to invert </span><span class="font64" style="font-weight:bold;font-style:italic;">£</span><span class="font64"> to evaluate&#160;the PDF. We can instead use a </span><span class="font64" style="font-weight:bold;font-style:italic;">precision matrix</span><span class="font64"> 3:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">N(x</span><span class="font64">;3 &#160;&#160;&#160;) =</span></p></div><div>
<p><span class="font64"><sup>1 ) exp</sup> </span><span class="font67" style="font-weight:bold;">(<sup>-</sup> </span><span class="font64">2<sup>(X </sup></span><span class="font67" style="font-weight:bold;"><sup>-</sup> </span><span class="font64">״</span><span class="font67" style="font-weight:bold;">d</span><span class="font64"><sup>3(x </sup></span><span class="font67" style="font-weight:bold;"><sup>-</sup> &#160;&#160;&#160;</span><span class="font64">•</span></p></div><div>
<p><span class="font64">(3.24)</span></p></div>
<p><span class="font64">We often fix the covariance matrix to be a diagonal matrix. An even simpler version is the </span><span class="font64" style="font-weight:bold;font-style:italic;">isotropic</span><span class="font64"> Gaussian distribution, whose covariance matrix is a scalar&#160;times the identity matrix.</span></p><h5><a id="bookmark17"></a><span class="font64" style="font-weight:bold;">3.9.4 Exponential and Laplace Distributions</span></h5>
<p><span class="font64">In the context of deep learning, we often want to have a probability distribution with a sharp point at </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> = 0. To accomplish this, we can use the </span><span class="font64" style="font-weight:bold;font-style:italic;">exponential&#160;distribution:</span></p>
<p><span class="font64">p(x; A) = A1<sub>x</sub>&gt;0exp(-Ax). &#160;&#160;&#160;(3.25)</span></p>
<p><span class="font64">The exponential distribution uses the indicator function 1<sub>x</sub>&gt;0 to assign probability zero to all negative values of x.</span></p>
<p><span class="font64">A closely related probability distribution that allows us to place a sharp peak of probability mass at an arbitrary point p is the </span><span class="font64" style="font-weight:bold;font-style:italic;">Laplace distribution</span></p><div>
<p><span class="font64">Laplace(x; p, y) = — exp <sup>2</sup></span><span class="font64" style="font-variant:small-caps;">y</span></p></div><div>
<p><span class="font64">(3.26)</span></p></div>
<p><span class="font64">1 &#160;&#160;&#160;/&#160;&#160;&#160;&#160;|x — p\</span></p><h5><a id="bookmark18"></a><span class="font64" style="font-weight:bold;">3.9.5 The Dirac Distribution and Empirical Distribution</span></h5>
<p><span class="font64">In some cases, we wish to specify that all of the mass in a probability distribution clusters around a single point. This can be accomplished by defining a PDF using&#160;the Dirac delta function, </span><span class="font64" style="font-weight:bold;font-style:italic;">5(x):</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(x)</span><span class="font64"> = </span><span class="font64" style="font-weight:bold;font-style:italic;">S(x — p).</span><span class="font64"> &#160;&#160;&#160;(3.27)</span></p>
<p><span class="font64">The Dirac delta function is defined such that it is zero-valued everywhere except 0, yet integrates to 1. The Dirac delta function is not an ordinary function that&#160;associates each value x with a real-valued output, instead it is a different kind of&#160;mathematical object called a </span><span class="font64" style="font-weight:bold;font-style:italic;">generalized function</span><span class="font64"> that is defined in terms of its&#160;properties when integrated. We can think of the Dirac delta function as being the&#160;limit point of a series of functions that put less and less mass on all points other&#160;than p.</span></p>
<p><span class="font64">By defining p( x) to be 5 shifted by </span><span class="font64" style="font-weight:bold;font-style:italic;">—p</span><span class="font64"> we obtain an infinitely narrow and infinitely high peak of probability mass where x = p.</span></p>
<p><span class="font64">A common use of the Dirac delta distribution is as a component of an </span><span class="font64" style="font-weight:bold;font-style:italic;">empirical distribution</span><span class="font64">,</span></p><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64"><sup>p(x)</sup> = </span><span class="font64" style="font-weight:bold;font-style:italic;">m^2<sup>5(x</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;">-x<sup>(i</sup></span></p></div><div>
<p><span class="font64">(3.28)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p></div>
<p><span class="font64">which puts probability mass m on each of the m points x<sup>(1)</sup>,..., xforming a given data set or collection of samples. The Dirac delta distribution is only&#160;necessary to define the empirical distribution over continuous variables. For discrete&#160;variables, the situation is simpler: an empirical distribution can be conceptualized&#160;as a multinoulli distribution, with a probability associated to each possible input&#160;value that is simply equal to the </span><span class="font64" style="font-weight:bold;font-style:italic;">empirical frequency</span><span class="font64"> of that value in the training&#160;set.</span></p>
<p><span class="font64">We can view the empirical distribution formed from a dataset of training examples as specifying the distribution that we sample from when we train a model&#160;on this dataset. Another important perspective on the empirical distribution is&#160;that it is the probability density that maximizes the likelihood of the training data&#160;(see Sec. 5.5).</span></p><h5><a id="bookmark19"></a><span class="font64" style="font-weight:bold;">3.9.6 Mixtures of Distributions</span></h5>
<p><span class="font64">It is also common to define probability distributions by combining other simpler probability distributions. One common way of combining distributions is to&#160;construct a </span><span class="font64" style="font-weight:bold;font-style:italic;">mixture distribution.</span><span class="font64"> A mixture distribution is made up of several&#160;component distributions. On each trial, the choice of which component distribution&#160;generates the sample is determined by sampling a component identity from a&#160;multinoulli distribution:</span></p>
<p><span class="font64">P(x) = ^</span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(c = </span><span class="font64" style="font-weight:bold;font-style:italic;">i)P</span><span class="font64">(x | c = </span><span class="font64" style="font-weight:bold;font-style:italic;">i)</span><span class="font64"> &#160;&#160;&#160;(3.29)</span></p>
<p><span class="font64">i</span></p>
<p><span class="font64">where P(c) is the multinoulli distribution over component identities.</span></p>
<p><span class="font64">We have already seen one example of a mixture distribution: the empirical distribution over real-valued variables is a mixture distribution with one Dirac&#160;component for each training example.</span></p>
<p><span class="font64">The mixture model is one simple strategy for combining probability distributions to create a richer distribution. In Chapter 16, we explore the art of building complex&#160;probability distributions from simple ones in more detail.</span></p>
<p><span class="font64">The mixture model allows us to briefly glimpse a concept that will be of paramount importance later—the </span><span class="font64" style="font-weight:bold;font-style:italic;">latent variable.</span><span class="font64"> A latent variable is a random&#160;variable that we cannot observe directly. The component identity variable c of the&#160;mixture model provides an example. Latent variables may be related to x through&#160;the joint distribution, in this case, P(x, c) = </span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(x | c)P(c). The distribution P(c)&#160;over the latent variable and the distribution P(x | c) relating the latent variables&#160;to the visible variables determines the shape of the distribution P (x) even though&#160;it is possible to describe P(x) without reference to the latent variable. Latent&#160;variables are discussed further in Sec. 16.5.</span></p>
<p><span class="font64">A very powerful and common type of mixture model is the </span><span class="font64" style="font-weight:bold;font-style:italic;">Gaussian mixture </span><span class="font64">model, in which the components</span><span class="font64" style="font-weight:bold;font-style:italic;">p(x</span><span class="font64"> | c = </span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64">) are Gaussians. Each component has&#160;a separately parametrized mean ^<sup>(i)</sup> and covariance S<sup>(i)</sup>. Some mixtures can have&#160;more constraints. For example, the covariances could be shared across components&#160;via the constraint S<sup>(i)</sup> = SVi. As with a single Gaussian distribution, the mixture&#160;of Gaussians might constrain the covariance matrix for each component to be&#160;diagonal or isotropic.</span></p>
<p><span class="font64">In addition to the means and covariances, the parameters of a Gaussian mixture specify the </span><span class="font64" style="font-weight:bold;font-style:italic;">prior probability a<sub>i</sub> = P</span><span class="font64">(c = i) given to each component i. The word&#160;“prior” indicates that it expresses the model’s beliefs about c before it has observed&#160;x. By comparison, P(c | x) is a </span><span class="font64" style="font-weight:bold;font-style:italic;">posterior probability,</span><span class="font64"> because it is computed after&#160;observation of x. A Gaussian mixture model is a </span><span class="font64" style="font-weight:bold;font-style:italic;">universal a,pproximator</span><span class="font64"> of&#160;densities, in the sense that any smooth density can be approximated with any&#160;specific, non-zero amount of error by a Gaussian mixture model with enough&#160;components.</span></p>
<p><span class="font64">Fig. 3.2 shows samples from a Gaussian mixture model.</span></p><h4><a id="bookmark20"></a><span class="font65" style="font-weight:bold;">3.10 Useful Properties of Common Functions</span></h4>
<p><span class="font64">Certain functions arise often while working with probability distributions, especially the probability distributions used in deep learning models.</span></p>
<p><span class="font64">One of these functions is the </span><span class="font64" style="font-weight:bold;font-style:italic;">logistic sigmoid:</span></p>
<p><span class="font64">1</span></p>
<p><span class="font64">״<sup>(x)</sup> = 1 + exp(-x)<sup>30</sup>'<sup>3)</sup> &#160;&#160;&#160;י<sup>)</sup></span></p>
<p><span class="font64">The logistic sigmoid is commonly used to produce the </span><span class="font64" style="font-weight:bold;font-style:italic;">&lt;p</span><span class="font64"> parameter of a Bernoulli distribution because its range is (0,1), which lies within the valid range of values&#160;for the ^ parameter. See Fig. 3.3 for a graph of the sigmoid function. The sigmoid</span></p>
<p><span class="font64">Figure 3.2: Samples from a Gaussian mixture model. In this example, there are three components. From left to right, the first component has an isotropic covariance matrix,&#160;meaning it has the same amount of variance in each direction. The second has a diagonal&#160;covariance matrix, meaning it can control the variance separately along each axis-aligned&#160;direction. This example has more variance along the</span><span class="font64" style="font-style:italic;">x<sub>2</sub></span><span class="font64"> axis than along the </span><span class="font64" style="font-style:italic;">x<sub>1</sub></span><span class="font64"> axis. The&#160;third component has a full-rank covariance matrix, allowing it to control the variance&#160;separately along an arbitrary basis of directions.</span></p>
<p><span class="font64">function </span><span class="font64" style="font-weight:bold;font-style:italic;">saturates</span><span class="font64"> when its argument is very positive or very negative, meaning that the function becomes very flat and insensitive to small changes in its input.</span></p>
<p><span class="font64">Another commonly encountered function is the </span><span class="font64" style="font-weight:bold;font-style:italic;">softplus</span><span class="font64"> function (Dugas </span><span class="font64" style="font-weight:bold;font-style:italic;">et al., </span><span class="font64">2001):</span></p>
<p><span class="font64">Z(x) = log (1 + exp(x)) . &#160;&#160;&#160;(3.31)</span></p>
<p><span class="font64">The softplus function can be useful for producing the </span><span class="font64" style="font-weight:bold;font-style:italic;">3</span><span class="font64"> or a parameter of a normal distribution because its range is (Q to ). It also arises commonly when manipulating&#160;expressions involving sigmoids. The name of the softplus function comes from the&#160;fact that it is a smoothed or “softened” version of</span></p><div>
<p><span class="font64">(3.32)</span></p></div>
<p><span class="font64">x+ = max(0, x). See Fig. 3.4 for a graph of the softplus function.</span></p>
<p><span class="font64">The following properties are all useful enough that you may wish to memorize them:</span></p><div>
<p><span class="font64">a</span></p></div><div>
<p><span class="font64">(x)</span></p></div><div>
<p><span class="font64">exp(x)</span></p></div><div>
<p><span class="font64">exp(x) + exp(0)</span></p></div><div>
<p><span class="font64">d</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dx</span></p></div><div>
<p><span class="font64">a(x) = a(x)(1 — a(x))</span></p></div><div>
<p><span class="font64">(3.33)</span></p>
<p><span class="font64">(3.34)</span></p></div><div>
<p><span class="font63" style="font-style:italic;font-variant:small-caps;">(x)d</span><span class="font25"> &#160;&#160;&#160;</span><span class="font63" style="font-style:italic;">(<sup>x</sup>)ל</span></p></div><div><div>
<p><span class="font64">The logistic sigmoid function</span></p><img src="main-22.jpg" alt=""/>
<p><span class="font64">Figure 3.3: The logistic sigmoid function.</span></p></div></div><div><div>
<p><span class="font64">The softplus function</span></p><img src="main-23.jpg" alt=""/>
<p><span class="font64">Figure 3.4: The softplus function.</span></p></div></div>
<p><span class="font64">1 — a(x) — a (—x) log a(x) = —Z (—x)</span></p><div>
<p><span class="font64">(3.35)</span></p>
<p><span class="font64">(3.36)</span></p>
<p><span class="font64">(3.37)</span></p>
<p><span class="font64">(3.38)</span></p>
<p><span class="font64">(3.39)</span></p>
<p><span class="font64">(3.40)</span></p>
<p><span class="font64">(3.41)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Tx<sup>c</sup></span><span class="font64"><sup> (x)</sup> = <sup>a (x)</sup></span></p>
<p><span class="font64">Vx e (0,1), a<sup>-1 (</sup>x<sup>)</sup> = 1^(ץ—</span></p>
<p><span class="font64">Vx &gt; 0, Z<sup>-1</sup>(x) — log(exp(x) — 1)</span></p>
<p><span class="font64">/x</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>a(</sup>y)dy </span><span class="font64">Z (x) — Z (—x) = x</span></p>
<p><span class="font64">The function a <sup>1</sup>(x) is called the </span><span class="font64" style="font-weight:bold;font-style:italic;">logit</span><span class="font64"> in statistics, but this term is more rarely used in machine learning.</span></p>
<p><span class="font64">Eq. 3.41 provides extra justification for the name “softplus.” The softplus function is intended as a smoothed version of the </span><span class="font64" style="font-weight:bold;font-style:italic;">positive part</span><span class="font64"> function, x + —&#160;max{0, x}. The positive part function is the counterpart of the </span><span class="font64" style="font-weight:bold;font-style:italic;">negative part&#160;</span><span class="font64">function, x<sup>-</sup> — max{0, —x}. To obtain a smooth function that is analogous to the&#160;negative part, one can use Z(—x). Just as x can be recovered from its positive part&#160;and negative part via the identity x x — x, it is also possible to recover x&#160;using the same relationship between Z(x) and Z(—x), as shown in Eq. 3.41.</span></p><h4><a id="bookmark21"></a><span class="font65" style="font-weight:bold;">3.11 Bayes’ Rule</span></h4>
<p><span class="font64">We often find ourselves in a situation where we know P(y | x) and need to know P(x | y). Fortunately, if we also know P(x), we can compute the desired quantity&#160;using </span><span class="font64" style="font-weight:bold;font-style:italic;">Bayes’ rule</span><span class="font64">:</span></p><div>
<p><span class="font64">P(x | y)</span></p></div><div>
<p><span class="font64">(3.42)</span></p></div>
<p><span class="font64">P(x)P(y | x)</span></p>
<p><span class="font64">P(y)</span></p>
<p><span class="font64">Note that while P (y) appears in the formula, it is usually feasible to compute P(y) — X}<sub>x</sub> P(y I x)P(x), so we do not need to begin with knowledge of P(y).</span></p>
<p><span class="font64">Bayes’ rule is straightforward to derive from the definition of conditional probability, but it is useful to know the name of this formula since many texts&#160;refer to it by name. It is named after the Reverend Thomas Bayes, who first&#160;discovered a special case of the formula. The general version presented here was&#160;independently discovered by Pierre-Simon Laplace.</span></p><h4><a id="bookmark22"></a><span class="font65" style="font-weight:bold;">3.12 Technical Details of Continuous Variables</span></h4>
<p><span class="font64">A proper formal understanding of continuous random variables and probability density functions requires developing probability theory in terms of a branch of&#160;mathematics known as </span><span class="font64" style="font-weight:bold;font-style:italic;">measure theory.</span><span class="font64"> Measure theory is beyond the scope of&#160;this textbook, but we can briefly sketch some of the issues that measure theory is&#160;employed to resolve.</span></p>
<p><span class="font64">In Sec. 3.3.2, we saw that the probability of a continuous vector-valued x lying in some set S is given by the integral of p (x) over the set S. Some choices of set S&#160;can produce paradoxes. For example, it is possible to construct two sets S! and&#160;S<sub>2</sub> such that p (x </span><span class="font64" style="font-weight:bold;font-style:italic;">E</span><span class="font64"> S!) + p(x </span><span class="font64" style="font-weight:bold;font-style:italic;">E</span><span class="font64"> S<sub>2</sub>) &gt; 1 but S! ח S<sub>2</sub> = 0. These sets are generally&#160;constructed making very heavy use of the infinite precision of real numbers, for&#160;example by making fractal-shaped sets or sets that are defined by transforming&#160;the set of rational numbers.<a id="footnote3"></a><sup><a href="#bookmark23">3</a></sup><sup></sup> One of the key contributions of measure theory is to&#160;provide a characterization of the set of sets that we can compute the probability&#160;of without encountering paradoxes. In this book, we only integrate over sets with&#160;relatively simple descriptions, so this aspect of measure theory never becomes a&#160;relevant concern.</span></p>
<p><span class="font64">For our purposes, measure theory is more useful for describing theorems that apply to most points in R<sup>n</sup> but do not apply to some corner cases. Measure theory&#160;provides a rigorous way of describing that a set of points is negligibly small. Such&#160;a set is said to have “ </span><span class="font64" style="font-weight:bold;font-style:italic;">measure zero.&quot;</span><span class="font64"> We do not formally define this concept in this&#160;textbook. However, it is useful to understand the intuition that a set of measure&#160;zero occupies no volume in the space we are measuring. For example, within R<a id="footnote3"></a><sup><a href="#bookmark23">3</a></sup><sup></sup>, a&#160;line has measure zero, while a filled polygon has positive measure. Likewise, an&#160;individual point has measure zero. Any union of countably many sets that each&#160;have measure zero also has measure zero (so the set of all the rational numbers&#160;has measure zero, for instance).</span></p>
<p><span class="font64">Another useful term from measure theory is “ </span><span class="font64" style="font-weight:bold;font-style:italic;">almost everywhere</span><span class="font64">.” A property that holds almost everywhere holds throughout all of space except for on a set of&#160;measure zero. Because the exceptions occupy a negligible amount of space, they&#160;can be safely ignored for many applications. Some important results in probability&#160;theory hold for all discrete values but only hold “almost everywhere” for continuous&#160;values.</span></p>
<p><span class="font64">Another technical detail of continuous variables relates to handling continuous random variables that are deterministic functions of one another. Suppose we have&#160;two random variables, x and y, such that </span><span class="font64" style="font-weight:bold;font-style:italic;">y = g(x</span><span class="font64">), where g is an invertible, continuous, differentiable transformation. One might expect that p<sub>y</sub> (y) = p<sub>x</sub> (g <sup>1</sup>(y)).&#160;This is actually not the case.</span></p>
<p><span class="font64">As a simple example, suppose we have scalar random variables x and y. Suppose y = X and x ~ U(0, 1). If we use the rule p<sub>y</sub>(y) = p<sub>x</sub>(2</span><span class="font64" style="font-weight:bold;font-style:italic;">y)</span><span class="font64"> then p<sub>y</sub> will be 0&#160;everywhere except the interval [0, </span><span class="font64" style="font-weight:bold;font-style:italic;">2</span><span class="font64">], and it will be 1 on this interval. This means</span></p><div>
<p><span class="font64">(3.43)</span></p></div>
<p><span class="font64">p y </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(</sup>y<sup>)d</sup>y</span><span class="font64"> = 2 ’</span></p>
<p><span class="font64">which violates the definition of a probability distribution.</span></p>
<p><span class="font64">This common mistake is wrong because it fails to account for the distortion of space introduced by the function g. Recall that the probability of x lying in&#160;an infinitesimally small region with volume </span><span class="font64" style="font-weight:bold;font-style:italic;">5x</span><span class="font64"> is given by p( </span><span class="font64" style="font-weight:bold;font-style:italic;">x)Sx.</span><span class="font64"> Since g can&#160;expand or contract space, the infinitesimal volume surrounding x in x space may&#160;have different volume in y space.</span></p>
<p><span class="font64">To see how to correct the problem, we return to the scalar case. We need to preserve the property</span></p><div>
<p><span class="font64">p x<sup>(x)</sup> = </span><span class="font64" style="font-weight:bold;font-style:italic;">py<sup>(</sup>g<sup>(x))</sup></span></p>
<table border="1">
<tr><td>
<p><span class="font64">px </span><span class="font64" style="font-weight:bold;font-style:italic;">(x)dx</span><span class="font64"> |.</span></p></td><td>
<p><span class="font64">(3.44)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64"><sup>(</sup>y</span><span class="font67" style="font-weight:bold;">» S</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(3.45)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">) </span><span class="font64" style="font-weight:bold;font-style:italic;">9g<sup>(x) </sup>dx</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(3.46)</span></p></td></tr>
</table></div>
<p><span class="font64"><sup>|</sup>py<sup>(</sup>g<sup>(x))d</sup>y<sup>|</sup></span></p>
<p><span class="font64">Solving from this, we obtain</span></p>
<p><span class="font64">py<sup>(</sup>y<sup>)</sup> = px <sup>(</sup>g <sup>1(</sup>y<sup>))</sup></span></p>
<p><span class="font64">or equivalently</span></p>
<p><span class="font64">In higher dimensions, the derivative generalizes to the determinant of the </span><span class="font64" style="font-weight:bold;font-style:italic;">Jacobian matrix</span><span class="font64">—the matrix with Jjj =&#160;&#160;&#160;&#160;. Thus, for real-valued vectors x and y,</span></p><div>
<p><span class="font64">p x <sup>(x)</sup> = </span><span class="font64" style="font-weight:bold;font-style:italic;">py<sup>(</sup>g<sup>(x))</sup></span></p></div><div>
<p><span class="font64">det</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:underline;"><sup>d</sup>g<sup>(x)</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d x</span></p></div><div>
<p><span class="font64">(3.47)</span></p></div><h4><a id="bookmark24"></a><span class="font65" style="font-weight:bold;">3.13 Information Theory</span></h4>
<p><span class="font64">Information theory is a branch of applied mathematics that revolves around quantifying how much information is present in a signal. It was originally invented&#160;to study sending messages from discrete alphabets over a noisy channel, such as&#160;communication via radio transmission. In this context, information theory tells how&#160;to design optimal codes and calculate the expected length of messages sampled from&#160;specific probability distributions using various encoding schemes. In the context of&#160;machine learning, we can also apply information theory to continuous variables&#160;where some of these message length interpretations do not apply. This field is&#160;fundamental to many areas of electrical engineering and computer science. In this&#160;textbook, we mostly use a few key ideas from information theory to characterize&#160;probability distributions or quantify similarity between probability distributions.&#160;For more detail on information theory, see Cover and Thomas (2006) or MacKay&#160;(2003).</span></p>
<p><span class="font64">The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has&#160;occurred. A message saying “the sun rose this morning” is so uninformative as&#160;to be unnecessary to send, but a message saying “there was a solar eclipse this&#160;morning” is very informative.</span></p>
<p><span class="font64">We would like to quantify information in a way that formalizes this intuition. Specifically,</span></p>
<p><span class="font64">• &#160;&#160;&#160;Likely events should have low information content, and in the extreme case,&#160;events that are guaranteed to happen should have no information content&#160;whatsoever.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Less likely events should have higher information content.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Independent events should have additive information. For example, finding&#160;out that a tossed coin has come up as heads twice should convey twice as&#160;much information as finding out that a tossed coin has come up as heads&#160;once.</span></p>
<p><span class="font64">In order to satisfy all three of these properties, we define the </span><span class="font64" style="font-weight:bold;font-style:italic;">self-information </span><span class="font64">of an event x = x to be</span></p>
<p><span class="font64">I (x) = — log P (x). &#160;&#160;&#160;(3.48)</span></p>
<p><span class="font64">In this book, we always use log to mean the natural logarithm, with base e. Our definition of I(x) is therefore written in units of </span><span class="font64" style="font-weight:bold;font-style:italic;">nats.</span><span class="font64"> One nat is the amount of&#160;information gained by observing an event of probability -<sup>1</sup>. Other texts use base-2&#160;logarithms and units called </span><span class="font64" style="font-weight:bold;font-style:italic;">bits</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">shannons</span><span class="font64">; information measured in bits is just&#160;a rescaling of information measured in nats.</span></p>
<p><span class="font64">When x is continuous, we use the same definition of information by analogy, but some of the properties from the discrete case are lost. For example, an event&#160;with unit density still has zero information, despite not being an event that is&#160;guaranteed to occur.</span></p>
<p><span class="font63">0.7 0.6&#160;0.5&#160;0.4&#160;0.3&#160;0.2&#160;0.1&#160;0.0</span></p>
<p><span class="font64">Figure 3.5: This plot shows how distributions that are closer to deterministic have low Shannon entropy while distributions that are close to uniform have high Shannon entropy.&#160;On the horizontal axis, we plot p, the probability of a binary random variable being equal&#160;to 1. The entropy is given by (p — 1) log(1 — p) — p logp. When p is near 0, the distribution&#160;is nearly deterministic, because the random variable is nearly always 0. Whenp is near 1,&#160;the distribution is nearly deterministic, because the random variable is nearly always 1.&#160;When p = 0.5, the entropy is maximal, because the distribution is uniform over the two&#160;outcomes.</span></p>
<p><span class="font64">Self-information deals only with a single outcome. We can quantify the amount of uncertainty in an entire probability distribution using the </span><span class="font64" style="font-weight:bold;font-style:italic;">Shannon entropy:</span></p><div>
<p><span class="font64">(3.49)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">H</span><span class="font64"> (x) = E </span><span class="font64" style="font-weight:bold;font-style:italic;">x~p [I</span><span class="font64"> (x)] = —E </span><span class="font64" style="font-weight:bold;font-style:italic;">x~p</span><span class="font64"> [log P (x)].</span></p>
<p><span class="font64">also denoted H(P). In other words, the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution. It gives&#160;a lower bound on the number of bits (if the logarithm is base 2, otherwise the units&#160;are different) needed on average to encode symbols drawn from a distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">P.&#160;</span><span class="font64">Distributions that are nearly deterministic (where the outcome is nearly certain)&#160;have low entropy; distributions that are closer to uniform have high entropy. See&#160;Fig. 3.5 for a demonstration. When x is continuous, the Shannon entropy is known&#160;as the </span><span class="font64" style="font-weight:bold;font-style:italic;">differential entropy.</span></p>
<p><span class="font64">If we have two separate probability distributions P(x) and Q(x) over the same random variable x, we can measure how different these two distributions are using&#160;the </span><span class="font64" style="font-weight:bold;font-style:italic;">Kullback-Leibler (KL) divergence:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">D</span><span class="font64"> KL (P ||Q) = Ex^p log</span></p></div><div>
<p><span class="font64">P (x)־</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Q<sup>(x)</sup>_</span></p></div><div>
<p><span class="font64">= Ex~p [log P(x) — log Q(x)]</span></p></div><div>
<p><span class="font64">(3.50)</span></p></div>
<p><span class="font64">In the case of discrete variables, it is the extra amount of information (measured in bits if we use the base 2 logarithm, but in machine learning we usually use nats&#160;and the natural logarithm) needed to send a message containing symbols drawn&#160;from probability distribution P, when we use a code that was designed to minimize&#160;the length of messages drawn from probability distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">Q.</span></p>
<p><span class="font64">The KL divergence has many useful properties, most notably that it is nonnegative. The KL divergence is 0 if and only if P and Q are the same distribution in the case of discrete variables, or equal “almost everywhere” in the case of continuous&#160;variables. Because the KL divergence is non-negative and measures the difference&#160;between two distributions, it is often conceptualized as measuring some sort of&#160;distance between these distributions. However, it is not a true distance measure&#160;because it is not symmetric: </span><span class="font64" style="font-weight:bold;font-style:italic;">D<sub>KL</sub>(P</span><span class="font64">||Q) = D<sub>KL</sub>(Q||P) for some P and Q. This&#160;asymmetry means that there are important consequences to the choice of whether&#160;to use D<sub>KL</sub>(P||Q) or D</span><span class="font64" style="font-variant:small-caps;">kl</span><span class="font64">(Q||P). See Fig. 3.6 for more detail.</span></p>
<p><span class="font64">A quantity that is closely related to the KL divergence is the </span><span class="font64" style="font-weight:bold;font-style:italic;">cross-entropy H(P,</span><span class="font64"> Q) = H(P) + D<sub>KL</sub>(P||Q), which is similar to the KL divergence but lacking&#160;the term on the left:</span></p>
<p><span class="font64" style="font-variant:small-caps;">H(P, Q) = -E^p log </span><span class="font64" style="font-weight:bold;font-style:italic;">Q(x).</span><span class="font64"> &#160;&#160;&#160;(3.51)</span></p>
<p><span class="font64">Minimizing the cross-entropy with respect to Q is equivalent to minimizing the KL divergence, because Q does not participate in the omitted term.</span></p>
<p><span class="font64">When computing many of these quantities, it is common to encounter expressions of the form 0log0. By convention, in the context of information theory, we treat these expressions as lim<sub>x</sub>^0 x log x = 0.</span></p><h4><a id="bookmark25"></a><span class="font65" style="font-weight:bold;">3.14 Structured Probabilistic Models</span></h4>
<p><span class="font64">Machine learning algorithms often involve probability distributions over a very large number of random variables. Often, these probability distributions involve&#160;direct interactions between relatively few variables. Using a single function to&#160;describe the entire joint probability distribution can be very inefficient (both&#160;computationally and statistically).</span></p>
<p><span class="font64">Instead of using a single function to represent a probability distribution, we can split a probability distribution into many factors that we multiply together.&#160;For example, suppose we have three random variables: a, b and c. Suppose that&#160;a influences the value of b and b influences the value of c, but that a and c are&#160;independent given b. We can represent the probability distribution over all three</span></p><div><div><img src="main-24.png" alt=""/></div></div><div><div>
<p><span class="font64">q * = argmin </span><span class="font64" style="font-style:italic;"><sub>q</sub>D</span><span class="font64" style="font-variant:small-caps;"> kl (p|q)</span></p><img src="main-25.jpg" alt=""/></div></div><div><div>
<p><span class="font64" style="font-style:italic;">q * =</span><span class="font64"> argmin </span><span class="font64" style="font-style:italic;font-variant:small-caps;"><sub>q</sub>Dkl</span><span class="font64"> (q||p)</span></p><img src="main-26.jpg" alt=""/></div></div>
<p><span class="font64">Figure 3.6: The KL divergence is asymmetric. Suppose we have a distribution</span><span class="font64" style="font-style:italic;">p(x</span><span class="font64">) and wish to approximate it with another distribution </span><span class="font64" style="font-style:italic;">q (</span><span class="font64">x). We have the choice of minimizing&#160;either Dkl (p||q) or Dkl (q||p). We illustrate the effect of this choice using a mixture of&#160;two Gaussians for p, and a single Gaussian for q. The choice of which direction of the&#160;KL divergence to use is problem-dependent. Some applications require an approximation&#160;that usually places high probability anywhere that the true distribution places high&#160;probability, while other applications require an approximation that rarely places high&#160;probability anywhere that the true distribution places low probability. The choice of the&#160;direction of the KL divergence reflects which of these considerations takes priority for each&#160;application. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> The effect of minimizingDkl(p||q). In this case, we select a q that has&#160;high probability where p has high probability. When p has multiple modes, q chooses to&#160;blur the modes together, in order to put high probability mass on all of them. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> The&#160;effect of minimizing Dkl (q||p). In this case, we select a q that has low probability where&#160;p has low probability. When p has multiple modes that are sufficiently widely separated,&#160;as in this figure, the KL divergence is minimized by choosing a single mode, in order to&#160;avoid putting probability mass in the low-probability areas between modes ofp. Here, we&#160;illustrate the outcome when q is chosen to emphasize the left mode. We could also have&#160;achieved an equal value of the KL divergence by choosing the right mode. If the modes&#160;are not separated by a sufficiently strong low probability region, then this direction of the&#160;KL divergence can still choose to blur the modes.</span></p>
<p><span class="font64">variables as a product of probability distributions over two variables:</span></p>
<p><span class="font64">p(a, b, c) = p(a)p(b | a)p(c | b). &#160;&#160;&#160;(3.52)</span></p>
<p><span class="font64">These factorizations can greatly reduce the number of parameters needed to describe the distribution. Each factor uses a number of parameters that is&#160;exponential in the number of variables in the factor. This means that we can greatly&#160;reduce the cost of representing a distribution if we are able to find a factorization&#160;into distributions over fewer variables.</span></p>
<p><span class="font64">We can describe these kinds of factorizations using graphs. Here we use the word “graph” in the sense of graph theory: a set of vertices that may be connected&#160;to each other with edges. When we represent the factorization of a probability&#160;distribution with a graph, we call it a </span><span class="font64" style="font-weight:bold;font-style:italic;">structured probabilistic model</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">graphical&#160;model.</span></p>
<p><span class="font64">There are two main kinds of structured probabilistic models: directed and undirected. Both kinds of graphical models use a graph G in which each node&#160;in the graph corresponds to a random variable, and an edge connecting two&#160;random variables means that the probability distribution is able to represent direct&#160;interactions between those two random variables.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Directed</span><span class="font64"> models use graphs with directed edges, and they represent factorizations into conditional probability distributions, as in the example above. Specifically, a directed model contains one factor for every random variable </span><span class="font64" style="font-weight:bold;font-style:italic;">xi</span><span class="font64"> in the distribution,&#160;and that factor consists of the conditional distribution over x<sub>i</sub> given the parents of&#160;xi, denoted </span><span class="font64" style="font-weight:bold;font-style:italic;">Pag(x<sub>i</sub></span><span class="font64">):</span></p>
<p><span class="font64">p(x) = JJp (xi<sup>1 Pa</sup>g </span><span class="font64" style="font-weight:bold;font-style:italic;">(xi</span><span class="font64">)). &#160;&#160;&#160;(3.53)</span></p>
<p><span class="font64">i</span></p>
<p><span class="font64">See Fig. 3.7 for an example of a directed graph and the factorization of probability distributions it represents.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Undirected</span><span class="font64"> models use graphs with undirected edges, and they represent factorizations into a set of functions; unlike in the directed case, these functions are usually not probability distributions of any kind. Any set of nodes that are all&#160;connected to each other in G is called a clique. Each clique C<sup>(i)</sup> in an undirected&#160;model is associated with a factor G<sup>(i)</sup>(C<sup>(i)</sup>). These factors are just functions, not&#160;probability distributions. The output of each factor must be non-negative, but&#160;there is no constraint that the factor must sum or integrate to 1 like a probability&#160;distribution.</span></p>
<p><span class="font64">The probability of a configuration of random variables is </span><span class="font64" style="font-weight:bold;font-style:italic;">proportional</span><span class="font64"> to the product of all of these factors—assignments that result in larger factor values are</span></p><div><img src="main-27.jpg" alt=""/></div>
<p><span class="font64">Figure 3.7: A directed graphical model over random variables a, b, c, d and e. This graph corresponds to probability distributions that can be factored as</span></p>
<p><span class="font64">p(a, b, c, d, e) = p(a)p(b | a)p(c | a, b)p(d | b)p(e | c). &#160;&#160;&#160;(3.54)</span></p>
<p><span class="font64">This graph allows us to quickly see some properties of the distribution. For example,a and c interact directly, but a and e interact only indirectly via c.</span></p>
<p><span class="font64">more likely. Of course, there is no guarantee that this product will sum to 1. We therefore divide by a normalizing constant </span><span class="font64" style="font-weight:bold;font-style:italic;">Z,</span><span class="font64"> defined to be the sum or integral&#160;over all states of the product of the ^ functions, in order to obtain a normalized&#160;probability distribution:</span></p>
<p><span class="font63" style="font-weight:bold;">ך</span></p>
<p><span class="font64">p(x) = <sub>Z</sub>nd<sup>־)</sup>(C«) . &#160;&#160;&#160;(3.55)</span></p>
<p><span class="font64">i</span></p>
<p><span class="font64">See Fig. 3.8 for an example of an undirected graph and the factorization of probability distributions it represents.</span></p>
<p><span class="font64">Keep in mind that these graphical representations of factorizations are a language for describing probability distributions. They are not mutually exclusive&#160;families of probability distributions. Being directed or undirected is not a property&#160;of a probability distribution; it is a property of a particular </span><span class="font64" style="font-weight:bold;font-style:italic;">description</span><span class="font64"> of a&#160;probability distribution, but any probability distribution may be described in both&#160;ways.</span></p>
<p><span class="font64">Throughout Part I and Part II of this book, we will use structured probabilistic models merely as a language to describe which direct probabilistic relationships&#160;different machine learning algorithms choose to represent. No further understanding&#160;of structured probabilistic models is needed until the discussion of research topics,&#160;in Part III, where we will explore structured probabilistic models in much greater&#160;detail.</span></p><div><img src="main-28.jpg" alt=""/></div>
<p><span class="font64">Figure 3.8: An undirected graphical model over random variablesa, b, c, d ande. This graph corresponds to probability distributions that can be factored as</span></p>
<p><span class="font64">p(a, b, c, d, e) = — ^&gt;<sup>(1)</sup> (a, b, c)^<sup>(2)</sup>(b, d)^&gt;<sup>(3)</sup> (c, e). &#160;&#160;&#160;(3.56)</span></p>
<p><span class="font64">Z</span></p>
<p><span class="font64">This graph allows us to quickly see some properties of the distribution. For example,a and c interact directly, but a and e interact only indirectly via c.</span></p>
<p><span class="font64">This chapter has reviewed the basic concepts of probability theory that are most relevant to deep learning. One more set of fundamental mathematical tools&#160;remains: numerical methods.</span></p>
<p><a id="bookmark2"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font64"> &#160;&#160;&#160;Incomplete modeling. When we use a model that must discard some of&#160;the information we have observed, the discarded information results in&#160;uncertainty in the model’s predictions. For example, suppose we build a&#160;robot that can exactly observe the location of every object around it. If the</span></p>
<p><a id="bookmark15"><sup><a href="#footnote2">2</a></sup></a></p>
<p><span class="font64"><sup></sup> “Multinoulli” is a term that was recently coined by Gustavo Lacerdo and popularized by Murphy (2012). The multinoulli distribution is a special case of the </span><span class="font64" style="font-style:italic;">multinomial</span><span class="font64"> distribution. A&#160;multinomial distribution is the distribution over vectors in {0,..., </span><span class="font64" style="font-style:italic;">n}<sup>k</sup></span><span class="font64"> representing how many&#160;times each of the k categories is visited when n samples are drawn from a multinoulli distribution.&#160;Many texts use the term “multinomial” to refer to multinoulli distributions without clarifying&#160;that they refer only to the n = 1 case.</span></p>
<p><a id="bookmark23"><sup><a href="#footnote3">3</a></sup></a></p>
<p><span class="font64"><sup></sup>The Banach-Tarski theorem provides a fun example of such sets.</span></p>
</body>
</html>