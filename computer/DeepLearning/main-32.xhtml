<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 17</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Monte Carlo Methods</span></h2>
<p><span class="font64" style="font-weight:bold;">Randomized algorithms fall into two rough categories: Las Vegas algorithms and Monte Carlo algorithms. Las Vegas algorithms always return precisely the correct&#160;answer (or report that they failed). These algorithms consume a random amount&#160;of resources, usually memory or time. In contrast, Monte Carlo algorithms return&#160;answers with a random amount of error. The amount of error can typically be&#160;reduced by expending more resources (usually running time and memory). For any&#160;fixed computational budget, a Monte Carlo algorithm can provide an approximate&#160;answer.</span></p>
<p><span class="font64" style="font-weight:bold;">Many problems in machine learning are so difficult that we can never expect to obtain precise answers to them. This excludes precise deterministic algorithms and&#160;Las Vegas algorithms. Instead, we must use deterministic approximate algorithms&#160;or Monte Carlo approximations. Both approaches are ubiquitous in machine&#160;learning. In this chapter, we focus on Monte Carlo methods.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">17.1 &#160;&#160;&#160;Sampling and Monte Carlo Methods</span></h4>
<p><span class="font64" style="font-weight:bold;">Many important technologies used to accomplish machine learning goals are based on drawing samples from some probability distribution and using these samples to&#160;form a Monte Carlo estimate of some desired quantity.</span></p><h5><a id="bookmark2"></a><span class="font64" style="font-weight:bold;">17.1.1 &#160;&#160;&#160;Why Sampling?</span></h5>
<p><span class="font64" style="font-weight:bold;">There are many reasons that we may wish to draw samples from a probability distribution. Sampling provides a flexible way to approximate many sums and&#160;integrals at reduced cost. Sometimes we use this to provide a significant speedup to&#160;a costly but tractable sum, as in the case when we subsample the full training cost&#160;with minibatches. In other cases, our learning algorithm requires us to approximate&#160;an intractable sum or integral, such as the gradient of the log partition function of&#160;an undirected model. In many other cases, sampling is actually our goal, in the&#160;sense that we want to train a model that can sample from the training distribution.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">17.1.2 Basics of Monte Carlo Sampling</span></h5>
<p><span class="font64" style="font-weight:bold;">When a sum or an integral cannot be computed exactly (for example the sum has an exponential number of terms and no exact simplification is known) it is&#160;often possible to approximate it using Monte Carlo sampling. The idea is to view&#160;the sum or integral as if it was an expectation under some distribution and to</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">approximate the expectation by a corresponding average.</span><span class="font64" style="font-weight:bold;"> Let</span></p>
<p><span class="font64" style="font-weight:bold;"><sup>s</sup> = ^ p(<sup>x</sup>)/(<sup>x</sup>) = </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">e<sub>p</sub></span><span class="font64" style="font-weight:bold;"> <sup>[/</sup> (x)<sup>] &#160;&#160;&#160;(</sup>m)</span></p>
<p><span class="font7">X</span></p>
<p><span class="font64" style="font-weight:bold;">or</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">s = J</span><span class="font64" style="font-weight:bold;"> p(x)/(x)dx = </span><span class="font64" style="font-weight:bold;font-style:italic;">E </span><span class="font11" style="font-style:italic;">p</span><span class="font64" style="font-weight:bold;font-style:italic;">[/</span><span class="font64" style="font-weight:bold;">(x)] &#160;&#160;&#160;(17.2)</span></p>
<p><span class="font64" style="font-weight:bold;">be the sum or integral to estimate, rewritten as an expectation, with the constraint that p is a probability distribution (for the sum) or a probability density (for the&#160;integral) over random variable x.</span></p>
<p><span class="font64" style="font-weight:bold;">We can approximate s by drawing n samples x<sup>(1)</sup>,..., x<sup>(n)</sup> from p and then forming the empirical average</span></p>
<p><span class="font64" style="font-weight:bold;"><sub>1</sub> <sup>n</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>S</sup></span><span class="font11" style="font-style:italic;"><sup>n</sup></span><span class="font63"> </span><span class="font64" style="font-weight:bold;">= &#160;&#160;&#160;<sup>(x(i))</sup>.&#160;&#160;&#160;&#160;<sup>(17</sup>.<sup>3)</sup></span></p>
<p><span class="font11" style="font-style:italic;">i=1</span></p>
<p><span class="font64" style="font-weight:bold;">This approximation is justified by a few different properties. The first trivial observation is that the estimator s is unbiased, since</span></p>
<p><span class="font11" style="font-style:italic;">nn</span></p>
<p><span class="font64" style="font-weight:bold;">E[Sn ] =- &#160;&#160;&#160;E[/(x<sup>(i)</sup>)] = - </span><span class="font64" style="font-weight:bold;font-style:italic;">s = s.</span><span class="font64" style="font-weight:bold;">&#160;&#160;&#160;&#160;(17.4)</span></p>
<p><span class="font64" style="font-weight:bold;">nn</span></p>
<p><span class="font11" style="font-style:italic;">i= 1 &#160;&#160;&#160;i= 1</span></p>
<p><span class="font64" style="font-weight:bold;">But in addition, the </span><span class="font64" style="font-weight:bold;font-style:italic;">law of large numbers</span><span class="font64" style="font-weight:bold;"> states that if the samples x<sup>(i)</sup> are i.i.d., then the average converges almost surely to the expected value:</span></p>
<p><span class="font64" style="font-weight:bold;">lim s<sub>n</sub> = s, &#160;&#160;&#160;(17.5)</span></p>
<p><span class="font64" style="font-weight:bold;">provided that the variance of the individual terms, Var </span><span class="font64" style="font-weight:bold;font-style:italic;">[f</span><span class="font64" style="font-weight:bold;"> (x<sup>(i)</sup>)], is bounded. To see this more clearly, consider the variance of S<sub>n</sub> as n increases. The variance Var[S<sub>n</sub>]&#160;decreases and converges to 0, so long as Var[f (x<sup>(i)</sup>)] &lt; to:</span></p><div>
<p><span class="font64" style="font-weight:bold;">Var[Sn ]</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">n</span></p></div><div>
<p><span class="font65" style="font-weight:bold;">J]<sup>Var[f (x)]</sup></span></p></div><div>
<p><span class="font11" style="font-style:italic;">i=1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;"><sup>Var[f (x)]</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;">n</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(17.6)</span></p>
<p><span class="font64" style="font-weight:bold;">(17.7)</span></p></div>
<p><span class="font64" style="font-weight:bold;">This convenient result also tells us how to estimate the uncertainty in a Monte Carlo average or equivalently the amount of expected error of the Monte Carlo&#160;approximation. We compute both the empirical average of the f (x<sup>(i)</sup>) and their&#160;empirical variance,<a id="footnote1"></a><sup><a href="#bookmark4">1</a></sup><sup></sup> and then divide the estimated variance by the number of&#160;samples n to obtain an estimator of Var[S<sub>n</sub>]. The </span><span class="font64" style="font-weight:bold;font-style:italic;">central limit theorem</span><span class="font64" style="font-weight:bold;"> tells us that&#160;the distribution of the average, S<sub>n</sub>, converges to a normal distribution with mean s&#160;and variance </span><span class="font64" style="font-weight:bold;text-decoration:line-through;"><sup>Var</sup></span><span class="font64" style="font-weight:bold;"><sup>[</sup></span><span class="font64" style="font-weight:bold;text-decoration:line-through;"><sup>f (x)</sup></span><span class="font64" style="font-weight:bold;"><sup>]</sup>. This allows us to estimate confidence intervals around the</span></p>
<p><span class="font64" style="font-weight:bold;">n</span></p>
<p><span class="font64" style="font-weight:bold;">estimate </span><span class="font64" style="font-weight:bold;font-style:italic;">S</span><span class="font64" style="font-weight:bold;"><sub>n</sub>, using the cumulative distribution of the normal density.</span></p>
<p><span class="font64" style="font-weight:bold;">However, all this relies on our ability to easily sample from the base distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x),</span><span class="font64" style="font-weight:bold;"> but doing so is not always possible. When it is not feasible to sample from&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">p,</span><span class="font64" style="font-weight:bold;"> an alternative is to use importance sampling, presented in Sec. 17.2. A more&#160;general approach is to form a sequence of estimators that converge towards the&#160;distribution of interest. That is the approach of Monte Carlo Markov chains&#160;(Sec. 17.3).</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">17.2 Importance Sampling</span></h4>
<p><span class="font64" style="font-weight:bold;">An important step in the decomposition of the integrand (or summand) used by the Monte Carlo method in Eq. 17.2 is deciding which part of the integrand should&#160;play the role the probability p(x) and which part of the integrand should play the&#160;role of the quantity f (x) whose expected value (under that probability distribution)&#160;is to be estimated. There is no unique decomposition because p(x)f (x) can always&#160;be rewritten as</span></p><div>
<p><span class="font64" style="font-weight:bold;">p<sup>(x)f (x)</sup> = q<sup>(x)</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(17.8)</span></p></div>
<p><span class="font64" style="font-weight:bold;">p<sup>(x)f (x)</sup></span></p>
<p><span class="font64" style="font-weight:bold;">q<sup>(x)</sup> where we now sample from q and average pf. In many cases, we wish to compute&#160;an expectation for a given p and an f, and the fact that the problem is specified&#160;from the start as an expectation suggests that this p and f would be a natural&#160;choice of decomposition. However, the original specification of the problem may&#160;not be the the optimal choice in terms of the number of samples required to obtain&#160;a given level of accuracy. Fortunately, the form of the optimal choice q* can be&#160;derived easily. The optimal q* corresponds to what is called optimal importance&#160;sampling.</span></p>
<p><span class="font64" style="font-weight:bold;">Because of the identity shown in Eq. 17.8, any Monte Carlo estimator</span></p><div>
<p><span class="font64" style="font-weight:bold;">(17.9)</span></p></div>
<p><span class="font64" style="font-weight:bold;"><sup>S</sup>p = </span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>n</sub> &#160;&#160;&#160;<sup>f</sup></span><span class="font64" style="font-weight:bold;"><sup> (x(i)</sup>)</span></p>
<p><span class="font64" style="font-weight:bold;">i=1,x</span></p>
<p><span class="font64" style="font-weight:bold;">can be transformed into an importance sampling estimator</span></p><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div><h3><a id="bookmark6"></a><span class="font66">־ E</span></h3>
<p><span class="font64" style="font-weight:bold;">n</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">p(x<sup>(i)</sup>)f (x<sup>(i)</sup>)</span></p>
<p><span class="font64" style="font-weight:bold;">q(x<sup>(i)</sup>)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(17.10)</span></p>
<p><span class="font64" style="font-weight:bold;">q! xw 1</span></p>
<p><span class="font64" style="font-weight:bold;">i=1,xW ~q</span></p>
<p><span class="font64" style="font-weight:bold;">We see readily that the expected value of the estimator does not depend on </span><span class="font64" style="font-weight:bold;font-style:italic;">q:</span></p>
<p><span class="font64" style="font-weight:bold;">Eq </span><span class="font64" style="font-weight:bold;font-style:italic;">[Sq</span><span class="font64" style="font-weight:bold;">] = Eq[Sp] = S. &#160;&#160;&#160;(17.11)</span></p>
<p><span class="font64" style="font-weight:bold;">However, the variance of an importance sampling estimator can be greatly sensitive to the choice of q. The variance is given by</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(17.12)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(17.13)</span></p></div>
<p><span class="font64" style="font-weight:bold;">Var[Sq ] = Var[ </span><span class="font64" style="font-weight:bold;text-decoration:line-through;"><sup>p(x</sup>.<sup>)f(x)</sup></span><span class="font64" style="font-weight:bold;">]/n.</span></p>
<p><span class="font64" style="font-weight:bold;"><sup>q(x)</sup></span></p>
<p><span class="font64" style="font-weight:bold;">The minimum variance occurs when q is</span></p>
<p><span class="font64" style="font-weight:bold;">q*(x) = hf,</span></p>
<p><span class="font64" style="font-weight:bold;">where Z is the normalization constant, chosen so that q* (x) sums or integrates to 1 as appropriate. Better importance sampling distributions put more weight where&#160;the integrand is larger. In fact, when f (x) does not change sign, Var [S<sub>q</sub>*] = 0,&#160;meaning that a single sample is sufficient when the optimal distribution is&#160;used. Of course, this is only because the computation of q* has essentially solved&#160;the original problem, so it is usually not practical to use this approach of drawing&#160;a single sample from the optimal distribution.</span></p>
<p><span class="font64" style="font-weight:bold;">Any choice of sampling distribution q is valid (in the sense of yielding the correct expected value) and q* is the optimal one (in the sense of yielding minimum&#160;variance). Sampling from q* is usually infeasible, but other choices of q can be&#160;feasible while still reducing the variance somewhat.</span></p>
<p><span class="font64" style="font-weight:bold;">Another approach is to use biased importance sampling, which has the advantage of not requiring normalized p or q. In the case of discrete variables, the&#160;biased importance sampling estimator is given by</span></p><div>
<p><span class="font64" style="font-style:italic;">E</span><span class="font11" style="font-style:italic;">n</span></p>
<p><span class="font64" style="font-weight:bold;">i— 1</span></p></div><div>
<p><span class="font11" style="font-style:italic;"><sup>s</sup>BIS =</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">p(x<sup>(i)</sup>) q(x<sup>(l)</sup>)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">n &#160;&#160;&#160;p(x</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64" style="font-weight:bold;">—<sup>1</sup> q </span><span class="font19">1</span><span class="font64" style="font-weight:bold;"> x</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">f</span><span class="font64" style="font-weight:bold;"> (*״&gt;)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">n &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">p(x<sup>(i)</sup>)</span></p>
<p><span class="font64" style="font-weight:bold;">q(xW)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">En</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i—1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(17.14)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">n</span><span class="font64" style="font-weight:bold;"> p<sup>(x(l))</sup> f </span><span class="font64" style="font-weight:bold;font-style:italic;">(<sub>x</sub>(i))</span></p>
<p><span class="font64" style="font-weight:bold;">i—1 </span><span class="font64" style="font-weight:bold;font-style:italic;">q(X-i</span><span class="font64" style="font-weight:bold;">)) <sup>f </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(X )</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">En</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i—1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(x<sup>(l)</sup>) q(x</span><span class="font64" style="font-weight:bold;"> W)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">n &#160;&#160;&#160;p</span><span class="font64" style="font-weight:bold;"> ( x</span></p>
<p><span class="font64" style="font-weight:bold;"><sup>i—1</sup> q </span><span class="font19">1</span><span class="font64" style="font-weight:bold;"> x</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(17.15)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(17.16)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">\^<sup>n</sup></span><span class="font64" style="font-weight:bold;"><sup> </sup></span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:underline;"><sup>p(x(i))</sup></span><span class="font64" style="font-weight:bold;font-style:italic;"> f</span><span class="font64" style="font-weight:bold;"> (<sub>x</sub>(i)) </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>i—1</sup> q(xi<sup>l)</sup>) <sup>f</sup></span><span class="font64" style="font-weight:bold;"><sup> (&#160;&#160;&#160;&#160;)</sup></span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">En</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i—1</span></p></div>
<p><span class="font64" style="font-weight:bold;">n </span><span class="font64" style="font-weight:bold;text-decoration:underline;">p(x<sup>(i)</sup>) </span><span class="font64" style="font-weight:bold;"><sup>i—1</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">q(x <sup>(i)</sup>)</span></p>
<p><span class="font64" style="font-weight:bold;">where </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64" style="font-weight:bold;"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">q</span><span class="font64" style="font-weight:bold;"> are the unnormalized forms ofp and q and the </span><span class="font64" style="font-weight:bold;font-style:italic;">x<sup>(i)</sup></span><span class="font64" style="font-weight:bold;"> are the samples from q. This estimator is biased because E[sbis] = s, except asymptotically when&#160;n —— x and the denominator of Eq. 17.14 converges to 1. Hence this estimator is&#160;called asymptotically unbiased.</span></p>
<p><span class="font64" style="font-weight:bold;">Although a good choice of q can greatly improve the efficiency of Monte Carlo estimation, a poor choice of q can make the efficiency much worse. Going back&#160;to Eq. 17.12, we see that if there are samples of q for which&#160;then the variance of the estimator can get very large. This may happen when&#160;q(x) is tiny while neither p(x) nor f (x) are small enough to cancel it. The q&#160;distribution is usually chosen to be a very simple distribution so that it is easy&#160;to sample from. When x is high-dimensional, this simplicity in q causes it to&#160;match p or p|f | poorly. When q(x<sup>(i) )</sup> » p<sup>(x(i))</sup>|f<sup>(</sup> x<sup>(i)</sup>) |, importance sampling&#160;collects useless samples (summing tiny numbers or zeros). On the other hand, when&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">q(x<sup>(i)</sup>)</span><span class="font64" style="font-weight:bold;"> ^ p(x <sup>(i)</sup>)lf ( x<sup>(i)</sup> )|, which will happen more rarely, the ratio can be huge.&#160;Because these latter events are rare, they may not show up in a typical sample,&#160;yielding typical underestimation of s, compensated rarely by gross overestimation.&#160;Such very large or very small numbers are typical when x is high dimensional,&#160;because in high dimension the dynamic range of joint probabilities can be very&#160;large.</span></p>
<p><span class="font64" style="font-weight:bold;">In spite of this danger, importance sampling and its variants have been found very useful in many machine learning algorithms, including deep learning algorithms.&#160;For example, see the use of importance sampling to accelerate training in neural&#160;language models with a large vocabulary (Sec. 12.4.3.3) or other neural nets&#160;with a large number of outputs. See also how importance sampling has been&#160;used to estimate a partition function (the normalization constant of a probability&#160;distribution) in Sec. 18.7, and to estimate the log-likelihood in deep directed models&#160;such as the variational autoencoder, in Sec. 20.10.3. Importance sampling may&#160;also be used to improve the estimate of the gradient of the cost function used to&#160;train model parameters with stochastic gradient descent, particularly for models&#160;such as classifiers where most of the total value of the cost function comes from a&#160;small number of misclassified examples. Sampling more difficult examples more&#160;frequently can reduce the variance of the gradient in such cases (Hinton, 2006).</span></p><h4><a id="bookmark7"></a><span class="font65" style="font-weight:bold;">17.3 Markov Chain Monte Carlo Methods</span></h4>
<p><span class="font64" style="font-weight:bold;">In many cases, we wish to use a Monte Carlo technique but there is no tractable method for drawing exact samples from the distribution p<sub>mo</sub>d</span><span class="font19"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;"> (x) or from a good&#160;(low variance) importance sampling distribution q (x). In the context of deep&#160;learning, this most often happens when p<sub>mo</sub>d<sub>e</sub></span><span class="font19">1</span><span class="font64" style="font-weight:bold;">(x) is represented by an undirected&#160;model. In these cases, we introduce a mathematical tool called a </span><span class="font64" style="font-weight:bold;font-style:italic;">Markov chain</span><span class="font64" style="font-weight:bold;"> to&#160;approximately sample from p<sub>mo</sub>d<sub>e</sub></span><span class="font19">1</span><span class="font64" style="font-weight:bold;">(x). The family of algorithms that use Markov&#160;chains to perform Monte Carlo estimates is called </span><span class="font64" style="font-weight:bold;font-style:italic;">Markov chain Monte Carlo&#160;methods</span><span class="font64" style="font-weight:bold;"> (MCMC). Markov chain Monte Carlo methods for machine learning are&#160;described at greater length in Koller and Friedman (2009). The most standard,&#160;generic guarantees for MCMC techniques are only applicable when the model&#160;does not assign zero probability to any state. Therefore, it is most convenient&#160;to present these techniques as sampling from an energy-based model (EBM)&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">p(x)</span><span class="font64" style="font-weight:bold;"> rc exp (—E(x)) as described in Sec. 16.2.4. In the EBM formulation, every&#160;state is guaranteed to have non-zero probability. MCMC methods are in fact&#160;more broadly applicable and can be used with many probability distributions that&#160;contain zero probability states. However, the theoretical guarantees concerning the&#160;behavior of MCMC methods must be proven on a case-by-case basis for different&#160;families of such distributions. In the context of deep learning, it is most common&#160;to rely on the most general theoretical guarantees that naturally apply to all&#160;energy-based models.</span></p>
<p><span class="font64" style="font-weight:bold;">To understand why drawing samples from an energy-based model is difficult, consider an EBM over just two variables, defining a distribution p(a, b). In order&#160;to sample a, we must draw a from p(a | b), and in order to sample b, we must&#160;draw it from p(b | a). It seems to be an intractable chicken-and-egg problem.&#160;Directed models avoid this because their graph is directed and acyclic. To perform&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">ancestral sampling</span><span class="font64" style="font-weight:bold;"> one simply samples each of the variables in topological order,&#160;conditioning on each variable’s parents, which are guaranteed to have already been&#160;sampled (Sec. 16.3). Ancestral sampling defines an efficient, single-pass method of&#160;obtaining a sample.</span></p>
<p><span class="font64" style="font-weight:bold;">In an EBM, we can avoid this chicken and egg problem by sampling using a Markov chain. The core idea of a Markov chain is to have a state x that begins&#160;as an arbitrary value. Over time, we randomly update x repeatedly. Eventually&#160;x becomes (very nearly) a fair sample from p( x). Formally, a Markov chain is&#160;defined by a random state x and a transition distribution T</span><span class="font64" style="font-weight:bold;font-style:italic;">(x'</span><span class="font64" style="font-weight:bold;"> | x) specifying&#160;the probability that a random update will go to state x' if it starts in state x.&#160;Running the Markov chain means repeatedly updating the state x to a value x'&#160;sampled from T(X | x).</span></p>
<p><span class="font64" style="font-weight:bold;">To gain some theoretical understanding of how MCMC methods work, it is useful to reparametrize the problem. First, we restrict our attention to the case&#160;where the random variable x has countably many states. In this case, we can&#160;represent the state as just a positive integer </span><span class="font64" style="font-weight:bold;font-style:italic;">x.</span><span class="font64" style="font-weight:bold;"> Different integer values of </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64" style="font-weight:bold;"> map&#160;back to different states x in the original problem.</span></p>
<p><span class="font64" style="font-weight:bold;">Consider what happens when we run infinitely many Markov chains in parallel. All of the states of the different Markov chains are drawn from some distribution&#160;q<sup>(t)</sup>(x), where t indicates the number of time steps that have elapsed. At the&#160;beginning, q<sup>(0)</sup> is some distribution that we used to arbitrarily initialize x for each&#160;Markov chain. Later, q<sup>(t)</sup> is influenced by all of the Markov chain steps that have&#160;run so far. Our goal is for q<sup>(t)</sup> (x) to converge to </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x).</span></p>
<p><span class="font64" style="font-weight:bold;">Because we have reparametrized the problem in terms of positive integer x, we can describe the probability distribution q using a vector v, with</span></p>
<p><span class="font64" style="font-weight:bold;">q(x = i) = </span><span class="font64" style="font-weight:bold;font-style:italic;">Vi.</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;(17.17)</span></p>
<p><span class="font64" style="font-weight:bold;">Consider what happens when we update a single Markov chain’s state x to a new state x'. The probability of a single state landing in state </span><span class="font19">2</span><span class="font64" style="font-weight:bold;">/ is given by</span></p>
<p><span class="font64" style="font-weight:bold;">q<sup>(t+1)</sup>(x') = &#160;&#160;&#160;q<sup>(t)</sup> (x)T(x' | x).&#160;&#160;&#160;&#160;(17.18)</span></p>
<p><span class="font64" style="font-weight:bold;">X</span></p>
<p><span class="font64" style="font-weight:bold;">Using our integer parametrization, we can represent the effect of the transition operator T using a matrix A. We define </span><span class="font64" style="font-weight:bold;font-style:italic;">A</span><span class="font64" style="font-weight:bold;"> so that</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Ai,j = T(x' = i</span><span class="font64" style="font-weight:bold;"> | x = </span><span class="font64" style="font-weight:bold;font-style:italic;">j</span><span class="font64" style="font-weight:bold;">). &#160;&#160;&#160;(17.19)</span></p>
<p><span class="font64" style="font-weight:bold;">Using this definition, we can now rewrite Eq. 17.18. Rather than writing it in terms of q and T to understand how a single state is updated, we may now use v&#160;and A to describe how the entire distribution over all the different Markov chains&#160;run in parallel shifts as we apply an update:</span></p>
<p><span class="font64" style="font-weight:bold;">v<sup>(t)</sup> = Av<sup>(t-1)</sup>. &#160;&#160;&#160;(17.20)</span></p>
<p><span class="font64" style="font-weight:bold;">Applying the Markov chain update repeatedly corresponds to multiplying by the matrix </span><span class="font63">A </span><span class="font64" style="font-weight:bold;">repeatedly. In other words, we can think of the process as exponentiating&#160;the matrix </span><span class="font63">A</span><span class="font64" style="font-weight:bold;">:</span></p>
<p><span class="font63">v</span><span class="font64" style="font-weight:bold;"><sup>(t)</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">= <sub>A</sub></span><span class="font11" style="font-style:italic;">t</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>v</sub></span><span class="font11" style="font-style:italic;">(0)</span><span class="font64" style="font-weight:bold;font-style:italic;">.</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;(17.21)</span></p>
<p><span class="font64" style="font-weight:bold;">The matrix </span><span class="font63">A </span><span class="font64" style="font-weight:bold;">has special structure because each of its columns represents a probability distribution. Such matrices are called </span><span class="font64" style="font-weight:bold;font-style:italic;">stochastic matrices.</span><span class="font64" style="font-weight:bold;"> If there is&#160;a non-zero probability of transitioning from any state x to any other state x' for&#160;some power t, then the Perron-Frobenius theorem (Perron, 1907; Frobenius, 1908)&#160;guarantees that the largest eigenvalue is real and equal to 1. Over time, we can&#160;see that all of the eigenvalues are exponentiated:</span></p>
<p><span class="font63">v</span><span class="font64" style="font-weight:bold;"><sup>(t)</sup> = </span><span class="font63"><sup>(</sup>V</span><span class="font64" style="font-weight:bold;">diag(</span><span class="font63">A</span><span class="font64" style="font-weight:bold;">)</span><span class="font63">V</span><span class="font64" style="font-weight:bold;"><sup>1</sup>־</span><span class="font63">^ v</span><span class="font64" style="font-weight:bold;"><sup>(0)</sup> = </span><span class="font63">V</span><span class="font64" style="font-weight:bold;">diag(</span><span class="font63">A</span><span class="font64" style="font-weight:bold;">)*</span><span class="font63">V</span><span class="font64" style="font-weight:bold;"><sup>-1</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">v</span><span class="font11" style="font-style:italic;"><sup>(0)</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">.</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;(17.22)</span></p>
<p><span class="font64" style="font-weight:bold;">This process causes all of the eigenvalues that are not equal to 1 to decay to zero. Under some additional mild conditions, </span><span class="font63">A </span><span class="font64" style="font-weight:bold;">is guaranteed to have only&#160;one eigenvector with eigenvalue 1. The process thus converges to a </span><span class="font64" style="font-weight:bold;font-style:italic;">stationary&#160;distribution,</span><span class="font64" style="font-weight:bold;"> sometimes also called the </span><span class="font64" style="font-weight:bold;font-style:italic;">equilibrium distribution.</span><span class="font64" style="font-weight:bold;"> At convergence,</span></p>
<p><span class="font63">v</span><span class="font64" style="font-weight:bold;">' = </span><span class="font63">Av </span><span class="font64" style="font-weight:bold;">= </span><span class="font63">v</span><span class="font64" style="font-weight:bold;">, &#160;&#160;&#160;(17.23)</span></p>
<p><span class="font64" style="font-weight:bold;">and this same condition holds for every additional step. This is an eigenvector equation. To be a stationary point, </span><span class="font63">v </span><span class="font64" style="font-weight:bold;">must be an eigenvector with corresponding&#160;eigenvalue 1. This condition guarantees that once we have reached the stationary&#160;distribution, repeated applications of the transition sampling procedure do not&#160;change the </span><span class="font64" style="font-weight:bold;font-style:italic;">distribution</span><span class="font64" style="font-weight:bold;"> over the states of all the various Markov chains (although&#160;transition operator does change each individual state, of course).</span></p>
<p><span class="font64" style="font-weight:bold;">If we have chosen T correctly, then the stationary distribution q will be equal to the distribution p we wish to sample from. We will describe how to choose T&#160;shortly, in Sec. 17.4.</span></p>
<p><span class="font64" style="font-weight:bold;">Most properties of Markov Chains with countable states can be generalized to continuous variables. In this situation, some authors call the Markov Chain&#160;a </span><span class="font64" style="font-weight:bold;font-style:italic;">Harris chain</span><span class="font64" style="font-weight:bold;"> but we use the term Markov Chain to describe both conditions.&#160;In general, a Markov chain with transition operator T will converge, under mild&#160;conditions, to a fixed point described by the equation</span></p>
<p><span class="font64" style="font-weight:bold;">q'(x') = Ex^qT(x' | x), &#160;&#160;&#160;(17.24)</span></p>
<p><span class="font64" style="font-weight:bold;">which in the discrete case is just rewriting Eq. 17.23. When x is discrete, the expectation corresponds to a sum, and when x is continuous, the expectation&#160;corresponds to an integral.</span></p>
<p><span class="font64" style="font-weight:bold;">Regardless of whether the state is continuous or discrete, all Markov chain methods consist of repeatedly applying stochastic updates until eventually the state&#160;begins to yield samples from the equilibrium distribution. Running the Markov&#160;chain until it reaches its equilibrium distribution is called “ </span><span class="font64" style="font-weight:bold;font-style:italic;">burning in&quot;&quot;</span><span class="font64" style="font-weight:bold;"> the Markov&#160;chain. After the chain has reached equilibrium, a sequence of infinitely many&#160;samples may be drawn from from the equilibrium distribution. They are identically&#160;distributed but any two successive samples will be highly correlated with each other.&#160;A finite sequence of samples may thus not be very representative of the equilibrium&#160;distribution. One way to mitigate this problem is to return only every n successive&#160;samples, so that our estimate of the statistics of the equilibrium distribution is&#160;not as biased by the correlation between an MCMC sample and the next several&#160;samples. Markov chains are thus expensive to use because of the time required to&#160;burn in to the equilibrium distribution and the time required to transition from&#160;one sample to another reasonably decorrelated sample after reaching equilibrium.&#160;If one desires truly independent samples, one can run multiple Markov chains&#160;in parallel. This approach uses extra parallel computation to eliminate latency.&#160;The strategy of using only a single Markov chain to generate all samples and the&#160;strategy of using one Markov chain for each desired sample are two extremes; deep&#160;learning practitioners usually use a number of chains that is similar to the number&#160;of examples in a minibatch and then draw as many samples as are needed from&#160;this fixed set of Markov chains. A commonly used number of Markov chains is 100.</span></p>
<p><span class="font64" style="font-weight:bold;">Another difficulty is that we do not know in advance how many steps the Markov chain must run before reaching its equilibrium distribution. This length of&#160;time is called the </span><span class="font64" style="font-weight:bold;font-style:italic;">mixing time.</span><span class="font64" style="font-weight:bold;"> It is also very difficult to test whether a Markov&#160;chain has reached equilibrium. We do not have a precise enough theory for guiding&#160;us in answering this question. Theory tells us that the chain will converge, but not&#160;much more. If we analyze the Markov chain from the point of view of a matrix A&#160;acting on a vector of probabilities v, then we know that the chain mixes when A*&#160;has effectively lost all of the eigenvalues from A besides the unique eigenvalue of 1.&#160;This means that the magnitude of the second largest eigenvalue will determine the&#160;mixing time. However, in practice, we cannot actually represent our Markov chain&#160;in terms of a matrix. The number of states that our probabilistic model can visit&#160;is exponentially large in the number of variables, so it is infeasible to represent&#160;v, A, or the eigenvalues of A. Due to these and other obstacles, we usually do&#160;not know whether a Markov chain has mixed. Instead, we simply run the Markov&#160;chain for an amount of time that we roughly estimate to be sufficient, and use&#160;heuristic methods to determine whether the chain has mixed. These heuristic&#160;methods include manually inspecting samples or measuring correlations between&#160;successive samples.</span></p><h4><a id="bookmark8"></a><span class="font65" style="font-weight:bold;">17.4 Gibbs Sampling</span></h4>
<p><span class="font64" style="font-weight:bold;">So far we have described how to draw samples from a distribution q(x) by repeatedly updating x ^ </span><span class="font64" style="font-weight:bold;font-style:italic;">x'&#160;&#160;&#160;&#160;T(x'</span><span class="font64" style="font-weight:bold;"> | x). However, we have not described how to ensure that</span></p>
<p><span class="font64" style="font-weight:bold;">q(x) is a useful distribution. Two basic approaches are considered in this book. The first one is to derive T from a given learned pm<sub>o</sub>d<sub>e</sub>1, described below with the&#160;case of sampling from EBMs. The second one is to directly parametrize T and&#160;learn it, so that its stationary distribution implicitly defines the p<sub>mo</sub>d</span><span class="font19"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;"> of interest.&#160;Examples of this second approach are discussed in Sec. 20.12 and Sec. 20.13.</span></p>
<p><span class="font64" style="font-weight:bold;">In the context of deep learning, we commonly use Markov chains to draw samples from an energy-based model defining a distributionp<sub>mo</sub>d</span><span class="font19"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;"> (x). In this case,&#160;we want the q(x) for the Markov chain to be p<sub>mo</sub>d</span><span class="font19"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;"> (x). To obtain the desired&#160;q(x), we must choose an appropriate T(x' | x).</span></p>
<p><span class="font64" style="font-weight:bold;">A conceptually simple and effective approach to building a Markov chain that samples from p<sub>mo</sub>d</span><span class="font19"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;">(x) is to use </span><span class="font64" style="font-weight:bold;font-style:italic;">Gibbs sampling,</span><span class="font64" style="font-weight:bold;"> in which sampling from&#160;T(x' | x) is accomplished by selecting one variable x^ and sampling it from p<sub>mo</sub>d</span><span class="font19"><sub>e</sub>1&#160;</span><span class="font64" style="font-weight:bold;">conditioned on its neighbors in the undirected graph G defining the structure of&#160;the energy-based model. It is also possible to sample several variables at the same&#160;time so long as they are conditionally independent given all of their neighbors. As&#160;shown in the RBM example in Sec. 16.7.1, all of the hidden units of an RBM may&#160;be sampled simultaneously because they are conditionally independent from each&#160;other given all of the visible units. Likewise, all of the visible units may be sampled&#160;simultaneously because they are conditionally independent from each other given&#160;all of the hidden units. Gibbs sampling approaches that update many variables&#160;simultaneously in this way are called </span><span class="font64" style="font-weight:bold;font-style:italic;">block Gibbs sampling.</span></p>
<p><span class="font64" style="font-weight:bold;">Alternate approaches to designing Markov chains to sample from </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64" style="font-weight:bold;"> <sub>mo</sub>d</span><span class="font19"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;"> are possible. For example, the Metropolis-Hastings algorithm is widely used in other&#160;disciplines. In the context of the deep learning approach to undirected modeling,&#160;it is rare to use any approach other than Gibbs sampling. Improved sampling&#160;techniques are one possible research frontier.</span></p><h4><a id="bookmark9"></a><span class="font65" style="font-weight:bold;">17.5 The Challenge of Mixing between Separated Modes</span></h4>
<p><span class="font64" style="font-weight:bold;">The primary difficulty involved with MCMC methods is that they have a tendency to </span><span class="font64" style="font-weight:bold;font-style:italic;">mix</span><span class="font64" style="font-weight:bold;"> poorly. Ideally, successive samples from a Markov chain designed to sample&#160;from p(x) would be completely independent from each other and would visit many&#160;different regions in x space proportional to their probability. Instead, especially&#160;in high dimensional cases, MCMC samples become very correlated. We refer</span></p>
<p><span class="font64" style="font-weight:bold;">to such behavior as slow mixing or even failure to mix. MCMC methods with slow mixing can be seen as inadvertently performing something resembling noisy&#160;gradient descent on the energy function, or equivalently noisy hill climbing on the&#160;probability, with respect to the state of the chain (the random variables being&#160;sampled). The chain tends to take small steps (in the space of the state of the&#160;Markov chain), from a configuration x<sup>(t-1)</sup> to a configuration x<sup>(t)</sup>, with the energy&#160;E(x<sup>(t)</sup>) generally lower or approximately equal to the energy E(x<sup>(t-1)</sup>), with a&#160;preference for moves that yield lower energy configurations. When starting from a&#160;rather improbable configuration (higher energy than the typical ones from p (x)),&#160;the chain tends to gradually reduce the energy of the state and only occasionally&#160;move to another mode. Once the chain has found a region of low energy (for&#160;example, if the variables are pixels in an image, a region of low energy might be&#160;a connected manifold of images of the same object), which we call a mode, the&#160;chain will tend to walk around that mode (following a kind of random walk). Once&#160;in a while it will step out of that mode and generally return to it or (if it finds&#160;an escape route) move towards another mode. The problem is that successful&#160;escape routes are rare for many interesting distributions, so the Markov chain will&#160;continue to sample the same mode longer than it should.</span></p>
<p><span class="font64" style="font-weight:bold;">This is very clear when we consider the Gibbs sampling algorithm (Sec. 17.4). In this context, consider the probability of going from one mode to a nearby&#160;mode within a given number of steps. What will determine that probability is&#160;the shape of the “energy barrier” between these modes. Transitions between two&#160;modes that are separated by a high energy barrier (a region of low probability)&#160;are exponentially less likely (in terms of the height of the energy barrier). This is&#160;illustrated in Fig. 17.1. The problem arises when there are multiple modes with&#160;high probability that are separated by regions of low probability, especially when&#160;each Gibbs sampling step must update only a small subset of variables whose&#160;values are largely determined by the other variables.</span></p>
<p><span class="font64" style="font-weight:bold;">As a simple example, consider an energy-based model over two variables a and b, which are both binary with a sign, taking on values —1 and 1. If E(a, b) = —wab&#160;for some large positive number w, then the model expresses a strong belief that a&#160;and b have the same sign. Consider updating b using a Gibbs sampling step with&#160;a = 1. The conditional distribution over b is given by </span><span class="font64" style="font-weight:bold;font-style:italic;">P(</span><span class="font64" style="font-weight:bold;"> b = 1 | a = 1) = a </span><span class="font64" style="font-weight:bold;font-style:italic;">(w).&#160;</span><span class="font64" style="font-weight:bold;">If w is large, the sigmoid saturates, and the probability of also assigning b to be&#160;1 is close to 1. Likewise, if a = —1, the probability of assigning b to be —1 is&#160;close to 1. According to P<sub>mo</sub>d</span><span class="font19"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;"> (a, b), both signs of both variables are equally likely.&#160;According to P<sub>mo</sub>d<sub>e</sub>i (a | b), both variables should have the same sign. This means&#160;that Gibbs sampling will only very rarely flip the signs of these variables.</span></p><div><img src="main-179.jpg" alt=""/>
<p><span class="font64">Figure 17.1: Paths followed by Gibbs sampling for three distributions, with the Markov chain initialized at the mode in both cases. </span><span class="font64" style="font-weight:bold;font-style:italic;">(Left)</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">A multivariate normal distribution&#160;with two independent variables. Gibbs sampling mixes well because the variables are&#160;independent. </span><span class="font64" style="font-weight:bold;font-style:italic;">(Center)</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">A multivariate normal distribution with highly correlated variables.&#160;The correlation between variables makes it difficult for the Markov chain to mix. Because&#160;each variable must be updated conditioned on the other, the correlation reduces the rate&#160;at which the Markov chain can move away from the starting point. </span><span class="font64" style="font-weight:bold;font-style:italic;">(Right)</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">A mixture of&#160;Gaussians with widely separated modes that are not axis-aligned. Gibbs sampling mixes&#160;very slowly because it is difficult to change modes while altering only one variable at a&#160;time.</span></p></div>
<p><span class="font64" style="font-weight:bold;">In more practical scenarios, the challenge is even greater because we care not only about making transitions between two modes but more generally between&#160;all the many modes that a real model might contain. If several such transitions&#160;are difficult because of the difficulty of mixing between modes, then it becomes&#160;very expensive to obtain a reliable set of samples covering most of the modes, and&#160;convergence of the chain to its stationary distribution is very slow.</span></p>
<p><span class="font64" style="font-weight:bold;">Sometimes this problem can be resolved by finding groups of highly dependent units and updating all of them simultaneously in a block. Unfortunately, when&#160;the dependencies are complicated, it can be computationally intractable to draw a&#160;sample from the group. After all, the problem that the Markov chain was originally&#160;introduced to solve is this problem of sampling from a large group of variables.</span></p>
<p><span class="font64" style="font-weight:bold;">In the context of models with latent variables, which define a joint distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p<sub>mo</sub></span><span class="font64" style="font-weight:bold;">d<sub>e</sub>i(x, h), we often draw samples of x by alternating between sampling from&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">p<sub>mo</sub></span><span class="font64" style="font-weight:bold;">d<sub>e</sub>i(x | h) and sampling fromp<sub>mo</sub>d<sub>e</sub></span><span class="font19">1</span><span class="font64" style="font-weight:bold;">(h | x). From the point of view of mixing&#160;rapidly, we would like p<sub>mo</sub>d</span><span class="font19"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;"> (h | x) to have very high entropy. However, from the&#160;point of view of learning a useful representation of h, we would like h to encode</span></p><div><div><img src="main-180.jpg" alt=""/></div></div><div>
<p><span class="font59" style="font-weight:bold;">EDnane□□□!!</span></p><h1 dir="rtl"><a id="bookmark10"></a><span class="font54" style="font-weight:bold;">□סם0םםםםםם</span></h1>
<p><span class="font53" style="font-weight:bold;">QDBDBQEIBBD</span></p>
<p><span class="font13" style="font-weight:bold;">BBBEIBnQBBQ</span></p>
<p><span class="font53" style="font-weight:bold;">BEIDBDQEIDDQ</span></p>
<p><span class="font53" style="font-weight:bold;">BE1BBBBBBEI□</span></p>
<p><span class="font53" style="font-weight:bold;">BQBQDBEIBD□</span></p>
<p><span class="font53" style="font-weight:bold;">BBBBBBBSBB</span></p>
<p><span class="font53" style="font-weight:bold;">BDBOBBBBBB</span></p>
<p><span class="font53" style="font-weight:bold;">BBDBBBBBDB</span></p></div>
<p><span class="font64">Figure 17.2: An illustration of the slow mixing problem in deep probabilistic models. Each panel should be read left to right, top to bottom. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> Consecutive samples from&#160;Gibbs sampling applied to a deep Boltzmann machine trained on the MNIST dataset.&#160;Consecutive samples are similar to each other. Because the Gibbs sampling is performed&#160;in a deep graphical model, this similarity is based more on semantic rather than raw visual&#160;features, but it is still difficult for the Gibbs chain to transition from one mode of the&#160;distribution to another, for example by changing the digit identity. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> Consecutive&#160;ancestral samples from a generative adversarial network. Because ancestral sampling&#160;generates each sample independently from the others, there is no mixing problem.</span></p>
<p><span class="font64" style="font-weight:bold;">enough information about x to reconstruct it well, which implies that h and </span><span class="font64" style="font-weight:bold;font-style:italic;">x </span><span class="font64" style="font-weight:bold;">should have very high mutual information. These two goals are at odds with each&#160;other. We often learn generative models that very precisely encode x into h but&#160;are not able to mix very well. This situation arises frequently with Boltzmann&#160;machines—the sharper the distribution a Boltzmann machine learns, the harder&#160;it is for a Markov chain sampling from the model distribution to mix well. This&#160;problem is illustrated in Fig. 17.2.</span></p>
<p><span class="font64" style="font-weight:bold;">All this could make MCMC methods less useful when the distribution of interest has a manifold structure with a separate manifold for each class: the distribution&#160;is concentrated around many modes and these modes are separated by vast regions&#160;of high energy. This type of distribution is what we expect in many classification&#160;problems and would make MCMC methods converge very slowly because of poor&#160;mixing between modes.</span></p><h5><a id="bookmark11"></a><span class="font64" style="font-weight:bold;">17.5.1 Tempering to Mix between Modes</span></h5>
<p><span class="font64" style="font-weight:bold;">When a distribution has sharp peaks of high probability surrounded by regions of low probability, it is difficult to mix between the different modes of the distribution.</span></p>
<p><span class="font64" style="font-weight:bold;">Several techniques for faster mixing are based on constructing alternative versions of the target distribution in which the peaks are not as high and the surrounding&#160;valleys are not as low. Energy-based models provide a particularly simple way to&#160;do so. So far, we have described an energy-based model as defining a probability&#160;distribution</span></p>
<p><span class="font64" style="font-weight:bold;">p(x) </span><span class="font64" style="font-weight:bold;font-style:italic;">rc</span><span class="font64" style="font-weight:bold;"> exp (—E(x)). &#160;&#160;&#160;(17.25)</span></p>
<p><span class="font64" style="font-weight:bold;">Energy-based models may be augmented with an extra parameter 3 controlling how sharply peaked the distribution is:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p!</span><span class="font64" style="font-style:italic;">3</span><span class="font64" style="font-weight:bold;">(x) rc exp(-3E(x)). &#160;&#160;&#160;(17.26)</span></p>
<p><span class="font64" style="font-weight:bold;">The 3 parameter is often described as being the reciprocal of the </span><span class="font64" style="font-weight:bold;font-style:italic;">temperature</span><span class="font64" style="font-weight:bold;">, reflecting the origin of energy-based models in statistical physics. When the&#160;temperature falls to zero and 3 rises to infinity, the energy-based model becomes&#160;deterministic. When the temperature rises to infinity and 3 falls to zero, the&#160;distribution (for discrete x) becomes uniform.</span></p>
<p><span class="font64" style="font-weight:bold;">Typically, a model is trained to be evaluated at 3 = 1. However, we can make use of other temperatures, particularly those where 3 &lt; 1. </span><span class="font64" style="font-weight:bold;font-style:italic;">Tempering</span><span class="font64" style="font-weight:bold;"> is a general&#160;strategy of mixing between modes of pi rapidly by drawing samples with 3 &lt; 1.</span></p>
<p><span class="font64" style="font-weight:bold;">Markov chains based on </span><span class="font64" style="font-weight:bold;font-style:italic;">tempered transitions</span><span class="font64" style="font-weight:bold;"> (Neal, 1994) temporarily sample from higher-temperature distributions in order to mix to different modes, then&#160;resume sampling from the unit temperature distribution. These techniques have&#160;been applied to models such as RBMs (Salakhutdinov, 2010). Another approach is&#160;to use </span><span class="font64" style="font-weight:bold;font-style:italic;">parallel tempering</span><span class="font64" style="font-weight:bold;"> (Iba, 2001), in which the Markov chain simulates many&#160;different states in parallel, at different temperatures. The highest temperature&#160;states mix slowly, while the lowest temperature states, at temperature 1, provide&#160;accurate samples from the model. The transition operator includes stochastically&#160;swapping states between two different temperature levels, so that a sufficiently high-probability sample from a high-temperature slot can jump into a lower temperature&#160;slot. This approach has also been applied to RBMs (Desjardins </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64" style="font-weight:bold;"> 2010; Cho&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64" style="font-weight:bold;"> 2010). Although tempering is a promising approach, at this point it has not&#160;allowed researchers to make a strong advance in solving the challenge of sampling&#160;from complex EBMs. One possible reason is that there are </span><span class="font64" style="font-weight:bold;font-style:italic;">critical temperatures&#160;</span><span class="font64" style="font-weight:bold;">around which the temperature transition must be very slow (as the temperature is&#160;gradually reduced) in order for tempering to be effective.</span></p><h5><a id="bookmark12"></a><span class="font64" style="font-weight:bold;">17.5.2 Depth May Help Mixing</span></h5>
<p><span class="font64" style="font-weight:bold;">When drawing samples from a latent variable model p(h, x), we have seen that if </span><span class="font64" style="font-weight:bold;font-style:italic;">p(h</span><span class="font64" style="font-weight:bold;"> | x) encodes x too well, then sampling from p(x | h) will not change x very&#160;much and mixing will be poor. One way to resolve this problem is to make h be a&#160;deep representation, that encodes x into h in such a way that a Markov chain in&#160;the space of h can mix more easily. Many representation learning algorithms, such&#160;as autoencoders and RBMs, tend to yield a marginal distribution over h that is&#160;more uniform and more unimodal than the original data distribution over x. It can&#160;be argued that this arises from trying to minimize reconstruction error while using&#160;all of the available representation space, because minimizing reconstruction error&#160;over the training examples will be better achieved when different training examples&#160;are easily distinguishable from each other in h-space, and thus well separated.&#160;Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;"> (2013a) observed that deeper stacks of regularized autoencoders or&#160;RBMs yield marginal distributions in the top-level h-space that appeared more&#160;spread out and more uniform, with less of a gap between the regions corresponding&#160;to different modes (categories, in the experiments). Training an RBM in that&#160;higher-level space allowed Gibbs sampling to mix faster between modes. It remains&#160;however unclear how to exploit this observation to help better train and sample&#160;from deep generative models.</span></p>
<p><span class="font64" style="font-weight:bold;">Despite the difficulty of mixing, Monte Carlo techniques are useful and are often the best tool available. Indeed, they are the primary tool used to confront&#160;the intractable partition function of undirected models, discussed next.</span></p>
<p><a id="bookmark4"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font64" style="font-weight:bold;"><sup></sup> The unbiased estimator of the variance is often preferred, in which the sum of squared differences is divided by </span><span class="font64" style="font-weight:bold;font-style:italic;">n —</span><span class="font64" style="font-weight:bold;"> 1 instead of n.</span></p>
</body>
</html>