<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 4</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Numerical Computation</span></h2>
<p><span class="font64">Machine learning algorithms usually require a high amount of numerical computation. This typically refers to algorithms that solve mathematical problems by methods that update estimates of the solution via an iterative process, rather than&#160;analytically deriving a formula providing a symbolic expression for the correct solution. Common operations include optimization (finding the value of an argument&#160;that minimizes or maximizes a function) and solving systems of linear equations.&#160;Even just evaluating a mathematical function on a digital computer can be difficult&#160;when the function involves real numbers, which cannot be represented precisely&#160;using a finite amount of memory.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">4.1 Overflow and Underflow</span></h4>
<p><span class="font64">The fundamental difficulty in performing continuous math on a digital computer is that we need to represent infinitely many real numbers with a finite number&#160;of bit patterns. This means that for almost all real numbers, we incur some&#160;approximation error when we represent the number in the computer. In many&#160;cases, this is just rounding error. Rounding error is problematic, especially when&#160;it compounds across many operations, and can cause algorithms that work in&#160;theory to fail in practice if they are not designed to minimize the accumulation of&#160;rounding error.</span></p>
<p><span class="font64">One form of rounding error that is particularly devastating is </span><span class="font64" style="font-weight:bold;font-style:italic;">underflow.</span><span class="font64"> Underflow occurs when numbers near zero are rounded to zero. Many functions behave qualitatively differently when their argument is zero rather than a small positive&#160;number. For example, we usually want to avoid division by zero (some software&#160;environments will raise exceptions when this occurs, others will return a result&#160;with a placeholder not-a-number value) or taking the logarithm of zero (this is&#160;usually treated as —to, which then becomes not-a-number if it is used for many&#160;further arithmetic operations).</span></p>
<p><span class="font64">Another highly damaging form of numerical error is </span><span class="font64" style="font-weight:bold;font-style:italic;">overflow.</span><span class="font64"> Overflow occurs when numbers with large magnitude are approximated as to or —to. Further&#160;arithmetic will usually change these infinite values into not-a-number values.</span></p>
<p><span class="font64">One example of a function that must be stabilized against underflow and overflow is the softmax function. The softmax function is often used to predict the&#160;probabilities associated with a multinoulli distribution. The softmax function is&#160;defined to be</span></p>
<p><span class="font64">softmax(®)! = &#160;&#160;&#160;</span><span class="font64" style="text-decoration:line-through;"><sub>n</sub><sup>exp(xi</sup>)</span><span class="font64">— .&#160;&#160;&#160;&#160;(4.1)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">j=1 <sup>ex</sup>P<sup>(</sup>X</span><span class="font64"><sup>)</sup></span></p>
<p><span class="font64">Consider what happens when all of the x! are equal to some constant c. Analytically, we can see that all of the outputs should be equal to n. Numerically, this may&#160;not occur when c has large magnitude. If c is very negative, then exp(c) will&#160;underflow. This means the denominator of the softmax will become 0, so the final&#160;result is undefined. When c is very large and positive, exp(c) will overflow, again&#160;resulting in the expression as a whole being undefined. Both of these difficulties&#160;can be resolved by instead evaluating softmax(z) where z = </span><span class="font64" style="font-weight:bold;font-style:italic;">x -</span><span class="font64"> max! x!. Simple&#160;algebra shows that the value of the softmax function is not changed analytically by&#160;adding or subtracting a scalar from the input vector. Subtracting max* x! results&#160;in the largest argument to exp being 0, which rules out the possibility of overflow.&#160;Likewise, at least one term in the denominator has a value of 1, which rules out&#160;the possibility of underflow in the denominator leading to a division by zero.</span></p>
<p><span class="font64">There is still one small problem. Underflow in the numerator can still cause the expression as a whole to evaluate to zero. This means that if we implement&#160;log softmax(x) by first running the softmax subroutine then passing the result to&#160;the log function, we could erroneously obtain —to. Instead, we must implement&#160;a separate function that calculates log softmax in a numerically stable way. The&#160;log softmax function can be stabilized using the same trick as we used to stabilize&#160;the softmax function.</span></p>
<p><span class="font64">For the most part, we do not explicitly detail all of the numerical considerations involved in implementing the various algorithms described in this book. Developers&#160;of low-level libraries should keep numerical issues in mind when implementing&#160;deep learning algorithms. Most readers of this book can simply rely on low-level libraries that provide stable implementations. In some cases, it is possible&#160;to implement a new algorithm and have the new implementation automatically</span></p>
<p><span class="font64">stabilized. Theano (Bergstra </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010; Bastien </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012) is an example of a software package that automatically detects and stabilizes many common&#160;numerically unstable expressions that arise in the context of deep learning.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">4.2 Poor Conditioning</span></h4>
<p><span class="font64">Conditioning refers to how rapidly a function changes with respect to small changes in its inputs. Functions that change rapidly when their inputs are perturbed slightly&#160;can be problematic for scientific computation because rounding errors in the inputs&#160;can result in large changes in the output.</span></p>
<p><span class="font64">Consider the function f (x) </span><span class="font64" style="font-weight:bold;font-style:italic;">= A<sup>-1</sup>x.</span><span class="font64"> When A </span><span class="font64" style="font-weight:bold;font-style:italic;">E</span><span class="font64"> R</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>nxn</sup></span><span class="font64"> has an eigenvalue decomposition, its </span><span class="font64" style="font-weight:bold;font-style:italic;">condition number</span><span class="font64"> is</span></p><div>
<p><span class="font64">max</span></p></div><div>
<p><span class="font64">A,</span></p>
<p><span class="font64">A״</span></p></div><div>
<p><span class="font64">(4.2)</span></p></div>
<p><span class="font64">This is the ratio of the magnitude of the largest and smallest eigenvalue. When this number is large, matrix inversion is particularly sensitive to error in the input.</span></p>
<p><span class="font64">This sensitivity is an intrinsic property of the matrix itself, not the result of rounding error during matrix inversion. Poorly conditioned matrices amplify&#160;pre-existing errors when we multiply by the true matrix inverse. In practice, the&#160;error will be compounded further by numerical errors in the inversion process itself.</span></p><h4><a id="bookmark3"></a><span class="font65" style="font-weight:bold;">4.3 Gradient-Based Optimization</span></h4>
<p><span class="font64">Most deep learning algorithms involve optimization of some sort. Optimization refers to the task of either minimizing or maximizing some function f (x) by altering&#160;x. We usually phrase most optimization problems in terms of minimizing f (x).&#160;Maximization may be accomplished via a minimization algorithm by minimizing</span></p>
<p><span class="font64"><sup>-f (x)</sup>.</span></p>
<p><span class="font64">The function we want to minimize or maximize is called the </span><span class="font64" style="font-weight:bold;font-style:italic;">objective function </span><span class="font64">or </span><span class="font64" style="font-weight:bold;font-style:italic;">criterion.</span><span class="font64"> When we are minimizing it, we may also call it the </span><span class="font64" style="font-weight:bold;font-style:italic;">cost function,&#160;loss function,</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">error function.</span><span class="font64"> In this book, we use these terms interchangeably,&#160;though some machine learning publications assign special meaning to some of these&#160;terms.</span></p>
<p><span class="font64">We often denote the value that minimizes or maximizes a function with a superscript *. For example, we might say x* = argminf (x).</span></p><div>
<p><span class="font64">Gradient descent</span></p><img src="main-29.jpg" alt=""/>
<p><span class="font64">Figure 4.1: An illustration of how the derivatives of a function can be used to follow the function downhill to a minimum. This technique is called </span><span class="font64" style="font-style:italic;">gradient descent.</span></p></div>
<p><span class="font64">We assume the reader is already familiar with calculus, but provide a brief review of how calculus concepts relate to optimization here.</span></p>
<p><span class="font64">Suppose we have a function y = f (x), where both </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> and y are real numbers. The </span><span class="font64" style="font-weight:bold;font-style:italic;">derivative</span><span class="font64"> of this function is denoted as f'(x) or as . The derivative f</span><span class="font64" style="font-weight:bold;font-style:italic;">'</span><span class="font64">(x)&#160;gives the slope of f (x) at the point x. In other words, it specifies how to scale&#160;a small change in the input in order to obtain the corresponding change in the&#160;output: f (x + e) ~ f (x) + ef '(<sup>x)</sup>.</span></p>
<p><span class="font64">The derivative is therefore useful for minimizing a function because it tells us how to change x in order to make a small improvement in y. For example, we&#160;know that f (x — e sign </span><span class="font64" style="font-weight:bold;font-style:italic;">(f (x)))</span><span class="font64"> is less than f (x) for small enough e. We can thus&#160;reduce f (x) by moving x in small steps with opposite sign of the derivative. This&#160;technique is called </span><span class="font64" style="font-weight:bold;font-style:italic;">gradient descent</span><span class="font64"> (Cauchy, 1847). See Fig. 4.1 for an example of&#160;this technique.</span></p>
<p><span class="font64">When f</span><span class="font64" style="font-weight:bold;font-style:italic;">'</span><span class="font64"> (x) = 0, the derivative provides no information about which direction to move. Points where f</span><span class="font64" style="font-weight:bold;font-style:italic;">'</span><span class="font64">(x) = 0 are known as </span><span class="font64" style="font-weight:bold;font-style:italic;">critical points</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">stationary points.&#160;</span><span class="font64">A </span><span class="font64" style="font-weight:bold;font-style:italic;">local minimum</span><span class="font64"> is a point where f (x) is lower than at all neighboring points,&#160;so it is no longer possible to decrease f (x) by making infinitesimal steps. A </span><span class="font64" style="font-weight:bold;font-style:italic;">local&#160;maximum</span><span class="font64"> is a point where f (x) is higher than at all neighboring points, so it is</span></p>
<p><span class="font64">Types of critical points</span></p><div><div>
<p><span class="font64">Minimum</span></p><img src="main-30.jpg" alt=""/></div></div>
<p><span class="font64">Maximum &#160;&#160;&#160;Saddle point</span></p><div>
<p><span class="font40">/A</span></p></div><div><div><img src="main-31.jpg" alt=""/></div></div>
<p><span class="font64">Figure 4.2: Examples of each of the three types of critical points in 1-D. A critical point is a point with zero slope. Such a point can either be a local minimum, which is lower than&#160;the neighboring points, a local maximum, which is higher than the neighboring points, or&#160;a saddle point, which has neighbors that are both higher and lower than the point itself.</span></p>
<p><span class="font64">not possible to increase f(x) by making infinitesimal steps. Some critical points are neither maxima nor minima. These are known as </span><span class="font64" style="font-weight:bold;font-style:italic;">saddle points.</span><span class="font64"> See Fig. 4.2&#160;for examples of each type of critical point.</span></p>
<p><span class="font64">A point that obtains the absolute lowest value of f (x) is a </span><span class="font64" style="font-weight:bold;font-style:italic;">global minimum.</span><span class="font64"> It is possible for there to be only one global minimum or multiple global minima of&#160;the function. It is also possible for there to be local minima that are not globally&#160;optimal. In the context of deep learning, we optimize functions that may have&#160;many local minima that are not optimal, and many saddle points surrounded by&#160;very flat regions. All of this makes optimization very difficult, especially when the&#160;input to the function is multidimensional. We therefore usually settle for finding a&#160;value of f that is very low, but not necessarily minimal in any formal sense. See&#160;Fig. 4.3 for an example.</span></p>
<p><span class="font64">We often minimize functions that have multiple inputs: f : R</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>n</sup> ^</span><span class="font64"> R. For the concept of “minimization” to make sense, there must still be only one (scalar)&#160;output.</span></p>
<p><span class="font64">For functions with multiple inputs, we must make use of the concept of </span><span class="font64" style="font-weight:bold;font-style:italic;">partial derivatives.</span><span class="font64"> The partial derivative&#160;&#160;&#160;&#160;f (x) measures how f changes as only the</span></p>
<p><span class="font64">variable x increases at point x. The </span><span class="font64" style="font-weight:bold;font-style:italic;">gradient</span><span class="font64"> generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of f is the&#160;vector containing all of the partial derivatives, denoted </span><span class="font64" style="font-weight:bold;font-style:italic;">V<sub>x</sub>f</span><span class="font64"> (x). Element </span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> of the&#160;gradient is the partial derivative of f with respect to Xi In multiple dimensions,</span></p><div><div>
<p><span class="font64">Approximate minimization</span></p>
<p><span class="font63">This local minimum performs nearly as well as&#160;the global one,&#160;so it is an acceptable&#160;halting point.</span></p>
<p><span class="font63">Ideally, we would like to arrive at the global&#160;minimum, but this&#160;might not be possible.</span></p><img src="main-32.jpg" alt=""/>
<p><span class="font63">This local minimum performs poorly, and should be avoided.</span></p>
<p><span class="font64">Figure 4.3: Optimization algorithms may fail to find a global minimum when there are multiple local minima or plateaus present. In the context of deep learning, we generally&#160;accept such solutions even though they are not truly minimal, so long as they correspond&#160;to significantly low values of the cost function.</span></p>
<p><span class="font39">X</span></p></div></div>
<p><span class="font64">critical points are points where every element of the gradient is equal to zero.</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">directional derivative</span><span class="font64"> in direction u (a unit vector) is the slope of the function f in direction </span><span class="font64" style="font-weight:bold;font-style:italic;">u.</span><span class="font64"> In other words, the directional derivative is the derivative&#160;of the function f (x + au) with respect to a, evaluated at a </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> 0. Using the chain&#160;rule, we can see that d־ f (x + au) = u<sup>T</sup> V<sub>x</sub>f (x).</span></p>
<p><span class="font64">To minimize f, we would like to find the direction in which f decreases the fastest. We can do this using the directional derivative:</span></p>
<p><span class="font64">min u<sup>T</sup>V </span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>x</sub>f</span><span class="font64"> (x) &#160;&#160;&#160;(4.3)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">u,u <sup>T</sup>u=1</span></p>
<p><span class="font64">= min ||u||</span><span class="font18">2</span><span class="font64">||V</span><span class="font64" style="font-weight:bold;font-style:italic;">x f</span><span class="font64"> (x)112 cos6 &#160;&#160;&#160;(4.4)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">u,u<sup>T</sup> u=1</span></p>
<p><span class="font64">where 6 is the angle between u and the gradient. Substituting in ||u</span><span class="font18">||2</span><span class="font64"> = 1 and ignoring factors that do not depend on u, this simplifies to min<sub>u</sub> cos 6. This is&#160;minimized when u points in the opposite direction as the gradient. In other&#160;words, the gradient points directly uphill, and the negative gradient points directly&#160;downhill. We can decrease f by moving in the direction of the negative gradient.&#160;This is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">method of steepest descent</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">gradient descent.</span></p>
<p><span class="font64">Steepest descent proposes a new point</span></p>
<p><span class="font64">x' = x — eVx f (x) &#160;&#160;&#160;(4.5)</span></p>
<p><span class="font64">where e is the </span><span class="font64" style="font-weight:bold;font-style:italic;">learning rate</span><span class="font64">, a positive scalar determining the size of the step. We can choose e in several different ways. A popular approach is to set e to a small&#160;constant. Sometimes, we can solve for the step size that makes the directional&#160;derivative vanish. Another approach is to evaluate f (x — eV<sub>x</sub>f (x)) for several&#160;values of e and choose the one that results in the smallest objective function value.&#160;This last strategy is called a </span><span class="font64" style="font-weight:bold;font-style:italic;">line search.</span></p>
<p><span class="font64">Steepest descent converges when every element of the gradient is zero (or, in practice, very close to zero). In some cases, we may be able to avoid running this&#160;iterative algorithm, and just jump directly to the critical point by solving the&#160;equation </span><span class="font64" style="font-weight:bold;font-style:italic;">V<sub>x</sub>f</span><span class="font64"> (x) = 0 for x.</span></p>
<p><span class="font64">Although gradient descent is limited to optimization in continuous spaces, the general concept of making small moves (that are approximately the best small move)&#160;towards better configurations can be generalized to discrete spaces. Ascending an&#160;objective function of discrete parameters is called </span><span class="font64" style="font-weight:bold;font-style:italic;">hill climbing</span><span class="font64"> (Russel and Norvig,&#160;2003).</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices</span></h5>
<p><span class="font64">Sometimes we need to find all of the partial derivatives of a function whose input and output are both vectors. The matrix containing all such partial derivatives is&#160;known as a </span><span class="font64" style="font-weight:bold;font-style:italic;">Jacobian matrix.</span><span class="font64"> Specifically, if we have a function f : R<sup>m</sup> ^ R<sup>n</sup>, then&#160;the Jacobian matrix J G R<sup>nxm</sup> of f is defined such that Jij = </span><span class="font64" style="font-weight:bold;font-style:italic;">JX. f (x)%.</span></p>
<p><span class="font64">We are also sometimes interested in a derivative of a derivative. This is known as a </span><span class="font64" style="font-weight:bold;font-style:italic;">second derivative.</span><span class="font64"> For example, for a function f : R<sup>n</sup> ^ R, the derivative&#160;with respect to x of the derivative of f with respect to </span><span class="font64" style="font-weight:bold;font-style:italic;">xj</span><span class="font64"> is denoted as</span></p>
<p><span class="font64">In a single dimension, we can denote ^־</span><span class="font18">2</span><span class="font64">־ f by f </span><span class="font64" style="font-weight:bold;font-style:italic;">&quot;(x</span><span class="font64">). The second derivative tells us how the first derivative will change as we vary the input. This is important&#160;because it tells us whether a gradient step will cause as much of an improvement&#160;as we would expect based on the gradient alone. We can think of the second&#160;derivative as measuring </span><span class="font64" style="font-weight:bold;font-style:italic;">curvature.</span><span class="font64"> Suppose we have a quadratic function (many&#160;functions that arise in practice are not quadratic but can be approximated well&#160;as quadratic, at least locally). If such a function has a second derivative of zero,&#160;then there is no curvature. It is a perfectly flat line, and its value can be predicted&#160;using only the gradient. If the gradient is 1, then we can make a step of size e&#160;along the negative gradient, and the cost function will decrease by e. If the second&#160;derivative is negative, the function curves downward, so the cost function will&#160;actually decrease by more than e. Finally, if the second derivative is positive, the&#160;function curves upward, so the cost function can decrease by less than e. See Fig.</span></p><div><div><img src="main-33.jpg" alt=""/></div></div><div><div>
<p><span class="font64">Negative curvature No curvature Positive curvature</span></p><img src="main-34.jpg" alt=""/></div></div><div><div><img src="main-35.jpg" alt=""/></div></div>
<p><span class="font64">Figure 4.4: The second derivative determines the curvature of a function. Here we show quadratic functions with various curvature. The dashed line indicates the value of the cost&#160;function we would expect based on the gradient information alone as we make a gradient&#160;step downhill. In the case of negative curvature, the cost function actually decreases&#160;faster than the gradient predicts. In the case of no curvature, the gradient predicts the&#160;decrease correctly. In the case of positive curvature, the function decreases slower than&#160;expected and eventually begins to increase, so too large of step sizes can actually increase&#160;the function inadvertently.</span></p>
<p><span class="font64">4.4 to see how different forms of curvature affect the relationship between the value&#160;of the cost function predicted by the gradient and the true value.</span></p>
<p><span class="font64">When our function has multiple input dimensions, there are many second derivatives. These derivatives can be collected together into a matrix called the&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">Hessian matrix.</span><span class="font64"> The Hessian matrix H(f )(x) is defined such that</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"><sup>2</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dxi dxj</span></p></div><div>
<p><span class="font64"><sup>f (x)</sup>•</span></p></div><div>
<p><span class="font64">(4.6)</span></p></div><div>
<p><span class="font64">Equivalently, the Hessian is the Jacobian of the gradient.</span></p>
<p><span class="font64">Anywhere that the second partial derivatives are continuous, the differential operators are commutative, i.e. their order can be swapped:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"><sup>2</sup> &#160;&#160;&#160;<sub>N</sub>&#160;&#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">d<sup>2</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dx <sub>i</sub>dx</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>f</sup></span><span class="font64"><sup> (x)</sup> =</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dxjdx</span></p></div><div>
<p><span class="font64"><sup>f (x)</sup>•</span></p></div><div>
<p><span class="font64">(4.7)</span></p></div>
<p><span class="font64">This implies that </span><span class="font64" style="font-weight:bold;font-style:italic;">H<sub>i</sub>,j = Hj,<sub>i</sub>,</span><span class="font64"> so the Hessian matrix is symmetric at such points. Most of the functions we encounter in the context of deep learning have a symmetric&#160;Hessian almost everywhere. Because the Hessian matrix is real and symmetric,&#160;we can decompose it into a set of real eigenvalues and an orthogonal basis of&#160;eigenvectors. The second derivative in a specific direction represented by a unit&#160;vector d is given by d</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>T</sup>Hd.</span><span class="font64"> When d is an eigenvector of H, the second derivative&#160;in that direction is given by the corresponding eigenvalue. For other directions of&#160;d, the directional second derivative is a weighted average of all of the eigenvalues,&#160;with weights between 0 and 1, and eigenvectors that have smaller angle with d&#160;receiving more weight. The maximum eigenvalue determines the maximum second&#160;derivative and the minimum eigenvalue determines the minimum second derivative.</span></p>
<p><span class="font64">The (directional) second derivative tells us how well we can expect a gradient descent step to perform. We can make a second-order Taylor series approximation&#160;to the function f (x) around the current point x<sup>(0)</sup>:</span></p>
<p><span class="font64">1</span></p>
<p><span class="font64">f (x) « f (x<sup>(0)</sup>) + (x - x<sup>(0)</sup>)<sup>T</sup>g + <sub>2</sub>(x - x<sup>(0)</sup>)<sup>T</sup>H(x - x<sup>(0)</sup>). &#160;&#160;&#160;(4.8)</span></p>
<p><span class="font64">2</span></p>
<p><span class="font64">where g is the gradient and H is the Hessian at x<sup>(0)</sup>. If we use a learning rate of e, then the new point x will be given by x<sup>(0)</sup> — eg. Substituting this into our&#160;approximation, we obtain</span></p>
<p><span class="font64"><sub>1</sub></span></p><div>
<p><span class="font64">(4.9)</span></p></div>
<p><span class="font64"><sup>f (x(0)</sup> - eg) « f (x<sup>(0)</sup>) - eg<sup>T</sup>g + <sub>2</sub>e<sup>2</sup>g<sup>TH</sup>g.</span></p>
<p><span class="font64">There are three terms here: the original value of the function, the expected improvement due to the slope of the function, and the correction we must apply&#160;to account for the curvature of the function. When this last term is too large, the&#160;gradient descent step can actually move uphill. When </span><span class="font64" style="font-weight:bold;font-style:italic;">g<sup>T</sup>Hg</span><span class="font64"> is zero or negative,&#160;the Taylor series approximation predicts that increasing e forever will decrease f&#160;forever. In practice, the Taylor series is unlikely to remain accurate for large e, so&#160;one must resort to more heuristic choices of e in this case. When </span><span class="font64" style="font-weight:bold;font-style:italic;">g<sup>T</sup>Hg</span><span class="font64"> is positive,&#160;solving for the optimal step size that decreases the Taylor series approximation of&#160;the function the most yields</span></p><div>
<p><span class="font64">g g</span></p></div><div>
<p><span class="font64">(4.10)</span></p></div>
<p><span class="font64">g<sup>T H</sup>g</span></p>
<p><span class="font64">In the worst case, when g aligns with the eigenvector of H corresponding to the maximal eigenvalue A<sub>max</sub>, then this optimal step size is given by ^ . To the&#160;extent that the function we minimize can be approximated well by a quadratic&#160;function, the eigenvalues of the Hessian thus determine the scale of the learning&#160;rate.</span></p>
<p><span class="font64">The second derivative can be used to determine whether a critical point is a local maximum, a local minimum, or saddle point. Recall that on a critical point,&#160;f </span><span class="font64" style="font-weight:bold;font-style:italic;">'(x)</span><span class="font64"> = 0. When f </span><span class="font64" style="font-weight:bold;font-style:italic;">&quot;(x) &gt;</span><span class="font64"> 0, this means that f </span><span class="font64" style="font-weight:bold;font-style:italic;">'(x)</span><span class="font64"> increases as we move to the&#160;right, and f</span><span class="font64" style="font-weight:bold;font-style:italic;">'</span><span class="font64"> (x) decreases as we move to the left. This means </span><span class="font64" style="font-weight:bold;font-style:italic;">f</span><span class="font64"> (x - e) &lt; 0 and&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">f '(x + e)</span><span class="font64"> &gt; 0 for small enough e. In other words, as we move right, the slope begins&#160;to point uphill to the right, and as we move left, the slope begins to point uphill&#160;to the left. Thus, when f' (x) = 0 and f'' (x) &gt; 0, we can conclude that x is a local&#160;minimum. Similarly, when f '(x) = 0 and f יי (x) &lt; 0, we can conclude that x is a&#160;local maximum. This is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">second derivative test.</span><span class="font64"> Unfortunately, when&#160;f''(x) = 0, the test is inconclusive. In this case x may be a saddle point, or a part&#160;of a flat region.</span></p>
<p><span class="font64">In multiple dimensions, we need to examine all of the second derivatives of the function. Using the eigendecomposition of the Hessian matrix, we can generalize&#160;the second derivative test to multiple dimensions. At a critical point, where&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">V<sub>x</sub> f(x) =</span><span class="font64"> 0, we can examine the eigenvalues of the Hessian to determine whether&#160;the critical point is a local maximum, local minimum, or saddle point. When the&#160;Hessian is positive definite (all its eigenvalues are positive), the point is a local&#160;minimum. This can be seen by observing that the directional second derivative&#160;in any direction must be positive, and making reference to the univariate second&#160;derivative test. Likewise, when the Hessian is negative definite (all its eigenvalues&#160;are negative), the point is a local maximum. In multiple dimensions, it is actually&#160;possible to find positive evidence of saddle points in some cases. When at least&#160;one eigenvalue is positive and at least one eigenvalue is negative, we know that&#160;x is a local maximum on one cross section of f but a local minimum on another&#160;cross section. See Fig. 4.5 for an example. Finally, the multidimensional second&#160;derivative test can be inconclusive, just like the univariate version. The test is&#160;inconclusive whenever all of the non-zero eigenvalues have the same sign, but at&#160;least one eigenvalue is zero. This is because the univariate second derivative test is&#160;inconclusive in the cross section corresponding to the zero eigenvalue.</span></p>
<p><span class="font64">In multiple dimensions, there can be a wide variety of different second derivatives at a single point, because there is a different second derivative for each direction.&#160;The condition number of the Hessian measures how much the second derivatives&#160;vary. When the Hessian has a poor condition number, gradient descent performs&#160;poorly. This is because in one direction, the derivative increases rapidly, while in&#160;another direction, it increases slowly. Gradient descent is unaware of this change&#160;in the derivative so it does not know that it needs to explore preferentially in&#160;the direction where the derivative remains negative for longer. It also makes it&#160;difficult to choose a good step size. The step size must be small enough to avoid&#160;overshooting the minimum and going uphill in directions with strong positive&#160;curvature. This usually means that the step size is too small to make significant&#160;progress in other directions with less curvature. See Fig. 4.6 for an example.</span></p>
<p><span class="font64">This issue can be resolved by using information from the Hessian matrix to</span></p><div><img src="main-36.jpg" alt=""/>
<p><span class="font64">Figure 4.5: A saddle point containing both positive and negative curvature. The function in this example is f (</span><span class="font64" style="font-style:italic;">x) = xf</span><span class="font64"> — xf. Along the axis corresponding to </span><span class="font64" style="font-style:italic;">x<sub>1:</sub></span><span class="font64"> the function&#160;curves upward. This axis is an eigenvector of the Hessian and has a positive eigenvalue.&#160;Along the axis corresponding to </span><span class="font64" style="font-style:italic;">x</span><span class="font64"> <sub>2</sub>, the function curves downward. This direction is an&#160;eigenvector of the Hessian with negative eigenvalue. The name “saddle point” derives from&#160;the saddle-like shape of this function. This is the quintessential example of a function&#160;with a saddle point. In more than one dimension, it is not necessary to have an eigenvalue&#160;of 0 in order to get a saddle point: it is only necessary to have both positive and negative&#160;eigenvalues. We can think of a saddle point with both signs of eigenvalues as being a local&#160;maximum within one cross section and a local minimum within another cross section.</span></p></div><div><div><img src="main-37.jpg" alt=""/>
<p><span class="font64">Figure 4.6: Gradient descent fails to exploit the curvature information contained in the Hessian matrix. Here we use gradient descent to minimize a quadratic function </span><span class="font64" style="font-style:italic;">f(</span><span class="font64"> x) whose&#160;Hessian matrix has condition number 5. This means that the direction of most curvature&#160;has five times more curvature than the direction of least curvature. In this case, the most&#160;curvature is in the direction [1,1]<sup>T</sup> and the least curvature is in the direction [1 — 1]<sup>T</sup>. The&#160;red lines indicate the path followed by gradient descent. This very elongated quadratic&#160;function resembles a long canyon. Gradient descent wastes time repeatedly descending&#160;canyon walls, because they are the steepest feature. Because the step size is somewhat&#160;too large, it has a tendency to overshoot the bottom of the function and thus needs to&#160;descend the opposite canyon wall on the next iteration. The large positive eigenvalue&#160;of the Hessian corresponding to the eigenvector pointed in this direction indicates that&#160;this directional derivative is rapidly increasing, so an optimization algorithm based on&#160;the Hessian could predict that the steepest direction is not actually a promising search&#160;direction in this context.</span></p></div></div>
<p><span class="font64">guide the search. The simplest method for doing so is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">Newton’s method. </span><span class="font64">Newton’s method is based on using a second-order Taylor series expansion to&#160;approximate f (x) near some point x<sup>(0)</sup>:</span></p>
<p><span class="font64">1</span></p>
<p><span class="font64">f (x) « f (x<sup>(0)</sup> ) + (x-x<sup>(0)</sup> )<sup>T</sup>V</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>x</sub>f</span><span class="font64"> (x<sup>(0)</sup>)+- (x-x<sup>(0)</sup>)<sup>T</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">H(f</span><span class="font64"> )(x<sup>(0)</sup>)(x—x<sup>(0)</sup>). (4.11)</span></p>
<p><span class="font64">—</span></p>
<p><span class="font64">If we then solve for the critical point of this function, we obtain:</span></p>
<p><span class="font64">x* = x<sup>(0)</sup> — H (f )(x<sup>(0)</sup> )<sup>-1</sup>V<sub>x</sub> f (x<sup>(0)</sup>). &#160;&#160;&#160;(4.12)</span></p>
<p><span class="font64">When f is a positive definite quadratic function, Newton’s method consists of applying Eq. 4.12 once to jump to the minimum of the function directly. When f is&#160;not truly quadratic but can be locally approximated as a positive definite quadratic,&#160;Newton’s method consists of applying Eq. 4.12 multiple times. Iteratively updating&#160;the approximation and jumping to the minimum of the approximation can reach&#160;the critical point much faster than gradient descent would. This is a useful property&#160;near a local minimum, but it can be a harmful property near a saddle point. As&#160;discussed in Sec. 8.2.3, Newton’s method is only appropriate when the nearby&#160;critical point is a minimum (all the eigenvalues of the Hessian are positive), whereas&#160;gradient descent is not attracted to saddle points unless the gradient points toward&#160;them.</span></p>
<p><span class="font64">Optimization algorithms such as gradient descent that use only the gradient are called </span><span class="font64" style="font-weight:bold;font-style:italic;">first-order optimization algorithms.</span><span class="font64"> Optimization algorithms such as Newton’s method that also use the Hessian matrix are called </span><span class="font64" style="font-weight:bold;font-style:italic;">second-order optimization&#160;algorithms</span><span class="font64"> (Nocedal and Wright, 2006).</span></p>
<p><span class="font64">The optimization algorithms employed in most contexts in this book are applicable to a wide variety of functions, but come with almost no guarantees. This&#160;is because the family of functions used in deep learning is quite complicated. In&#160;many other fields, the dominant approach to optimization is to design optimization&#160;algorithms for a limited family of functions.</span></p>
<p><span class="font64">In the context of deep learning, we sometimes gain some guarantees by restricting ourselves to functions that are either </span><span class="font64" style="font-weight:bold;font-style:italic;">Lipschitz continuous</span><span class="font64"> or have Lipschitz continuous derivatives. A Lipschitz continuous function is a function f whose rate&#160;of change is bounded by a </span><span class="font64" style="font-weight:bold;font-style:italic;">Lipschitz consta,nt L:</span></p>
<p><span class="font64">Vx, Vy, |f(x) — f(y)| &lt; L||x — </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64">||</span><span class="font18">2</span><span class="font64">. &#160;&#160;&#160;(4.13)</span></p>
<p><span class="font64">This property is useful because it allows us to quantify our assumption that a small change in the input made by an algorithm such as gradient descent will have&#160;a small change in the output. Lipschitz continuity is also a fairly weak constraint,&#160;and many optimization problems in deep learning can be made Lipschitz continuous&#160;with relatively minor modifications.</span></p>
<p><span class="font64">Perhaps the most successful field of specialized optimization is </span><span class="font64" style="font-weight:bold;font-style:italic;">convex optimization.</span><span class="font64"> Convex optimization algorithms are able to provide many more guarantees by making stronger restrictions. Convex optimization algorithms are applicable&#160;only to convex functions—functions for which the Hessian is positive semidefinite&#160;everywhere. Such functions are well-behaved because they lack saddle points and&#160;all of their local minima are necessarily global minima. However, most problems&#160;in deep learning are difficult to express in terms of convex optimization. Convex&#160;optimization is used only as a subroutine of some deep learning algorithms. Ideas&#160;from the analysis of convex optimization algorithms can be useful for proving the&#160;convergence of deep learning algorithms. However, in general, the importance of&#160;convex optimization is greatly diminished in the context of deep learning. For&#160;more information about convex optimization, see Boyd and Vandenberghe (2004)&#160;or Rockafellar (1997).</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">4.4 Constrained Optimization</span></h4>
<p><span class="font64">Sometimes we wish not only to maximize or minimize a function f (x) over all possible values of x. Instead we may wish to find the maximal or minimal value of&#160;f (x) for values of x in some set S. This is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">constrained optimization.</span><span class="font64"> Points&#160;x that lie within the set S are called </span><span class="font64" style="font-weight:bold;font-style:italic;">feasible</span><span class="font64"> points in constrained optimization&#160;terminology.</span></p>
<p><span class="font64">We often wish to find a solution that is small in some sense. A common approach in such situations is to impose a norm constraint, such as ||x|| &lt; 1.</span></p>
<p><span class="font64">One simple approach to constrained optimization is simply to modify gradient descent taking the constraint into account. If we use a small constant step size e,&#160;we can make gradient descent steps, then project the result back into S. If we use&#160;a line search, we can search only over step sizes e that yield new x points that are&#160;feasible, or we can project each point on the line back into the constraint region.&#160;When possible, this method can be made more efficient by projecting the gradient&#160;into the tangent space of the feasible region before taking the step or beginning&#160;the line search (Rosen, 1960).</span></p>
<p><span class="font64">A more sophisticated approach is to design a different, unconstrained optimization problem whose solution can be converted into a solution to the original, constrained optimization problem. For example, if we want to minimize f( x) for&#160;x £ R<sup>2</sup> with x constrained to have exactly unit L<sup>2</sup> norm, we can instead minimize&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">g(9</span><span class="font64">) = f ([cos </span><span class="font64" style="font-weight:bold;font-style:italic;">9</span><span class="font64">, sin </span><span class="font64" style="font-weight:bold;font-style:italic;">9</span><span class="font64">]<sup>T</sup>) with respect to </span><span class="font64" style="font-weight:bold;font-style:italic;">9</span><span class="font64">, then return [cos </span><span class="font64" style="font-weight:bold;font-style:italic;">9</span><span class="font64">, sin </span><span class="font64" style="font-weight:bold;font-style:italic;">9]</span><span class="font64"> as the solution&#160;to the original problem. This approach requires creativity; the transformation&#160;between optimization problems must be designed specifically for each case we&#160;encounter.</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">Karush-Kuhn-Tucker</span><span class="font64"> (KKT) approach<a id="footnote1"></a><sup><a href="#bookmark6">1</a></sup><sup></sup> provides a very general solution to constrained optimization. With the KKT approach, we introduce a new function&#160;called the </span><span class="font64" style="font-weight:bold;font-style:italic;">generalized La,grangian</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">generalized Lagrange function.</span></p>
<p><span class="font64">To define the Lagrangian, we first need to describe S in terms of equations and inequalities. We want a description of S in terms of m functions g<sup>(i)</sup> and n&#160;functions h<sup>(j)</sup> so that S = {x | Vi,g<sup>(i)</sup>(x) = 0 and </span><span class="font64" style="font-weight:bold;font-style:italic;">Vj,</span><span class="font64"> h<sup>(j)</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">(x)</span><span class="font64"> &lt; 0}. The equations&#160;involving g<sup>(i)</sup> are called the </span><span class="font64" style="font-weight:bold;font-style:italic;">equality constraints</span><span class="font64"> and the inequalities involving h<sup>(j)&#160;</sup>are called </span><span class="font64" style="font-weight:bold;font-style:italic;">inequality constraints.</span></p>
<p><span class="font64">We introduce new variables A<sub>i</sub> and aj for each constraint, these are called the KKT multipliers. The generalized Lagrangian is then defined as</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">L(x,</span><span class="font64"> A, a) = f (x) + ^ Aig<sup>(i)</sup>(x) + ^ajh<sup>(j) (x)</sup>. &#160;&#160;&#160;(4T4)</span></p>
<p><span class="font64">ij</span></p>
<p><span class="font64">We can now solve a constrained minimization problem using unconstrained optimization of the generalized Lagrangian. Observe that, so long as at least one&#160;feasible point exists and f (x) is not permitted to have value to, then</span></p><div>
<p><span class="font64">(4.15)</span></p></div>
<p><span class="font64">min max max L(x, A, a). </span><span class="font12" style="font-style:italic;">x A a,a</span><span class="font11" style="font-weight:bold;"> 0</span></p>
<p><span class="font64">has the same optimal objective function value and set of optimal points x as</span></p><div>
<p><span class="font64">(4.16)</span></p></div><div>
<p><span class="font64">(4.17)</span></p></div><div>
<p><span class="font64">(4.18)</span></p></div>
<p><span class="font64">min f (x).</span></p>
<p><span class="font11" style="font-weight:bold;">xSS</span></p>
<p><span class="font64">This follows because any time the constraints are satisfied,</span></p>
<p><span class="font64">max max L(x, A, a) = f (x),</span></p>
<p><span class="font11" style="font-weight:bold;">A a,a&gt;0</span></p>
<p><span class="font64">while any time a constraint is violated,</span></p>
<p><span class="font64">max max L(x, A, a) = oo. </span><span class="font11" style="font-weight:bold;">A a,a 0</span></p>
<p><span class="font64">These properties guarantee that no infeasible point will ever be optimal, and that the optimum within the feasible points is unchanged.</span></p>
<p><span class="font64">To perform constrained maximization, we can construct the generalized Lagrange function of — f (x), which leads to this optimization problem:</span></p><div>
<p><span class="font64">min max max — f (x</span></p>
<p><span class="font11" style="font-weight:bold;">x A a,a&gt;0</span></p></div><div>
<p><span class="font64"><sup>f </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(x)</sup></span><span class="font64"> + &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">^<sup>ig</sup></span><span class="font64"><sup>(i)</sup>(x) +&#160;&#160;&#160;&#160;ah<sup>(j)</sup> (x).</span></p></div><div>
<p><span class="font64">(4.19)</span></p></div>
<p><span class="font64">We may also convert this to a problem with maximization in the outer loop:</span></p><div>
<p><span class="font64">maxmin min </span><span class="font12" style="font-style:italic;">x A a,a&gt;</span><span class="font11" style="font-weight:bold;"> 0</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>f</sup></span><span class="font64"><sup> (x)</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">+ ^2<sup>Xi9</sup></span><span class="font64"><sup> (i)</sup></span></p>
<p><span class="font64">i</span></p></div><div>
<p><span class="font64">(x) — &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">aj</span><span class="font64"> h<sup>(j)</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">(x).</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>j</sup></span></p></div><div>
<p><span class="font64">(4.20)</span></p></div>
<p><span class="font64">The sign of the term for the equality constraints does not matter; we may define it with addition or subtraction as we wish, because the optimization is free to choose&#160;any sign for each A<sub>i</sub>.</span></p>
<p><span class="font64">The inequality constraints are particularly interesting. We say that a constraint h<sup>(i)</sup> (x) is </span><span class="font64" style="font-weight:bold;font-style:italic;">active</span><span class="font64"> if h<sup>(i)</sup>(x*) = 0. If a constraint is not active, then the solution to&#160;the problem found using that constraint would remain at least a local solution if&#160;that constraint were removed. It is possible that an inactive constraint excludes&#160;other solutions. For example, a convex problem with an entire region of globally&#160;optimal points (a wide, flat, region of equal cost) could have a subset of this&#160;region eliminated by constraints, or a non-convex problem could have better local&#160;stationary points excluded by a constraint that is inactive at convergence. However,&#160;the point found at convergence remains a stationary point whether or not the&#160;inactive constraints are included. Because an inactive h<sup>(i)</sup> has negative value, then&#160;the solution to min<sub>x</sub> maxA max</span><span class="font18"><sub>a</sub>,<sub>a</sub>&gt;0</span><span class="font64"> L(x, A, a) will have a<sub>i</sub> = 0. We can thus&#160;observe that at the solution, ah(x) = 0 .In other words, for all i, we know that at&#160;least one of the constraints a<sub>i</sub> &gt; 0 and h<sup>(i)</sup>(x) &lt; 0 must be active at the solution.&#160;To gain some intuition for this idea, we can say that either the solution is on&#160;the boundary imposed by the inequality and we must use its KKT multiplier to&#160;influence the solution to x, or the inequality has no influence on the solution and&#160;we represent this by zeroing out its KKT multiplier.</span></p>
<p><span class="font64">The properties that the gradient of the generalized Lagrangian is zero, all constraints on both x and the KKT multipliers are satisfied, and a 0 h(x) = 0&#160;are called the Karush-Kuhn-Tucker (KKT) conditions (Karush, 1939; Kuhn and&#160;Tucker, 1951). Together, these properties describe the optimal points of constrained&#160;optimization problems.</span></p>
<p><span class="font64">For more information about the KKT approach, see Nocedal and Wright (2006).</span></p><h4><a id="bookmark7"></a><span class="font65" style="font-weight:bold;">4.5 Example: Linear Least Squares</span></h4>
<p><span class="font64">Suppose we want to find the value of x that minimizes</span></p>
<p><span class="font64">1</span></p>
<p><span class="font64"><sup>f</sup>(<sup>x</sup>) = 2 <sup>1|Ax - b||</sup></span><span class="font11" style="font-weight:bold;">2</span><span class="font64">• &#160;&#160;&#160;(<sup>4</sup>•<sup>21</sup>)</span></p>
<p><span class="font64">There are specialized linear algebra algorithms that can solve this problem efficiently. However, we can also explore how to solve it using gradient-based optimization as&#160;a simple example of how these techniques work.</span></p>
<p><span class="font64">First, we need to obtain the gradient:</span></p>
<p><span class="font64">V<sub>x</sub>f (x) = A<sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">(Ax — b) =</span><span class="font64"> A<sup>T</sup>Ax </span><span class="font64" style="font-weight:bold;font-style:italic;">— A b.</span><span class="font64"> &#160;&#160;&#160;(4.22)</span></p>
<p><span class="font64">We can then follow this gradient downhill, taking small steps. See Algorithm 4.1 for details.</span></p>
<p><span class="font64">Algorithm 4.1 An algorithm to minimize f (x) = </span><span class="font11" style="font-weight:bold;">1 </span><span class="font64">||Ax — b||</span><span class="font11" style="font-weight:bold;">2 </span><span class="font64">with respect to x using gradient descent.</span></p>
<p><span class="font64">Set the step size (e) and tolerance (5) to small, positive numbers. while ||A</span><span class="font64" style="font-variant:small-caps;"><sup>t</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">Ax —</span><span class="font64"> A<sup>T</sup>b||</span><span class="font11" style="font-weight:bold;"><sub>2</sub> </span><span class="font64" style="font-weight:bold;font-style:italic;">&gt;5</span><span class="font64"> do&#160;x ^ x — e (A<sup>T</sup>Ax — A</span><span class="font64" style="font-variant:small-caps;"><sup>t</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">bj&#160;</span><span class="font64">end while</span></p>
<p><span class="font64">One can also solve this problem using Newton’s method. In this case, because the true function is quadratic, the quadratic approximation employed by Newton’s&#160;method is exact, and the algorithm converges to the global minimum in a single&#160;step.</span></p>
<p><span class="font64">Now suppose we wish to minimize the same function, but subject to the constraint x<sup>T</sup>x &lt; 1. To do so, we introduce the Lagrangian</span></p>
<p><span class="font64">L(x, A) = f (x) + A ^x<sup>T</sup>x — 1^ • &#160;&#160;&#160;(4.23)</span></p>
<p><span class="font64">We can now solve the problem</span></p>
<p><span class="font64">min max L(x, A). &#160;&#160;&#160;(4.24)</span></p>
<p><span class="font11" style="font-weight:bold;">x A,A&gt;0</span></p>
<p><span class="font64">The smallest-norm solution to the unconstrained least squares problem may be found using the Moore-Penrose pseudoinverse: x = A</span><span class="font11" style="font-weight:bold;">+ </span><span class="font64">b. If this point is feasible,&#160;then it is the solution to the constrained problem. Otherwise, we must find a&#160;solution where the constraint is active. By differentiating the Lagrangian with&#160;respect to x, we obtain the equation</span></p>
<p><span class="font64">A<sup>T</sup>Ax </span><span class="font64" style="font-weight:bold;font-style:italic;">—</span><span class="font64"> Ab + 2Ax = 0. &#160;&#160;&#160;(4.25)</span></p>
<p><span class="font64">This tells us that the solution will take the form</span></p>
<p><span class="font64">x = (A<sup>t</sup>A + 2A1)<sup>-1</sup>A b. &#160;&#160;&#160;(4.26)</span></p>
<p><span class="font64">The magnitude of A must be chosen such that the result obeys the constraint. We can find this value by performing gradient ascent on A. To do so, observe</span></p>
<p><span class="font64">d</span></p>
<p><span class="font64">L(x, A) = x<sup>T</sup> x — 1. &#160;&#160;&#160;(4.27)</span></p>
<p><span class="font64">dA</span></p>
<p><span class="font64">When the norm of x exceeds 1, this derivative is positive, so to follow the derivative uphill and increase the Lagrangian with respect to A, we increase A. Because the&#160;coefficient on the x<sup>T</sup>x penalty has increased, solving the linear equation for x will&#160;now yield a solution with smaller norm. The process of solving the linear equation&#160;and adjusting A continues until x has the correct norm and the derivative on A is</span></p>
<p><span class="font64">0.</span></p>
<p><span class="font64">This concludes the mathematical preliminaries that we use to develop machine learning algorithms. We are now ready to build and analyze some full-fledged&#160;learning systems.</span></p>
<p><a id="bookmark6"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font64"><sup></sup>The KKT approach generalizes the method of </span><span class="font64" style="font-style:italic;">Lagrange multipliers</span><span class="font64"> which allows equality constraints but not inequality constraints.</span></p>
</body>
</html>