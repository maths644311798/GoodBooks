<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 6</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Deep Feedforward Networks</span></h2>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Deep feedforward networks</span><span class="font64">, also often called </span><span class="font64" style="font-weight:bold;font-style:italic;">feedforward neural networks</span><span class="font64">, or </span><span class="font64" style="font-weight:bold;font-style:italic;">multilayer perceptrons</span><span class="font64"> (</span><span class="font64" style="font-weight:bold;font-style:italic;">MLPs</span><span class="font64">), are the quintessential deep learning models. The goal of a feedforward network is to approximate some function /*. For example, for&#160;a classifier, y = /*(x) maps an input x to a category y. A feedforward network&#160;defines a mapping y = / (x; 6) and learns the value of the parameters 6 that result&#160;in the best function approximation.</span></p>
<p><span class="font64">These models are called </span><span class="font64" style="font-weight:bold;font-style:italic;">feedforward</span><span class="font64"> because information flows through the function being evaluated from x, through the intermediate computations used to&#160;define /, and finally to the output y. There are no </span><span class="font64" style="font-weight:bold;font-style:italic;">feedback</span><span class="font64"> connections in which&#160;outputs of the model are fed back into itself. When feedforward neural networks&#160;are extended to include feedback connections, they are called </span><span class="font64" style="font-weight:bold;font-style:italic;">recurrent neural&#160;networks,</span><span class="font64"> presented in Chapter 10.</span></p>
<p><span class="font64">Feedforward networks are of extreme importance to machine learning practitioners. They form the basis of many important commercial applications. For example, the convolutional networks used for object recognition from photos are a&#160;specialized kind of feedforward network. Feedforward networks are a conceptual&#160;stepping stone on the path to recurrent networks, which power many natural&#160;language applications.</span></p>
<p><span class="font64">Feedforward neural networks are called </span><span class="font64" style="font-weight:bold;font-style:italic;">networks</span><span class="font64"> because they are typically represented by composing together many different functions. The model is associated with a directed acyclic graph describing how the functions are composed together.&#160;For example, we might have three functions /<sup>(1)</sup>, /<sup>(2)</sup>, and /<sup>(3)</sup> connected in a&#160;chain, to form /(x) = /<sup>(3)</sup> (/<sup>(2)</sup> (/<sup>(1)</sup>(x))). These chain structures are the most&#160;commonly used structures of neural networks. In this case, /<sup>(1)</sup> is called the </span><span class="font64" style="font-weight:bold;font-style:italic;">first&#160;layer</span><span class="font64"> of the network, /<sup>(2)</sup> is called the </span><span class="font64" style="font-weight:bold;font-style:italic;">second layer,</span><span class="font64"> and so on. The overall length</span></p>
<p><span class="font64">of the chain gives the </span><span class="font64" style="font-weight:bold;font-style:italic;">depth</span><span class="font64"> of the model. It is from this terminology that the name “deep learning” arises. The final layer of a feedforward network is called the&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">output layer.</span><span class="font64"> During neural network training, we drive f(x) to match f *(x). The&#160;training data provides us with noisy, approximate examples of f *(x) evaluated at&#160;different training points. Each example x is accompanied by a label y&#160;&#160;&#160;&#160;f*(x).</span></p>
<p><span class="font64">The training examples specify directly what the output layer must do at each point x; it must produce a value that is close to y. The behavior of the other layers is&#160;not directly specified by the training data. The learning algorithm must decide&#160;how to use those layers to produce the desired output, but the training data does&#160;not say what each individual layer should do. Instead, the learning algorithm must&#160;decide how to use these layers to best implement an approximation of f *. Because&#160;the training data does not show the desired output for each of these layers, these&#160;layers are called </span><span class="font64" style="font-weight:bold;font-style:italic;">hidden layers.</span></p>
<p><span class="font64">Finally, these networks are called </span><span class="font64" style="font-weight:bold;font-style:italic;">neural</span><span class="font64"> because they are loosely inspired by neuroscience. Each hidden layer of the network is typically vector-valued. The&#160;dimensionality of these hidden layers determines the </span><span class="font64" style="font-weight:bold;font-style:italic;">width</span><span class="font64"> of the model. Each&#160;element of the vector may be interpreted as playing a role analogous to a neuron.&#160;Rather than thinking of the layer as representing a single vector-to-vector function,&#160;we can also think of the layer as consisting of many </span><span class="font64" style="font-weight:bold;font-style:italic;">units</span><span class="font64"> that act in parallel,&#160;each representing a vector-to-scalar function. Each unit resembles a neuron in&#160;the sense that it receives input from many other units and computes its own&#160;activation value. The idea of using many layers of vector-valued representation&#160;is drawn from neuroscience. The choice of the functions f(x) used to compute&#160;these representations is also loosely guided by neuroscientific observations about&#160;the functions that biological neurons compute. However, modern neural network&#160;research is guided by many mathematical and engineering disciplines, and the&#160;goal of neural networks is not to perfectly model the brain. It is best to think of&#160;feedforward networks as function approximation machines that are designed to&#160;achieve statistical generalization, occasionally drawing some insights from what we&#160;know about the brain, rather than as models of brain function.</span></p>
<p><span class="font64">One way to understand feedforward networks is to begin with linear models and consider how to overcome their limitations. Linear models, such as logistic&#160;regression and linear regression, are appealing because they may be fit efficiently&#160;and reliably, either in closed form or with convex optimization. Linear models also&#160;have the obvious defect that the model capacity is limited to linear functions, so&#160;the model cannot understand the interaction between any two input variables.</span></p>
<p><span class="font64">To extend linear models to represent nonlinear functions of x, we can apply the linear model not to x itself but to a transformed input ^(x), where ^ is a&#160;nonlinear transformation. Equivalently, we can apply the kernel trick described in&#160;Sec. 5.7.2, to obtain a nonlinear learning algorithm based on implicitly applying&#160;the ^ mapping. We can think of ^ as providing a set of features describing x, or&#160;as providing a new representation for x.</span></p>
<p><span class="font64">The question is then how to choose the mapping ^.</span></p>
<p><span class="font64">1. &#160;&#160;&#160;One option is to use a very generic ^&gt;, such as the infinite-dimensional ^ that&#160;is implicitly used by kernel machines based on the RBF kernel. If ^(x) is&#160;of high enough dimension, we can always have enough capacity to fit the&#160;training set, but generalization to the test set often remains poor. Very&#160;generic feature mappings are usually based only on the principle of local&#160;smoothness and do not encode enough prior information to solve advanced&#160;problems.</span></p>
<p><span class="font64">2. &#160;&#160;&#160;Another option is to manually engineer ^. Until the advent of deep learning,&#160;this was the dominant approach. This approach requires decades of human&#160;effort for each separate task, with practitioners specializing in different&#160;domains such as speech recognition or computer vision, and with little&#160;transfer between domains.</span></p>
<p><span class="font64">3. &#160;&#160;&#160;The strategy of deep learning is to learn </span><span class="font64" style="font-weight:bold;font-style:italic;">&lt;p.</span><span class="font64"> In this approach, we have a model&#160;y = f (x; 6, w) = 0(x; </span><span class="font64" style="font-weight:bold;font-style:italic;">0)<sup>T</sup>w</span><span class="font64">. We now have parameters 6 that we use to learn&#160;^ from a broad class of functions, and parameters w that map from ^&gt;( x) to&#160;the desired output. This is an example of a deep feedforward network, with&#160;^ defining a hidden layer. This approach is the only one of the three that&#160;gives up on the convexity of the training problem, but the benefits outweigh&#160;the harms. In this approach, we parametrize the representation as ^(x; 6)&#160;and use the optimization algorithm to find the 6 that corresponds to a good&#160;representation. If we wish, this approach can capture the benefit of the first&#160;approach by being highly generic—we do so by using a very broad family&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">&lt;p(x; 6</span><span class="font64">). This approach can also capture the benefit of the second approach.&#160;Human practitioners can encode their knowledge to help generalization by&#160;designing families ^(x; 6) that they expect will perform well. The advantage&#160;is that the human designer only needs to find the right general function&#160;family rather than finding precisely the right function.</span></p>
<p><span class="font64">This general principle of improving models by learning features extends beyond the feedforward networks described in this chapter. It is a recurring theme of deep&#160;learning that applies to all of the kinds of models described throughout this book.&#160;Feedforward networks are the application of this principle to learning deterministic&#160;mappings from x to y that lack feedback connections. Other models presented&#160;later will apply these principles to learning stochastic mappings, learning functions&#160;with feedback, and learning probability distributions over a single vector.</span></p>
<p><span class="font64">We begin this chapter with a simple example of a feedforward network. Next, we address each of the design decisions needed to deploy a feedforward network.&#160;First, training a feedforward network requires making many of the same design&#160;decisions as are necessary for a linear model: choosing the optimizer, the cost&#160;function, and the form of the output units. We review these basics of gradient-based&#160;learning, then proceed to confront some of the design decisions that are unique&#160;to feedforward networks. Feedforward networks have introduced the concept of a&#160;hidden layer, and this requires us to choose the </span><span class="font64" style="font-weight:bold;font-style:italic;">activation functions</span><span class="font64"> that will be&#160;used to compute the hidden layer values. We must also design the architecture of&#160;the network, including how many layers the network should contain, how these&#160;networks should be connected to each other, and how many units should be in&#160;each layer. Learning in deep neural networks requires computing the gradients of&#160;complicated functions. We present the </span><span class="font64" style="font-weight:bold;font-style:italic;">back-propagation</span><span class="font64"> algorithm and its modern&#160;generalizations, which can be used to efficiently compute these gradients. Finally,&#160;we close with some historical perspective.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">6.1 Example: Learning XOR</span></h4>
<p><span class="font64">To make the idea of a feedforward network more concrete, we begin with an example of a fully functioning feedforward network on a very simple task: learning&#160;the XOR function.</span></p>
<p><span class="font64">The XOR function (“exclusive or”) is an operation on two binary values, </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font62" style="font-style:italic;">1 </span><span class="font64">and x</span><span class="font18">2</span><span class="font64">. When exactly one of these binary values is equal to 1, the XOR function&#160;returns 1. Otherwise, it returns 0. The XOR function provides the target function&#160;y = f*(x) that we want to learn. Our model provides a function y = f (x;0</span><span class="font64" style="font-weight:bold;font-style:italic;">)</span><span class="font64"> and&#160;our learning algorithm will adapt the parameters 0 to make f as similar as possible&#160;to f *.</span></p>
<p><span class="font64">In this simple example, we will not be concerned with statistical generalization. We want our network to perform correctly on the four points X = {[0,0]<sup>T</sup>, [0,1]<sup>T</sup>,&#160;[1, 0]<sup>T</sup>, and [1,1]<sup>T</sup>}. We will train the network on all four of these points. The&#160;only challenge is to fit the training set.</span></p>
<p><span class="font64">We can treat this problem as a regression problem and use a mean squared error loss function. We choose this loss function to simplify the math for this example&#160;as much as possible. We will see later that there are other, more appropriate&#160;approaches for modeling binary data.</span></p>
<p><span class="font64">Evaluated on our whole training set, the MSE loss function is</span></p>
<p><span class="font64">1</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64">(6) = <sub>4</sub> &#160;&#160;&#160;(f</span><span class="font64" style="font-weight:bold;font-style:italic;">*</span><span class="font64"> (x) - </span><span class="font64" style="font-weight:bold;font-style:italic;">f</span><span class="font64"> (x; 6))<sup>2</sup> .&#160;&#160;&#160;&#160;(6.1)</span></p>
<p><span class="font64">xSX</span></p>
<p><span class="font64">Now we must choose the form of our model, f (x; </span><span class="font64" style="font-weight:bold;font-style:italic;">6</span><span class="font64">). Suppose that we choose a linear model, with 6 consisting of w and b. Our model is defined to be</span></p>
<p><span class="font64">f (x; w, b) = xw + b. &#160;&#160;&#160;(6-2)</span></p>
<p><span class="font64">We can minimize J(6) in closed form with respect to w and b using the normal equations.</span></p>
<p><span class="font64">After solving the normal equations, we obtain w = </span><span class="font64" style="font-weight:bold;">0 </span><span class="font64">and b = 1 The linear model simply outputs 0.5 everywhere. Why does this happen? Fig. 6.1 shows how&#160;a linear model is not able to represent the XOR function. One way to solve this&#160;problem is to use a model that learns a different feature space in which a linear&#160;model is able to represent the solution.</span></p>
<p><span class="font64">Specifically, we will introduce a very simple feedforward network with one hidden layer containing two hidden units. See Fig. 6.2 for an illustration of&#160;this model. This feedforward network has a vector of hidden units h that are&#160;computed by a function f <sup>(1)</sup>(x; </span><span class="font64" style="font-weight:bold;font-style:italic;">W,</span><span class="font64"> c). The values of these hidden units are then&#160;used as the input for a second layer. The second layer is the output layer of the&#160;network. The output layer is still just a linear regression model, but now it is&#160;applied to h rather than to x. The network now contains two functions chained&#160;together: h = f<sup>(1)</sup>(x; W, c) and y = f <sup>(2)</sup>(h; w, b), with the complete model being&#160;f (x; W, c, </span><span class="font64" style="font-weight:bold;font-style:italic;">w,b)</span><span class="font64"> = f <sup>(2)</sup>(f <sup>(1)</sup>(x)).</span></p>
<p><span class="font64">What function should f<sup>(1)</sup> compute? Linear models have served us well so far, and it may be tempting to make f<sup>(1)</sup> be linear as well. Unfortunately, if f<sup>(1)</sup> were&#160;linear, then the feedforward network as a whole would remain a linear function of&#160;its input. Ignoring the intercept terms for the moment, suppose f <sup>(1)</sup>(x ) = W<sup>T</sup> x&#160;and f <sup>(2)</sup>(h) = h<sup>T</sup> w. Then f (x) = w<sup>T</sup> W<sup>T</sup> x. We could represent this function as&#160;f (x) = x<sup>T</sup> w where </span><span class="font64" style="font-weight:bold;font-style:italic;">uf</span><span class="font64"> = Ww.</span></p>
<p><span class="font64">Clearly, we must use a nonlinear function to describe the features. Most neural networks do so using an affine transformation controlled by learned parameters,&#160;followed by a fixed, nonlinear function called an activation function. We use that&#160;strategy here, by defining h = g( W<sup>T</sup>x + c), where W provides the weights of a&#160;linear transformation and c the biases. Previously, to describe a linear regression&#160;model, we used a vector of weights and a scalar bias parameter to describe an</span></p><div><img src="main-51.jpg" alt=""/>
<p><span class="font64">Figure 6.1: Solving the XOR problem by learning a representation. The bold numbers printed on the plot indicate the value that the learned function must output at each point.&#160;</span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> A linear model applied directly to the original input cannot implement the XOR&#160;function. When </span><span class="font64" style="font-style:italic;">x<sub>1</sub> =</span><span class="font64"> 0, the model’s output must increase as </span><span class="font64" style="font-style:italic;">x <sub>2</sub></span><span class="font64"> increases. When </span><span class="font64" style="font-style:italic;">x<sub>1</sub> =</span><span class="font64"> 1,&#160;the model’s output must decrease as x<sub>2</sub> increases. A linear model must apply a fixed&#160;coefficient </span><span class="font64" style="font-style:italic;">w<sub>2</sub></span><span class="font64"> to x<sub>2</sub>. The linear model therefore cannot use the value of x<sub>1</sub> to change&#160;the coefficient on x<sub>2</sub> and cannot solve this problem. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> In the transformed space&#160;represented by the features extracted by a neural network, a linear model can now solve&#160;the problem. In our example solution, the two points that must have output 1 have been&#160;collapsed into a single point in feature space. In other words, the nonlinear features have&#160;mapped both </span><span class="font65" style="font-style:italic;">x =</span><span class="font64"> [1, 0]<sup>T</sup> and </span><span class="font65" style="font-style:italic;">x =</span><span class="font64"> [0,1]<sup>T</sup> to a single point in feature space, </span><span class="font65" style="font-style:italic;">h =</span><span class="font64"> [1,0]<sup>T</sup>.&#160;The linear model can now describe the function as increasing in </span><span class="font64" style="font-style:italic;">h <sub>1</sub></span><span class="font64"> and decreasing in </span><span class="font64" style="font-style:italic;">h<sub>2</sub>.&#160;</span><span class="font64">In this example, the motivation for learning the feature space is only to make the model&#160;capacity greater so that it can fit the training set. In more realistic applications, learned&#160;representations can also help the model to generalize.</span></p></div><div><div><img src="main-52.jpg" alt=""/>
<p><span class="font64">Figure 6.2: An example of a feedforward network, drawn in two different styles. Specifically, this is the feedforward network we use to solve the XOR example. It has a single hidden&#160;layer containing two units. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> In this style, we draw every unit as a node in the&#160;graph. This style is very explicit and unambiguous but for networks larger than this&#160;example it can consume too much space. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> In this style, we draw a node in the&#160;graph for each entire vector representing a layer’s activations. This style is much more&#160;compact. Sometimes we annotate the edges in this graph with the name of the parameters&#160;that describe the relationship between two layers. Here, we indicate that a matrix </span><span class="font64" style="font-style:italic;">W&#160;</span><span class="font64">describes the mapping from x to h, and a vector </span><span class="font64" style="font-style:italic;">w</span><span class="font64"> describes the mapping from h to y.&#160;We typically omit the intercept parameters associated with each layer when labeling this&#160;kind of drawing.</span></p></div></div>
<p><span class="font64">affine transformation from an input vector to an output scalar. Now, we describe an affine transformation from a vector </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">to a vector </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">, so an entire vector of bias&#160;parameters is needed. The activation function g is typically chosen to be a function&#160;that is applied element-wise, with h </span><span class="font64" style="font-weight:bold;font-style:italic;">= g</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x<sup>T</sup> W<sub>:</sub></span><span class="font64"><sub>i</sub> + c). In modern neural networks,&#160;the default recommendation is to use the </span><span class="font64" style="font-weight:bold;font-style:italic;">rectified linear unit</span><span class="font64"> or ReLU (Jarrett&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009; Nair and Hinton, 2010; Glorot </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011a) defined by the activation&#160;function g(z) = max{0, z} depicted in Fig. 6.3.</span></p>
<p><span class="font64">We can now specify our complete network as</span></p>
<p><span class="font64">f (</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">W</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">c</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">, b) </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">w<sup>T</sup> </span><span class="font64">max{0, </span><span class="font64" style="font-weight:bold;">W<sup>T</sup>x </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;">c</span><span class="font64">} + b. &#160;&#160;&#160;(6.3)</span></p>
<p><span class="font64">We can now specify a solution to the XOR problem. Let</span></p><div>
<p><span class="font64" style="font-weight:bold;">W =</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">c =</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">w =</span></p></div><div>
<p><span class="font64">1 1 1 1 <sup>,</sup></span></p></div><div>
<p><span class="font64">(6.4)</span></p></div><div>
<p><span class="font64">0</span></p>
<p><span class="font64">-1 <sup>, </sup>1</span></p>
<p><span class="font64">-2 <sup>,</sup></span></p></div><div>
<p><span class="font64">(6.5)</span></p>
<p><span class="font64">(6.6)</span></p></div><div><img src="main-53.jpg" alt=""/>
<p><span class="font64">Figure 6.3: The rectified linear activation function. This activation function is the default activation function recommended for use with most feedforward neural networks. Applying&#160;this function to the output of a linear transformation yields a nonlinear transformation.&#160;However, the function remains very close to linear, in the sense that is a piecewise linear&#160;function with two linear pieces. Because rectified linear units are nearly linear, they&#160;preserve many of the properties that make linear models easy to optimize with gradient-based methods. They also preserve many of the properties that make linear models&#160;generalize well. A common principle throughout computer science is that we can build&#160;complicated systems from minimal components. Much as a Turing machine’s memory&#160;needs only to be able to store 0 or 1 states, we can build a universal function approximator&#160;from rectified linear functions.</span></p></div>
<p><span class="font64">and </span><span class="font64" style="font-weight:bold;font-style:italic;">b =</span><span class="font64"> 0.</span></p>
<p><span class="font64">We can now walk through the way that the model processes a batch of inputs. Let X be the design matrix containing all four points in the binary input space,&#160;with one example per row:</span></p><div>
<p><span class="font64">(6.7)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">X =</span></p></div>
<p><span class="font64">00 0 1&#160;1 0&#160;1 1</span></p><div>
<p><span class="font64">The first step in the neural network is to multiply the input matrix by the first layer’s weight matrix:</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">XW</span></p></div><div>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font64">-1</span></p>
<p><span class="font64">O</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td></tr>
<tr><td>
<p><span class="font64">to</span></p></td><td>
<p><span class="font64">to</span></p></td></tr>
</table></div><div>
<p><span class="font64">(6.8)</span></p></div><div>
<p><span class="font64">Next, we add the bias vector </span><span class="font64" style="font-weight:bold;">c</span><span class="font64">, to obtain</span></p></div><div></div><div>
<p><span class="font64">(6.9)</span></p></div><div>
<p><span class="font64">In this space, all of the examples lie along a line with slope 1. As we move along this line, the output needs to begin at 0, then rise to 1, then drop back down to 0.&#160;A linear model cannot implement such a function. To finish computing the value&#160;of </span><span class="font64" style="font-weight:bold;">h </span><span class="font64">for each example, we apply the rectified linear transformation:</span></p>
<p><span class="font64">' 0 0 1 0&#160;1 0&#160;21</span></p></div><div>
<p><span class="font64">(6.10)</span></p></div><div>
<p><span class="font64">This transformation has changed the relationship between the examples. They no longer lie on a single line. As shown in Fig. 6.1, they now lie in a space where a&#160;linear model can solve the problem.</span></p>
<p><span class="font64">We finish by multiplying by the weight vector </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">:</span></p></div><div>
<p><span class="font64">0</span></p>
<p><span class="font64">1</span></p>
<p><span class="font64">1</span></p>
<p><span class="font64">0</span></p></div><div>
<p><span class="font64">(6.11)</span></p></div>
<p><span class="font64">The neural network has obtained the correct answer for every example in the batch.</span></p>
<p><span class="font64">In this example, we simply specified the solution, then showed that it obtained zero error. In a real situation, there might be billions of model parameters and&#160;billions of training examples, so one cannot simply guess the solution as we did&#160;here. Instead, a gradient-based optimization algorithm can find parameters that&#160;produce very little error. The solution we described to the XOR problem is at a&#160;global minimum of the loss function, so gradient descent could converge to this&#160;point. There are other equivalent solutions to the XOR problem that gradient&#160;descent could also find. The convergence point of gradient descent depends on the&#160;initial values of the parameters. In practice, gradient descent would usually not&#160;find clean, easily understood, integer-valued solutions like the one we presented&#160;here.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">6.2 Gradient-Based Learning</span></h4>
<p><span class="font64">Designing and training a neural network is not much different from training any other machine learning model with gradient descent. In Sec. 5.10, we described&#160;how to build a machine learning algorithm by specifying an optimization procedure,&#160;a cost function, and a model family.</span></p>
<p><span class="font64">The largest difference between the linear models we have seen so far and neural networks is that the nonlinearity of a neural network causes most interesting loss&#160;functions to become non-convex. This means that neural networks are usually&#160;trained by using iterative, gradient-based optimizers that merely drive the cost&#160;function to a very low value, rather than the linear equation solvers used to train&#160;linear regression models or the convex optimization algorithms with global convergence guarantees used to train logistic regression or SVMs. Convex optimization&#160;converges starting from any initial parameters (in theory—in practice it is very&#160;robust but can encounter numerical problems). Stochastic gradient descent applied&#160;to non-convex loss functions has no such convergence guarantee, and is sensitive&#160;to the values of the initial parameters. For feedforward neural networks, it is&#160;important to initialize all weights to small random values. The biases may be&#160;initialized to zero or to small positive values. The iterative gradient-based optimization algorithms used to train feedforward networks and almost all other deep&#160;models will be described in detail in Chapter 8, with parameter initialization in&#160;particular discussed in Sec. 8.4. For the moment, it suffices to understand that&#160;the training algorithm is almost always based on using the gradient to descend the&#160;cost function in one way or another. The specific algorithms are improvements&#160;and refinements on the ideas of gradient descent, introduced in Sec. 4.3, and,&#160;more specifically, are most often improvements of the stochastic gradient descent&#160;algorithm, introduced in Sec. 5.9.</span></p>
<p><span class="font64">We can of course, train models such as linear regression and support vector machines with gradient descent too, and in fact this is common when the training&#160;set is extremely large. From this point of view, training a neural network is not&#160;much different from training any other model. Computing the gradient is slightly&#160;more complicated for a neural network, but can still be done efficiently and exactly.&#160;Sec. 6.5 will describe how to obtain the gradient using the back-propagation&#160;algorithm and modern generalizations of the back-propagation algorithm.</span></p>
<p><span class="font64">As with other machine learning models, to apply gradient-based learning we must choose a cost function, and we must choose how to represent the output of&#160;the model. We now revisit these design considerations with special emphasis on&#160;the neural networks scenario.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">6.2.1 &#160;&#160;&#160;Cost Functions</span></h5>
<p><span class="font64">An important aspect of the design of a deep neural network is the choice of the cost function. Fortunately, the cost functions for neural networks are more or less&#160;the same as those for other parametric models, such as linear models.</span></p>
<p><span class="font64">In most cases, our parametric model defines a distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p( y</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;font-style:italic;">x; Q</span><span class="font64">) and we simply use the principle of maximum likelihood. This means we use the&#160;cross-entropy between the training data and the model’s predictions as the cost&#160;function.</span></p>
<p><span class="font64">Sometimes, we take a simpler approach, where rather than predicting a complete probability distribution over </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">, we merely predict some statistic of </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">conditioned&#160;on </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">. Specialized loss functions allow us to train a predictor of these estimates.</span></p>
<p><span class="font64">The total cost function used to train a neural network will often combine one of the primary cost functions described here with a regularization term. We have&#160;already seen some simple examples of regularization applied to linear models in Sec.&#160;5.2.2. The weight decay approach used for linear models is also directly applicable&#160;to deep neural networks and is among the most popular regularization strategies.&#160;More advanced regularization strategies for neural networks will be described in&#160;Chapter 7.</span></p>
<p><span class="font64" style="font-weight:bold;">6.2.1.1 &#160;&#160;&#160;Learning Conditional Distributions with Maximum Likelihood</span></p>
<p><span class="font64">Most modern neural networks are trained using maximum likelihood. This means that the cost function is simply the negative log-likelihood, equivalently described</span></p>
<p><span class="font64">as the cross-entropy between the training data and the model distribution. This cost function is given by</span></p>
<p><span class="font64"><sup>J (6) &#160;&#160;&#160;— E</sup>x,y~p3data <sup>10g p</sup>mode1<sup>(y 1 x)</sup>•&#160;&#160;&#160;&#160;(<sup>6</sup>.<sup>12</sup>)</span></p>
<p><span class="font64">The specific form of the cost function changes from model to model, depending on the specific form of logp<sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64">• The expansion of the above equation typically&#160;yields some terms that do not depend on the model parameters and may be&#160;discarded. For example, as we saw in Sec. 5.5.1, if pm<sub>o</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (y | x) </span><span class="font64" style="font-weight:bold;font-style:italic;">= N(y; f</span><span class="font64"> (x; </span><span class="font64" style="font-weight:bold;font-style:italic;">6</span><span class="font64">), I),&#160;then we recover the mean squared error cost,</span></p>
<p><span class="font64"><sup>J </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(</sup>0<sup>)</sup> = ~E</span><span class="font64"> x,y~p3data</span><span class="font64" style="font-weight:bold;font-style:italic;">\\y <sup>- f (X; 6</sup></span><span class="font64"><sup> )|12</sup> +<sup>COnSt &#160;&#160;&#160;(6</sup>.<sup>13)</sup></span></p>
<p><span class="font64">up to a scaling factor of 2־ and a term that does not depend on 6. The discarded constant is based on the variance of the Gaussian distribution, which in this case&#160;we chose not to parametrize. Previously, we saw that the equivalence between&#160;maximum likelihood estimation with an output distribution and minimization of&#160;mean squared error holds for a linear model, but in fact, the equivalence holds&#160;regardless of the f (x; 6) used to predict the mean of the Gaussian.</span></p>
<p><span class="font64">An advantage of this approach of deriving the cost function from maximum likelihood is that it removes the burden of designing cost functions for each model.&#160;Specifying a model </span><span class="font64" style="font-weight:bold;font-style:italic;">p(y</span><span class="font64"> \ x) automatically determines a cost function log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(y</span><span class="font64"> \ x).</span></p>
<p><span class="font64">One recurring theme throughout neural network design is that the gradient of the cost function must be large and predictable enough to serve as a good guide&#160;for the learning algorithm. Functions that saturate (become very flat) undermine&#160;this objective because they make the gradient become very small. In many cases&#160;this happens because the activation functions used to produce the output of the&#160;hidden units or the output units saturate. The negative log-likelihood helps to&#160;avoid this problem for many models. Many output units involve an exp function&#160;that can saturate when its argument is very negative. The log function in the&#160;negative log-likelihood cost function undoes the exp of some output units. We will&#160;discuss the interaction between the cost function and the choice of output unit in&#160;Sec. 6.2.2.</span></p>
<p><span class="font64">One unusual property of the cross-entropy cost used to perform maximum likelihood estimation is that it usually does not have a minimum value when applied&#160;to the models commonly used in practice. For discrete output variables, most&#160;models are parametrized in such a way that they cannot represent a probability&#160;of zero or one, but can come arbitrarily close to doing so. Logistic regression&#160;is an example of such a model. For real-valued output variables, if the model&#160;can control the density of the output distribution (for example, by learning the&#160;variance parameter of a Gaussian output distribution) then it becomes possible&#160;to assign extremely high density to the correct training set outputs, resulting in&#160;cross-entropy approaching negative infinity. Regularization techniques described&#160;in Chapter 7 provide several different ways of modifying the learning problem so&#160;that the model cannot reap unlimited reward in this way.</span></p>
<p><span class="font64" style="font-weight:bold;">6.2.1.2 Learning Conditional Statistics</span></p>
<p><span class="font64">Instead of learning a full probability distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p(y | x; 6</span><span class="font64">) we often want to learn just one conditional statistic of y given x.</span></p>
<p><span class="font64">For example, we may have a predictor f (x; 6) that we wish to predict the mean of y.</span></p>
<p><span class="font64">If we use a sufficiently powerful neural network, we can think of the neural network as being able to represent any function f from a wide class of functions,&#160;with this class being limited only by features such as continuity and boundedness&#160;rather than by having a specific parametric form. From this point of view, we&#160;can view the cost function as being a </span><span class="font64" style="font-weight:bold;font-style:italic;">functional</span><span class="font64"> rather than just a function. A&#160;functional is a mapping from functions to real numbers. We can thus think of&#160;learning as choosing a function rather than merely choosing a set of parameters.&#160;We can design our cost functional to have its minimum occur at some specific&#160;function we desire. For example, we can design the cost functional to have its&#160;minimum lie on the function that maps x to the expected value of y given x.&#160;Solving an optimization problem with respect to a function requires a mathematical&#160;tool called </span><span class="font64" style="font-weight:bold;font-style:italic;">calculus of variations,</span><span class="font64"> described in Sec. 19.4.2. It is not necessary to&#160;understand calculus of variations to understand the content of this chapter. At&#160;the moment, it is only necessary to understand that calculus of variations may be&#160;used to derive the following two results.</span></p>
<p><span class="font64">Our first result derived using calculus of variations is that solving the optimization problem</span></p>
<p><span class="font64"><sup>f</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> <sup>a</sup>rgmrn <sup>E</sup>X,y~<sub>P</sub>data<sup>||</sup>y <sup>- f</sup> (<sup>x)|</sup>!<sup>2</sup> &#160;&#160;&#160;(6.14)</span></p>
<p><span class="font64">f</span></p>
<p><span class="font64">yields</span></p>
<p><span class="font64"><sup>f</sup> *<sup>(x)</sup> = Ey^pdata(y|x) </span><span class="font64" style="font-weight:bold;font-style:italic;">[y],</span><span class="font64"> &#160;&#160;&#160;<sup>(6</sup>.<sup>15)</sup></span></p>
<p><span class="font64">so long as this function lies within the class we optimize over. In other words, if we could train on infinitely many samples from the true data-generating distribution,&#160;minimizing the mean squared error cost function gives a function that predicts the&#160;mean of y for each value of x.</span></p>
<p><span class="font64">Different cost functions give different statistics. A second result derived using calculus of variations is that</span></p>
<p><span class="font64"><sup>f</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> <sup>ar</sup>g<sup>min E</sup>X,y~p data<sup>11</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">y <sup>- f</sup></span><span class="font64"><sup> (</sup></span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>)||</sup>1 &#160;&#160;&#160;(<sup>6</sup>•<sup>16</sup>)</span></p>
<p><span class="font64">f</span></p>
<p><span class="font64">yields a function that predicts the </span><span class="font64" style="font-weight:bold;">median </span><span class="font64">value of </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">for each </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">, so long as such a function may be described by the family of functions we optimize over. This cost&#160;function is commonly called </span><span class="font64" style="font-weight:bold;font-style:italic;">mean absolute error</span><span class="font64">.</span></p>
<p><span class="font64">Unfortunately, mean squared error and mean absolute error often lead to poor results when used with gradient-based optimization. Some output units that&#160;saturate produce very small gradients when combined with these cost functions.&#160;This is one reason that the cross-entropy cost function is more popular than mean&#160;squared error or mean absolute error, even when it is not necessary to estimate an&#160;entire distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p(y</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">).</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">6.2.2 Output Units</span></h5>
<p><span class="font64">The choice of cost function is tightly coupled with the choice of output unit. Most of the time, we simply use the cross-entropy between the data distribution and the&#160;model distribution. The choice of how to represent the output then determines&#160;the form of the cross-entropy function.</span></p>
<p><span class="font64">Any kind of neural network unit that may be used as an output can also be used as a hidden unit. Here, we focus on the use of these units as outputs of the&#160;model, but in principle they can be used internally as well. We revisit these units&#160;with additional detail about their use as hidden units in Sec. 6.3.</span></p>
<p><span class="font64">Throughout this section, we suppose that the feedforward network provides a set of hidden features defined by </span><span class="font64" style="font-weight:bold;font-style:italic;">h = f</span><span class="font64"> (</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">6</span><span class="font64">). The role of the output layer is then&#160;to provide some additional transformation from the features to complete the task&#160;that the network must perform.</span></p>
<p><span class="font64" style="font-weight:bold;">6.2.2.1 Linear Units for Gaussian Output Distributions</span></p>
<p><span class="font64">One simple kind of output unit is an output unit based on an affine transformation with no nonlinearity. These are often just called linear units.</span></p>
<p><span class="font64">Given features </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">, a layer of linear output units produces a vector </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">= </span><span class="font64" style="font-weight:bold;font-style:italic;">W<sup>T</sup></span><span class="font64"> </span><span class="font64" style="font-weight:bold;">h</span><span class="font64">+ </span><span class="font64" style="font-weight:bold;">b</span><span class="font64">. Linear output layers are often used to produce the mean of a conditional&#160;Gaussian distribution:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p<sup>(</sup>y <sup>1</sup></span><span class="font64"><sup> </sup></span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>)</sup> = <sup>N(</sup></span><span class="font64" style="font-weight:bold;">y</span><span class="font64">; </span><span class="font64" style="font-weight:bold;font-style:italic;">y, I</span><span class="font64">). &#160;&#160;&#160;(6.17)</span></p>
<p><span class="font64">Maximizing the log-likelihood is then equivalent to minimizing the mean squared error.</span></p>
<p><span class="font64">The maximum likelihood framework makes it straightforward to learn the covariance of the Gaussian too, or to make the covariance of the Gaussian be a&#160;function of the input. However, the covariance must be constrained to be a positive&#160;definite matrix for all inputs. It is difficult to satisfy such constraints with a linear&#160;output layer, so typically other output units are used to parametrize the covariance.&#160;Approaches to modeling the covariance are described shortly, in Sec. 6.2.2.4.</span></p>
<p><span class="font64">Because linear units do not saturate, they pose little difficulty for gradient-based optimization algorithms and may be used with a wide variety of optimization algorithms.</span></p>
<p><span class="font64" style="font-weight:bold;">6.2.2.2 Sigmoid Units for Bernoulli Output Distributions</span></p>
<p><span class="font64">Many tasks require predicting the value of a binary variable y. Classification problems with two classes can be cast in this form.</span></p>
<p><span class="font64">The maximum-likelihood approach is to define a Bernoulli distribution over y conditioned on x.</span></p>
<p><span class="font64">A Bernoulli distribution is defined by just a single number. The neural net needs to predict only P(y = 1 | x). For this number to be a valid probability, it&#160;must lie in the interval [0, 1].</span></p>
<p><span class="font64">Satisfying this constraint requires some careful design effort. Suppose we were to use a linear unit, and threshold its value to obtain a valid probability:</span></p>
<p><span class="font64">P(y = 1 | x) = max |0,min j 1, w </span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font64"> + b j j . &#160;&#160;&#160;(6.18)</span></p>
<p><span class="font64">This would indeed define a valid conditional distribution, but we would not be able to train it very effectively with gradient descent. Any time that w<sup>T</sup>h + b strayed&#160;outside the unit interval, the gradient of the output of the model with respect to&#160;its parameters would be </span><span class="font64" style="font-weight:bold;">0</span><span class="font64">. A gradient of </span><span class="font64" style="font-weight:bold;">0 </span><span class="font64">is typically problematic because the&#160;learning algorithm no longer has a guide for how to improve the corresponding&#160;parameters.</span></p>
<p><span class="font64">Instead, it is better to use a different approach that ensures there is always a strong gradient whenever the model has the wrong answer. This approach is based&#160;on using sigmoid output units combined with maximum likelihood.</span></p>
<p><span class="font64">A sigmoid output unit is defined by</span></p>
<p><span class="font64">y = a </span><span class="font64" style="font-weight:bold;font-style:italic;">(w h</span><span class="font64"> + b^ &#160;&#160;&#160;(6.19)</span></p>
<p><span class="font64">where a is the logistic sigmoid function described in Sec. 3.10.</span></p>
<p><span class="font64">We can think of the sigmoid output unit as having two components. First, it uses a linear layer to compute z = w<sup>T</sup>h + b. Next, it uses the sigmoid activation&#160;function to convert z into a probability.</span></p>
<p><span class="font64">We omit the dependence on x for the moment to discuss how to define a probability distribution over y using the value z. The sigmoid can be motivated&#160;by constructing an unnormalized probability distribution P(y), which does not&#160;sum to 1. We can then divide by an appropriate constant to obtain a valid&#160;probability distribution. If we begin with the assumption that the unnormalized log&#160;probabilities are linear in y and z, we can exponentiate to obtain the unnormalized&#160;probabilities. We then normalize to see that this yields a Bernoulli distribution&#160;controlled by a sigmoidal transformation of z:</span></p><div>
<p><span class="font64">log </span><span class="font64" style="font-weight:bold;font-style:italic;">P{y</span><span class="font64"><sup>)</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">P{y</span><span class="font64">) </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">p (y</span><span class="font64">)</span></p></div><div>
<p><span class="font64">(6.20)</span></p>
<p><span class="font64">(6.21)</span></p>
<p><span class="font64">(6.22)</span></p>
<p><span class="font64">(6.23)</span></p></div>
<p><span class="font64">yz</span></p>
<p><span class="font64">exp(yz) exp(yz)</span></p>
<p><span class="font64"><sup>ex</sup>p<sup>(y1 z) P (</sup>y<sup>)</sup> = <sup>a ((2</sup>y <sup>- 1)z)</sup> •</span></p>
<p><span class="font64">Probability distributions based on exponentiation and normalization are common throughout the statistical modeling literature. The z variable defining such a&#160;distribution over binary variables is called a </span><span class="font64" style="font-weight:bold;font-style:italic;">logit.</span></p>
<p><span class="font64">This approach to predicting the probabilities in log-space is natural to use with maximum likelihood learning. Because the cost function used with maximum&#160;likelihood is — log P(y | x), the log in the cost function undoes the exp of the&#160;sigmoid. Without this effect, the saturation of the sigmoid could prevent gradient-based learning from making good progress. The loss function for maximum&#160;likelihood learning of a Bernoulli parametrized by a sigmoid is</span></p>
<p><span class="font64">J(0) = — log P(y | x) &#160;&#160;&#160;(6.24)</span></p>
<p><span class="font64">= — log a ((2y — 1)z) &#160;&#160;&#160;(6.25)</span></p>
<p><span class="font64">= &lt; ((1 — </span><span class="font64" style="font-weight:bold;font-style:italic;">2y)z</span><span class="font64">) • &#160;&#160;&#160;(6.26)</span></p>
<p><span class="font64">This derivation makes use of some properties from Sec. 3.10. By rewriting the loss in terms of the softplus function, we can see that it saturates only when&#160;(1 — 2y )z is very negative. Saturation thus occurs only when the model already&#160;has the right answer—when y = 1 and z is very positive, or y = 0 and z is very&#160;negative. When z has the wrong sign, the argument to the softplus function,&#160;(1 — 2y)z, may be simplified to |z|. As |z| becomes large while z has the wrong sign,&#160;the softplus function asymptotes toward simply returning its argument |z|. The&#160;derivative with respect to z asymptotes to sign(z), so, in the limit of extremely&#160;incorrect z, the softplus function does not shrink the gradient at all. This property&#160;is very useful because it means that gradient-based learning can act to quickly&#160;correct a mistaken z.</span></p>
<p><span class="font64">When we use other loss functions, such as mean squared error, the loss can saturate anytime a</span><span class="font64" style="font-weight:bold;font-style:italic;">(z)</span><span class="font64"> saturates. The sigmoid activation function saturates to 0&#160;when z becomes very negative and saturates to 1 when z becomes very positive.&#160;The gradient can shrink too small to be useful for learning whenever this happens,&#160;whether the model has the correct answer or the incorrect answer. For this reason,&#160;maximum likelihood is almost always the preferred approach to training sigmoid&#160;output units.</span></p>
<p><span class="font64">Analytically, the logarithm of the sigmoid is always defined and finite, because the sigmoid returns values restricted to the open interval (0,1), rather than using&#160;the entire closed interval of valid probabilities [0, 1]. In software implementations,&#160;to avoid numerical problems, it is best to write the negative log-likelihood as a&#160;function of z, rather than as a function of </span><span class="font64" style="font-weight:bold;font-style:italic;">y = a(z).</span><span class="font64"> If the sigmoid function&#160;underflows to zero, then taking the logarithm of y yields negative infinity.</span></p>
<p><span class="font64" style="font-weight:bold;">6.2.2.3 Softmax Units for Multinoulli Output Distributions</span></p>
<p><span class="font64">Any time we wish to represent a probability distribution over a discrete variable with n possible values, we may use the softmax function. This can be seen as a&#160;generalization of the sigmoid function which was used to represent a probability&#160;distribution over a binary variable.</span></p>
<p><span class="font64">Softmax functions are most often used as the output of a classifier, to represent the probability distribution over n different classes. More rarely, softmax functions&#160;can be used inside the model itself, if we wish the model to choose between one of&#160;n different options for some internal variable.</span></p>
<p><span class="font64">In the case of binary variables, we wished to produce a single number</span></p>
<p><span class="font64">y = </span><span class="font64" style="font-weight:bold;font-style:italic;">P(y = 1</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;font-style:italic;">x).</span><span class="font64"> &#160;&#160;&#160;(6.27)</span></p>
<p><span class="font64">Because this number needed to lie between 0 and 1, and because we wanted the logarithm of the number to be well-behaved for gradient-based optimization of&#160;the log-likelihood, we chose to instead predict a number z = log</span><span class="font64" style="font-weight:bold;font-style:italic;">P(y</span><span class="font64"> = 1 | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">).&#160;Exponentiating and normalizing gave us a Bernoulli distribution controlled by the&#160;sigmoid function.</span></p>
<p><span class="font64">To generalize to the case of a discrete variable with n values, we now need to produce a vector </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64">, with y<sub>i</sub> </span><span class="font64" style="font-weight:bold;font-style:italic;">= P(y = i</span><span class="font64"> | x). We require not only that each&#160;element of y<sub>i</sub> be between 0 and 1, but also that the entire vector sums to 1 so that&#160;it represents a valid probability distribution. The same approach that worked for&#160;the Bernoulli distribution generalizes to the multinoulli distribution. First, a linear&#160;layer predicts unnormalized log probabilities:</span></p>
<p><span class="font64">z = </span><span class="font64" style="font-weight:bold;font-style:italic;">W</span><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">b,</span><span class="font64"> &#160;&#160;&#160;(6.28)</span></p>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;font-style:italic;">z<sub>i</sub> =</span><span class="font64"> log </span><span class="font64" style="font-weight:bold;font-style:italic;">P(y = i</span><span class="font64"> | x). The softmax function can then exponentiate and normalize z to obtain the desired y. Formally, the softmax function is given by</span></p>
<p><span class="font64">softmax(z)<sub>i</sub> = &#160;&#160;&#160;</span><span class="font64" style="text-decoration:line-through;"><sup>6</sup>&#160;&#160;&#160;&#160;)</span><span class="font64"> .&#160;&#160;&#160;&#160;(6.29)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">j</span><span class="font64"> <sup>ex</sup>P<sup>(z</sup>j<sup>)</sup></span></p>
<p><span class="font64">As with the logistic sigmoid, the use of the exp function works very well when training the softmax to output a target value y using maximum log-likelihood. In&#160;this case, we wish to maximize log P (y = i; z) = logsoftmax(z)<sub>i</sub>. Defining the&#160;softmax in terms of exp is natural because the log in the log-likelihood can undo&#160;the exp of the softmax:</span></p>
<p><span class="font64">logsoftmax(z)<sub>i</sub> = </span><span class="font64" style="font-weight:bold;font-style:italic;">z</span><span class="font64"> — log^^ exp(z;). &#160;&#160;&#160;(6.30)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>j</sup></span></p>
<p><span class="font64">The first term of Eq. 6.30 shows that the input </span><span class="font64" style="font-weight:bold;font-style:italic;">z</span><span class="font64"> always has a direct contribution to the cost function. Because this term cannot saturate, we know that learning can proceed, even if the contribution of z<sub>i</sub> to the second term of Eq. 6.30&#160;becomes very small. When maximizing the log-likelihood, the first term encourages&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">z</span><span class="font64"> to be pushed up, while the second term encourages all of z to be pushed down.&#160;To gain some intuition for the second term, log^jexp(zj), observe that this term&#160;can be roughly approximated by maxj zj. This approximation is based on the idea&#160;that exp(zk) is insignificant for any zk that is noticeably less than maxj zj. The&#160;intuition we can gain from this approximation is that the negative log-likelihood&#160;cost function always strongly penalizes the most active incorrect prediction. If the&#160;correct answer already has the largest input to the softmax, then the — z<sub>i</sub> term&#160;and the log^j exp(zj) ~ maxj zj = z<sub>i</sub> terms will roughly cancel. This example&#160;will then contribute little to the overall training cost, which will be dominated by&#160;other examples that are not yet correctly classified.</span></p>
<p><span class="font64">So far we have discussed only a single example. Overall, unregularized maximum likelihood will drive the model to learn parameters that drive the softmax to predict&#160;the fraction of counts of each outcome observed in the training set:</span></p>
<p><span class="font64"><sup>m</sup> 1</span></p>
<p><span class="font64">softmax(z(x; 0)) ~ —</span><span class="font64" style="text-decoration:line-through;"><sup>j</sup>=<sup>1</sup><sub>m</sub> <sup>y</sup> 1 =<sup>i,x(J</sup></span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:line-through;"><sup>]</sup>~<sup>x</sup></span><span class="font64" style="font-weight:bold;font-style:italic;"> .</span><span class="font64"> &#160;&#160;&#160;(6.31)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">j=</span><span class="font64">1 </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>1</sup>x<sup>(j)</sup> =x</span></p>
<p><span class="font64">Because maximum likelihood is a consistent estimator, this is guaranteed to happen so long as the model family is capable of representing the training distribution. In&#160;practice, limited model capacity and imperfect optimization will mean that the&#160;model is only able to approximate these fractions.</span></p>
<p><span class="font64">Many objective functions other than the log-likelihood do not work as well with the softmax function. Specifically, objective functions that do not use a log to&#160;undo the exp of the softmax fail to learn when the argument to the exp becomes&#160;very negative, causing the gradient to vanish. In particular, squared error is a&#160;poor loss function for softmax units, and can fail to train the model to change its&#160;output, even when the model makes highly confident incorrect predictions (Bridle,&#160;1990). To understand why these other loss functions can fail, we need to examine&#160;the softmax function itself.</span></p>
<p><span class="font64">Like the sigmoid, the softmax activation can saturate. The sigmoid function has a single output that saturates when its input is extremely negative or extremely&#160;positive. In the case of the softmax, there are multiple output values. These&#160;output values can saturate when the differences between input values become&#160;extreme. When the softmax saturates, many cost functions based on the softmax&#160;also saturate, unless they are able to invert the saturating activating function.</span></p>
<p><span class="font64">To see that the softmax function responds to the difference between its inputs, observe that the softmax output is invariant to adding the same scalar to all of its&#160;inputs:</span></p>
<p><span class="font64">softmax(z) = softmax(z + c). &#160;&#160;&#160;(6.32)</span></p>
<p><span class="font64">Using this property, we can derive a numerically stable variant of the softmax:</span></p>
<p><span class="font64">softmax(z) = softmax(z — max z). &#160;&#160;&#160;(6.33)</span></p>
<p><span class="font64">i</span></p>
<p><span class="font64">The reformulated version allows us to evaluate softmax with only small numerical errors even when z contains extremely large or extremely negative numbers. Examining the numerically stable variant, we see that the softmax function is driven&#160;by the amount that its arguments deviate from max<sub>i</sub> </span><span class="font64" style="font-weight:bold;font-style:italic;">z<sub>i</sub></span><span class="font64">.</span></p>
<p><span class="font64">An output softmax(z )<sub>i</sub> saturates to 1 when the corresponding input is maximal (z<sub>i</sub> = maxj z<sub>i</sub>) and z<sub>i</sub> is much greater than all of the other inputs. The output&#160;softmax(z)i can also saturate to 0 when z is not maximal and the maximum is&#160;much greater. This is a generalization of the way that sigmoid units saturate, and&#160;can cause similar difficulties for learning if the loss function is not designed to&#160;compensate for it.</span></p>
<p><span class="font64">The argument </span><span class="font64" style="font-weight:bold;">z </span><span class="font64">to the softmax function can be produced in two different ways. The most common is simply to have an earlier layer of the neural network output&#160;every element of </span><span class="font64" style="font-weight:bold;">z</span><span class="font64">, as described above using the linear layer </span><span class="font64" style="font-weight:bold;">z </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">W</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>T</sup></span><span class="font64" style="font-weight:bold;"> h </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;">b</span><span class="font64">. While&#160;straightforward, this approach actually overparametrizes the distribution. The&#160;constraint that the n outputs must sum to 1 means that only </span><span class="font64" style="font-weight:bold;font-style:italic;">n — 1</span><span class="font64"> parameters are&#160;necessary; the probability of the n-th value may be obtained by subtracting the&#160;first n — 1 probabilities from 1. We can thus impose a requirement that one element&#160;of </span><span class="font64" style="font-weight:bold;">z </span><span class="font64">be fixed. For example, we can require that </span><span class="font64" style="font-weight:bold;font-style:italic;">zn</span><span class="font64"> =0. Indeed, this is exactly&#160;what the sigmoid unit does. Defining P(y = 1 | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) = a</span><span class="font64" style="font-weight:bold;font-style:italic;">(z</span><span class="font64">) is equivalent to defining&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">P(y = 1</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) = softmax(</span><span class="font64" style="font-weight:bold;">z</span><span class="font18">)1</span><span class="font64"> with a two-dimensional </span><span class="font64" style="font-weight:bold;">z </span><span class="font64">and z! = 0. Both the n — 1&#160;argument and the n argument approaches to the softmax can describe the same&#160;set of probability distributions, but have different learning dynamics. In practice,&#160;there is rarely much difference between using the overparametrized version or the&#160;restricted version, and it is simpler to implement the overparametrized version.</span></p>
<p><span class="font64">From a neuroscientific point of view, it is interesting to think of the softmax as a way to create a form of competition between the units that participate in it: the&#160;softmax outputs always sum to 1 so an increase in the value of one unit necessarily&#160;corresponds to a decrease in the value of others. This is analogous to the lateral&#160;inhibition that is believed to exist between nearby neurons in the cortex. At the&#160;extreme (when the difference between the maximal a and the others is large in&#160;magnitude) it becomes a form of </span><span class="font64" style="font-weight:bold;font-style:italic;">winner-take-all</span><span class="font64"> (one of the outputs is nearly 1&#160;and the others are nearly 0).</span></p>
<p><span class="font64">The name “softmax” can be somewhat confusing. The function is more closely related to the argmax function than the max function. The term “soft” derives&#160;from the fact that the softmax function is continuous and differentiable. The&#160;argmax function, with its result represented as a one-hot vector, is not continuous&#160;or differentiable. The softmax function thus provides a “softened” version of the&#160;argmax. The corresponding soft version of the maximum function is softmax( </span><span class="font64" style="font-weight:bold;">z</span><span class="font64">)</span><span class="font64" style="font-weight:bold;"><sup>T</sup>z</span><span class="font64">.&#160;It would perhaps be better to call the softmax function “softargmax,” but the&#160;current name is an entrenched convention.</span></p>
<p><span class="font64" style="font-weight:bold;">6.2.2.4 Other Output Types</span></p>
<p><span class="font64">The linear, sigmoid, and softmax output units described above are the most common. Neural networks can generalize to almost any kind of output layer that&#160;we wish. The principle of maximum likelihood provides a guide for how to design&#160;a good cost function for nearly any kind of output layer.</span></p>
<p><span class="font64">In general, if we define a conditional distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p (y</span><span class="font64"> | x; </span><span class="font64" style="font-weight:bold;font-style:italic;">6</span><span class="font64">), the principle of maximum likelihood suggests we use — logp(y | x; 6) as our cost function.</span></p>
<p><span class="font64">In general, we can think of the neural network as representing a function f (x; 6). The outputs of this function are not direct predictions of the value y. Instead,&#160;f (x;6) = u provides the parameters for a distribution over y. Our loss function&#160;can then be interpreted as — logp(y; u(x)).</span></p>
<p><span class="font64">For example, we may wish to learn the variance of a conditional Gaussian for y, given x. In the simple case, where the variance a<sup>2</sup> is a constant, there is a&#160;closed form expression because the maximum likelihood estimator of variance is&#160;simply the empirical mean of the squared difference between observations y and&#160;their expected value. A computationally more expensive approach that does not&#160;require writing special-case code is to simply include the variance as one of the&#160;properties of the distribution p(y | x) that is controlled by u = f(x;6). The&#160;negative log-likelihood — logp (y; u(x)) will then provide a cost function with the&#160;appropriate terms necessary to make our optimization procedure incrementally&#160;learn the variance. In the simple case where the standard deviation does not depend&#160;on the input, we can make a new parameter in the network that is copied directly&#160;into u. This new parameter might be </span><span class="font64" style="font-weight:bold;font-style:italic;">a</span><span class="font64"> itself or could be a parameter v representing&#160;a<sup>2</sup> or it could be a parameter 3 representing , depending on how we choose to&#160;parametrize the distribution. We may wish our model to predict a different amount&#160;of variance in y for different values of x. This is called a </span><span class="font64" style="font-weight:bold;font-style:italic;">heteroscedastic</span><span class="font64"> model.&#160;In the heteroscedastic case, we simply make the specification of the variance be&#160;one of the values output by f (x; 6). A typical way to do this is to formulate the&#160;Gaussian distribution using precision, rather than variance, as described in Eq.&#160;3.22. In the multivariate case it is most common to use a diagonal precision matrix</span></p>
<p><span class="font64">diag(fl). &#160;&#160;&#160;(6.34)</span></p>
<p><span class="font64">This formulation works well with gradient descent because the formula for the log-likelihood of the Gaussian distribution parametrized by involves only multiplication by </span><span class="font64" style="font-weight:bold;font-style:italic;">(3%</span><span class="font64"> and addition of log . The gradient of multiplication, addition,&#160;and logarithm operations is well-behaved. By comparison, if we parametrized the&#160;output in terms of variance, we would need to use division. The division function&#160;becomes arbitrarily steep near zero. While large gradients can help learning,&#160;arbitrarily large gradients usually result in instability. If we parametrized the&#160;output in terms of standard deviation, the log-likelihood would still involve division,&#160;and would also involve squaring. The gradient through the squaring operation&#160;can vanish near zero, making it difficult to learn parameters that are squared.</span></p>
<p><span class="font64">Regardless of whether we use standard deviation, variance, or precision, we must ensure that the covariance matrix of the Gaussian is positive definite. Because&#160;the eigenvalues of the precision matrix are the reciprocals of the eigenvalues of&#160;the covariance matrix, this is equivalent to ensuring that the precision matrix is&#160;positive definite. If we use a diagonal matrix, or a scalar times the diagonal matrix,&#160;then the only condition we need to enforce on the output of the model is positivity.&#160;If we suppose that a is the raw activation of the model used to determine the&#160;diagonal precision, we can use the softplus function to obtain a positive precision&#160;vector:&#160;&#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">= Z</span><span class="font64">(a). This same strategy applies equally if using variance or standard</span></p>
<p><span class="font64">deviation rather than precision or if using a scalar times identity rather than diagonal matrix.</span></p>
<p><span class="font64">It is rare to learn a covariance or precision matrix with richer structure than diagonal. If the covariance is full and conditional, then a parametrization must&#160;be chosen that guarantees positive-definiteness of the predicted covariance matrix.&#160;This can be achieved by writing £(x) = </span><span class="font64" style="font-weight:bold;font-style:italic;">B</span><span class="font64">(x </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">)B<sup>t</sup></span><span class="font64"> (x), where B is an unconstrained&#160;square matrix. One practical issue if the matrix is full rank is that computing the&#160;likelihood is expensive, with a d x </span><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"> matrix requiring </span><span class="font64" style="font-weight:bold;font-style:italic;">O</span><span class="font64"> (</span><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"><sup>3</sup>) computation for the&#160;determinant and inverse of £(x) (or equivalently, and more commonly done, its&#160;eigendecomposition or that of </span><span class="font64" style="font-weight:bold;font-variant:small-caps;">B(x)).</span></p>
<p><span class="font64">We often want to perform multimodal regression, that is, to predict real values that come from a conditional distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p(y</span><span class="font64"> | x) that can have several different&#160;peaks in y space for the same value of x. In this case, a Gaussian mixture is&#160;a natural representation for the output (Jacobs </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1991; Bishop, 1994).&#160;Neural networks with Gaussian mixtures as their output are often called </span><span class="font64" style="font-weight:bold;font-style:italic;">mixture&#160;density networks.</span><span class="font64"> A Gaussian mixture output with n components is defined by the&#160;conditional probability distribution</span></p><div>
<p><span class="font64">(6.35)</span></p></div>
<p><span class="font64">p(y | x) = &#160;&#160;&#160;p(c = i | x)N(y; M<sup>W</sup>(x), </span><span class="font64" style="font-weight:bold;font-style:italic;">£</span><span class="font64"> <sup>(i)</sup>(x)).</span></p>
<p><span class="font64">The neural network must have three outputs: a vector defining p(c = i | x), a matrix providing </span><span class="font64" style="font-weight:bold;font-style:italic;">f</span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(i)</sup>(x)</span><span class="font64"> for all i, and a tensor providing £<sup>(i)</sup>(x) for all i. These&#160;outputs must satisfy different constraints:</span></p>
<p><span class="font64">1. Mixture components p(c = i | x): these form a multinoulli distribution over the n different components associated with latent variable<sup>1</sup> c, and can</span></p>
<p><span class="font64"><sup>1</sup> We consider c to be latent because we do not observe it in the data: given input </span><span class="font11" style="font-weight:bold;">x </span><span class="font64">and target </span><span class="font11" style="font-weight:bold;">y</span><span class="font64">, it is not possible to know with certainty which Gaussian component was responsible for </span><span class="font11" style="font-weight:bold;">y</span><span class="font64">, but&#160;we can imagine that </span><span class="font11" style="font-weight:bold;">y </span><span class="font64">was generated by picking one of them, and make that unobserved choice a&#160;random variable.</span></p>
<p><span class="font64">typically be obtained by a softmax over an n-dimensional vector, to guarantee that these outputs are positive and sum to 1.</span></p>
<p><span class="font64">2. Means &#160;&#160;&#160;x): these indicate the center or mean associated with the i-th</span></p>
<p><span class="font64">Gaussian component, and are unconstrained (typically with no nonlinearity at all for these output units). If y is a d-vector, then the network must output&#160;an n </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> d matrix containing all </span><span class="font64" style="font-weight:bold;font-style:italic;">n</span><span class="font64"> of these d-dimensional vectors. Learning&#160;these means with maximum likelihood is slightly more complicated than&#160;learning the means of a distribution with only one output mode. We only&#160;want to update the mean for the component that actually produced the&#160;observation. In practice, we do not know which component produced each&#160;observation. The expression for the negative log-likelihood naturally weights&#160;each example’s contribution to the loss for each component by the probability&#160;that the component produced the example.</span></p>
<p><span class="font64">3. Covariances </span><span class="font64" style="font-weight:bold;font-style:italic;">£</span><span class="font64"> <sup>(i)</sup>(x): these specify the covariance matrix for each component&#160;i. As when learning a single Gaussian component, we typically use a diagonal&#160;matrix to avoid needing to compute determinants. As with learning the means&#160;of the mixture, maximum likelihood is complicated by needing to assign&#160;partial responsibility for each point to each mixture component. Gradient&#160;descent will automatically follow the correct process if given the correct&#160;specification of the negative log-likelihood under the mixture model.</span></p>
<p><span class="font64">It has been reported that gradient-based optimization of conditional Gaussian mixtures (on the output of neural networks) can be unreliable, in part because one&#160;gets divisions (by the variance) which can be numerically unstable (when some&#160;variance gets to be small for a particular example, yielding very large gradients).&#160;One solution is to </span><span class="font64" style="font-weight:bold;font-style:italic;">clip gradients</span><span class="font64"> (see Sec. 10.11.1) while another is to scale the&#160;gradients heuristically (Murray and Larochelle, 2014).</span></p>
<p><span class="font64">Gaussian mixture outputs are particularly effective in generative models of speech (Schuster, 1999) or movements of physical objects (Graves, 2013). The&#160;mixture density strategy gives a way for the network to represent multiple output&#160;modes and to control the variance of its output, which is crucial for obtaining&#160;a high degree of quality in these real-valued domains. An example of a mixture&#160;density network is shown in Fig. 6.4.</span></p>
<p><span class="font64">In general, we may wish to continue to model larger vectors y containing more variables, and to impose richer and richer structures on these output variables. For&#160;example, we may wish for our neural network to output a sequence of characters&#160;that forms a sentence. In these cases, we may continue to use the principle&#160;of maximum likelihood applied to our model p(y; w(x)), but the model we use</span></p><div><img src="main-54.jpg" alt=""/>
<p><span class="font64">Figure 6.4: Samples drawn from a neural network with a mixture density output layer. The input x is sampled from a uniform distribution and the output y is sampled from&#160;p<sub>mo</sub>d</span><span class="font19"><sub>e</sub>1</span><span class="font64"> (y | x). The neural network is able to learn nonlinear mappings from the input to&#160;the parameters of the output distribution. These parameters include the probabilities&#160;governing which of three mixture components will generate the output as well as the&#160;parameters for each mixture component. Each mixture component is Gaussian with&#160;predicted mean and variance. All of these aspects of the output distribution are able to&#160;vary with respect to the input x, and to do so in nonlinear ways.</span></p></div>
<p><span class="font64">to describe y becomes complex enough to be beyond the scope of this chapter. Chapter 10 describes how to use recurrent neural networks to define such models&#160;over sequences, and Part III describes advanced techniques for modeling arbitrary&#160;probability distributions.</span></p>
</body>
</html>