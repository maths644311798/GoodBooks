<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Bibliography</span></h2>
<p><span class="font64">Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfehow, I., Harp, A., Irving, G., Isard, M.,&#160;Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R.,&#160;Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I.,&#160;Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden,&#160;P., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. (2015). TensorFlow: Large-scale&#160;machine learning on heterogeneous systems. Software available from tensorflow.org. 25,&#160;212, 448</span></p>
<p><span class="font64">Ackley, D. H., Hinton, G. E., and Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. </span><span class="font64" style="font-style:italic;">Cognitive Science</span><span class="font64">, 9, 147-169. 572, 656</span></p>
<p><span class="font64">Alain, G. and Bengio, Y. (2013). What regularized auto-encoders learn from the data generating distribution. In </span><span class="font64" style="font-style:italic;">ICLR’2013, arXiv:1211.4246</span><span class="font64">. 509, 515, 523</span></p>
<p><span class="font64">Alain, G., Bengio, Y., Yao, L., Eric Thibodeau-Laufer, Yosinski, J., and Vincent, P. (2015). GSNs: Generative stochastic networks. arXiv:1503.05571. 512, 715</span></p>
<p><span class="font64">Anderson, E. (1935). The Irises of the Gaspe Peninsula. </span><span class="font64" style="font-style:italic;">Bulletin of the American Iris Society</span><span class="font64">, 59, 2-5. 21</span></p>
<p><span class="font64">Ba, J., Mnih, V., and Kavukcuoglu, K. (2014). Multiple object recognition with visual attention. </span><span class="font64" style="font-style:italic;">arXiv:1412.7755</span><span class="font64">. 693</span></p>
<p><span class="font64">Bachman, P. and Precup, D. (2015). Variational generative stochastic networks with collaborative shaping. In </span><span class="font64" style="font-style:italic;">Proceedings of the 32nd International Conference on Machine&#160;Learning, ICML 2015, Lille, France, 6-11 July 2015</span><span class="font64">, pages 1964-1972. 718</span></p>
<p><span class="font64">Bacon, P.-L., Bengio, E., Pineau, J., and Precup, D. (2015). Conditional computation in neural networks using a decision-theoretic approach. In </span><span class="font64" style="font-style:italic;">2nd Multidisciplinary Conference&#160;on Reinforcement Learning and Decision Making (RLDM 2015).</span><span class="font64"> 452</span></p>
<p><span class="font64">Bagnell, J. A. and Bradley, D. M. (2009). Differentiable sparse coding. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information&#160;Processing Systems 21 (NIPS’08)</span><span class="font64">, pages 113-120. 500</span></p>
<p><span class="font64">Bahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. In </span><span class="font64" style="font-style:italic;">ICLR’2015, arXiv:1f09.0473</span><span class="font64">. 25, 101, 398, 420, 421,&#160;467, 477</span></p>
<p><span class="font64">Bahl, L. R., Brown, P., de Souza, P. V., and Mercer, R. L. (1987). Speech recognition with continuous-parameter hidden Markov models. </span><span class="font64" style="font-style:italic;">Computer, Speech and Language</span><span class="font64">,2,&#160;219-234. 460</span></p>
<p><span class="font64">Baldi, P. and Hornik, K. (1989). Neural networks and principal component analysis: Learning from examples without local minima. </span><span class="font64" style="font-style:italic;">Neural Networks</span><span class="font64">, 2, 53-58. 286</span></p>
<p><span class="font64">Baldi, P., Brunak, S., Frasconi, P., Soda, G., and Pollastri, G. (1999). Exploiting the past and the future in protein secondary structure prediction. </span><span class="font64" style="font-style:italic;">Bioinformatics</span><span class="font64">, 15(11),&#160;937-946. 395</span></p>
<p><span class="font64">Baldi, P., Sadowski, P., and Whiteson, D. (2014). Searching for exotic particles in high-energy physics with deep learning. </span><span class="font64" style="font-style:italic;">Nature communications</span><span class="font64">, 5. 26</span></p>
<p><span class="font64">Ballard, D. H., Hinton, G. E., and Sejnowski, T. J. (1983). Parallel vision computation.</span></p>
<p><span class="font64" style="font-style:italic;">Nature</span><span class="font64">. 454</span></p>
<p><span class="font64">Barlow, H. B. (1989). Unsupervised learning. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 1, 295-311. 146</span></p>
<p><span class="font64">Barron, A. E. (1993). Universal approximation bounds for superpositions of a sigmoidal function. </span><span class="font64" style="font-style:italic;">IEEE Trans. on Information Theory</span><span class="font64">, 39, 930-945. 198</span></p>
<p><span class="font64">Bartholomew, D. J. (1987). </span><span class="font64" style="font-style:italic;">Latent variable models and factor analysis</span><span class="font64">. Oxford University Press. 492</span></p>
<p><span class="font64">Basilevsky, A. (1994). </span><span class="font64" style="font-style:italic;">Statistical Factor Analysis and Related Methods: Theory and Applications</span><span class="font64">. Wiley. 492</span></p>
<p><span class="font64">Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements.&#160;Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop. 25, 82, 212,&#160;222, 448</span></p>
<p><span class="font64">Basu, S. and Christensen, J. (2013). Teaching classification boundaries to humans. In </span><span class="font64" style="font-style:italic;">AAAI’2013</span><span class="font64">. 328</span></p>
<p><span class="font64">Baxter, J. (1995). Learning internal representations. In </span><span class="font64" style="font-style:italic;">Proceedings of the 8th International Conference on Computational Learning Theory (COLT’95),</span><span class="font64"> pages 311-320, Santa Cruz,&#160;California. ACM Press. 246</span></p>
<p><span class="font64">Bayer, J. and Osendorfer, C. (2014). Learning stochastic recurrent networks. </span><span class="font64" style="font-style:italic;">ArXiv e-prints</span><span class="font64">. 265</span></p>
<p><span class="font64">Becker, S. and Hinton, G. (1992). A self-organizing neural network that discovers surfaces in random-dot stereograms. </span><span class="font64" style="font-style:italic;">Nature</span><span class="font64">, 355, 161-163. 543</span></p>
<p><span class="font64">Behnke, S. (2001). Learning iterative image reconstruction in the neural abstraction pyramid. </span><span class="font64" style="font-style:italic;">Int. J. Computational Intelligence and Applications</span><span class="font64">, 1(4), 427-438. 517</span></p>
<p><span class="font64">Beiu, V., Quintana, J. M., and Avedillo, M. J. (2003). VLSI implementations of threshold logic-a comprehensive survey. </span><span class="font64" style="font-style:italic;">Neural Networks, IEEE Transactions on</span><span class="font64">, 14(5), 12171243. 453</span></p>
<p><span class="font64">Belkin, M. and Niyogi, P. (2002). Laplacian eigenmaps and spectral techniques for embedding and clustering. In T. Dietterich, S. Becker, and Z. Ghahramani, editors,&#160;</span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 14 (NIPS’01),</span><span class="font64"> Cambridge, MA.&#160;MIT Press. 244</span></p>
<p><span class="font64">Belkin, M. and Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and data representation. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 15(6), 1373-1396. 163, 520</span></p>
<p><span class="font64">Bengio, E., Bacon, P.-L., Pineau, J., and Precup, D. (2015a). Conditional computation in neural networks for faster models. arXiv:1511.06297. 452</span></p>
<p><span class="font64">Bengio, S. and Bengio, Y. (2000a). Taking on the curse of dimensionality in joint distributions using neural networks. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Neural Networks, special&#160;issue on Data Mining and Knowledge Discovery</span><span class="font64">, 11(3), 550-557. 709</span></p>
<p><span class="font64">Bengio, S., Vinyals, O., Jaitly, N., and Shazeer, N. (2015b). Scheduled sampling for sequence prediction with recurrent neural networks. Technical report, arXiv:1506.03099.&#160;384</span></p>
<p><span class="font64">Bengio, Y. (1991). </span><span class="font64" style="font-style:italic;">Artificial Neural Networks and their Application to Sequence Recognition</span><span class="font64">. Ph.D. thesis, McGill University, (Computer Science), Montreal, Canada. 408</span></p>
<p><span class="font64">Bengio, Y. (2000). Gradient-based optimization of hyperparameters. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 12(8), 1889-1900. 437</span></p>
<p><span class="font64">Bengio, Y. (2002). New distributed probabilistic language models. Technical Report 1215, Dept. IRO, Universite de Montreal. 469</span></p>
<p><span class="font64">Bengio, Y. (2009). </span><span class="font64" style="font-style:italic;">Learning deep architectures for AI</span><span class="font64">. Now Publishers. 200, 624</span></p>
<p><span class="font64">Bengio, Y. (2013). Deep learning of representations: looking forward. In </span><span class="font64" style="font-style:italic;">Statistical Language and Speech Processing</span><span class="font64">, volume 7978 of </span><span class="font64" style="font-style:italic;">Lecture Notes in Computer Science</span><span class="font64">,&#160;pages 1-37. Springer, also in arXiv at </span><a href="http://arxiv.org/abs/1305.0445"><span class="font64">http://arxiv.org/abs/1305.0445</span></a><span class="font64">. 450</span></p>
<p><span class="font64">Bengio, Y. (2015). Early inference in energy-based models approximates back-propagation. Technical Report arXiv:1510.02777, Universite de Montreal. 658</span></p>
<p><span class="font64">Bengio, Y. and Bengio, S. (2000b). Modeling high-dimensional discrete data with multilayer neural networks. In </span><span class="font64" style="font-style:italic;">NIPS 12</span><span class="font64">, pages 400-406. MIT Press. 707, 709, 710, 712</span></p>
<p><span class="font64">Bengio, Y. and Delalleau, O. (2009). Justifying and generalizing contrastive divergence. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 21(6), 1601-1621. 515, 613</span></p>
<p><span class="font64">Bengio, Y. and Grandvalet, Y. (2004). No unbiased estimator of the variance of k-fold cross-validation. In S. Thrun, L. Saul, and B. Scholkopf, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural&#160;Information Processing Systems 16 (NIPS’03),</span><span class="font64"> Cambridge, MA. MIT Press, Cambridge.&#160;122</span></p>
<p><span class="font64">Bengio, Y. and LeCun, Y. (2007). Scaling learning algorithms towards AI. In </span><span class="font64" style="font-style:italic;">Large Scale Kernel Machines</span><span class="font64">. 19</span></p>
<p><span class="font64">Bengio, Y. and Monperrus, M. (2005). Non-local manifold tangent learning. In L. Saul, Y. Weiss, and L. Bottou, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems&#160;17 (NIPS’04)</span><span class="font64">, pages 129-136. MIT Press. 159, 521</span></p>
<p><span class="font64">Bengio, Y. and Senecal, J.-S. (2003). Quick training of probabilistic neural nets by importance sampling. In </span><span class="font64" style="font-style:italic;">Proceedings of AISTATS 2003</span><span class="font64">. 472</span></p>
<p><span class="font64">Bengio, Y. and Senecal, J.-S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. </span><span class="font64" style="font-style:italic;">IEEE Trans. Neural Networks</span><span class="font64">,19 (4), 713-722.&#160;472</span></p>
<p><span class="font64">Bengio, Y., De Mori, R., Flammia, G., and Kompe, R. (1991). Phonetically motivated acoustic parameters for continuous speech recognition using artificial neural networks.&#160;In </span><span class="font64" style="font-style:italic;">Proceedings of EuroSpeech’91</span><span class="font64">. 27, 461</span></p>
<p><span class="font64">Bengio, Y., De Mori, R., Flammia, G., and Kompe, R. (1992). Neural network-Gaussian mixture hybrid for speech recognition or density estimation. In </span><span class="font64" style="font-style:italic;">NIPS 4</span><span class="font64">, pages 175-182.&#160;Morgan Kaufmann. 461</span></p>
<p><span class="font64">Bengio, Y., Frasconi, P., and Simard, P. (1993). The problem of learning long-term dependencies in recurrent networks. In </span><span class="font64" style="font-style:italic;">IEEE International Conference on Neural&#160;Networks</span><span class="font64">, pages 1183-1195, San Francisco. IEEE Press. (invited paper). 404</span></p>
<p><span class="font64">Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. </span><span class="font64" style="font-style:italic;">IEEE Tr. Neural Nets</span><span class="font64">. 18, 402, 404, 405, 413</span></p>
<p><span class="font64">Bengio, Y., Latendresse, S., and Dugas, C. (1999). Gradient-based learning of hyperparameters. Learning Conference, Snowbird. 437</span></p>
<p><span class="font64">Bengio, Y., Ducharme, R., and Vincent, P. (2001). A neural probabilistic language model. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, </span><span class="font64" style="font-style:italic;">NIPS’2000</span><span class="font64">, pages 932-938. MIT&#160;Press. 18, 449, 465, 468, 474, 479, 484</span></p>
<p><span class="font64">Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A neural probabilistic language model. </span><span class="font64" style="font-style:italic;">JMLR,</span><span class="font64"> 3, 1137-1155. 468, 474</span></p>
<p><span class="font64">Bengio, Y., Le Roux, N., Vincent, P., Delalleau, O., and Marcotte, P. (2006a). Convex neural networks. In </span><span class="font64" style="font-style:italic;">NIPS’2005</span><span class="font64">, pages 123-130. 258</span></p>
<p><span class="font64">Bengio, Y., Delalleau, O., and Le Roux, N. (2006b). The curse of highly variable functions for local kernel machines. In </span><span class="font64" style="font-style:italic;">NIPS’2005</span><span class="font64">. 157</span></p>
<p><span class="font64">Bengio, Y., Larochelle, H., and Vincent, P. (2006c). Non-local manifold Parzen windows. In </span><span class="font64" style="font-style:italic;">NIPS’2005</span><span class="font64">. MIT Press. 159, 522</span></p>
<p><span class="font64">Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy layer-wise training of deep networks. In </span><span class="font64" style="font-style:italic;">NIPS’2006</span><span class="font64">. 14, 19, 200, 323, 324, 530, 532</span></p>
<p><span class="font64">Bengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In </span><span class="font64" style="font-style:italic;">ICML’09</span><span class="font64">. 328</span></p>
<p><span class="font64">Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013a). Better mixing via deep representations. In </span><span class="font64" style="font-style:italic;">ICML’2013</span><span class="font64">. 606</span></p>
<p><span class="font64">Bengio, Y., Leonard, N., and Courville, A. (2013b). Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv:1308.3432. 450, 452,&#160;691, 693</span></p>
<p><span class="font64">Bengio, Y., Yao, L., Alain, G., and Vincent, P. (2013c). Generalized denoising autoencoders as generative models. In </span><span class="font64" style="font-style:italic;">NIPS’2013</span><span class="font64">. 509, 713, 715</span></p>
<p><span class="font64">Bengio, Y., Courville, A., and Vincent, P. (2013d). Representation learning: A review and new perspectives. </span><span class="font64" style="font-style:italic;">IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI),&#160;</span><span class="font64">35(8), 1798-1828. 557</span></p>
<p><span class="font64">Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014). Deep generative stochastic networks trainable by backprop. In </span><span class="font64" style="font-style:italic;">ICML’2014</span><span class="font64">. 713, 714, 715, 716, 717</span></p>
<p><span class="font64">Bennett, C. (1976). Efficient estimation of free energy differences from Monte Carlo data. </span><span class="font64" style="font-style:italic;">Journal of Computational Physics</span><span class="font64">, 22(2), 245-268. 630</span></p>
<p><span class="font64">Bennett, J. and Lanning, S. (2007). The Netflix prize. 481</span></p>
<p><span class="font64">Berger, A. L., Della Pietra, V. J., and Della Pietra, S. A. (1996). A maximum entropy approach to natural language processing. </span><span class="font64" style="font-style:italic;">Computational Linguistics</span><span class="font64">, 22, 39-71. 475</span></p>
<p><span class="font64">Berglund, M. and Raiko, T. (2013). Stochastic gradient estimate variance in contrastive divergence and persistent contrastive divergence. </span><span class="font64" style="font-style:italic;">CoRR,</span><span class="font64"> abs/1312.6002. 616</span></p>
<p><span class="font64">Bergstra, J. (2011). </span><span class="font64" style="font-style:italic;">Incorporating Complex Cells into Neural Networks for Pattern Classification</span><span class="font64">. Ph.D. thesis, Universite de Montreal. 255</span></p>
<p><span class="font64">Bergstra, J. and Bengio, Y. (2009). Slow, decorrelated features for pretraining complex cell-like networks. In </span><span class="font64" style="font-style:italic;">NIPS’2009</span><span class="font64">. 496</span></p>
<p><span class="font64">Bergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization. </span><span class="font64" style="font-style:italic;">J. Machine Learning Res.,</span><span class="font64"> 13, 281-305. 436, 437</span></p>
<p><span class="font64">Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression&#160;compiler. In </span><span class="font64" style="font-style:italic;">Proc. SciPy</span><span class="font64">. 25, 82, 212, 222, 448</span></p>
<p><span class="font64">Bergstra, J., Bardenet, R., Bengio, Y., and Kegl, B. (2011). Algorithms for hyper-parameter optimization. In </span><span class="font64" style="font-style:italic;">NIPS’2011</span><span class="font64">. 438</span></p>
<p><span class="font64">Berkes, P. and Wiskott, L. (2005). Slow feature analysis yields a rich repertoire of complex cell properties. </span><span class="font64" style="font-style:italic;">Journal of Vision</span><span class="font64">, 5(6), 579-602. 497</span></p>
<p><span class="font64">Bertsekas, D. P. and Tsitsiklis, J. (1996). </span><span class="font64" style="font-style:italic;">Neuro-Dynamic Programming</span><span class="font64">. Athena Scientific. 106</span></p>
<p><span class="font64">Besag, J. (1975). Statistical analysis of non-lattice data. </span><span class="font64" style="font-style:italic;">The Statistician</span><span class="font64">, 24(3), 179-195. 617</span></p>
<p><span class="font64">Bishop, C. M. (1994). Mixture density networks. 188</span></p>
<p><span class="font64">Bishop, C. M. (1995a). Regularization and complexity control in feed-forward networks. In </span><span class="font64" style="font-style:italic;">Proceedings International Conference on Artificial Neural Networks ICANN’95</span><span class="font64">,&#160;volume 1, page 141-148. 242, 250</span></p>
<p><span class="font64">Bishop, C. M. (1995b). Training with noise is equivalent to Tikhonov regularization. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 7(1), 108-116. 242</span></p>
<p><span class="font64">Bishop, C. M. (2006). </span><span class="font64" style="font-style:italic;">Pattern Recognition and Machine Learning</span><span class="font64">. Springer. 98, 145</span></p>
<p><span class="font64">Blum, A. L. and Rivest, R. L. (1992). Training a 3-node neural network is NP-complete. 293</span></p>
<p><span class="font64">Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. (1989). Learnability and the Vapnik-Chervonenkis dimension. </span><span class="font64" style="font-style:italic;">Journal of the ACM</span><span class="font64">, 36(4), 929—865. 114</span></p>
<p><span class="font64">Bonnet, G. (1964). Transformations des signaux aleatoires a travers les systemes non lineaires sans memoire. </span><span class="font64" style="font-style:italic;">Annales des Telecommunications</span><span class="font64">, 19(9-10), 203-220. 691</span></p>
<p><span class="font64">Bordes, A., Weston, J., Collobert, R., and Bengio, Y. (2011). Learning structured embeddings of knowledge bases. In </span><span class="font64" style="font-style:italic;">AAAI 2011</span><span class="font64">. 486</span></p>
<p><span class="font64">Bordes, A., Glorot, X., Weston, J., and Bengio, Y. (2012). Joint learning of words and meaning representations for open-text semantic parsing. </span><span class="font64" style="font-style:italic;">AISTATS’2012</span><span class="font64">. 402, 486</span></p>
<p><span class="font64">Bordes, A., Glorot, X., Weston, J., and Bengio, Y. (2013a). A semantic matching energy function for learning with multi-relational data. </span><span class="font64" style="font-style:italic;">Machine Learning: Special Issue on&#160;Learning Semantics</span><span class="font64">. 485</span></p>
<p><span class="font64">Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O. (2013b). Translating embeddings for modeling multi-relational data. In C. Burges, L. Bottou,&#160;M. Welling, Z. Ghahramani, and K. Weinberger, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information&#160;Processing Systems 26</span><span class="font64">, pages 2787-2795. Curran Associates, Inc. 486</span></p>
<p><span class="font64">Bornschein, J. and Bengio, Y. (2015). Reweighted wake-sleep. In </span><span class="font64" style="font-style:italic;">ICLR’2015, arXiv:1406.2751</span><span class="font64">. 695</span></p>
<p><span class="font64">Bornschein, J., Shabanian, S., Fischer, A., and Bengio, Y. (2015). Training bidirectional Helmholtz machines. Technical report, arXiv:1506.03877. 695</span></p>
<p><span class="font64">Boser, B. E., Guyon, I. M., and Vapnik, V. N. (1992). A training algorithm for optimal margin classifiers. In </span><span class="font64" style="font-style:italic;">COLT ’92: Proceedings of the fifth annual workshop on Computational learning theory</span><span class="font64">, pages 144-152, New York, NY, USA. ACM. 18, 140</span></p>
<p><span class="font64">Bottou, L. (1998). Online algorithms and stochastic approximations. In D. Saad, editor, </span><span class="font64" style="font-style:italic;">Online Learning in Neural Networks</span><span class="font64">. Cambridge University Press, Cambridge, UK. 296</span></p>
<p><span class="font64">Bottou, L. (2011). From machine learning to machine reasoning. Technical report, arXiv.1102.1808. 400, 402</span></p>
<p><span class="font64">Bottou, L. (2015). Multilayer neural networks. Deep Learning Summer School. 442</span></p>
<p><span class="font64">Bottou, L. and Bousquet, O. (2008). The tradeoffs of large scale learning. In </span><span class="font64" style="font-style:italic;">NIPS’2008</span><span class="font64">. 282, 295</span></p>
<p><span class="font64">Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012). Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation&#160;and transcription. In </span><span class="font64" style="font-style:italic;">ICML’12</span><span class="font64">. 687, 688</span></p>
<p><span class="font64">Boureau, Y., Ponce, J., and LeCun, Y. (2010). A theoretical analysis of feature pooling in vision algorithms. In </span><span class="font64" style="font-style:italic;">Proc. International Conference on Machine learning (ICML’10)</span><span class="font64">.&#160;345</span></p>
<p><span class="font64">Boureau, Y., Le Roux, N., Bach, F., Ponce, J., and LeCun, Y. (2011). Ask the locals: multi-way local pooling for image recognition. In </span><span class="font64" style="font-style:italic;">Proc. International Conference on&#160;Computer Vision (ICCV’11).</span><span class="font64"> IEEE. 345</span></p>
<p><span class="font64">Bourlard, H. and Kamp, Y. (1988). Auto-association by multilayer perceptrons and singular value decomposition. </span><span class="font64" style="font-style:italic;">Biological Cybernetics</span><span class="font64">, 59, 291-294. 504</span></p>
<p><span class="font64">Bourlard, H. and Wellekens, C. (1989). Speech pattern discrimination and multi-layered perceptrons. </span><span class="font64" style="font-style:italic;">Computer Speech and Language</span><span class="font64">, 3, 1-19. 461</span></p>
<p><span class="font64">Boyd, S. and Vandenberghe, L. (2004). </span><span class="font64" style="font-style:italic;">Convex Optimization</span><span class="font64">. Cambridge University Press, New York, NY, USA. 93</span></p>
<p><span class="font64">Brady, M. L., Raghavan, R., and Slawny, J. (1989). Back-propagation fails to separate where perceptrons succeed. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Circuits and Systems</span><span class="font64">, 36, 665-674.&#160;284</span></p>
<p><span class="font64">Brakel, P., Stroobandt, D., and Schrauwen, B. (2013). Training energy-based models for time-series imputation. </span><span class="font64" style="font-style:italic;">Journal of Machine Learning Research</span><span class="font64">, 14, 2771-2797. 676,&#160;700</span></p>
<p><span class="font64">Brand, M. (2003). Charting a manifold. In </span><span class="font64" style="font-style:italic;">NIPS’2002</span><span class="font64">, pages 961-968. MIT Press. 163, 520</span></p>
<p><span class="font64">Breiman, L. (1994). Bagging predictors. </span><span class="font64" style="font-style:italic;">Machine Learning</span><span class="font64">, 24(2), 123-140. 256</span></p>
<p><span class="font64">Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. (1984). </span><span class="font64" style="font-style:italic;">Classification and Regression Trees</span><span class="font64">. Wadsworth International Group, Belmont, CA. 145</span></p>
<p><span class="font64">Bridle, J. S. (1990). Alphanets: a recurrent ‘neural’ network architecture with a hidden Markov model interpretation. </span><span class="font64" style="font-style:italic;">Speech Communication</span><span class="font64">, 9(1), 83-92. 185</span></p>
<p><span class="font64">Briggman, K., Denk, W., Seung, S., Helmstaedter, M. N., and Turaga, S. C. (2009). Maximin affinity learning of image segmentation. In </span><span class="font64" style="font-style:italic;">NIPS’2009</span><span class="font64">, pages 1865-1873. 359</span></p>
<p><span class="font64">Brown, P. F., Cocke, J., Pietra, S. A. D., Pietra, V. J. D., Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. (1990). A statistical approach to machine translation.&#160;</span><span class="font64" style="font-style:italic;">Computational linguistics</span><span class="font64">, 16(2), 79-85. 21</span></p>
<p><span class="font64">Brown, P. F., Pietra, V. J. D., DeSouza, P. V., Lai, J. C., and Mercer, R. L. (1992). Class-based n-gram models of natural language. </span><span class="font64" style="font-style:italic;">Computational Linguistics</span><span class="font64">, 18, 467-479. 465</span></p>
<p><span class="font64">Bryson, A. and Ho, Y. (1969). </span><span class="font64" style="font-style:italic;">Applied optimal control: optimization, estimation, and control</span><span class="font64">. Blaisdell Pub. Co. 224</span></p>
<p><span class="font64">Bryson, Jr., A. E. and Denham, W. F. (1961). A steepest-ascent method for solving optimum programming problems. Technical Report BR-1303, Raytheon Company,&#160;Missle and Space Division. 224</span></p>
<p><span class="font64">Bucilua, C., Caruana, R., and Niculescu-Mizil, A. (2006). Model compression. In </span><span class="font64" style="font-style:italic;">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery&#160;and data mining</span><span class="font64">, pages 535-541. ACM. 450</span></p>
<p><span class="font64">Burda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1509.00519</span><span class="font64">. 700</span></p>
<p><span class="font64">Cai, M., Shi, Y., and Liu, J. (2013). Deep maxout neural networks for speech recognition. In </span><span class="font64" style="font-style:italic;">Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop&#160;on</span><span class="font64">, pages 291-296. IEEE. 193</span></p>
<p><span class="font64">Carreira-Perpinan, M. A. and Hinton, G. E. (2005). On contrastive divergence learning. In R. G. Cowell and Z. Ghahramani, editors, </span><span class="font64" style="font-style:italic;">Proceedings of the Tenth International&#160;Workshop on Artificial Intelligence and Statistics (AISTATS’05),</span><span class="font64"> pages 33-40. Society&#160;for Artificial Intelligence and Statistics. 613</span></p>
<p><span class="font64">Caruana, R. (1993). Multitask connectionist learning. In </span><span class="font64" style="font-style:italic;">Proc. 1993 Connectionist Models Summer School</span><span class="font64">, pages 372-379. 244</span></p>
<p><span class="font64">Cauchy, A. (1847). Methode generale pour la resolution de systemes d’equations simul-tanees. In </span><span class="font64" style="font-style:italic;">Compte rendu des seances de l’academie des sciences</span><span class="font64">, pages 536-538. 83, 224</span></p>
<p><span class="font64">Cayton, L. (2005). Algorithms for manifold learning. Technical Report CS2008-0923, UCSD. 163</span></p>
<p><span class="font64">Chandola, V., Banerjee, A., and Kumar, V. (2009). Anomaly detection: A survey. </span><span class="font64" style="font-style:italic;">ACM computing surveys (CSUR),</span><span class="font64"> 41(3), 15. 102</span></p>
<p><span class="font64">Chapelle, O., Weston, J., and Scholkopf, B. (2003). Cluster kernels for semi-supervised learning. In S. Becker, S. Thrun, and K. Obermayer, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural&#160;Information Processing Systems 15 (NIPS’02),</span><span class="font64"> pages 585-592, Cambridge, MA. MIT&#160;Press. 244</span></p>
<p><span class="font64">Chapelle, O., Scholkopf, B., and Zien, A., editors (2006). </span><span class="font64" style="font-style:italic;">Semi-Supervised Learning</span><span class="font64">. MIT Press, Cambridge, MA. 244, 543</span></p>
<p><span class="font64">Chellapilla, K., Puri, S., and Simard, P. (2006). High Performance Convolutional Neural Networks for Document Processing. In Guy Lorette, editor, </span><span class="font64" style="font-style:italic;">Tenth International&#160;Workshop on Frontiers in Handwriting Recognition</span><span class="font64">, La Baule (France). Universite de&#160;Rennes 1, Suvisoft. </span><a href="http://www.suvisoft.com"><span class="font64">http://www.suvisoft.com</span></a><span class="font64">. 24, 27, 447</span></p>
<p><span class="font64">Chen, B., Ting, J.-A., Marlin, B. M., and de Freitas, N. (2010). Deep learning of invariant spatio-temporal features from video. NIPS*2010 Deep Learning and Unsupervised&#160;Feature Learning Workshop. 360</span></p>
<p><span class="font64">Chen, S. F. and Goodman, J. T. (1999). An empirical study of smoothing techniques for language modeling. </span><span class="font64" style="font-style:italic;">Computer, Speech and Language</span><span class="font64">, 13(4), 359-393. 464, 475</span></p>
<p><span class="font64">Chen, T., Du, Z., Sun, N., Wang, J., Wu, C., Chen, Y., and Temam, O. (2014a). DianNao: A small-footprint high-throughput accelerator for ubiquitous machine-learning. In </span><span class="font64" style="font-style:italic;">Proceedings of the 19th international conference on Architectural support for programming&#160;languages and operating systems</span><span class="font64">, pages 269-284. ACM. 453</span></p>
<p><span class="font64">Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang, C., and Zhang, Z. (2015). MXNet: A flexible and efficient machine learning library for&#160;heterogeneous distributed systems. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1512.01274</span><span class="font64">. 25</span></p>
<p><span class="font64">Chen, Y., Luo, T., Liu, S., Zhang, S., He, L., Wang, J., Li, L., Chen, T., Xu, Z., Sun, N., </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2014b). DaDianNao: A machine-learning supercomputer. In </span><span class="font64" style="font-style:italic;">Microarchitecture&#160;(MICRO), 2014 47th Annual IEEE/ACM International Symposium on</span><span class="font64">, pages 609-622.&#160;IEEE. 453</span></p>
<p><span class="font64">Chilimbi, T., Suzue, Y., Apacible, J., and Kalyanaraman, K. (2014). Project Adam: Building an efficient and scalable deep learning training system. In </span><span class="font64" style="font-style:italic;">11th USENIX&#160;Symposium on Operating Systems Design and Implementation (OSDI’14)</span><span class="font64">. 449</span></p>
<p><span class="font64">Cho, K., Raiko, T., and Ilin, A. (2010). Parallel tempering is efficient for learning restricted Boltzmann machines. In </span><span class="font64" style="font-style:italic;">IJCNN’2010</span><span class="font64">. 605, 616</span></p>
<p><span class="font64">Cho, K., Raiko, T., and Ilin, A. (2011). Enhanced gradient and adaptive learning rate for training restricted Boltzmann machines. In </span><span class="font64" style="font-style:italic;">ICML’2011</span><span class="font64">, pages 105-112. 676</span></p>
<p><span class="font64">Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y. (2014a). Learning phrase representations using RNN encoder-decoder for statistical&#160;machine translation. In </span><span class="font64" style="font-style:italic;">Proceedings of the Empiricial Methods in Natural Language&#160;Processing (EMNLP 2014).</span><span class="font64"> 396, 476, 477</span></p>
<p><span class="font64">Cho, K., Van Merrienboer, B., Bahdanau, D., and Bengio, Y. (2014b). On the properties of neural machine translation: Encoder-decoder approaches. </span><span class="font64" style="font-style:italic;">ArXiv e-prints</span><span class="font64">, abs/1409.1259. 413</span></p>
<p><span class="font64">Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B., and LeCun, Y. (2014). The loss surface of multilayer networks. 285, 286</span></p>
<p><span class="font64">Chorowski, J., Bahdanau, D., Cho, K., and Bengio, Y. (2014). End-to-end continuous speech recognition using attention-based recurrent NN: First results. arXiv:1412.1602.&#160;462</span></p>
<p><span class="font64">Christianson, B. (1992). Automatic Hessians by reverse accumulation. </span><span class="font64" style="font-style:italic;">IMA Journal of Numerical Analysis</span><span class="font64">, 12(2), 135-150. 224</span></p>
<p><span class="font64">Chrupala, G., Kadar, A., and Alishahi, A. (2015). Learning language through pictures. arXiv 1506.03694. 413</span></p>
<p><span class="font64">Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. NIPS’2014 Deep Learning workshop,&#160;arXiv 1412.3555. 413, 462</span></p>
<p><span class="font64">Chung, J., Gulgehre, Q., Cho, K., and Bengio, Y. (2015a). Gated feedback recurrent neural networks. In </span><span class="font64" style="font-style:italic;">ICML’15</span><span class="font64">. 413</span></p>
<p><span class="font64">Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A., and Bengio, Y. (2015b). A recurrent latent variable model for sequential data. In </span><span class="font64" style="font-style:italic;">NIPS’2015</span><span class="font64">. 700</span></p>
<p><span class="font64">Ciresan, D., Meier, U., Masci, J., and Schmidhuber, J. (2012). Multi-column deep neural network for traffic sign classification. </span><span class="font64" style="font-style:italic;">Neural Networks</span><span class="font64">, 32, 333-338. 23, 200</span></p>
<p><span class="font64">Ciresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber, J. (2010). Deep big simple neural nets for handwritten digit recognition. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 22, 1-14.&#160;24, 27, 448</span></p>
<p><span class="font64">Coates, A. and Ng, A. Y. (2011). The importance of encoding versus training with sparse coding and vector quantization. In </span><span class="font64" style="font-style:italic;">ICML’2011</span><span class="font64">. 27, 256, 500</span></p>
<p><span class="font64">Coates, A., Lee, H., and Ng, A. Y. (2011). An analysis of single-layer networks in unsupervised feature learning. In </span><span class="font64" style="font-style:italic;">Proceedings of the Thirteenth International Conference&#160;on Artificial Intelligence and Statistics (AISTATS 2011).</span><span class="font64"> 363, 364, 457</span></p>
<p><span class="font64">Coates, A., Huval, B., Wang, T., Wu, D., Catanzaro, B., and Andrew, N. (2013). Deep learning with COTS HPC systems. In S. Dasgupta and D. McAllester, editors,&#160;</span><span class="font64" style="font-style:italic;">Proceedings of the 30th International Conference on Machine Learning (ICML-13),&#160;</span><span class="font64">volume 28 (3), pages 1337-1345. JMLR Workshop and Conference Proceedings. 24, 27,&#160;364, 449</span></p>
<p><span class="font64">Cohen, N., Sharir, O., and Shashua, A. (2015). On the expressive power of deep learning: A tensor analysis. arXiv:1509.05009. 556</span></p>
<p><span class="font64">Collobert, R. (2004). </span><span class="font64" style="font-style:italic;">Large Scale Machine Learning</span><span class="font64">. Ph.D. thesis, Universite de Paris VI, LIP6. 196</span></p>
<p><span class="font64">Collobert, R. (2011). Deep learning for efficient discriminative parsing. In </span><span class="font64" style="font-style:italic;">AISTATS’2011</span><span class="font64">. 101, 479</span></p>
<p><span class="font64">Collobert, R. and Weston, J. (2008a). A unified architecture for natural language processing: Deep neural networks with multitask learning. In </span><span class="font64" style="font-style:italic;">ICML’2008</span><span class="font64">. 473, 479</span></p>
<p><span class="font64">Collobert, R. and Weston, J. (2008b). A unified architecture for natural language processing: Deep neural networks with multitask learning. In </span><span class="font64" style="font-style:italic;">ICML’2008</span><span class="font64">. 537</span></p>
<p><span class="font64">Collobert, R., Bengio, S., and Bengio, Y. (2001). A parallel mixture of SVMs for very large scale problems. Technical Report IDIAP-RR-01-12, IDIAP. 452</span></p>
<p><span class="font64">Collobert, R., Bengio, S., and Bengio, Y. (2002). Parallel mixture of SVMs for very large scale problems. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 14(5), 1105-1114. 452</span></p>
<p><span class="font64">Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011a). Natural language processing (almost) from scratch. </span><span class="font64" style="font-style:italic;">The Journal of Machine Learning&#160;Research</span><span class="font64">, 12, 2493-2537. 328, 479, 537, 538</span></p>
<p><span class="font64">Collobert, R., Kavukcuoglu, K., and Farabet, C. (2011b). Torch7: A Matlab-like environment for machine learning. In </span><span class="font64" style="font-style:italic;">BigLearn, NIPS Workshop</span><span class="font64">. 25, 210, 448</span></p>
<p><span class="font64">Comon, P. (1994). Independent component analysis - a new concept? </span><span class="font64" style="font-style:italic;">Signal Processing</span><span class="font64">, 36, 287-314. 493</span></p>
<p><span class="font64">Cortes, C. and Vapnik, V. (1995). Support vector networks. </span><span class="font64" style="font-style:italic;">Machine Learning</span><span class="font64">, 20, 273-297. 18, 140</span></p>
<p><span class="font64">Couprie, C., Farabet, C., Najman, L., and LeCun, Y. (2013). Indoor semantic segmentation using depth information. In </span><span class="font64" style="font-style:italic;">International Conference on Learning Representations&#160;(ICLR2013).</span><span class="font64"> 23, 200</span></p>
<p><span class="font64">Courbariaux, M., Bengio, Y., and David, J.-P. (2015). Low precision arithmetic for deep learning. In </span><span class="font64" style="font-style:italic;">Arxiv:1f12.7024, ICLR’2015 Workshop</span><span class="font64">. 454</span></p>
<p><span class="font64">Courville, A., Bergstra, J., and Bengio, Y. (2011). Unsupervised models of images by spike-and-slab RBMs. In </span><span class="font64" style="font-style:italic;">ICML’11</span><span class="font64">. 563, 683</span></p>
<p><span class="font64">Courville, A., Desjardins, G., Bergstra, J., and Bengio, Y. (2014). The spike-and-slab RBM and extensions to discrete and sparse data distributions. </span><span class="font64" style="font-style:italic;">Pattern Analysis and&#160;Machine Intelligence, IEEE Transactions on</span><span class="font64">, 36(9), 1874-1887. 684</span></p>
<p><span class="font64">Cover, T. M. and Thomas, J. A. (2006). </span><span class="font64" style="font-style:italic;">Elements of Information Theory, 2nd Edition</span><span class="font64">. Wiley-Interscience. 73</span></p>
<p><span class="font64">Cox, D. and Pinto, N. (2011). Beyond simple features: A large-scale feature search approach to unconstrained face recognition. In </span><span class="font64" style="font-style:italic;">Automatic Face &amp; Gesture Recognition&#160;and Workshops (FG 2011), 2011 IEEE International Conference on</span><span class="font64">, pages 8-15. IEEE.&#160;363</span></p>
<p><span class="font64">Cramer, H. (1946). </span><span class="font64" style="font-style:italic;">Mathematical methods of statistics</span><span class="font64">. Princeton University Press. 135, 295</span></p>
<p><span class="font64">Crick, F. H. C. and Mitchison, G. (1983). The function of dream sleep. </span><span class="font64" style="font-style:italic;">Nature</span><span class="font64">,304, 111-114. 611</span></p>
<p><span class="font64">Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. </span><span class="font64" style="font-style:italic;">Mathematics of Control, Signals, and Systems</span><span class="font64">, 2, 303-314. 197</span></p>
<p><span class="font64">Dahl, G. E., Ranzato, M., Mohamed, A., and Hinton, G. E. (2010). Phone recognition with the mean-covariance restricted Boltzmann machine. In </span><span class="font64" style="font-style:italic;">NIPS’2010</span><span class="font64">. 23</span></p>
<p><span class="font64">Dahl, G. E., Yu, D., Deng, L., and Acero, A. (2012). Context-dependent pre-trained deep neural networks for large vocabulary speech recognition. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Audio,&#160;Speech, and Language Processing</span><span class="font64">, 20(1), 33-42. 461</span></p>
<p><span class="font64">Dahl, G. E., Sainath, T. N., and Hinton, G. E. (2013). Improving deep neural networks for LVCSR using rectified linear units and dropout. In </span><span class="font64" style="font-style:italic;">ICASSP’2013</span><span class="font64">. 461</span></p>
<p><span class="font64">Dahl, G. E., Jaitly, N., and Salakhutdinov, R. (2014). Multi-task neural networks for QSAR predictions. arXiv:1406.1231. 26</span></p>
<p><span class="font64">Dauphin, Y. and Bengio, Y. (2013). Stochastic ratio matching of RBMs for sparse high-dimensional inputs. In </span><span class="font64" style="font-style:italic;">NIPS26</span><span class="font64">. NIPS Foundation. 621</span></p>
<p><span class="font64">Dauphin, Y., Glorot, X., and Bengio, Y. (2011). Large-scale learning of embeddings with reconstruction sampling. In </span><span class="font64" style="font-style:italic;">ICML’2011</span><span class="font64">. 473</span></p>
<p><span class="font64">Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex&#160;optimization. In </span><span class="font64" style="font-style:italic;">NIPS’2014</span><span class="font64">. 285, 286, 288</span></p>
<p><span class="font64">Davis, A., Rubinstein, M., Wadhwa, N., Mysore, G., Durand, F., and Freeman, W. T. (2014). The visual microphone: Passive recovery of sound from video. </span><span class="font64" style="font-style:italic;">ACM Transactions&#160;on Graphics (Proc. SIGGRAPH),</span><span class="font64"> 33(4), 79:1-79:10. 454</span></p>
<p><span class="font64">Dayan, P. (1990). Reinforcement comparison. In </span><span class="font64" style="font-style:italic;">Connectionist Models: Proceedings of the 1990 Connectionist Summer School</span><span class="font64">, San Mateo, CA. 693</span></p>
<p><span class="font64">Dayan, P. and Hinton, G. E. (1996). Varieties of Helmholtz machine. </span><span class="font64" style="font-style:italic;">Neural Networks</span><span class="font64">, 9(8), 1385-1403. 695</span></p>
<p><span class="font64">Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. (1995). The Helmholtz machine. </span><span class="font64" style="font-style:italic;">Neural computation</span><span class="font64">, 7(5), 889-904. 695</span></p>
<p><span class="font64">Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., and Ng, A. Y. (2012). Large scale distributed deep&#160;networks. In </span><span class="font64" style="font-style:italic;">NIPS’2012</span><span class="font64">. 25, 449</span></p>
<p><span class="font64">Dean, T. and Kanazawa, K. (1989). A model for reasoning about persistence and causation. </span><span class="font64" style="font-style:italic;">Computational Intelligence</span><span class="font64">, 5(3), 142-150. 664</span></p>
<p><span class="font64">Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990). Indexing by latent semantic analysis. </span><span class="font64" style="font-style:italic;">Journal of the American Society for Information&#160;Science</span><span class="font64">, 41(6), 391-407. 478, 484</span></p>
<p><span class="font64">Delalleau, O. and Bengio, Y. (2011). Shallow vs. deep sum-product networks. In </span><span class="font64" style="font-style:italic;">NIPS</span><span class="font64">. 19, 556</span></p>
<p><span class="font64">Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A Large-Scale Hierarchical Image Database. In </span><span class="font64" style="font-style:italic;">CVPR09</span><span class="font64">. 21</span></p>
<p><span class="font64">Deng, J., Berg, A. C., Li, K., and Fei-Fei, L. (2010a). What does classifying more than 10,000 image categories tell us? In </span><span class="font64" style="font-style:italic;">Proceedings of the 11th European Conference on&#160;Computer Vision: Part V</span><span class="font64">, ECCV’10, pages 71-84, Berlin, Heidelberg. Springer-Verlag.&#160;21</span></p>
<p><span class="font64">Deng, L. and Yu, D. (2014). Deep learning - methods and applications. </span><span class="font64" style="font-style:italic;">Foundations and Trends in Signal Processing</span><span class="font64">. 462</span></p>
<p><span class="font64">Deng, L., Seltzer, M., Yu, D., Acero, A., Mohamed, A., and Hinton, G. (2010b). Binary coding of speech spectrograms using a deep auto-encoder. In </span><span class="font64" style="font-style:italic;">Interspeech 2010</span><span class="font64">, Makuhari,&#160;Chiba, Japan. 23</span></p>
<p><span class="font64">Denil, M., Bazzani, L., Larochelle, H., and de Freitas, N. (2012). Learning where to attend with deep architectures for image tracking. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">,24(8), 2151-2184. 367</span></p>
<p><span class="font64">Denton, E., Chintala, S., Szlam, A., and Fergus, R. (2015). Deep generative image models using a Laplacian pyramid of adversarial networks. </span><span class="font64" style="font-style:italic;">NIPS</span><span class="font64">. 703, 704, 720</span></p>
<p><span class="font64">Desjardins, G. and Bengio, Y. (2008). Empirical evaluation of convolutional RBMs for vision. Technical Report 1327, Departement d’Informatique et de Recherche Opera-tionnelle, Universite de Montreal. 685</span></p>
<p><span class="font64">Desjardins, G., Courville, A. C., Bengio, Y., Vincent, P., and Delalleau, O. (2010). Tempered Markov chain Monte Carlo for training of restricted Boltzmann machines. In&#160;</span><span class="font64" style="font-style:italic;">International Conference on Artificial Intelligence and Statistics</span><span class="font64">, pages 145-152. 605,&#160;616</span></p>
<p><span class="font64">Desjardins, G., Courville, A., and Bengio, Y. (2011). On tracking the partition function. In </span><span class="font64" style="font-style:italic;">NIPS’2011</span><span class="font64">. 631</span></p>
<p><span class="font64">Desjardins, G., Simonyan, K., Pascanu, R., </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2015). Natural neural networks. In </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems</span><span class="font64">, pages 2062-2070. 320</span></p>
<p><span class="font64">Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. In </span><span class="font64" style="font-style:italic;">Proc.&#160;ACL’2014</span><span class="font64">. 475</span></p>
<p><span class="font64">Devroye, L. (2013). </span><span class="font64" style="font-style:italic;">Non-Uniform Random Variate Generation</span><span class="font64">. SpringerLink : Bucher. Springer New York. 696</span></p>
<p><span class="font64">DiCarlo, J. J. (2013). Mechanisms underlying visual object recognition: Humans vs. neurons vs. machines. NIPS Tutorial. 26, 366</span></p>
<p><span class="font64">Dinh, L., Krueger, D., and Bengio, Y. (2014). NICE: Non-linear independent components estimation. arXiv:1410.8516. 495</span></p>
<p><span class="font64">Donahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., and Darrell, T. (2014). Long-term recurrent convolutional networks for visual&#160;recognition and description. arXiv:1411.4389. 102</span></p>
<p><span class="font64">Donoho, D. L. and Grimes, C. (2003). Hessian eigenmaps: new locally linear embedding techniques for high-dimensional data. Technical Report 2003-08, Dept. Statistics,&#160;Stanford University. 163, 521</span></p>
<p><span class="font64">Dosovitskiy, A., Springenberg, J. T., and Brox, T. (2015). Learning to generate chairs with convolutional neural networks. In </span><span class="font64" style="font-style:italic;">Proceedings of the IEEE Conference on Computer&#160;Vision and Pattern Recognition</span><span class="font64">, pages 1538-1546. 697, 706, 707</span></p>
<p><span class="font64">Doya, K. (1993). Bifurcations of recurrent neural networks in gradient descent learning. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Neural Networks</span><span class="font64">, 1, 75-80. 402, 405</span></p>
<p><span class="font64">Dreyfus, S. E. (1962). The numerical solution of variational problems. </span><span class="font64" style="font-style:italic;">Journal of Mathematical Analysis and Applications</span><span class="font64">, 5(1), 30-45. 224</span></p>
<p><span class="font64">Dreyfus, S. E. (1973). The computational solution of optimal control problems with time lag. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Automatic Control</span><span class="font64">, 18(4), 383-385. 224</span></p>
<p><span class="font64">Drucker, H. and LeCun, Y. (1992). Improving generalisation performance using double back-propagation. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Neural Networks</span><span class="font64">, 3(6), 991-997. 271</span></p>
<p><span class="font64">Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. </span><span class="font64" style="font-style:italic;">Journal of Machine Learning Research</span><span class="font64">. 307</span></p>
<p><span class="font64">Dudik, M., Langford, J., and Li, L. (2011). Doubly robust policy evaluation and learning.</span></p>
<p><span class="font64">In </span><span class="font64" style="font-style:italic;">Proceedings of the 28th International Conference on Machine learning</span><span class="font64">, ICML ’11. 484</span></p>
<p><span class="font64">Dugas, C., Bengio, Y., Belisle, F., and Nadeau, C. (2001). Incorporating second-order functional knowledge for better option pricing. In T. Leen, T. Dietterich, and V. Tresp,&#160;editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 13 (NIPS’00)</span><span class="font64">, pages&#160;472-478. MIT Press. 68, 196</span></p>
<p><span class="font64">Dziugaite, G. K., Roy, D. M., and Ghahramani, Z. (2015). Training generative neural networks via maximum mean discrepancy optimization. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1505.03906</span><span class="font64">. 705</span></p>
<p><span class="font64">El Hihi, S. and Bengio, Y. (1996). Hierarchical recurrent neural networks for long-term dependencies. In </span><span class="font64" style="font-style:italic;">NIPS’1995</span><span class="font64">. 400, 409</span></p>
<p><span class="font64">Elkahky, A. M., Song, Y., and He, X. (2015). A multi-view deep learning approach for cross domain user modeling in recommendation systems. In </span><span class="font64" style="font-style:italic;">Proceedings of the 24th&#160;International Conference on World Wide Web</span><span class="font64">, pages 278-288. 482</span></p>
<p><span class="font64">Elman, J. L. (1993). Learning and development in neural networks: The importance of starting small. </span><span class="font64" style="font-style:italic;">Cognition</span><span class="font64">, 48, 781-799. 328</span></p>
<p><span class="font64">Erhan, D., Manzagol, P.-A., Bengio, Y., Bengio, S., and Vincent, P. (2009). The difficulty of training deep architectures and the effect of unsupervised pre-training. In </span><span class="font64" style="font-style:italic;">Proceedings&#160;of A IS TA TS ’2009</span><span class="font64">. 200</span></p>
<p><span class="font64">Erhan, D., Bengio, Y., Courville, A., Manzagol, P., Vincent, P., and Bengio, S. (2010). Why does unsupervised pre-training help deep learning? </span><span class="font64" style="font-style:italic;">J. Machine Learning Res.&#160;</span><span class="font64">531, 535, 536</span></p>
<p><span class="font64">Fahlman, S. E., Hinton, G. E., and Sejnowski, T. J. (1983). Massively parallel architectures for AI: NETL, thistle, and Boltzmann machines. In </span><span class="font64" style="font-style:italic;">Proceedings of the National&#160;Conference on Artificial Intelligence AAAI-83</span><span class="font64">. 572, 656</span></p>
<p><span class="font64">Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Dollar, P., Gao, J., He, X., Mitchell, M., Platt, J. C., Zitnick, C. L., and Zweig, G. (2015). From captions to visual&#160;concepts and back. arXiv:1411.4952. 102</span></p>
<p><span class="font64">Farabet, C., LeCun, Y., Kavukcuoglu, K., Culurciello, E., Martini, B., Akselrod, P., and Talay, S. (2011). Large-scale FPGA-based convolutional networks. In R. Bekkerman,&#160;M. Bilenko, and J. Langford, editors, </span><span class="font64" style="font-style:italic;">Scaling up Machine Learning: Parallel and&#160;Distributed Approaches</span><span class="font64">. Cambridge University Press. 525</span></p>
<p><span class="font64">Farabet, C., Couprie, C., Najman, L., and LeCun, Y. (2013). Learning hierarchical features for scene labeling. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font64">,&#160;35(8), 1915-1929. 23, 200, 359</span></p>
<p><span class="font64">Fei-Fei, L., Fergus, R., and Perona, P. (2006). One-shot learning of object categories. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font64">,28(4), 594-611. 540</span></p>
<p><span class="font64">Finn, C., Tan, X. Y., Duan, Y., Darrell, T., Levine, S., and Abbeel, P. (2015). Learning visual feature spaces for robotic manipulation with deep spatial autoencoders. </span><span class="font64" style="font-style:italic;">arXiv&#160;preprint arXiv:1509.06113</span><span class="font64">. 25</span></p>
<p><span class="font64">Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. </span><span class="font64" style="font-style:italic;">Annals of Eugenics</span><span class="font64">, 7, 179-188. 21, 105</span></p>
<p><span class="font64">Foldiak, P. (1989). Adaptive network for optimal linear feature extraction. In </span><span class="font64" style="font-style:italic;">International Joint Conference on Neural Networks (IJCNN)</span><span class="font64">, volume 1, pages 401-405, Washington&#160;1989. IEEE, New York. 496</span></p>
<p><span class="font64">Franzius, M., Sprekeler, H., and Wiskott, L. (2007). Slowness and sparseness lead to place, head-direction, and spatial-view cells. 497</span></p>
<p><span class="font64">Franzius, M., Wilbert, N., and Wiskott, L. (2008). Invariant object recognition with slow feature analysis. In </span><span class="font64" style="font-style:italic;">Artificial Neural Networks-ICANN 2008</span><span class="font64">, pages 961-970. Springer.&#160;498</span></p>
<p><span class="font64">Frasconi, P., Gori, M., and Sperduti, A. (1997). On the efficient classification of data structures by neural networks. In </span><span class="font64" style="font-style:italic;">Proc. Int. Joint Conf. on Artificial Intelligence</span><span class="font64">. 400,&#160;402</span></p>
<p><span class="font64">Frasconi, P., Gori, M., and Sperduti, A. (1998). A general framework for adaptive processing of data structures. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Neural Networks</span><span class="font64">,9(5), 768-786.&#160;400, 402</span></p>
<p><span class="font64">Freund, Y. and Schapire, R. E. (1996a). Experiments with a new boosting algorithm. In </span><span class="font64" style="font-style:italic;">Machine Learning: Proceedings of Thirteenth International Conference</span><span class="font64">, pages 148-156,&#160;USA. ACM. 258</span></p>
<p><span class="font64">Freund, Y. and Schapire, R. E. (1996b). Game theory, on-line prediction and boosting. In</span></p>
<p><span class="font64" style="font-style:italic;">Proceedings of the Ninth Annual Conference on Computational Learning Theory</span><span class="font64">, pages 325-332. 258</span></p>
<p><span class="font64">Frey, B. J. (1998). </span><span class="font64" style="font-style:italic;">Graphical models for machine learning and digital communication</span><span class="font64">. MIT Press. 707, 708</span></p>
<p><span class="font64">Frey, B. J., Hinton, G. E., and Dayan, P. (1996). Does the wake-sleep algorithm learn good density estimators? In D. Touretzky, M. Mozer, and M. Hasselmo, editors, </span><span class="font64" style="font-style:italic;">Advances&#160;in Neural Information Processing Systems 8 (NIPS’95)</span><span class="font64">, pages 661-670. MIT Press,&#160;Cambridge, MA. 653</span></p>
<p><span class="font64">Frobenius, G. (1908). Uber matrizen aus positiven elementen, s. </span><span class="font64" style="font-style:italic;">B. Preuss. Akad. Wiss. Berlin, Germany.</span><span class="font64"> 599</span></p>
<p><span class="font64">Fukushima, K. (1975). Cognitron: A self-organizing multilayered neural network. </span><span class="font64" style="font-style:italic;">Biological Cybernetics</span><span class="font64">, 20, 121-136. 16, 225, 530</span></p>
<p><span class="font64">Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. </span><span class="font64" style="font-style:italic;">Biological Cybernetics</span><span class="font64">,&#160;36, 193-202. 16, 24, 27, 225, 367</span></p>
<p><span class="font64">Gal, Y. and Ghahramani, Z. (2015). Bayesian convolutional neural networks with Bernoulli approximate variational inference. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1506.02158</span><span class="font64">. 264</span></p>
<p><span class="font64">Gallinari, P., LeCun, Y., Thiria, S., and Fogelman-Soulie, F. (1987). Memoires associatives distribuees. In </span><span class="font64" style="font-style:italic;">Proceedings of COGNITIVA 87</span><span class="font64">, Paris, La Villette. 517</span></p>
<p><span class="font64">Garcia-Duran, A., Bordes, A., Usunier, N., and Grandvalet, Y. (2015). Combining two and three-way embeddings models for link prediction in knowledge bases. </span><span class="font64" style="font-style:italic;">arXiv preprint&#160;arXiv:1506.00999</span><span class="font64">. 486</span></p>
<p><span class="font64">Garofolo, J. S., Lamel, L. F., Fisher, W. M., Fiscus, J. G., and Pallett, D. S. (1993). Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1.&#160;</span><span class="font64" style="font-style:italic;">NASA STI/Recon Technical Report N</span><span class="font64">, 93, 27403. 461</span></p>
<p><span class="font64">Garson, J. (1900). The metric system of identification of criminals, as used in Great Britain and Ireland. </span><span class="font64" style="font-style:italic;">The Journal of the Anthropological Institute of Great Britain and&#160;Ireland</span><span class="font64">, (2), 177-227. 21</span></p>
<p><span class="font64">Gers, F. A., Schmidhuber, J., and Cummins, F. (2000). Learning to forget: Continual prediction with LSTM. </span><span class="font64" style="font-style:italic;">Neural computation</span><span class="font64">, 12(10), 2451-2471. 410, 414</span></p>
<p><span class="font64">Ghahramani, Z. and Hinton, G. E. (1996). The EM algorithm for mixtures of factor analyzers. Technical Report CRG-TR-96-1, Dpt. of Comp. Sci., Univ. of Toronto. 491</span></p>
<p><span class="font64">Gillick, D., Brunk, C., Vinyals, O., and Subramanya, A. (2015). Multilingual language processing from bytes. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1512.00103</span><span class="font64">. 479</span></p>
<p><span class="font64">Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2015). Region-based convolutional networks for accurate object detection and segmentation. 428</span></p>
<p><span class="font64">Giudice, M. D., Manera, V., and Keysers, C. (2009). Programmed to learn? The ontogeny of mirror neurons. </span><span class="font64" style="font-style:italic;">Dev. Sci.,</span><span class="font64"> 12(2), 350—363. 658</span></p>
<p><span class="font64">Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In </span><span class="font64" style="font-style:italic;">AISTATS’2010</span><span class="font64">. 303</span></p>
<p><span class="font64">Glorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectifier neural networks. In </span><span class="font64" style="font-style:italic;">AISTATS’2011</span><span class="font64">. 16, 173, 196, 226</span></p>
<p><span class="font64">Glorot, X., Bordes, A., and Bengio, Y. (2011b). Domain adaptation for large-scale sentiment classification: A deep learning approach. In </span><span class="font64" style="font-style:italic;">ICML’2011</span><span class="font64">. 509, 539</span></p>
<p><span class="font64">Goldberger, J., Roweis, S., Hinton, G. E., and Salakhutdinov, R. (2005). Neighbourhood components analysis. In L. Saul, Y. Weiss, and L. Bottou, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural&#160;Information Processing Systems 17 (NIPS’04).</span><span class="font64"> MIT Press. 115</span></p>
<p><span class="font64">Gong, S., McKenna, S., and Psarrou, A. (2000). </span><span class="font64" style="font-style:italic;">Dynamic Vision: From Images to Face Recognition</span><span class="font64">. Imperial College Press. 164, 521</span></p>
<p><span class="font64">Goodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009). Measuring invariances in deep networks. In </span><span class="font64" style="font-style:italic;">NIPS’2009</span><span class="font64">, pages 646-654. 255</span></p>
<p><span class="font64">Goodfellow, I., Koenig, N., Muja, M., Pantofaru, C., Sorokin, A., and Takayama, L. (2010). Help me help you: Interfaces for personal robots. In </span><span class="font64" style="font-style:italic;">Proc. of Human Robot Interaction&#160;(HRI),</span><span class="font64"> Osaka, Japan. ACM Press, ACM Press. 100</span></p>
<p><span class="font64">Goodfellow, I. J. (2010). Technical report: Multidimensional, downsampled convolution for autoencoders. Technical report, Universite de Montreal. 357</span></p>
<p><span class="font64">Goodfellow, I. J. (2014). On distinguishability criteria for estimating generative models. In </span><span class="font64" style="font-style:italic;">International Conference on Learning Representations, Workshops Track</span><span class="font64">. 624, 702,&#160;703</span></p>
<p><span class="font64">Goodfellow, I. J., Courville, A., and Bengio, Y. (2011). Spike-and-slab sparse coding for unsupervised feature discovery. In </span><span class="font64" style="font-style:italic;">NIPS Workshop on Challenges in Learning&#160;Hierarchical Models</span><span class="font64">. 534, 540</span></p>
<p><span class="font64">Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013a). Maxout networks. In S. Dasgupta and D. McAllester, editors, </span><span class="font64" style="font-style:italic;">ICML’13</span><span class="font64">, pages 13191327. 192, 264, 344, 365, 457</span></p>
<p><span class="font64">Goodfellow, I. J., Mirza, M., Courville, A., and Bengio, Y. (2013b). Multi-prediction deep Boltzmann machines. In </span><span class="font64" style="font-style:italic;">NIPS26</span><span class="font64">. NIPS Foundation. 100, 619, 673, 674, 675, 676, 677,&#160;700</span></p>
<p><span class="font64">Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., and Bengio, Y. (2013c). Pylearn2: a machine learning research&#160;library. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1308.4214</span><span class="font64">. 25,448</span></p>
<p><span class="font64">Goodfellow, I. J., Courville, A., and Bengio, Y. (2013d). Scaling up spike-and-slab models for unsupervised feature learning. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine&#160;Intelligence</span><span class="font64">, 35(8), 1902-1914. 499, 500, 501, 652, 685</span></p>
<p><span class="font64">Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., and Bengio, Y. (2014a). An empirical investigation of catastrophic forgeting in gradient-based neural networks. In </span><span class="font64" style="font-style:italic;">ICLR ’2014</span><span class="font64">.&#160;193</span></p>
<p><span class="font64">Goodfellow, I. J., Shlens, J., and Szegedy, C. (2014b). Explaining and harnessing adversarial examples. </span><span class="font64" style="font-style:italic;">CoRR</span><span class="font64">, abs/1412.6572. 268, 269, 271, 557, 558</span></p>
<p><span class="font64">Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014c). Generative adversarial networks. In </span><span class="font64" style="font-style:italic;">NIPS’2014</span><span class="font64">.&#160;546, 691, 702, 703, 706</span></p>
<p><span class="font64">Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. (2014d). Multi-digit number recognition from Street View imagery using deep convolutional neural networks.&#160;In </span><span class="font64" style="font-style:italic;">International Conference on Learning Representations</span><span class="font64">. 25, 101, 200, 201, 202, 390,&#160;424, 451</span></p>
<p><span class="font64">Goodfellow, I. J., Vinyals, O., and Saxe, A. M. (2015). Qualitatively characterizing neural network optimization problems. In </span><span class="font64" style="font-style:italic;">International Conference on Learning Representations</span><span class="font64">. 285, 286, 287, 291</span></p>
<p><span class="font64">Goodman, J. (2001). Classes for fast maximum entropy training. In </span><span class="font64" style="font-style:italic;">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="font64">, Utah. 469</span></p>
<p><span class="font64">Gori, M. and Tesi, A. (1992). On the problem of local minima in backpropagation. </span><span class="font64" style="font-style:italic;">IEEE</span></p>
<p><span class="font64" style="font-style:italic;">Transactions on Pattern Analysis and Machine Intelligence</span><span class="font64">, PAMI-14(1), 76-86. 284</span></p>
<p><span class="font64">Gosset, W. S. (1908). The probable error of a mean. </span><span class="font64" style="font-style:italic;">Biometrika</span><span class="font64">, 6(1), 1-25. Originally published under the pseudonym “Student”. 21</span></p>
<p><span class="font64">Gouws, S., Bengio, Y., and Corrado, G. (2014). BilBOWA: Fast bilingual distributed representations without word alignments. Technical report, arXiv:1410.2455. 478, 541</span></p>
<p><span class="font64">Graf, H. P. and Jackel, L. D. (1989). Analog electronic neural network circuits. </span><span class="font64" style="font-style:italic;">Circuits and Devices Magazine, IEEE</span><span class="font64">, 5(4), 44-49. 453</span></p>
<p><span class="font64">Graves, A. (2011). Practical variational inference for neural networks. In </span><span class="font64" style="font-style:italic;">NIPS’2011</span><span class="font64">. 242</span></p>
<p><span class="font64">Graves, A. (2012). </span><span class="font64" style="font-style:italic;">Supervised Sequence Labelling with Recurrent Neural Networks</span><span class="font64">. Studies in Computational Intelligence. Springer. 374, 395, 413, 462</span></p>
<p><span class="font64">Graves, A. (2013). Generating sequences with recurrent neural networks. Technical report, arXiv:1308.0850. 189, 410, 417, 421</span></p>
<p><span class="font64">Graves, A. and Jaitly, N. (2014). Towards end-to-end speech recognition with recurrent neural networks. In </span><span class="font64" style="font-style:italic;">ICML ’2014</span><span class="font64">. 410</span></p>
<p><span class="font64">Graves, A. and Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional LSTM and other neural network architectures. </span><span class="font64" style="font-style:italic;">Neural Networks</span><span class="font64">, 18(5), 602-610. 395</span></p>
<p><span class="font64">Graves, A. and Schmidhuber, J. (2009). Offline handwriting recognition with multidimensional recurrent neural networks. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, </span><span class="font64" style="font-style:italic;">NIPS’2008</span><span class="font64">, pages 545-552. 395</span></p>
<p><span class="font64">Graves, A., Fernandez, S., Gomez, F., and Schmidhuber, J. (2006). Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In&#160;</span><span class="font64" style="font-style:italic;">ICML’2006</span><span class="font64">, pages 369-376, Pittsburgh, USA. 462</span></p>
<p><span class="font64">Graves, A., Liwicki, M., Bunke, H., Schmidhuber, J., and Fernandez, S. (2008). Unconstrained on-line handwriting recognition with recurrent neural networks. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, </span><span class="font64" style="font-style:italic;">NIPS’2007</span><span class="font64">, pages 577-584. 395</span></p>
<p><span class="font64">Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., and Schmidhuber, J. (2009). A novel connectionist system for unconstrained handwriting recognition. </span><span class="font64" style="font-style:italic;">Pattern&#160;Analysis and Machine Intelligence, IEEE Transactions on</span><span class="font64">, 31(5), 855-868. 410</span></p>
<p><span class="font64">Graves, A., Mohamed, A., and Hinton, G. (2013). Speech recognition with deep recurrent neural networks. In </span><span class="font64" style="font-style:italic;">ICASSP’2013</span><span class="font64">, pages 6645-6649. 395, 398, 400, 410, 412, 413, 462</span></p>
<p><span class="font64">Graves, A., Wayne, G., and Danihelka, I. (2014a). Neural Turing machines. arXiv:1410.5401. 25</span></p>
<p><span class="font64">Graves, A., Wayne, G., and Danihelka, I. (2014b). Neural Turing machines. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1410.5401</span><span class="font64">. 418, 420</span></p>
<p><span class="font64">Grefenstette, E., Hermann, K. M., Suleyman, M., and Blunsom, P. (2015). Learning to transduce with unbounded memory. In </span><span class="font64" style="font-style:italic;">NIPS’2015</span><span class="font64">. 420</span></p>
<p><span class="font64">Greff, K., Srivastava, R. K., Koutmk, J., Steunebrink, B. R., and Schmidhuber, J. (2015). LSTM: a search space odyssey. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1503.04 069</span><span class="font64"> . 414</span></p>
<p><span class="font64">Gregor, K. and LeCun, Y. (2010a). Emergence of complex-like cells in a temporal product network with local receptive fields. Technical report, arXiv:1006.0448. 352</span></p>
<p><span class="font64">Gregor, K. and LeCun, Y. (2010b). Learning fast approximations of sparse coding. In L. Bottou and M. Littman, editors, </span><span class="font64" style="font-style:italic;">Proceedings of the Twenty-seventh International&#160;Conference on Machine Learning (ICML-10)</span><span class="font64">. ACM. 654</span></p>
<p><span class="font64">Gregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. (2014). Deep autoregressive networks. In </span><span class="font64" style="font-style:italic;">International Conference on Machine Learning (ICML’2014)</span><span class="font64">.&#160;695</span></p>
<p><span class="font64">Gregor, K., Danihelka, I., Graves, A., and Wierstra, D. (2015). DRAW: A recurrent neural network for image generation. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1502.04623</span><span class="font64">. 700</span></p>
<p><span class="font64">Gretton, A., Borgwardt, K. M., Rasch, M. J., Scholkopf, B., and Smola, A. (2012). A kernel two-sample test. </span><span class="font64" style="font-style:italic;">The Journal of Machine Learning Research</span><span class="font64">, 13(1), 723-773.&#160;705</span></p>
<p><span class="font64">Gfilgehre, Q. and Bengio, Y. (2013). Knowledge matters: Importance of prior information for optimization. In </span><span class="font64" style="font-style:italic;">International Conference on Learning Representations (ICLR’2013).&#160;</span><span class="font64">25</span></p>
<p><span class="font64">Guo, H. and Gelfand, S. B. (1992). Classification trees with neural network feature extraction. </span><span class="font64" style="font-style:italic;">Neural Networks, IEEE Transactions on</span><span class="font64">, 3(6), 923-933. 452</span></p>
<p><span class="font64">Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. (2015). Deep learning with limited numerical precision. </span><span class="font64" style="font-style:italic;">CoRR,</span><span class="font64"> abs/1502.02551. 454</span></p>
<p><span class="font64">Gutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In </span><span class="font64" style="font-style:italic;">Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS’10)</span><span class="font64">. 622</span></p>
<p><span class="font64">Hadsell, R., Sermanet, P., Ben, J., Erkan, A., Han, J., Muller, U., and LeCun, Y. (2007). Online learning for offroad robots: Spatial label propagation to learn long-range&#160;traversability. In </span><span class="font64" style="font-style:italic;">Proceedings of Robotics: Science and Systems</span><span class="font64">, Atlanta, GA, USA. 455</span></p>
<p><span class="font64">Hajnal, A., Maass, W., Pudlak, P., Szegedy, M., and Turan, G. (1993). Threshold circuits of bounded depth. </span><span class="font64" style="font-style:italic;">J. Comput. System. Sci.,</span><span class="font64"> 46, 129-154. 198</span></p>
<p><span class="font64">Hastad, J. (1986). Almost optimal lower bounds for small depth circuits. In </span><span class="font64" style="font-style:italic;">Proceedings of the 18th annual ACM Symposium on Theory of Computing</span><span class="font64">, pages 6-20, Berkeley,&#160;California. ACM Press. 198</span></p>
<p><span class="font64">Hastad, J. and Goldmann, M. (1991). On the power of small-depth threshold circuits. </span><span class="font64" style="font-style:italic;">Computational Complexity</span><span class="font64">, 1, 113-129. 198</span></p>
<p><span class="font64">Hastie, T., Tibshirani, R., and Friedman, J. (2001). </span><span class="font64" style="font-style:italic;">The elements of statistical learning: data mining, inference and prediction</span><span class="font64">. Springer Series in Statistics. Springer Verlag.&#160;145</span></p>
<p><span class="font64">He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1502.01852</span><span class="font64">.&#160;28, 192</span></p>
<p><span class="font64">Hebb, D. O. (1949). </span><span class="font64" style="font-style:italic;">The Organization of Behavior</span><span class="font64">. Wiley, New York. 14, 17, 658</span></p>
<p><span class="font64">Henaff, M., Jarrett, K., Kavukcuoglu, K., and LeCun, Y. (2011). Unsupervised learning of sparse features for scalable audio classification. In </span><span class="font64" style="font-style:italic;">ISMIR’11</span><span class="font64">. 525</span></p>
<p><span class="font64">Henderson, J. (2003). Inducing history representations for broad coverage statistical parsing. In </span><span class="font64" style="font-style:italic;">HLT-NAACL,</span><span class="font64"> pages 103-110. 479</span></p>
<p><span class="font64">Henderson, J. (2004). Discriminative training of a neural network statistical parser. In</span></p>
<p><span class="font64" style="font-style:italic;">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</span><span class="font64">, page 95. 479</span></p>
<p><span class="font64">Henniges, M., Puertas, G., Bornschein, J., Eggert, J., and Lucke, J. (2010). Binary sparse coding. In </span><span class="font64" style="font-style:italic;">Latent Variable Analysis and Signal Separation</span><span class="font64">, pages 450-457. Springer.&#160;642</span></p>
<p><span class="font64">Herault, J. and Ans, B. (1984). Circuits neuronaux a synapses modifiables: Decodage de messages composites par apprentissage non supervise. </span><span class="font64" style="font-style:italic;">Comptes Rendus de l’Academie&#160;des Sciences</span><span class="font64">, 299(111-13), 525—528. 493</span></p>
<p><span class="font64">Hinton, G. (2012). Neural networks for machine learning. Coursera, video lectures. 307</span></p>
<p><span class="font64">Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. (2012a). Deep neural networks for acoustic&#160;modeling in speech recognition. </span><span class="font64" style="font-style:italic;">IEEE Signal Processing Magazine</span><span class="font64">, 29(6), 82-97. 23,&#160;462</span></p>
<p><span class="font64">Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the knowledge in a neural network. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1503.02531</span><span class="font64">. 450</span></p>
<p><span class="font64">Hinton, G. E. (1989). Connectionist learning procedures. </span><span class="font64" style="font-style:italic;">Artificial Intelligence</span><span class="font64">, 40, 185-234. 496</span></p>
<p><span class="font64">Hinton, G. E. (1990). Mapping part-whole hierarchies into connectionist networks. </span><span class="font64" style="font-style:italic;">Artificial Intelligence</span><span class="font64">, 46(1), 47-75. 420</span></p>
<p><span class="font64">Hinton, G. E. (1999). Products of experts. In </span><span class="font64" style="font-style:italic;">ICANN’1999</span><span class="font64">. 572</span></p>
<p><span class="font64">Hinton, G. E. (2000). Training products of experts by minimizing contrastive divergence. Technical Report GCNU TR 2000-004, Gatsby Unit, University College London. 612,&#160;678</span></p>
<p><span class="font64">Hinton, G. E. (2006). To recognize shapes, first learn to generate images. Technical Report UTML TR 2006-003, University of Toronto. 530, 597</span></p>
<p><span class="font64">Hinton, G. E. (2007a). How to do backpropagation in a brain. Invited talk at the NIPS’2007 Deep Learning Workshop. 658</span></p>
<p><span class="font64">Hinton, G. E. (2007b). Learning multiple layers of representation. </span><span class="font64" style="font-style:italic;">Trends in cognitive sciences</span><span class="font64">, 11(10), 428-434. 662</span></p>
<p><span class="font64">Hinton, G. E. (2010). A practical guide to training restricted Boltzmann machines. Technical Report UTML TR 2010-003, Department of Computer Science, University of&#160;Toronto. 612</span></p>
<p><span class="font64">Hinton, G. E. and Ghahramani, Z. (1997). Generative models for discovering sparse distributed representations. </span><span class="font64" style="font-style:italic;">Philosophical Transactions of the Royal Society of London</span><span class="font64">.&#160;146</span></p>
<p><span class="font64">Hinton, G. E. and McClelland, J. L. (1988). Learning representations by recirculation. In </span><span class="font64" style="font-style:italic;">NIPS’1987</span><span class="font64">, pages 358-366. 504</span></p>
<p><span class="font64">Hinton, G. E. and Roweis, S. (2003). Stochastic neighbor embedding. In </span><span class="font64" style="font-style:italic;">NIPS’2002</span><span class="font64">. 521</span></p>
<p><span class="font64">Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimensionality of data with neural networks. </span><span class="font64" style="font-style:italic;">Science</span><span class="font64">, 313(5786), 504-507. 511, 526, 530, 531, 536</span></p>
<p><span class="font64">Hinton, G. E. and Sejnowski, T. J. (1986). Learning and relearning in Boltzmann machines. In D. E. Rumelhart and J. L. McClelland, editors, </span><span class="font64" style="font-style:italic;">Parallel Distributed Processing</span><span class="font64">,&#160;volume 1, chapter 7, pages 282-317. MIT Press, Cambridge. 572, 656</span></p>
<p><span class="font64">Hinton, G. E. and Sejnowski, T. J. (1999). </span><span class="font64" style="font-style:italic;">Unsupervised learning: foundations of neural computation</span><span class="font64">. MIT press. 543</span></p>
<p><span class="font64">Hinton, G. E. and Shallice, T. (1991). Lesioning an attractor network: investigations of acquired dyslexia. </span><span class="font64" style="font-style:italic;">Psychologica,l review</span><span class="font64">, 98(1), 74. 13</span></p>
<p><span class="font64">Hinton, G. E. and Zemel, R. S. (1994). Autoencoders, minimum description length, and Helmholtz free energy. In </span><span class="font64" style="font-style:italic;">NIPS’1993</span><span class="font64">. 504</span></p>
<p><span class="font64">Hinton, G. E., Sejnowski, T. J., and Ackley, D. H. (1984). Boltzmann machines: Constraint satisfaction networks that learn. Technical Report TR-CMU-CS-84-119, Carnegie-Mellon&#160;University, Dept. of Computer Science. 572, 656</span></p>
<p><span class="font64">Hinton, G. E., McClelland, J., and Rumelhart, D. (1986). Distributed representations. In D. E. Rumelhart and J. L. McClelland, editors, </span><span class="font64" style="font-style:italic;">Parallel Distributed Processing:&#160;Explorations in the Microstructure of Cognition</span><span class="font64">, volume 1, pages 77-109. MIT Press,&#160;Cambridge. 17, 225, 528</span></p>
<p><span class="font64">Hinton, G. E., Revow, M., and Dayan, P. (1995a). Recognizing handwritten digits using mixtures of linear models. In G. Tesauro, D. Touretzky, and T. Leen, editors, </span><span class="font64" style="font-style:italic;">Advances&#160;in Neural Information Processing Systems</span><span class="font64"> 7 </span><span class="font64" style="font-style:italic;">(NIPS’94),</span><span class="font64"> pages 1015-1022. MIT Press,&#160;Cambridge, MA. 491</span></p>
<p><span class="font64">Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995b). The wake-sleep algorithm for unsupervised neural networks. </span><span class="font64" style="font-style:italic;">Science</span><span class="font64">, 268, 1558-1161. 506, 653</span></p>
<p><span class="font64">Hinton, G. E., Dayan, P., and Revow, M. (1997). Modelling the manifolds of images of handwritten digits. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Neural Networks</span><span class="font64">, 8, 65-74. 501</span></p>
<p><span class="font64">Hinton, G. E., Welling, M., Teh, Y. W., and Osindero, S. (2001). A new view of ICA. In</span></p>
<p><span class="font64" style="font-style:italic;">Proceedings of 3rd International Conference on Independent Component Analysis and Blind Signal Separation (ICA’01),</span><span class="font64"> pages 746-751, San Diego, CA. 493</span></p>
<p><span class="font64">Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 18, 1527-1554. 14, 19, 27, 142, 530, 531, 662, 663</span></p>
<p><span class="font64">Hinton, G. E., Deng, L., Yu, D., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T. N., and Kingsbury, B. (2012b). Deep neural&#160;networks for acoustic modeling in speech recognition: The shared views of four research&#160;groups. </span><span class="font64" style="font-style:italic;">IEEE Signal Process. Mag.,</span><span class="font64"> 29(6), 82-97. 101</span></p>
<p><span class="font64">Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012c). Improving neural networks by preventing co-adaptation of feature detectors. Technical&#160;report, arXiv:1207.0580. 238, 263, 267</span></p>
<p><span class="font64">Hinton, G. E., Vinyals, O., and Dean, J. (2014). Dark knowledge. Invited talk at the BayLearn Bay Area Machine Learning Symposium. 450</span></p>
<p><span class="font64">Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, T.U. Munchen. 18, 402, 404</span></p>
<p><span class="font64">Hochreiter, S. and Schmidhuber, J. (1995). Simplifying neural nets by discovering flat minima. In </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 7</span><span class="font64">, pages 529-536. MIT&#160;Press. 243</span></p>
<p><span class="font64">Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 9(8), 1735-1780. 18, 410, 413</span></p>
<p><span class="font64">Hochreiter, S., Bengio, Y., and Frasconi, P. (2001). Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. In J. Kolen and S. Kremer, editors, </span><span class="font64" style="font-style:italic;">Field&#160;Guide to Dynamical Recurrent Networks</span><span class="font64">. IEEE Press. 413</span></p>
<p><span class="font64">Holi, J. L. and Hwang, J.-N. (1993). Finite precision error analysis of neural network hardware implementations. </span><span class="font64" style="font-style:italic;">Computers, IEEE Transactions on</span><span class="font64">, 42(3), 281-290. 453</span></p>
<p><span class="font64">Holt, J. L. and Baker, T. E. (1991). Back propagation simulations using limited precision calculations. In </span><span class="font64" style="font-style:italic;">Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on</span><span class="font64">, volume 2, pages 121-126. IEEE. 453</span></p>
<p><span class="font64">Hornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are universal approximators. </span><span class="font64" style="font-style:italic;">Neural Networks</span><span class="font64">, 2, 359-366. 197</span></p>
<p><span class="font64">Hornik, K., Stinchcombe, M., and White, H. (1990). Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. </span><span class="font64" style="font-style:italic;">Neural&#160;networks</span><span class="font64">, 3(5), 551-560. 197</span></p>
<p><span class="font64">Hsu, F.-H. (2002). </span><span class="font64" style="font-style:italic;">Behind Deep Blue: Building the Computer That Defeated the World Chess Champion</span><span class="font64">. Princeton University Press, Princeton, NJ, USA. 2</span></p>
<p><span class="font64">Huang, F. and Ogata, Y. (2002). Generalized pseudo-likelihood estimates for Markov random fields on lattice. </span><span class="font64" style="font-style:italic;">Annals of the Institute of Statistical Mathematics</span><span class="font64">, 54(1), 1-18.&#160;618</span></p>
<p><span class="font64">Huang, P.-S., He, X., Gao, J., Deng, L., Acero, A., and Heck, L. (2013). Learning deep structured semantic models for web search using clickthrough data. In </span><span class="font64" style="font-style:italic;">Proceedings of&#160;the 22nd ACM international conference on Conference on information &amp; knowledge&#160;management</span><span class="font64">, pages 2333-2338. ACM. 482</span></p>
<p><span class="font64">Hubel, D. and Wiesel, T. (1968). Receptive fields and functional architecture of monkey striate cortex. </span><span class="font64" style="font-style:italic;">Journal of Physiology (London),</span><span class="font64"> 195, 215-243. 364</span></p>
<p><span class="font64">Hubel, D. H. and Wiesel, T. N. (1959). Receptive fields of single neurons in the cat’s striate cortex. </span><span class="font64" style="font-style:italic;">Journal of Physiology</span><span class="font64">, 148, 574-591. 364</span></p>
<p><span class="font64">Hubel, D. H. and Wiesel, T. N. (1962). Receptive fields, binocular interaction, and functional architecture in the cat’s visual cortex. </span><span class="font64" style="font-style:italic;">Journal of Physiology (London),</span><span class="font64"> 160,&#160;106-154. 364</span></p>
<p><span class="font64">Huszar, F. (2015). How (not) to train your generative model: schedule sampling, likelihood, adversary? </span><span class="font64" style="font-style:italic;">arXiv:1511.05101</span><span class="font64">. 699</span></p>
<p><span class="font64">Hutter, F., Hoos, H., and Leyton-Brown, K. (2011). Sequential model-based optimization for general algorithm configuration. In </span><span class="font64" style="font-style:italic;">LION-5</span><span class="font64">. Extended version as UBC Tech report&#160;TR-2010-10. 438</span></p>
<p><span class="font64">Hyotyniemi, H. (1996). Turing machines are recurrent neural networks. In </span><span class="font64" style="font-style:italic;">STeP’96</span><span class="font64">, pages 13-24. 379</span></p>
<p><span class="font64">Hyvarinen, A. (1999). Survey on independent component analysis. </span><span class="font64" style="font-style:italic;">Neural Computing Surveys</span><span class="font64">, 2, 94-128. 493</span></p>
<p><span class="font64">Hyvarinen, A. (2005). Estimation of non-normalized statistical models using score matching. </span><span class="font64" style="font-style:italic;">Journal of Machine Learning Research</span><span class="font64">, 6, 695-709. 515, 619</span></p>
<p><span class="font64">Hyvarinen, A. (2007a). Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Neural&#160;Networks</span><span class="font64">, 18, 1529-1531. 620</span></p>
<p><span class="font64">Hyvarinen, A. (2007b). Some extensions of score matching. </span><span class="font64" style="font-style:italic;">Computational Statistics and Data Analysis</span><span class="font64">, 51, 2499-2512. 620</span></p>
<p><span class="font64">Hyvarinen, A. and Hoyer, P. O. (1999). Emergence of topography and complex cell properties from natural images using extensions of ica. In </span><span class="font64" style="font-style:italic;">NIPS</span><span class="font64">, pages 827-833. 495</span></p>
<p><span class="font64">Hyvarinen, A. and Pajunen, P. (1999). Nonlinear independent component analysis: Existence and uniqueness results. </span><span class="font64" style="font-style:italic;">Neural Networks</span><span class="font64">, 12(3), 429-439. 495</span></p>
<p><span class="font64">Hyvarinen, A., Karhunen, J., and Oja, E. (2001a). </span><span class="font64" style="font-style:italic;">Independent Component Analysis</span><span class="font64">. Wiley-Interscience. 493</span></p>
<p><span class="font64">Hyvarinen, A., Hoyer, P. O., and Inki, M. O. (2001b). Topographic independent component analysis. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 13(7), 1527-1558. 495</span></p>
<p><span class="font64">Hyvarinen, A., Hurri, J., and Hoyer, P. O. (2009). </span><span class="font64" style="font-style:italic;">Natural Image Statistics: A probabilistic approach to early computational vision</span><span class="font64">. Springer-Verlag. 370</span></p>
<p><span class="font64">Iba, Y. (2001). Extended ensemble Monte Carlo. </span><span class="font64" style="font-style:italic;">International Journal of Modern Physics</span><span class="font64">, C12, 623-656. 605</span></p>
<p><span class="font64">Inayoshi, H. and Kurita, T. (2005). Improved generalization by adding both autoassociation and hidden-layer noise to neural-network-based-classifiers. </span><span class="font64" style="font-style:italic;">IEEE Workshop on Machine Learning for Signal Processing</span><span class="font64">, pages 141—-146. 517</span></p>
<p><span class="font64">Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. 100, 317, 320</span></p>
<p><span class="font64">Jacobs, R. A. (1988). Increased rates of convergence through learning rate adaptation. </span><span class="font64" style="font-style:italic;">Neural networks</span><span class="font64">, 1(4), 295-307. 307</span></p>
<p><span class="font64">Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive mixtures of local experts. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 3, 79-87. 188, 452</span></p>
<p><span class="font64">Jaeger, H. (2003). Adaptive nonlinear system identification with echo state networks. In</span></p>
<p><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 15</span><span class="font64">. 405</span></p>
<p><span class="font64">Jaeger, H. (2007a). Discovering multiscale dynamical features with hierarchical echo state networks. Technical report, Jacobs University. 400</span></p>
<p><span class="font64">Jaeger, H. (2007b). Echo state network. </span><span class="font64" style="font-style:italic;">Scholarpedia</span><span class="font64">, 2(9), 2330. 405</span></p>
<p><span class="font64">Jaeger, H. (2012). Long short-term memory in echo state networks: Details of a simulation study. Technical report, Technical report, Jacobs University Bremen. 406</span></p>
<p><span class="font64">Jaeger, H. and Haas, H. (2004). Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication. </span><span class="font64" style="font-style:italic;">Science</span><span class="font64">, 304(5667), 78-80. 27, 405</span></p>
<p><span class="font64">Jaeger, H., Lukosevicius, M., Popovici, D., and Siewert, U. (2007). Optimization and applications of echo state networks with leaky- integrator neurons. </span><span class="font64" style="font-style:italic;">Neural Networks</span><span class="font64">,&#160;20(3), 335-352. 409</span></p>
<p><span class="font64">Jain, V., Murray, J. F., Roth, F., Turaga, S., Zhigulin, V., Briggman, K. L., Helmstaedter, M. N., Denk, W., and Seung, H. S. (2007). Supervised learning of image restoration&#160;with convolutional networks. In </span><span class="font64" style="font-style:italic;">Computer Vision, 2007. ICCV 2007. IEEE 11th&#160;International Conference on</span><span class="font64">, pages 1-8. IEEE. 359</span></p>
<p><span class="font64">Jaitly, N. and Hinton, G. (2011). Learning a better representation of speech soundwaves using restricted Boltzmann machines. In </span><span class="font64" style="font-style:italic;">Acoustics, Speech and Signal Processing&#160;(ICASSP), 2011 IEEE International Conference on</span><span class="font64">, pages 5884-5887. IEEE. 460</span></p>
<p><span class="font64">Jaitly, N. and Hinton, G. E. (2013). Vocal tract length perturbation (VTLP) improves speech recognition. In </span><span class="font64" style="font-style:italic;">ICML’2013</span><span class="font64">. 241</span></p>
<p><span class="font64">Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best multi-stage architecture for object recognition? In </span><span class="font64" style="font-style:italic;">ICCV’09</span><span class="font64">. 16, 24, 27, 173, 192, 226,&#160;363, 364, 525</span></p>
<p><span class="font64">Jarzynski, C. (1997). Nonequilibrium equality for free energy differences. </span><span class="font64" style="font-style:italic;">Phys. Rev. Lett., </span><span class="font64">78, 2690-2693. 627, 630</span></p>
<p><span class="font64">Jaynes, E. T. (2003). </span><span class="font64" style="font-style:italic;">Probability Theory: The Logic of Science</span><span class="font64">. Cambridge University Press. 53</span></p>
<p><span class="font64">Jean, S., Cho, K., Memisevic, R., and Bengio, Y. (2014). On using very large target vocabulary for neural machine translation. arXiv:1412.2007. 476</span></p>
<p><span class="font64">Jelinek, F. and Mercer, R. L. (1980). Interpolated estimation of Markov source parameters from sparse data. In E. S. Gelsema and L. N. Kanal, editors, </span><span class="font64" style="font-style:italic;">Pattern Recognition in&#160;Practice</span><span class="font64">. North-Holland, Amsterdam. 464, 475</span></p>
<p><span class="font64">Jia, Y. (2013). Caffe: An open source convolutional architecture for fast feature embedding. </span><a href="http://caffe.berkeleyvision.org/"><span class="font64">http://caffe.berkeleyvision.org/</span></a><span class="font64">. 25,210</span></p>
<p><span class="font64">Jia, Y., Huang, C., and Darrell, T. (2012). Beyond spatial pyramids: Receptive field learning for pooled image features. In </span><span class="font64" style="font-style:italic;">Computer Vision and Pattern Recognition&#160;(CVPR), 2012 IEEE Conference on</span><span class="font64">, pages 3370-3377. IEEE. 345</span></p>
<p><span class="font64">Jim, K.-C., Giles, C. L., and Horne, B. G. (1996). An analysis of noise in recurrent neural networks: convergence and generalization. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Neural Networks</span><span class="font64">,&#160;7(6), 1424-1438. 242</span></p>
<p><span class="font64">Jordan, M. I. (1998). </span><span class="font64" style="font-style:italic;">Learning in Graphical Models</span><span class="font64">. Kluwer, Dordrecht, Netherlands. 18</span></p>
<p><span class="font64">Joulin, A. and Mikolov, T. (2015). Inferring algorithmic patterns with stack-augmented recurrent nets. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1503.01007</span><span class="font64">. 420</span></p>
<p><span class="font64">Jozefowicz, R., Zaremba, W., and Sutskever, I. (2015). An empirical evaluation of recurrent network architectures. In </span><span class="font64" style="font-style:italic;">ICML’2015</span><span class="font64">. 306, 413, 414</span></p>
<p><span class="font64">Judd, J. S. (1989). </span><span class="font64" style="font-style:italic;">Neural Network Design and the Complexity of Learning</span><span class="font64">. MIT press. 293</span></p>
<p><span class="font64">Jutten, C. and Herault, J. (1991). Blind separation of sources, part I: an adaptive algorithm based on neuromimetic architecture. </span><span class="font64" style="font-style:italic;">Signal Processing</span><span class="font64">, 24, 1-10. 493</span></p>
<p><span class="font64">Kahou, S. E., Pal, C., Bouthillier, X., Froumenty, P., Gulgehre, c., Memisevic, R., Vincent, P., Courville, A., Bengio, Y., Ferrari, R. C., Mirza, M., Jean, S., Carrier, P. L., Dauphin,&#160;Y., Boulanger-Lewandowski, N., Aggarwal, A., Zumer, J., Lamblin, P., Raymond,&#160;J.-P., Desjardins, G., Pascanu, R., Warde-Farley, D., Torabi, A., Sharma, A., Bengio,&#160;E., Cote, M., Konda, K. R., and Wu, Z. (2013). Combining modality specific deep&#160;neural networks for emotion recognition in video. In </span><span class="font64" style="font-style:italic;">Proceedings of the 15th ACM on&#160;International Conference on Multimodal Interaction</span><span class="font64">. 200</span></p>
<p><span class="font64">Kalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In </span><span class="font64" style="font-style:italic;">EMNLP’2013</span><span class="font64">. 476</span></p>
<p><span class="font64">Kalchbrenner, N., Danihelka, I., and Graves, A. (2015). Grid long short-term memory. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1507.01526</span><span class="font64">. 396</span></p>
<p><span class="font64">Kamyshanska, H. and Memisevic, R. (2015). The potential energy of an autoencoder. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font64">. 517</span></p>
<p><span class="font64">Karpathy, A. and Li, F.-F. (2015). Deep visual-semantic alignments for generating image descriptions. In </span><span class="font64" style="font-style:italic;">CVPR’2015</span><span class="font64">. arXiv:1412.2306. 102</span></p>
<p><span class="font64">Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., and Fei-Fei, L. (2014). Large-scale video classification with convolutional neural networks. In </span><span class="font64" style="font-style:italic;">CVPR.</span><span class="font64"> 21</span></p>
<p><span class="font64">Karush, W. (1939). </span><span class="font64" style="font-style:italic;">Minima of Functions of Several Variables with Inequalities as Side Constraints</span><span class="font64">. Master’s thesis, Dept. of Mathematics, Univ. of Chicago. 95</span></p>
<p><span class="font64">Katz, S. M. (1987). Estimation of probabilities from sparse data for the language model component of a speech recognizer. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Acoustics, Speech, and Signal&#160;Processing</span><span class="font64">, ASSP-35(3), 400-401. 464, 475</span></p>
<p><span class="font64">Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008). Fast inference in sparse coding algorithms with applications to object recognition. Technical report, Computational and&#160;Biological Learning Lab, Courant Institute, NYU. Tech Report CBLL-TR-2008-12-01.&#160;525</span></p>
<p><span class="font64">Kavukcuoglu, K., Ranzato, M.-A., Fergus, R., and LeCun, Y. (2009). Learning invariant features through topographic filter maps. In </span><span class="font64" style="font-style:italic;">CVPR’2009</span><span class="font64">. 525</span></p>
<p><span class="font64">Kavukcuoglu, K., Sermanet, P., Boureau, Y.-L., Gregor, K., Mathieu, M., and LeCun, Y. (2010). Learning convolutional feature hierarchies for visual recognition. In </span><span class="font64" style="font-style:italic;">NIPS’2010</span><span class="font64">.&#160;364, 525</span></p>
<p><span class="font64">Kelley, H. J. (1960). Gradient theory of optimal flight paths. </span><span class="font64" style="font-style:italic;">ARS Journal</span><span class="font64">, 30(10), 947-954. 224</span></p>
<p><span class="font64">Khan, F., Zhu, X., and Mutlu, B. (2011). How do humans teach: On curriculum learning and teaching dimension. In </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 24&#160;(NIPS’11),</span><span class="font64"> pages 1449-1457. 328</span></p>
<p><span class="font64">Kim, S. K., McAfee, L. C., McMahon, P. L., and Olukotun, K. (2009). A highly scalable restricted Boltzmann machine FPGA implementation. In </span><span class="font64" style="font-style:italic;">Field Programmable Logic&#160;and Applications, 2009. FPL 2009. International Conference on</span><span class="font64">, pages 367-372. IEEE.&#160;453</span></p>
<p><span class="font64">Kindermann, R. (1980). </span><span class="font64" style="font-style:italic;">Markov Random Fields and Their Applications (Contemporary Mathematics ; V. 1).</span><span class="font64"> American Mathematical Society. 568</span></p>
<p><span class="font64">Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1412.6980</span><span class="font64">. 308</span></p>
<p><span class="font64">Kingma, D. and LeCun, Y. (2010). Regularized estimation of image statistics by score matching. In </span><span class="font64" style="font-style:italic;">NIPS’2010</span><span class="font64">. 515, 622</span></p>
<p><span class="font64">Kingma, D., Rezende, D., Mohamed, S., and Welling, M. (2014). Semi-supervised learning with deep generative models. In </span><span class="font64" style="font-style:italic;">NIPS’2014</span><span class="font64">. 428</span></p>
<p><span class="font64">Kingma, D. P. (2013). Fast gradient-based inference with continuous latent variable models in auxiliary form. Technical report, arxiv:1306.0733. 654, 691, 698</span></p>
<p><span class="font64">Kingma, D. P. and Welling, M. (2014a). Auto-encoding variational bayes. In </span><span class="font64" style="font-style:italic;">Proceedings of the International Conference on Learning Representations (ICLR).</span><span class="font64"> 691, 701</span></p>
<p><span class="font64">Kingma, D. P. and Welling, M. (2014b). Efficient gradient-based inference through transformations between bayes nets and neural nets. Technical report, arxiv:1402.0480.&#160;691</span></p>
<p><span class="font64">Kirkpatrick, S., Jr., C. D. G., , and Vecchi, M. P. (1983). Optimization by simulated annealing. </span><span class="font64" style="font-style:italic;">Science</span><span class="font64">, 220, 671-680. 327</span></p>
<p><span class="font64">Kiros, R., Salakhutdinov, R., and Zemel, R. (2014a). Multimodal neural language models. In </span><span class="font64" style="font-style:italic;">ICML ’2014</span><span class="font64">. 102</span></p>
<p><span class="font64">Kiros, R., Salakhutdinov, R., and Zemel, R. (2014b). Unifying visual-semantic embeddings with multimodal neural language models. arXiv:1411.2539 [cs.LG]. 102, 410</span></p>
<p><span class="font64">Klementiev, A., Titov, I., and Bhattarai, B. (2012). Inducing crosslingual distributed representations of words. In </span><span class="font64" style="font-style:italic;">Proceedings of COLING 2012</span><span class="font64">. 478, 541</span></p>
<p><span class="font64">Knowles-Barley, S., Jones, T. R., Morgan, J., Lee, D., Kasthuri, N., Lichtman, J. W., and Pfister, H. (2014). Deep learning for the connectome. </span><span class="font64" style="font-style:italic;">GPU Technology Conference</span><span class="font64">. 26</span></p>
<p><span class="font64">Koller, D. and Friedman, N. (2009). </span><span class="font64" style="font-style:italic;">Probabilistic Graphical Models: Principles and Techniques</span><span class="font64">. MIT Press. 585, 597, 647</span></p>
<p><span class="font64">Konig, Y., Bourlard, H., and Morgan, N. (1996). REMAP: Recursive estimation and maximization of a posteriori probabilities - application to transition-based connectionist&#160;speech recognition. In D. Touretzky, M. Mozer, and M. Hasselmo, editors, </span><span class="font64" style="font-style:italic;">Advances in&#160;Neural Information Processing Systems 8 (NIPS’95).</span><span class="font64"> MIT Press, Cambridge, MA. 461</span></p>
<p><span class="font64">Koren, Y. (2009). The BellKor solution to the Netflix grand prize. 258, 481</span></p>
<p><span class="font64">Kotzias, D., Denil, M., de Freitas, N., and Smyth, P. (2015). From group to individual labels using deep features. In </span><span class="font64" style="font-style:italic;">ACM SIGKDD</span><span class="font64">. 106</span></p>
<p><span class="font64">Koutnik, J., Greff, K., Gomez, F., and Schmidhuber, J. (2014). A clockwork RNN. In </span><span class="font64" style="font-style:italic;">ICML ’2014</span><span class="font64">. 409</span></p>
<p><span class="font64">Kocisky, T., Hermann, K. M., and Blunsom, P. (2014). Learning Bilingual Word Representations by Marginalizing Alignments. In </span><span class="font64" style="font-style:italic;">Proceedings of ACL.</span><span class="font64"> 478</span></p>
<p><span class="font64">Krause, O., Fischer, A., Glasmachers, T., and Igel, C. (2013). Approximation properties of DBNs with binary hidden units and real-valued visible units. In </span><span class="font64" style="font-style:italic;">ICML’2013</span><span class="font64">. 555</span></p>
<p><span class="font64">Krizhevsky, A. (2010). Convolutional deep belief networks on CIFAR-10. Technical report, University of Toronto. Unpublished Manuscript: </span><a href="http://www.cs.utoronto.ca/"><span class="font64">http://www.cs.utoronto.ca/</span></a><span class="font64"> kriz/conv-cifar10-aug2010.pdf. 448</span></p>
<p><span class="font64">Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images. Technical report, University of Toronto. 21, 563</span></p>
<p><span class="font64">Krizhevsky, A. and Hinton, G. E. (2011). Using very deep autoencoders for content-based image retrieval. In </span><span class="font64" style="font-style:italic;">ESANN</span><span class="font64">. 527</span></p>
<p><span class="font64">Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In </span><span class="font64" style="font-style:italic;">NIPS’2012</span><span class="font64">. 23, 24, 27, 100, 200, 371, 456, 460</span></p>
<p><span class="font64">Krueger, K. A. and Dayan, P. (2009). Flexible shaping: how learning in small steps helps. </span><span class="font64" style="font-style:italic;">Cognition</span><span class="font64">, 110, 380-394. 328</span></p>
<p><span class="font64">Kuhn, H. W. and Tucker, A. W. (1951). Nonlinear programming. In </span><span class="font64" style="font-style:italic;">Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability</span><span class="font64">, pages 481-492,&#160;Berkeley, Calif. University of California Press. 95</span></p>
<p><span class="font64">Kumar, A., Irsoy, O., Su, J., Bradbury, J., English, R., Pierce, B., Ondruska, P., Iyyer, M., Gulrajani, I., and Socher, R. (2015). Ask me anything: Dynamic memory networks&#160;for natural language processing. </span><span class="font64" style="font-style:italic;">arXiv:1506.07285</span><span class="font64">. 420, 487</span></p>
<p><span class="font64">Kumar, M. P., Packer, B., and Koller, D. (2010). Self-paced learning for latent variable models. In </span><span class="font64" style="font-style:italic;">NIPS’2010</span><span class="font64">. 328</span></p>
<p><span class="font64">Lang, K. J. and Hinton, G. E. (1988). The development of the time-delay neural network architecture for speech recognition. Technical Report CMU-CS-88-152, Carnegie-Mellon&#160;University. 367, 374, 408</span></p>
<p><span class="font64">Lang, K. J., Waibel, A. H., and Hinton, G. E. (1990). A time-delay neural network architecture for isolated word recognition. </span><span class="font64" style="font-style:italic;">Neural networks</span><span class="font64">, 3(1), 23-43. 374</span></p>
<p><span class="font64">Langford, J. and Zhang, T. (2008). The epoch-greedy algorithm for contextual multi-armed bandits. In </span><span class="font64" style="font-style:italic;">NIPS’2008</span><span class="font64">, pages 1096—1103. 482</span></p>
<p><span class="font64">Lappalainen, H., Giannakopoulos, X., Honkela, A., and Karhunen, J. (2000). Nonlinear independent component analysis using ensemble learning: Experiments and discussion.&#160;In </span><span class="font64" style="font-style:italic;">Proc. ICA.</span><span class="font64"> Citeseer. 495</span></p>
<p><span class="font64">Larochelle, H. and Bengio, Y. (2008). Classification using discriminative restricted Boltzmann machines. In </span><span class="font64" style="font-style:italic;">ICML’2008</span><span class="font64">. 244, 255, 532, 688, 717</span></p>
<p><span class="font64">Larochelle, H. and Hinton, G. E. (2010). Learning to combine foveal glimpses with a third-order Boltzmann machine. In </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems&#160;23</span><span class="font64">, pages 1243-1251. 367</span></p>
<p><span class="font64">Larochelle, H. and Murray, I. (2011). The Neural Autoregressive Distribution Estimator. In </span><span class="font64" style="font-style:italic;">AISTATS’2011</span><span class="font64">. 707, 710</span></p>
<p><span class="font64">Larochelle, H., Erhan, D., and Bengio, Y. (2008). Zero-data learning of new tasks. In </span><span class="font64" style="font-style:italic;">AAAI Conference on Artificial Intelligence</span><span class="font64">. 541</span></p>
<p><span class="font64">Larochelle, H., Bengio, Y., Louradour, J., and Lamblin, P. (2009). Exploring strategies for training deep neural networks. </span><span class="font64" style="font-style:italic;">Journal of Machine Learning Research</span><span class="font64">, 10, 1-40. 537</span></p>
<p><span class="font64">Lasserre, J. A., Bishop, C. M., and Minka, T. P. (2006). Principled hybrids of generative and discriminative models. In </span><span class="font64" style="font-style:italic;">Proceedings of the Computer Vision and Pattern Recognition&#160;Conference (CVPR’06)</span><span class="font64">, pages 87-94, Washington, DC, USA. IEEE Computer Society.&#160;244, 253</span></p>
<p><span class="font64">Le, Q., Ngiam, J., Chen, Z., hao Chia, D. J., Koh, P. W., and Ng, A. (2010). Tiled convolutional neural networks. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor,&#160;R. Zemel, and A. Culotta, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems&#160;23 (NIPS’10)</span><span class="font64">, pages 1279-1287. 352</span></p>
<p><span class="font64">Le, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng, A. (2011). On optimization methods for deep learning. In </span><span class="font64" style="font-style:italic;">Proc. ICML’2011</span><span class="font64">. ACM. 316</span></p>
<p><span class="font64">Le, Q., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., and Ng, A. (2012). Building high-level features using large scale unsupervised learning. In&#160;</span><span class="font64" style="font-style:italic;">ICML’2012</span><span class="font64">. 24, 27</span></p>
</body>
</html>