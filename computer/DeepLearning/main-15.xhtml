<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h4><a id="bookmark0"></a><span class="font65" style="font-weight:bold;">7.9 Parameter Tying and Parameter Sharing</span></h4>
<p><span class="font64">Thus far, in this chapter, when we have discussed adding constraints or penalties to the parameters, we have always done so with respect to a fixed region or point.&#160;For example, L<sup>2</sup> regularization (or weight decay) penalizes model parameters for&#160;deviating from the fixed value of zero. However, sometimes we may need other&#160;ways to express our prior knowledge about suitable values of the model parameters.&#160;Sometimes we might not know precisely what values the parameters should take&#160;but we know, from knowledge of the domain and model architecture, that there&#160;should be some dependencies between the model parameters.</span></p>
<p><span class="font64">A common type of dependency that we often want to express is that certain parameters should be close to one another. Consider the following scenario: we&#160;have two models performing the same classification task (with the same set of&#160;classes) but with somewhat different input distributions. Formally, we have model&#160;A with parameters </span><span class="font64" style="font-weight:bold;">w</span><span class="font64"><sup>(A)</sup> and model B with parameters </span><span class="font64" style="font-weight:bold;">w</span><span class="font64"><sup>(B)</sup>. The two models&#160;map the input to two different, but related outputs: y<sup>(A)</sup> = f(</span><span class="font64" style="font-weight:bold;">w</span><span class="font64"><sup>(A)</sup>, </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) and&#160;y= g(</span><span class="font64" style="font-weight:bold;">w</span><span class="font64"><sup>(B)</sup></span><span class="font64" style="font-weight:bold;">, x</span><span class="font64">).</span></p>
<p><span class="font64">Let us imagine that the tasks are similar enough (perhaps with similar input and output distributions) that we believe the model parameters should be close&#160;to each other: Vi,&#160;&#160;&#160;&#160;should be close to . We can leverage this information</span></p>
<p><span class="font64">through regularization. Specifically, we can use a parameter norm penalty of the form: </span><span class="font64" style="font-weight:bold;font-style:italic;">0,(w</span><span class="font64"><sup>(A)</sup>, </span><span class="font64" style="font-weight:bold;">w</span><span class="font64"><sup>(B)</sup>) = || </span><span class="font64" style="font-weight:bold;">w</span><span class="font64"><sup>(A)</sup> — </span><span class="font64" style="font-weight:bold;">w</span><span class="font64"><sup>(B)</sup>||2. Here we used an L<sup>2</sup> penalty, but other&#160;choices are also possible.</span></p>
<p><span class="font64">This kind of approach was proposed by Lasserre </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2006), who regularized the parameters of one model, trained as a classifier in a supervised paradigm, to&#160;be close to the parameters of another model, trained in an unsupervised paradigm&#160;(to capture the distribution of the observed input data). The architectures were&#160;constructed such that many of the parameters in the classifier model could be&#160;paired to corresponding parameters in the unsupervised model.</span></p>
<p><span class="font64">While a parameter norm penalty is one way to regularize parameters to be close to one another, the more popular way is to use constraints: </span><span class="font64" style="font-weight:bold;">to force sets of&#160;parameters to be equal</span><span class="font64">. This method of regularization is often referred to as&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">parameter sharing,</span><span class="font64"> where we interpret the various models or model components as&#160;sharing a unique set of parameters. A significant advantage of parameter sharing&#160;over regularizing the parameters to be close (via a norm penalty) is that only a&#160;subset of the parameters (the unique set) need to be stored in memory. In certain&#160;models—such as the convolutional neural network—this can lead to significant&#160;reduction in the memory footprint of the model.</span></p>
<p><span class="font64" style="font-weight:bold;">Convolutional Neural Networks </span><span class="font64">By far the most popular and extensive use of parameter sharing occurs in </span><span class="font64" style="font-weight:bold;font-style:italic;">convolutional neural networks</span><span class="font64"> (CNNs) applied to&#160;computer vision.</span></p>
<p><span class="font64">Natural images have many statistical properties that are invariant to translation. For example, a photo of a cat remains a photo of a cat if it is translated one pixel&#160;to the right. CNNs take this property into account by sharing parameters across&#160;multiple image locations. The same feature (a hidden unit with the same weights)&#160;is computed over different locations in the input. This means that we can find a&#160;cat with the same cat detector whether the cat appears at column i or column&#160;i + 1 in the image.</span></p>
<p><span class="font64">Parameter sharing has allowed CNNs to dramatically lower the number of unique model parameters and to significantly increase network sizes without requiring a&#160;corresponding increase in training data. It remains one of the best examples of&#160;how to effectively incorporate domain knowledge into the network architecture.</span></p>
<p><span class="font64">CNNs will be discussed in more detail in Chapter 9.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">7.10 Sparse Representations</span></h4>
<p><span class="font64">Weight decay acts by placing a penalty directly on the model parameters. Another strategy is to place a penalty on the activations of the units in a neural network,&#160;encouraging their activations to be sparse. This indirectly imposes a complicated&#160;penalty on the model parameters.</span></p>
<p><span class="font64">We have already discussed (in Sec. 7.1.2) how L<sup>1</sup> penalization induces a sparse parametrization—meaning that many of the parameters become zero (or close to&#160;zero). Representational sparsity, on the other hand, describes a representation&#160;where many of the elements of the representation are zero (or close to zero).&#160;A simplified view of this distinction can be illustrated in the context of linear&#160;regression:</span></p><div>
<table border="1">
<tr><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td colspan="2">
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font64">2</span></p></td></tr>
<tr><td>
<p><span class="font64">' 18 &quot;</span></p></td><td>
<p></p></td><td>
<p><span class="font64">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">-2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td>
<p></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font64">3</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font64">5</span></p></td><td rowspan="2">
<p></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font64">0</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font64">0</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font64">-1</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font64">0</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font64">3</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font64">0</span></p></td><td rowspan="2">
<p></p></td></tr>
<tr><td rowspan="2">
<p><span class="font64">-2</span></p></td></tr>
<tr><td>
<p><span class="font64">15</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td>
<p><span class="font64">5</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font64">-5</span></p></td></tr>
<tr><td>
<p><span class="font64">-9</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">-1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td>
<p><span class="font64">-4</span></p></td><td>
<p></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font64">1</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">-3 _</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">-5</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p><span class="font64">4</span></p></td></tr>
<tr><td colspan="2">
<p><span class="font64" style="font-weight:bold;">y </span><span class="font64" style="font-weight:bold;font-style:italic;">e R<sup>m</sup></span></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p><span class="font64" style="font-weight:bold;font-style:italic;">A e</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">R</span></p></td><td>
<p><span class="font62">mxn</span></p></td><td>
<p></p></td><td>
<p></p></td><td colspan="2">
<p><span class="font64" style="font-weight:bold;font-style:italic;">x e</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">R</span></p></td></tr>
</table></div>
<p><span class="font64">(7.46)</span></p>
<p><span class="font64">In the first expression, we have an example of a sparsely parametrized linear regression model. In the second, we have linear regression with a sparse representation h of the data x. That is, h is a function of x that, in some sense, represents&#160;the information present in x, but does so with a sparse vector.</span></p>
<p><span class="font64">Representational regularization is accomplished by the same sorts of mechanisms that we have used in parameter regularization.</span></p>
<p><span class="font64">Norm penalty regularization of representations is performed by adding to the loss function </span><span class="font64" style="font-weight:bold;">J </span><span class="font64">a norm penalty on the </span><span class="font64" style="font-weight:bold;">representation</span><span class="font64">. This penalty is denoted&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">Q(h).</span><span class="font64"> As before, we denote the regularized loss function by </span><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64">:</span></p><div>
<p><span class="font64">(7.48)</span></p></div><div>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font64">-1</span></p>
<p><span class="font64">1</span></p>
<p><span class="font64">_1</span></p></td><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font64">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">—1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">—5</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font64">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">—3</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">—1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">3</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">19</span></p></td><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font64">— 1</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">4</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">—3</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">—2</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">2</span></p></td><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font64">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">—3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">0</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">—3</span></p></td></tr>
<tr><td>
<p><span class="font64">23</span></p></td><td>
<p></p></td><td>
<p><span class="font64">—5</span></p></td><td>
<p><span class="font64">4</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">—2</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">2</span></p></td><td>
<p><span class="font64">—5</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">—1</span></p></td></tr>
</table></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">y e</span><span class="font64"> R<sup>T</sup></span></p></div><div>
<p><span class="font64">Be R<sup>r</sup></span></p></div><div>
<p><span class="font64">0 2&#160;0&#160;0</span></p>
<p><span class="font64">-3 0</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">he</span><span class="font64"> R</span></p></div><div>
<p><span class="font64">(7.47)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">J(e; X</span><span class="font64">, y) = J( </span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font64">; X, y) + afi(h)</span></p>
<p><span class="font64">where a e </span><span class="font64" style="font-variant:small-caps;">[0, to) weights the relative contribution of the norm penalty term, with larger values of a corresponding to more regularization.</span></p>
<p><span class="font64">Just as an L<sup>1</sup> penalty on the parameters induces parameter sparsity, an L<sup>1 </sup>penalty on the elements of the representation induces representational sparsity:&#160;Q(h) = ||h||</span><span class="font18">1</span><span class="font64"> = ^<sub>i</sub> |hi|. Of course, the L<sup>1</sup> penalty is only one choice of penalty&#160;that can result in a sparse representation. Others include the penalty derived from&#160;a Student- prior on the representation (Olshausen and Field, 1996; Bergstra, 2011)&#160;and KL divergence penalties (Larochelle and Bengio, 2008) that are especially&#160;useful for representations with elements constrained to lie on the unit interval.&#160;Lee </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2008) and Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2009) both provide examples of strategies&#160;based on regularizing the average activation across several examples, A&#160;&#160;&#160;&#160;h<sup>(i)</sup>, to</span></p>
<p><span class="font64">be near some target value, such as a vector with .01 for each entry.</span></p>
<p><span class="font64">Other approaches obtain representational sparsity with a hard constraint on the activation values. For example, </span><span class="font64" style="font-weight:bold;font-style:italic;">orthogonal matching pursuit</span><span class="font64"> (Pati </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">1993) encodes an input x with the representation h that solves the constrained&#160;optimization problem</span></p>
<p><span class="font64">argmin ||x — Wh||<sup>2</sup>, &#160;&#160;&#160;(7.49)</span></p>
<p><span class="font64" style="font-weight:bold;"><sup>h</sup>,||<sup>h</sup>||0 </span><span class="font64" style="font-weight:bold;font-style:italic;">&lt;k</span></p>
<p><span class="font64">where ||h||<sub>0</sub> is the number of non-zero entries of h. This problem can be solved efficiently when W is constrained to be orthogonal. This method is often called</span></p>
<p><span class="font64">OMP-k with the value of k specified to indicate the number of non-zero features allowed. Coates and Ng (2011) demonstrated that OMP-1 can be a very effective&#160;feature extractor for deep architectures.</span></p>
<p><span class="font64">Essentially any model that has hidden units can be made sparse. Throughout this book, we will see many examples of sparsity regularization used in a variety of&#160;contexts.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">7.11 Bagging and Other Ensemble Methods</span></h4>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Bagging</span><span class="font64"> (short for </span><span class="font64" style="font-weight:bold;font-style:italic;">bootstrap aggregating)</span><span class="font64"> is a technique for reducing generalization error by combining several models (Breiman, 1994). The idea is to train several&#160;different models separately, then have all of the models vote on the output for test&#160;examples. This is an example of a general strategy in machine learning called </span><span class="font64" style="font-weight:bold;font-style:italic;">model&#160;averaging.</span><span class="font64"> Techniques employing this strategy are known as </span><span class="font64" style="font-weight:bold;font-style:italic;">ensemble methods.</span></p>
<p><span class="font64">The reason that model averaging works is that different models will usually not make all the same errors on the test set.</span></p>
<p><span class="font64">Consider for example a set of k regression models. Suppose that each model makes an error e<sub>i</sub> on each example, with the errors drawn from a zero-mean&#160;multivariate normal distribution with variances E[e <sup>2</sup>] = v and covariances E[e<sub>i</sub>e j =&#160;c. Then the error made by the average prediction of all the ensemble models is&#160;k </span><span class="font64" style="font-weight:bold;font-style:italic;">Yli e</span><span class="font64"><sub>i</sub>. The expected squared error of the ensemble predictor is</span></p><div>
<p><span class="font64">E</span></p></div><div>
<p><span class="font64">(k <sup>?</sup> ״)</span></p></div><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64">5ZI<sup>e</sup>•<sup>2</sup> + <sup>eie</sup>j</span></p>
<p><span class="font64"><sup>i</sup> \ &#160;&#160;&#160;<sup>j</sup>=<sup>i</sup></span></p>
<p><span class="font64">1 k-1</span></p></div><div>
<p><span class="font64">k<sup>2 E</sup></span></p></div><div>
<p><span class="font64">= k<sup>v</sup> + k <sup>c</sup>'</span></p></div><div>
<p><span class="font64">(7.50)</span></p>
<p><span class="font64">(7.51)</span></p></div>
<p><span class="font64">In the case where the errors are perfectly correlated and c = v, the mean squared error reduces to v, so the model averaging does not help at all. In the case where&#160;the errors are perfectly uncorrelated and c = 0, the expected squared error of the&#160;ensemble is only -|,v. This means that the expected squared error of the ensemble&#160;decreases linearly with the ensemble size. In other words, on average, the ensemble&#160;will perform at least as well as any of its members, and if the members make&#160;independent errors, the ensemble will perform significantly better than its members.</span></p>
<p><span class="font64">Different ensemble methods construct the ensemble of models in different ways. For example, each member of the ensemble could be formed by training a completely&#160;different kind of model using a different algorithm or objective function. Bagging</span></p><div><div>
<p><span class="font63">Original dataset</span></p><img src="main-70.jpg" alt=""/></div></div><div><div>
<p><span class="font63">First resampled dataset</span></p><img src="main-71.jpg" alt=""/>
<p><span class="font63">Second resampled dataset ©@®</span></p></div></div><div><div><img src="main-72.jpg" alt=""/></div></div>
<p><span class="font64">Figure 7.5: A cartoon depiction of how bagging works. Suppose we train an ‘8’ detector on the dataset depicted above, containing an ‘8’, a ‘6’ and a ‘9’. Suppose we make two&#160;different resampled datasets. The bagging training procedure is to construct each of these&#160;datasets by sampling with replacement. The first dataset omits the ‘9’ and repeats the ‘8’.&#160;On this dataset, the detector learns that a loop on top of the digit corresponds to an ‘8’.&#160;On the second dataset, we repeat the ‘9’ and omit the ‘6’. In this case, the detector learns&#160;that a loop on the bottom of the digit corresponds to an ‘8’. Each of these individual&#160;classification rules is brittle, but if we average their output then the detector is robust,&#160;achieving maximal confidence only when both loops of the ‘8’ are present.</span></p>
<p><span class="font64">is a method that allows the same kind of model, training algorithm and objective function to be reused several times.</span></p>
<p><span class="font64">Specifically, bagging involves constructing k different datasets. Each dataset has the same number of examples as the original dataset, but each dataset is&#160;constructed by sampling with replacement from the original dataset. This means&#160;that, with high probability, each dataset is missing some of the examples from the&#160;original dataset and also contains several duplicate examples (on average around&#160;2/3 of the examples from the original dataset are found in the resulting training&#160;set, if it has the same size as the original). Model i is then trained on dataset&#160;i. The differences between which examples are included in each dataset result in&#160;differences between the trained models. See Fig. 7.5 for an example.</span></p>
<p><span class="font64">Neural networks reach a wide enough variety of solution points that they can often benefit from model averaging even if all of the models are trained on the same&#160;dataset. Differences in random initialization, random selection of minibatches,&#160;differences in hyperparameters, or different outcomes of non-deterministic implementations of neural networks are often enough to cause different members of the&#160;ensemble to make partially independent errors.</span></p>
<p><span class="font64">Model averaging is an extremely powerful and reliable method for reducing generalization error. Its use is usually discouraged when benchmarking algorithms&#160;for scientific papers, because any machine learning algorithm can benefit substantially from model averaging at the price of increased computation and memory.&#160;For this reason, benchmark comparisons are usually made using a single model.</span></p>
<p><span class="font64">Machine learning contests are usually won by methods using model averaging over dozens of models. A recent prominent example is the Netflix Grand Prize (Koren, 2009).</span></p>
<p><span class="font64">Not all techniques for constructing ensembles are designed to make the ensemble more regularized than the individual models. For example, a technique called&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">boosting</span><span class="font64"> (Freund and Schapire, 1996b,a) constructs an ensemble with higher capacity&#160;than the individual models. Boosting has been applied to build ensembles of neural&#160;networks (Schwenk and Bengio, 1998) by incrementally adding neural networks to&#160;the ensemble. Boosting has also been applied interpreting an individual neural&#160;network as an ensemble (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006a), incrementally adding hidden units&#160;to the neural network.</span></p><h4><a id="bookmark3"></a><span class="font65" style="font-weight:bold;">7.12 Dropout</span></h4>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Dropout</span><span class="font64"> (Srivastava </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014) provides a computationally inexpensive but powerful method of regularizing a broad family of models. To a first approximation,&#160;dropout can be thought of as a method of making bagging practical for ensembles&#160;of very many large neural networks. Bagging involves training multiple models,&#160;and evaluating multiple models on each test example. This seems impractical&#160;when each model is a large neural network, since training and evaluating such&#160;networks is costly in terms of runtime and memory. It is common to use ensembles&#160;of five to ten neural networks—Szegedy </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014a) used six to win the ILSVRC—&#160;but more than this rapidly becomes unwieldy. Dropout provides an inexpensive&#160;approximation to training and evaluating a bagged ensemble of exponentially many&#160;neural networks.</span></p>
<p><span class="font64">Specifically, dropout trains the ensemble consisting of all sub-networks that can be formed by removing non-output units from an underlying base network,&#160;as illustrated in Fig. 7.6. In most modern neural networks, based on a series of&#160;affine transformations and nonlinearities, we can effectively remove a unit from a&#160;network by multiplying its output value by zero. This procedure requires some&#160;slight modification for models such as radial basis function networks, which take&#160;the difference between the unit’s state and some reference value. Here, we present&#160;the dropout algorithm in terms of multiplication by zero for simplicity, but it can&#160;be trivially modified to work with other operations that remove a unit from the&#160;network.</span></p>
<p><span class="font64">Recall that to learn with bagging, we define k different models, construct </span><span class="font64" style="font-weight:bold;font-style:italic;">k </span><span class="font64">different datasets by sampling from the training set with replacement, and then&#160;train model i on dataset </span><span class="font64" style="font-weight:bold;font-style:italic;">i.</span><span class="font64"> Dropout aims to approximate this process, but with an&#160;exponentially large number of neural networks. Specifically, to train with dropout,&#160;we use a minibatch-based learning algorithm that makes small steps, such as&#160;stochastic gradient descent. Each time we load an example into a minibatch, we&#160;randomly sample a different binary mask to apply to all of the input and hidden&#160;units in the network. The mask for each unit is sampled independently from all of&#160;the others. The probability of sampling a mask value of one (causing a unit to be&#160;included) is a hyperparameter fixed before training begins. It is not a function&#160;of the current value of the model parameters or the input example. Typically,&#160;an input unit is included with probability 0.8 and a hidden unit is included with&#160;probability 0.5. We then run forward propagation, back-propagation, and the&#160;learning update as usual. Fig. 7.7 illustrates how to run forward propagation with&#160;dropout.</span></p>
<p><span class="font64">More formally, suppose that a mask vector </span><span class="font64" style="font-weight:bold;font-style:italic;">fi</span><span class="font64"> specifies which units to include, and J(6, i) defines the cost of the model defined by parameters 6 and mask !׳.&#160;Then dropout training consists in minimizing E^ J(6, i). The expectation contains&#160;exponentially many terms but we can obtain an unbiased estimate of its gradient&#160;by sampling values of !׳.</span></p>
<p><span class="font64">Dropout training is not quite the same as bagging training. In the case of bagging, the models are all independent. In the case of dropout, the models share&#160;parameters, with each model inheriting a different subset of parameters from the&#160;parent neural network. This parameter sharing makes it possible to represent an&#160;exponential number of models with a tractable amount of memory. In the case of&#160;bagging, each model is trained to convergence on its respective training set. In the&#160;case of dropout, typically most models are not explicitly trained at all—usually,&#160;the model is large enough that it would be infeasible to sample all possible subnetworks within the lifetime of the universe. Instead, a tiny fraction of the possible&#160;sub-networks are each trained for a single step, and the parameter sharing causes&#160;the remaining sub-networks to arrive at good settings of the parameters. These&#160;are the only differences. Beyond these, dropout follows the bagging algorithm. For&#160;example, the training set encountered by each sub-network is indeed a subset of&#160;the original training set sampled with replacement.</span></p>
<p><span class="font64">To make a prediction, a bagged ensemble must accumulate votes from all of its members. We refer to this process as </span><span class="font64" style="font-weight:bold;font-style:italic;">inference</span><span class="font64"> in this context. So far, our</span></p><div><img src="main-73.jpg" alt=""/></div><div><img src="main-74.jpg" alt=""/></div>
<p><span class="font64">description of bagging and dropout has not required that the model be explicitly probabilistic. Now, we assume that the model’s role is to output a probability&#160;distribution. In the case of bagging, each model i produces a probability distribution</span></p><div>
<p><span class="font64">pM</span></p>
<p><span class="font64">of these distributions,</span></p></div>
<p><span class="font64">x). The prediction of the ensemble is given by the arithmetic mean of all</span></p><div>
<p><span class="font64">(7.52)</span></p></div>
<p><span class="font64">1<sup>x)</sup>׳</span></p>
<p><span class="font64">In the case of dropout, each sub-model defined by mask vector i defines a probability distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p(y</span><span class="font64"> | x, i). The arithmetic mean over all masks is given by</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p<sup>(</sup>i)p<sup>(</sup>v</span><span class="font64"> <sup>1 </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>x,</sup> i)</span><span class="font64"> &#160;&#160;&#160;(<sup>7</sup>.<sup>53</sup>)</span></p>
<p><span class="font64">M</span></p>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;font-style:italic;">p(l)</span><span class="font64"> is the probability distribution that was used to sample i at training time.</span></p>
<p><span class="font64">Because this sum includes an exponential number of terms, it is intractable to evaluate except in cases where the structure of the model permits some form&#160;of simplification. So far, deep neural nets are not known to permit any tractable&#160;simplification. Instead, we can approximate the inference with sampling, by&#160;averaging together the output from many masks. Even 10-20 masks are often&#160;sufficient to obtain good performance.</span></p>
<p><span class="font64">However, there is an even better approach, that allows us to obtain a good approximation to the predictions of the entire ensemble, at the cost of only one&#160;forward propagation. To do so, we change to using the geometric mean rather than&#160;the arithmetic mean of the ensemble members’ predicted distributions. Warde-Farley </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) present arguments and empirical evidence that the geometric&#160;mean performs comparably to the arithmetic mean in this context.</span></p>
<p><span class="font64">The geometric mean of multiple probability distributions is not guaranteed to be a probability distribution. To guarantee that the result is a probability distribution,&#160;we impose the requirement that none of the sub-models assigns probability 0 to any&#160;event, and we renormalize the resulting distribution. The unnormalized probability&#160;distribution defined directly by the geometric mean is given by</span></p><div>
<p><span class="font64">(7.54)</span></p></div>
<p><span class="font64">pensemble(v | x) = </span><span class="font18">2</span><span class="font64"><sup>d</sup> &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">p(v</span><span class="font64"> | X, l)</span></p>
<p><span class="font64">M</span></p>
<p><span class="font64">where d is the number of units that may be dropped. Here we use a uniform distribution over </span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> to simplify the presentation, but non-uniform distributions are</span></p>
<p><span class="font64">also possible. To make predictions we must re-normalize the ensemble:</span></p><div>
<p><span class="font64">(7.55)</span></p></div><div>
<p><span class="font64">Pensemble<sup>(y 1 x)</sup> —</span></p></div>
<p><span class="font64">Ttnsemble <sup>(y 1 x)</sup></span></p>
<p><span class="font64">׳ <sup>p</sup>ensemb1e<sup>(y</sup>' <sup>1 x)</sup></span></p>
<p><span class="font64">A key insight (Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012c) involved in dropout is that we can approximate p<sub>ensem</sub>b</span><span class="font18">1</span><span class="font64"><sub>e</sub> by evaluating </span><span class="font64" style="font-weight:bold;font-style:italic;">p(y</span><span class="font64"> | x) in one model: the model with all units, but with the weights going out of unit i multiplied by the probability of including unit&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">i.</span><span class="font64"> The motivation for this modification is to capture the right expected value of&#160;the output from that unit. We call this approach the </span><span class="font64" style="font-weight:bold;font-style:italic;">weight scaling inference rule.&#160;</span><span class="font64">There is not yet any theoretical argument for the accuracy of this approximate&#160;inference rule in deep nonlinear networks, but empirically it performs very well.</span></p>
<p><span class="font64">Because we usually use an inclusion probability of 2, the weight scaling rule usually amounts to dividing the weights by 2 at the end of training, and then using&#160;the model as usual. Another way to achieve the same result is to multiply the&#160;states of the units by 2 during training. Either way, the goal is to make sure that&#160;the expected total input to a unit at test time is roughly the same as the expected&#160;total input to that unit at train time, even though half the units at train time are&#160;missing on average.</span></p>
<p><span class="font64">For many classes of models that do not have nonlinear hidden units, the weight scaling inference rule is exact. For a simple example, consider a softmax regression&#160;classifier with n input variables represented by the vector v:</span></p><div>
<p><span class="font64">(7.56)</span></p></div>
<p><span class="font64">P(y — y | v) — softmax ^W<sup>T</sup>v + b &#160;&#160;&#160;.</span></p>
<p><span class="font64">We can index into the family of sub-models by element-wise multiplication of the input with a binary vector d:</span></p><div>
<p><span class="font64">(7.57)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(y — y | v; </span><span class="font64" style="font-weight:bold;font-style:italic;">d) —</span><span class="font64"> softmax ^W<sup>T</sup>(d 0 v) + b .</span></p>
<p><span class="font64">The ensemble predictor is defined by re-normalizing the geometric mean over all ensemble members’ predictions:</span></p><div>
<p><span class="font64"><sup>P</sup>ensemble<sup>(y — y | v) —</sup></span></p></div><div>
<p><span class="font64">P3nsemb1e<sup>(y</sup> — <sup>y 1 v)</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">ensemb1e(y — y' | v)</span></p></div><div>
<p><span class="font64">(7.58)</span></p></div>
<p><span class="font64">where</span></p><div>
<p><span class="font64">(7.59)</span></p></div>
<p><span class="font64">Pensemb1e(y — </span><span class="font64" style="font-weight:bold;font-style:italic;">V</span><span class="font64"> | v) — </span><span class="font18">2</span><span class="font64">? &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(y — </span><span class="font64" style="font-weight:bold;font-style:italic;">V</span><span class="font64"> | v; d) .</span></p>
<p><span class="font64">de{0,1}<sup>n</sup></span></p>
<p><span class="font64">To see that the weight scaling rule is exact, we can simplify P<sub>ensem</sub>bi<sub>e</sub>:</span></p><div>
<p><span class="font64" style="font-weight:bold;">(7.60)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(7.61)</span></p></div>
<p><span class="font64" style="font-weight:bold;">-</span><span class="font64">Pensembie(y = V | v) = </span><span class="font18">2</span><span class="font64">&quot;/ &#160;&#160;&#160;<sup>P</sup>^ = V I v; d)</span></p>
<p><span class="font64">de{0,1}&quot;</span></p>
<p><span class="font64"><sub>2</sub> &#160;&#160;&#160;softmax (</span><span class="font64" style="font-weight:bold;">W</span><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">(d 0</span><span class="font64" style="font-weight:bold;"> v</span><span class="font64">) + </span><span class="font64" style="font-weight:bold;font-style:italic;">b)<sub>y</sub></span></p>
<p><span class="font64">de{0,1} ״</span></p><div>
<p><span class="font64" style="font-weight:bold;">2</span></p></div><div>
<p><span class="font21">n</span></p></div><div>
<p><span class="font64">exp (</span><span class="font64" style="font-weight:bold;">W</span><span class="font64">T:(</span><span class="font64" style="font-weight:bold;"><sup>d</sup> </span><span class="font64">0 </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">) + </span><span class="font64" style="font-weight:bold;">b</span></p></div><div>
<p><span class="font64">de{0<sup>1</sup>1}״ Ey׳ exp </span><span class="font64" style="font-weight:bold;font-style:italic;">(W</span><span class="font62">J. <sup>(d 0 v)</sup> + <sup>b</sup>)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(7.62)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">2</span></p></div><div>
<p><span class="font64">^{0,1}״ <sup>exp</sup> (<sup>w</sup>T(<sup>d</sup> 0 v)+b</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">2</span></p></div><div>
<p><span class="font64">d<sub>E</sub>{0.1f^y' <sup>eX</sup>P (W<sup>T</sup>,:<sup>(d 0 v)</sup> + <sup>b</sup>)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(7.63)</span></p></div>
<p><span class="font64">Because P will be normalized, we can safely ignore multiplication by factors that are constant with respect to V:</span></p>
<p><span class="font64" style="font-weight:bold;">■P</span><span class="font64">ensembie(</span><span class="font64" style="font-weight:bold;">y </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">V </span><span class="font64">I v) « </span><span class="font18">2</span><span class="font64">&quot; &#160;&#160;&#160;exp (</span><span class="font64" style="font-weight:bold;">Wy</span><span class="font64">T (</span><span class="font64" style="font-weight:bold;">d </span><span class="font64">0 v) + </span><span class="font64" style="font-weight:bold;">b)&#160;&#160;&#160;&#160;(7.64)</span></p>
<p><span class="font64">de{0,1}&quot;</span></p><div>
<p><span class="font64" style="font-weight:bold;">(7.65)</span></p></div>
<p><span class="font64"><sub>1</sub></span></p>
<p><span class="font64"><sup>ex</sup>P I <sub>2</sub>n &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>W</sup>Z<sup>(d 0</sup></span><span class="font64"><sup> v)</sup> + <sup>b</sup> I</span></p>
<p><span class="font64">de{0,1}״</span></p><div>
<p><span class="font64"><sup>1</sup>2</span></p></div><div>
<p><span class="font64">exp ( </span><span class="font64" style="font-weight:bold;font-style:italic;">rWy,<sub>:</sub></span><span class="font64" style="font-weight:bold;"> v </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;">b</span></p></div><div><h3 dir="rtl"><a id="bookmark4"></a><span class="font66">(׳</span></h3></div><div>
<p><span class="font64" style="font-weight:bold;">(7.66)</span></p></div>
<p><span class="font64">Substituting this back into Eq. 7.58 we obtain a softmax classifier with weights </span><span class="font64" style="font-weight:bold;font-style:italic;">2w</span><span class="font64">.</span></p>
<p><span class="font64">The weight scaling rule is also exact in other settings, including regression networks with conditionally normal outputs, and deep networks that have hidden&#160;layers without nonlinearities. However, the weight scaling rule is only an approximation for deep models that have nonlinearities. Though the approximation has&#160;not been theoretically characterized, it often works well, empirically. Goodfellow&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013a) found experimentally that the weight scaling approximation can work&#160;better (in terms of classification accuracy) than Monte Carlo approximations to the&#160;ensemble predictor. This held true even when the Monte Carlo approximation was&#160;allowed to sample up to 1,000 sub-networks. Gal and Ghahramani (2015) found&#160;that some models obtain better classification accuracy using twenty samples and&#160;the Monte Carlo approximation. It appears that the optimal choice of inference&#160;approximation is problem-dependent.</span></p>
<p><span class="font64">Srivastava </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) showed that dropout is more effective than other standard computationally inexpensive regularizers, such as weight decay, filter&#160;norm constraints and sparse activity regularization. Dropout may also be combined&#160;with other forms of regularization to yield a further improvement.</span></p>
<p><span class="font64">One advantage of dropout is that it is very computationally cheap. Using dropout during training requires only O(n) computation per example per update,&#160;to generate n random binary numbers and multiply them by the state. Depending&#160;on the implementation, it may also require O (n) memory to store these binary&#160;numbers until the back-propagation stage. Running inference in the trained model&#160;has the same cost per-example as if dropout were not used, though we must pay&#160;the cost of dividing the weights by 2 once before beginning to run inference on&#160;examples.</span></p>
<p><span class="font64">Another significant advantage of dropout is that it does not significantly limit the type of model or training procedure that can be used. It works well with nearly&#160;any model that uses a distributed representation and can be trained with stochastic&#160;gradient descent. This includes feedforward neural networks, probabilistic models&#160;such as restricted Boltzmann machines (Srivastava </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014), and recurrent&#160;neural networks (Bayer and Osendorfer, 2014; Pascanu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014a). Many other&#160;regularization strategies of comparable power impose more severe restrictions on&#160;the architecture of the model.</span></p>
<p><span class="font64">Though the cost per-step of applying dropout to a specific model is negligible, the cost of using dropout in a complete system can be significant. Because dropout&#160;is a regularization technique, it reduces the effective capacity of a model. To offset&#160;this effect, we must increase the size of the model. Typically the optimal validation&#160;set error is much lower when using dropout, but this comes at the cost of a much&#160;larger model and many more iterations of the training algorithm. For very large&#160;datasets, regularization confers little reduction in generalization error. In these&#160;cases, the computational cost of using dropout and larger models may outweigh&#160;the benefit of regularization.</span></p>
<p><span class="font64">When extremely few labeled training examples are available, dropout is less effective. Bayesian neural networks (Neal, 1996) outperform dropout on the&#160;Alternative Splicing Dataset (Xiong </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011) where fewer than 5,000 examples&#160;are available (Srivastava </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014). When additional unlabeled data is available,&#160;unsupervised feature learning can gain an advantage over dropout.</span></p>
<p><span class="font64">Wager </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013) showed that, when applied to linear regression, dropout is equivalent to L<sup>2</sup> weight decay, with a different weight decay coefficient for&#160;each input feature. The magnitude of each feature’s weight decay coefficient is&#160;determined by its variance. Similar results hold for other linear models. For deep&#160;models, dropout is not equivalent to weight decay.</span></p>
<p><span class="font64">The stochasticity used while training with dropout is not necessary for the approach’s success. It is just a means of approximating the sum over all submodels. Wang and Manning (2013) derived analytical approximations to this&#160;marginalization. Their approximation, known as </span><span class="font64" style="font-weight:bold;font-style:italic;">fast dropout</span><span class="font64"> resulted in faster&#160;convergence time due to the reduced stochasticity in the computation of the&#160;gradient. This method can also be applied at test time, as a more principled&#160;(but also more computationally expensive) approximation to the average over all&#160;sub-networks than the weight scaling approximation. Fast dropout has been used&#160;to nearly match the performance of standard dropout on small neural network&#160;problems, but has not yet yielded a significant improvement or been applied to a&#160;large problem.</span></p>
<p><span class="font64">Just as stochasticity is not necessary to achieve the regularizing effect of dropout, it is also not sufficient. To demonstrate this, Warde-Farley </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014)&#160;designed control experiments using a method called </span><span class="font64" style="font-weight:bold;font-style:italic;">dropout boosting</span><span class="font64"> that they&#160;designed to use exactly the same mask noise as traditional dropout but lack&#160;its regularizing effect. Dropout boosting trains the entire ensemble to jointly&#160;maximize the log-likelihood on the training set. In the same sense that traditional&#160;dropout is analogous to bagging, this approach is analogous to boosting. As&#160;intended, experiments with dropout boosting show almost no regularization effect&#160;compared to training the entire network as a single model. This demonstrates that&#160;the interpretation of dropout as bagging has value beyond the interpretation of&#160;dropout as robustness to noise. The regularization effect of the bagged ensemble is&#160;only achieved when the stochastically sampled ensemble members are trained to&#160;perform well independently of each other.</span></p>
<p><span class="font64">Dropout has inspired other stochastic approaches to training exponentially large ensembles of models that share weights. DropConnect is a special case of&#160;dropout where each product between a single scalar weight and a single hidden&#160;unit state is considered a unit that can be dropped (Wan </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013). Stochastic&#160;pooling is a form of randomized pooling (see Sec. 9.3) for building ensembles&#160;of convolutional networks with each convolutional network attending to different&#160;spatial locations of each feature map. So far, dropout remains the most widely&#160;used implicit ensemble method.</span></p>
<p><span class="font64">One of the key insights of dropout is that training a network with stochastic behavior and making predictions by averaging over multiple stochastic decisions&#160;implements a form of bagging with parameter sharing. Earlier, we described&#160;dropout as bagging an ensemble of models formed by including or excluding&#160;units. However, there is no need for this model averaging strategy to be based on&#160;inclusion and exclusion. In principle, any kind of random modification is admissible.&#160;In practice, we must choose modification families that neural networks are able&#160;to learn to resist. Ideally, we should also use model families that allow a fast&#160;approximate inference rule. We can think of any form of modification parametrized&#160;by a vector p as training an ensemble consisting of </span><span class="font64" style="font-weight:bold;font-style:italic;">p(y</span><span class="font64"> | x, </span><span class="font64" style="font-weight:bold;font-style:italic;">p)</span><span class="font64"> for all possible&#160;values of p. There is no requirement that </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> have a finite number of values. For&#160;example, </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> can be real-valued. Srivastava </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) showed that multiplying the&#160;weights by </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> ~ N(1,1) can outperform dropout based on binary masks. Because&#160;E[</span><span class="font64" style="font-weight:bold;font-style:italic;">p]</span><span class="font64"> = 1 the standard network automatically implements approximate inference&#160;in the ensemble, without needing any weight scaling.</span></p>
<p><span class="font64">So far we have described dropout purely as a means of performing efficient, approximate bagging. However, there is another view of dropout that goes further&#160;than this. Dropout trains not just a bagged ensemble of models, but an ensemble&#160;of models that share hidden units. This means each hidden unit must be able to&#160;perform well regardless of which other hidden units are in the model. Hidden units&#160;must be prepared to be swapped and interchanged between models. Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(2012c) were inspired by an idea from biology: sexual reproduction, which involves&#160;swapping genes between two different organisms, creates evolutionary pressure for&#160;genes to become not just good, but to become readily swapped between different&#160;organisms. Such genes and such features are very robust to changes in their&#160;environment because they are not able to incorrectly adapt to unusual features&#160;of any one organism or model. Dropout thus regularizes each hidden unit to be&#160;not merely a good feature but a feature that is good in many contexts. Warde-Farley </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) compared dropout training to training of large ensembles and&#160;concluded that dropout offers additional improvements to generalization error&#160;beyond those obtained by ensembles of independent models.</span></p>
<p><span class="font64">It is important to understand that a large portion of the power of dropout arises from the fact that the masking noise is applied to the hidden units. This&#160;can be seen as a form of highly intelligent, adaptive destruction of the information&#160;content of the input rather than destruction of the raw values of the input. For&#160;example, if the model learns a hidden unit </span><span class="font64" style="font-weight:bold;font-style:italic;">hi</span><span class="font64"> that detects a face by finding the nose,&#160;then dropping h<sub>i</sub> corresponds to erasing the information that there is a nose in&#160;the image. The model must learn another h<sub>i</sub>, either that redundantly encodes the&#160;presence of a nose, or that detects the face by another feature, such as the mouth.&#160;Traditional noise injection techniques that add unstructured noise at the input are&#160;not able to randomly erase the information about a nose from an image of a face&#160;unless the magnitude of the noise is so great that nearly all of the information in&#160;the image is removed. Destroying extracted features rather than original values&#160;allows the destruction process to make use of all of the knowledge about the input&#160;distribution that the model has acquired so far.</span></p>
<p><span class="font64">Another important aspect of dropout is that the noise is multiplicative. If the noise were additive with fixed scale, then a rectified linear hidden unit </span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font64"><sub>i</sub> with&#160;added noise e could simply learn to have </span><span class="font64" style="font-weight:bold;font-style:italic;">hi</span><span class="font64"> become very large in order to make&#160;the added noise e insignificant by comparison. Multiplicative noise does not allow&#160;such a pathological solution to the noise robustness problem.</span></p>
<p><span class="font64">Another deep learning algorithm, batch normalization, reparametrizes the model in a way that introduces both additive and multiplicative noise on the&#160;hidden units at training time. The primary purpose of batch normalization is to&#160;improve optimization, but the noise can have a regularizing effect, and sometimes&#160;makes dropout unnecessary. Batch normalization is described further in Sec. 8.7.1.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">7.13 Adversarial Training</span></h4>
<p><span class="font64">In many cases, neural networks have begun to reach human performance when evaluated on an i.i.d. test set. It is natural therefore to wonder whether these&#160;models have obtained a true human-level understanding of these tasks. In order&#160;to probe the level of understanding a network has of the underlying task, we can&#160;search for examples that the model misclassifies. Szegedy </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014b) found that&#160;even neural networks that perform at human level accuracy have a nearly 100%&#160;error rate on examples that are intentionally constructed by using an optimization&#160;procedure to search for an input x' near a data point </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> such that the model output&#160;is very different at x'. In many cases, x' can be so similar to x that a human&#160;observer cannot tell the difference between the original example and the </span><span class="font64" style="font-weight:bold;font-style:italic;">adversarial&#160;example,</span><span class="font64"> but the network can make highly different predictions. See Fig. 7.8 for an&#160;example.</span></p>
<p><span class="font64">Adversarial examples have many implications, for example, in computer security, that are beyond the scope of this chapter. However, they are interesting in the&#160;context of regularization because one can reduce the error rate on the original i.i.d.&#160;test set via </span><span class="font64" style="font-weight:bold;font-style:italic;">adversarial training</span><span class="font64">—training on adversarially perturbed examples&#160;from the training set (Szegedy </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014b; Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014b).</span></p>
<p><span class="font64">Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014b) showed that one of the primary causes of these adversarial examples is excessive linearity. Neural networks are built out of&#160;primarily linear building blocks. In some experiments the overall function they&#160;implement proves to be highly linear as a result. These linear functions are easy</span></p><div>
<p><span class="font64">x</span></p>
<p><span class="font64">y =“panda” w/ 57.7%&#160;confidence</span></p></div><div><div><img src="main-75.jpg" alt=""/></div></div><div>
<p><span class="font64">sign(Vx J(6, x,y))</span></p>
<p><span class="font64">“nematode” w/ 8.2%&#160;confidence</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> +</span></p>
<p><span class="font64">e sign(V*J(6, x,y)) “gibbon”&#160;w/ 99.3 %&#160;confidence</span></p></div>
<p><span class="font64">Figure 7.8: A demonstration of adversarial example generation applied to GoogLeNet (Szegedy </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2014a) on ImageNet. By adding an imperceptibly small vector whose&#160;elements are equal to the sign of the elements of the gradient of the cost function with&#160;respect to the input, we can change GoogLeNet’s classification of the image. Reproduced&#160;with permission from Goodfellow </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2014b).</span></p>
<p><span class="font64">to optimize. Unfortunately, the value of a linear function can change very rapidly if it has numerous inputs. If we change each input by e, then a linear function&#160;with weights w can change by as much as e||w|| </span><span class="font18">1</span><span class="font64">, which can be a very large&#160;amount if w is high-dimensional. Adversarial training discourages this highly&#160;sensitive locally linear behavior by encouraging the network to be locally constant&#160;in the neighborhood of the training data. This can be seen as a way of explicitly&#160;introducing a local constancy prior into supervised neural nets.</span></p>
<p><span class="font64">Adversarial training helps to illustrate the power of using a large function family in combination with aggressive regularization. Purely linear models, like&#160;logistic regression, are not able to resist adversarial examples because they are&#160;forced to be linear. Neural networks are able to represent functions that can range&#160;from nearly linear to nearly locally constant and thus have the flexibility to capture&#160;linear trends in the training data while still learning to resist local perturbation.</span></p>
<p><span class="font64">Adversarial examples also provide a means of accomplishing semi-supervised learning. At a point x that is not associated with a label in the dataset, the&#160;model itself assigns some label y. The model’s label </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"> may not be the true label,&#160;but if the model is high quality, then y has a high probability of providing the&#160;true label. We can seek an adversarial example </span><span class="font64" style="font-weight:bold;font-style:italic;">x'</span><span class="font64"> that causes the classifier to&#160;output a label y' with y' = y. Adversarial examples generated using not the&#160;true label but a label provided by a trained model are called </span><span class="font64" style="font-weight:bold;font-style:italic;">virtual adversarial&#160;examples</span><span class="font64"> (Miyato </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015). The classifier may then be trained to assign the&#160;same label to x and x'. This encourages the classifier to learn a function that is&#160;robust to small changes anywhere along the manifold where the unlabeled data&#160;lies. The assumption motivating this approach is that different classes usually lie&#160;on disconnected manifolds, and a small perturbation should not be able to jump&#160;from one class manifold to another class manifold.</span></p><h4><a id="bookmark6"></a><span class="font65" style="font-weight:bold;">7.14 Tangent Distance, Tangent Prop, and Manifold&#160;Tangent Classifier</span></h4>
<p><span class="font64">Many machine learning algorithms aim to overcome the curse of dimensionality by assuming that the data lies near a low-dimensional manifold, as described in&#160;Sec. 5.11.3.</span></p>
<p><span class="font64">One of the early attempts to take advantage of the manifold hypothesis is the </span><span class="font64" style="font-weight:bold;font-style:italic;">tangent distance</span><span class="font64"> algorithm (Simard </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1993, 1998). It is a non-parametric&#160;nearest-neighbor algorithm in which the metric used is not the generic Euclidean&#160;distance but one that is derived from knowledge of the manifolds near which&#160;probability concentrates. It is assumed that we are trying to classify examples and&#160;that examples on the same manifold share the same category. Since the classifier&#160;should be invariant to the local factors of variation that correspond to movement&#160;on the manifold, it would make sense to use as nearest-neighbor distance between&#160;points x! and </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font62" style="font-style:italic;">2</span><span class="font64"> the distance between the manifolds </span><span class="font64" style="font-weight:bold;font-style:italic;">M!</span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">M</span><span class="font62" style="font-style:italic;">2</span><span class="font64"> to which they&#160;respectively belong. Although that may be computationally difficult (it would&#160;require solving an optimization problem, to find the nearest pair of points on M!&#160;and M</span><span class="font18">2</span><span class="font64">), a cheap alternative that makes sense locally is to approximate </span><span class="font64" style="font-weight:bold;font-style:italic;">Mi</span><span class="font64"> by its&#160;tangent plane at x<sub>i</sub> and measure the distance between the two tangents, or between&#160;a tangent plane and a point. That can be achieved by solving a low-dimensional&#160;linear system (in the dimension of the manifolds). Of course, this algorithm requires&#160;one to specify the tangent vectors.</span></p>
<p><span class="font64">In a related spirit, the </span><span class="font64" style="font-weight:bold;font-style:italic;">tangent prop</span><span class="font64"> algorithm (Simard </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1992) (Fig. 7.9) trains a neural net classifier with an extra penalty to make each output f (x) of&#160;the neural net locally invariant to known factors of variation. These factors of&#160;variation correspond to movement along the manifold near which examples of the&#160;same class concentrate. Local invariance is achieved by requiring </span><span class="font64" style="font-weight:bold;font-style:italic;">V<sub>x</sub>f</span><span class="font64"> (x) to be&#160;orthogonal to the known manifold tangent vectors v<sup>(i)</sup> at x, or equivalently that&#160;the directional derivative of f at x in the directions v<sup>(i)</sup> be small by adding a&#160;regularization penalty 0:</span></p>
<p><span class="font64">2</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">0(f</span><span class="font64">) = £((V.f(x))<sup>T</sup> v« &#160;&#160;&#160;.&#160;&#160;&#160;&#160;(7.67)</span></p>
<p><span class="font62">i</span></p>
<p><span class="font64">This regularizer can of course by scaled by an appropriate hyperparameter, and, for most neural networks, we would need to sum over many outputs rather than the lone&#160;output f (x) described here for simplicity. As with the tangent distance algorithm,&#160;the tangent vectors are derived a priori, usually from the formal knowledge of&#160;the effect of transformations such as translation, rotation, and scaling in images.&#160;Tangent prop has been used not just for supervised learning (Simard </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1992)&#160;but also in the context of reinforcement learning (Thrun, 1995).</span></p>
<p><span class="font64">Tangent propagation is closely related to dataset augmentation. In both cases, the user of the algorithm encodes his or her prior knowledge of the task&#160;by specifying a set of transformations that should not alter the output of the&#160;network. The difference is that in the case of dataset augmentation, the network is&#160;explicitly trained to correctly classify distinct inputs that were created by applying&#160;more than an infinitesimal amount of these transformations. Tangent propagation&#160;does not require explicitly visiting a new input point. Instead, it analytically&#160;regularizes the model to resist perturbation in the directions corresponding to&#160;the specified transformation. While this analytical approach is intellectually&#160;elegant, it has two major drawbacks. First, it only regularizes the model to resist&#160;infinitesimal perturbation. Explicit dataset augmentation confers resistance to&#160;larger perturbations. Second, the infinitesimal approach poses difficulties for models&#160;based on rectified linear units. These models can only shrink their derivatives&#160;by turning units off or shrinking their weights. They are not able to shrink their&#160;derivatives by saturating at a high value with large weights, as sigmoid or tanh&#160;units can. Dataset augmentation works well with rectified linear units because&#160;different subsets of rectified units can activate for different transformed versions of&#160;each original input.</span></p>
<p><span class="font64">Tangent propagation is also related to </span><span class="font64" style="font-weight:bold;font-style:italic;">double backprop</span><span class="font64"> (Drucker and LeCun, 1992) and adversarial training (Szegedy </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014b; Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014b).&#160;Double backprop regularizes the Jacobian to be small, while adversarial training&#160;finds inputs near the original inputs and trains the model to produce the same&#160;output on these as on the original inputs. Tangent propagation and dataset&#160;augmentation using manually specified transformations both require that the&#160;model should be invariant to certain specified directions of change in the input.&#160;Double backprop and adversarial training both require that the model should be&#160;invariant to </span><span class="font64" style="font-weight:bold;font-style:italic;">all</span><span class="font64"> directions of change in the input so long as the change is small. Just&#160;as dataset augmentation is the non-infinitesimal version of tangent propagation,&#160;adversarial training is the non-infinitesimal version of double backprop.</span></p>
<p><span class="font64">The manifold tangent classifier (Rifai </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011c), eliminates the need to know the tangent vectors a priori. As we will see in Chapter 14, autoencoders can</span></p><div><img src="main-76.jpg" alt=""/></div>
<p><span class="font64">Figure 7.9: Illustration of the main idea of the tangent prop algorithm (Simard </span><span class="font64" style="font-style:italic;">et al </span><span class="font64">1992) and manifold tangent classifier (Rifai </span><span class="font64" style="font-style:italic;">et al</span><span class="font64">., 2011c), which both regularize the&#160;classifier output function </span><span class="font64" style="font-style:italic;">f(x).</span><span class="font64"> Each curve represents the manifold for a different class,&#160;illustrated here as a one-dimensional manifold embedded in a two-dimensional space.&#160;On one curve, we have chosen a single point and drawn a vector that is tangent to the&#160;class manifold (parallel to and touching the manifold) and a vector that is normal to the&#160;class manifold (orthogonal to the manifold). In multiple dimensions there may be many&#160;tangent directions and many normal directions. We expect the classification function to&#160;change rapidly as it moves in the direction normal to the manifold, and not to change as&#160;it moves along the class manifold. Both tangent propagation and the manifold tangent&#160;classifier regularize f (x) to not change very much as x moves along the manifold. Tangent&#160;propagation requires the user to manually specify functions that compute the tangent&#160;directions (such as specifying that small translations of images remain in the same class&#160;manifold) while the manifold tangent classifier estimates the manifold tangent directions&#160;by training an autoencoder to fit the training data. The use of autoencoders to estimate&#160;manifolds will be described in Chapter 14.</span></p>
<p><span class="font64">estimate the manifold tangent vectors. The manifold tangent classifier makes use of this technique to avoid needing user-specified tangent vectors. As illustrated&#160;in Fig. 14.10, these estimated tangent vectors go beyond the classical invariants&#160;that arise out of the geometry of images (such as translation, rotation and scaling)&#160;and include factors that must be learned because they are object-specific (such as&#160;moving body parts). The algorithm proposed with the manifold tangent classifier&#160;is therefore simple: (1) use an autoencoder to learn the manifold structure by&#160;unsupervised learning, and (2) use these tangents to regularize a neural net classifier&#160;as in tangent prop (Eq. 7.67).</span></p>
<p><span class="font64">This chapter has described most of the general strategies used to regularize neural networks. Regularization is a central theme of machine learning and as such</span></p>
<p><span class="font64">will be revisited periodically by most of the remaining chapters. theme of machine learning is optimization, described next.</span></p><div>
<p><span class="font64">Another central</span></p></div><div><div><img src="main-77.jpg" alt=""/>
<p><span class="font63">Ensemble of Sub-Networks</span></p>
<p><span class="font64">Figure 7.6: Dropout trains an ensemble consisting of all sub-networks that can be constructed by removing non-output units from an underlying base network. Here, we&#160;begin with a base network with two visible units and two hidden units. There are sixteen&#160;possible subsets of these four units. We show all sixteen subnetworks that may be formed&#160;by dropping out different subsets of units from the original network. In this small example,&#160;a large proportion of the resulting networks have no input units or no path connecting&#160;the input to the output. This problem becomes insignificant for networks with wider&#160;layers, where the probability of dropping all possible paths from inputs to outputs becomes&#160;smaller.</span></p></div></div><div><div><img src="main-78.jpg" alt=""/>
<p><span class="font64">Figure 7.7: An example of forward propagation through a feedforward network using dropout. </span><span class="font64" style="font-style:italic;">(Top)</span><span class="font64"> In this example, we use a feedforward network with two input units, one&#160;hidden layer with two hidden units, and one output unit. </span><span class="font64" style="font-style:italic;">(Bottom)</span><span class="font64"> To perform forward&#160;propagation with dropout, we randomly sample a vector p with one entry for each input&#160;or hidden unit in the network. The entries of p are binary and are sampled independently&#160;from each other. The probability of each entry being 1 is a hyperparameter, usually 05&#160;for the hidden layers and 0.8 for the input. Each unit in the network is multiplied by&#160;the corresponding mask, and then forward propagation continues through the rest of the&#160;network as usual. This is equivalent to randomly selecting one of the sub-networks from&#160;Fig. 7.6 and running forward propagation through it.</span></p></div></div>
</body>
</html>