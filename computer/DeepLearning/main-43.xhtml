<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body>
<p><span class="font64">Le Roux, N. and Bengio, Y. (2008). Representational power of restricted Boltzmann machines and deep belief networks. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 20(6), 1631-1649. 555, 657</span></p>
<p><span class="font64">Le Roux, N. and Bengio, Y. (2010). Deep belief networks are compact universal approximators. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 22(8), 2192-2207. 555</span></p>
<p><span class="font64">LeCun, Y. (1985). Une procedure d’apprentissage pour Reseau a seuil assymetrique. In </span><span class="font64" style="font-style:italic;">Cognitiva 85: A la Frontiere de l’Intelligence Artificielle, des Sciences de la Connaissance&#160;et des Neurosciences</span><span class="font64">, pages 599-604, Paris 1985. CESTA, Paris. 224</span></p>
<p><span class="font64">LeCun, Y. (1986). Learning processes in an asymmetric threshold network. In F. Fogelman-Soulie, E. Bienenstock, and G. Weisbuch, editors, </span><span class="font64" style="font-style:italic;">Disordered Systems and Biological Organization</span><span class="font64">, pages 233-240. Springer-Verlag, Les Houches, France. 350</span></p>
<p><span class="font64">LeCun, Y. (1987). </span><span class="font64" style="font-style:italic;">Modeles connexionistes de l’apprentissage</span><span class="font64">. Ph.D. thesis, Universite de Paris VI. 18, 504, 517</span></p>
<p><span class="font64">LeCun, Y. (1989). Generalization and network design strategies. Technical Report CRG-TR-89-4, University of Toronto. 330, 350</span></p>
<p><span class="font64">LeCun, Y., Jackel, L. D., Boser, B., Denker, J. S., Graf, H. P., Guyon, I., Henderson, D., Howard, R. E., and Hubbard, W. (1989). Handwritten digit recognition: Applications&#160;of neural network chips and automatic learning. </span><span class="font64" style="font-style:italic;">IEEE Communications Magazine</span><span class="font64">,&#160;27(11), 41-46. 368</span></p>
<p><span class="font64">LeCun, Y., Bottou, L., Orr, G. B., and Muller, K.-R. (1998a). Efficient backprop. In </span><span class="font64" style="font-style:italic;">Neural Networks, Tricks of the Trade</span><span class="font64">, Lecture Notes in Computer Science LNCS 1524.&#160;Springer Verlag. 310, 431</span></p>
<p><span class="font64">LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998b). Gradient based learning applied to document recognition. </span><span class="font64" style="font-style:italic;">Proc. IEEE</span><span class="font64">. 16, 18, 21, 27, 371, 460, 462</span></p>
<p><span class="font64">LeCun, Y., Kavukcuoglu, K., and Farabet, C. (2010). Convolutional networks and applications in vision. In </span><span class="font64" style="font-style:italic;">Circuits and Systems (ISCAS), Proceedings of 2010 IEEE&#160;International Symposium on</span><span class="font64">, pages 253-256. IEEE. 371</span></p>
<p><span class="font64">L’Ecuyer, P. (1994). Efficiency improvement and variance reduction. In </span><span class="font64" style="font-style:italic;">Proceedings of the 1994 Winter Simulation Conference</span><span class="font64">, pages 122—132. 692</span></p>
<p><span class="font64">Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., and Tu, Z. (2014). Deeply-supervised nets. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1409.5185</span><span class="font64">. 326</span></p>
<p><span class="font64">Lee, H., Battle, A., Raina, R., and Ng, A. (2007). Efficient sparse coding algorithms. In B. Scholkopf, J. Platt, and T. Hoffman, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information&#160;Processing Systems 19 (NIPS’06)</span><span class="font64">, pages 801-808. MIT Press. 639</span></p>
<p><span class="font64">Lee, H., Ekanadham, C., and Ng, A. (2008). Sparse deep belief net model for visual area V2. In </span><span class="font64" style="font-style:italic;">NIPS’07</span><span class="font64">. 255</span></p>
<p><span class="font64">Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009). Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In L. Bottou&#160;and M. Littman, editors, </span><span class="font64" style="font-style:italic;">Proceedings of the Twenty-sixth International Conference on&#160;Machine Learning (ICML’09).</span><span class="font64"> ACM, Montreal, Canada. 363, 685, 686</span></p>
<p><span class="font64">Lee, Y. J. and Grauman, K. (2011). Learning the easy things first: self-paced visual category discovery. In </span><span class="font64" style="font-style:italic;">CVPR ’2011</span><span class="font64">. 328</span></p>
<p><span class="font64">Leibniz, G. W. (1676). Memoir using the chain rule. (Cited in TMME 7:2&amp;3 p 321-332, 2010). 224</span></p>
<p><span class="font64">Lenat, D. B. and Guha, R. V. (1989). </span><span class="font64" style="font-style:italic;">Building large knowledge-based systems; representation and inference in the Cyc project</span><span class="font64">. Addison-Wesley Longman Publishing Co., Inc. 2</span></p>
<p><span class="font64">Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. (1993). Multilayer feedforward networks with a nonpolynomial activation function can approximate any function.&#160;</span><span class="font64" style="font-style:italic;">Neural Networks</span><span class="font64">, 6, 861—867. 197, 198</span></p>
<p><span class="font64">Levenberg, K. (1944). A method for the solution of certain non-linear problems in least squares. </span><span class="font64" style="font-style:italic;">Quarterly Journal of Applied Mathematics</span><span class="font64">, II(2), 164-168. 312</span></p>
<p><span class="font64">L’Hopital, G. F. A. (1696). </span><span class="font64" style="font-style:italic;">Analyse des infiniment petits, pour intelligence des lignes courbes</span><span class="font64">. Paris: L’Imprimerie Royale. 224</span></p>
<p><span class="font64">Li, Y., Swersky, K., and Zemel, R. S. (2015). Generative moment matching networks. </span><span class="font64" style="font-style:italic;">CoRR,</span><span class="font64"> abs/1502.02761. 705</span></p>
<p><span class="font64">Lin, T., Horne, B. G., Tino, P., and Giles, C. L. (1996). Learning long-term dependencies is not as difficult with NARX recurrent neural networks. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Neural&#160;Networks</span><span class="font64">, 7(6), 1329-1338. 408</span></p>
<p><span class="font64">Lin, Y., Liu, Z., Sun, M., Liu, Y., and Zhu, X. (2015). Learning entity and relation embeddings for knowledge graph completion. In </span><span class="font64" style="font-style:italic;">Proc. AAAI’15</span><span class="font64">. 486</span></p>
<p><span class="font64">Linde, N. (1992). The machine that changed the world, episode 3. Documentary miniseries. 2</span></p>
<p><span class="font64">Lindsey, C. and Lindblad, T. (1994). Review of hardware neural networks: a user’s perspective. In </span><span class="font64" style="font-style:italic;">Proc. Third Workshop on Neural Networks: From Biology to High&#160;Energy Physics</span><span class="font64">, pages 195—202, Isola d’Elba, Italy. 453</span></p>
<p><span class="font64">Linnainmaa, S. (1976). Taylor expansion of the accumulated rounding error. </span><span class="font64" style="font-style:italic;">BIT Numerical Mathematics</span><span class="font64">, 16(2), 146-160. 224</span></p>
<p><span class="font64">LISA (2008). Deep learning tutorials: Restricted Boltzmann machines. Technical report, LISA Lab, Universite de Montreal. 590</span></p>
<p><span class="font64">Long, P. M. and Servedio, R. A. (2010). Restricted Boltzmann machines are hard to approximately evaluate or simulate. In </span><span class="font64" style="font-style:italic;">Proceedings of the 27th International Conference&#160;on Machine Learning (ICML’10).</span><span class="font64"> 660</span></p>
<p><span class="font64">Lotter, W., Kreiman, G., and Cox, D. (2015). Unsupervised learning of visual structure using predictive generative networks. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1511.06380</span><span class="font64">. 546, 547</span></p>
<p><span class="font64">Lovelace, A. (1842). Notes upon L. F. Menabrea’s “Sketch of the Analytical Engine invented by Charles Babbage”. 1</span></p>
<p><span class="font64">Lu, L., Zhang, X., Cho, K., and Renals, S. (2015). A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition. In </span><span class="font64" style="font-style:italic;">Proc. Interspeech</span><span class="font64">. 462</span></p>
<p><span class="font64">Lu, T., Pal, D., and Pal, M. (2010). Contextual multi-armed bandits. In </span><span class="font64" style="font-style:italic;">International Conference on Artificial Intelligence and Statistics</span><span class="font64">, pages 485-492. 482</span></p>
<p><span class="font64">Luenberger, D. G. (1984). </span><span class="font64" style="font-style:italic;">Linear and Nonlinear Programming</span><span class="font64">. Addison Wesley. 316</span></p>
<p><span class="font64">Lukosevicius, M. and Jaeger, H. (2009). Reservoir computing approaches to recurrent neural network training. </span><span class="font64" style="font-style:italic;">Computer Science Review</span><span class="font64">, 3(3), 127-149. 405</span></p>
<p><span class="font64">Luo, H., Shen, R., Niu, C., and Ullrich, C. (2011). Learning class-relevant features and class-irrelevant features via a hybrid third-order RBM. In </span><span class="font64" style="font-style:italic;">International Conference on&#160;Artificial Intelligence and Statistics</span><span class="font64">, pages 470-478. 688</span></p>
<p><span class="font64">Luo, H., Carrier, P. L., Courville, A., and Bengio, Y. (2013). Texture modeling with convolutional spike-and-slab RBMs and deep extensions. In </span><span class="font64" style="font-style:italic;">AISTATS’2013</span><span class="font64">. 102</span></p>
<p><span class="font64">Lyu, S. (2009). Interpretation and generalization of score matching. In </span><span class="font64" style="font-style:italic;">Proceedings of the Twenty-fifth Conference in Uncertainty in Artificial Intelligence (UAI’09).</span><span class="font64"> 620</span></p>
<p><span class="font64">Ma, J., Sheridan, R. P., Liaw, A., Dahl, G. E., and Svetnik, V. (2015). Deep neural nets as a method for quantitative structure - activity relationships. </span><span class="font64" style="font-style:italic;">J. Chemical information&#160;and modeling</span><span class="font64">. 532</span></p>
<p><span class="font64">Maas, A. L., Hannun, A. Y., and Ng, A. Y. (2013). Rectifier nonlinearities improve neural network acoustic models. In </span><span class="font64" style="font-style:italic;">ICML Workshop on Deep Learning for Audio, Speech, and&#160;Language Processing</span><span class="font64">. 192</span></p>
<p><span class="font64">Maass, W. (1992). Bounds for the computational power and learning complexity of analog neural nets (extended abstract). In </span><span class="font64" style="font-style:italic;">Proc. of the 25th ACM Symp. Theory of Computing</span><span class="font64">,&#160;pages 335-344. 198</span></p>
<p><span class="font64">Maass, W., Schnitger, G., and Sontag, E. D. (1994). A comparison of the computational power of sigmoid and Boolean threshold circuits. </span><span class="font64" style="font-style:italic;">Theoretical Advances in Neural&#160;Computation and Learning</span><span class="font64">, pages 127-151. 198</span></p>
<p><span class="font64">Maass, W., Natschlaeger, T., and Markram, H. (2002). Real-time computing without stable states: A new framework for neural computation based on perturbations. </span><span class="font64" style="font-style:italic;">Neural&#160;Computation</span><span class="font64">, 14(11), 2531-2560. 405</span></p>
<p><span class="font64">MacKay, D. (2003). </span><span class="font64" style="font-style:italic;">Information Theory, Inference and Learning Algorithms</span><span class="font64">. Cambridge University Press. 73</span></p>
<p><span class="font64">Maclaurin, D., Duvenaud, D., and Adams, R. P. (2015). Gradient-based hyperparameter optimization through reversible learning. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1502.03492</span><span class="font64">. 437</span></p>
<p><span class="font64">Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., and Yuille, A. L. (2015). Deep captioning with multimodal recurrent neural networks. In </span><span class="font64" style="font-style:italic;">ICLR’2015</span><span class="font64">. arXiv:1410.1090. 102</span></p>
<p><span class="font64">Marcotte, P. and Savard, G. (1992). Novel approaches to the discrimination problem. </span><span class="font64" style="font-style:italic;">Zeitschrift fur Operations Research (Theory),</span><span class="font64"> 36, 517-545. 276</span></p>
<p><span class="font64">Marlin, B. and de Freitas, N. (2011). Asymptotic efficiency of deterministic estimators for discrete energy-based models: Ratio matching and pseudolikelihood. In </span><span class="font64" style="font-style:italic;">UAI’2011</span><span class="font64">. 619,&#160;621</span></p>
<p><span class="font64">Marlin, B., Swersky, K., Chen, B., and de Freitas, N. (2010). Inductive principles for restricted Boltzmann machine learning. In </span><span class="font64" style="font-style:italic;">Proceedings of The Thirteenth International&#160;Conference on Artificial Intelligence and Statistics (AISTATS’10),</span><span class="font64"> volume 9, pages&#160;509-516. 615, 620, 621</span></p>
<p><span class="font64">Marquardt, D. W. (1963). An algorithm for least-squares estimation of non-linear parameters. </span><span class="font64" style="font-style:italic;">Journal of the Society of Industrial and Applied Mathematics</span><span class="font64">, 11(2), 431-441. 312</span></p>
<p><span class="font64">Marr, D. and Poggio, T. (1976). Cooperative computation of stereo disparity. </span><span class="font64" style="font-style:italic;">Science</span><span class="font64">, 194. 367</span></p>
<p><span class="font64">Martens, J. (2010). Deep learning via Hessian-free optimization. In L. Bottou and M. Littman, editors, </span><span class="font64" style="font-style:italic;">Proceedings of the Twenty-seventh International Conference on&#160;Machine Learning (ICML-10),</span><span class="font64"> pages 735-742. ACM. 304</span></p>
<p><span class="font64">Martens, J. and Medabalimi, V. (2014). On the expressive efficiency of sum product networks. </span><span class="font64" style="font-style:italic;">arXiv:1411.7717</span><span class="font64">. 556</span></p>
<p><span class="font64">Martens, J. and Sutskever, I. (2011). Learning recurrent neural networks with Hessian-free optimization. In </span><span class="font64" style="font-style:italic;">Proc. ICML’2011</span><span class="font64">. ACM. 414</span></p>
<p><span class="font64">Mase, S. (1995). Consistency of the maximum pseudo-likelihood estimator of continuous state space Gibbsian processes. </span><span class="font64" style="font-style:italic;">The Annals of Applied Probability</span><span class="font64">, 5(3), pp. 603-612.&#160;618</span></p>
<p><span class="font64">McClelland, J., Rumelhart, D., and Hinton, G. (1995). The appeal of parallel distributed processing. In </span><span class="font64" style="font-style:italic;">Computation &amp; intelligence</span><span class="font64">, pages 305-341. American Association for&#160;Artificial Intelligence. 17</span></p>
<p><span class="font64">McCulloch, W. S. and Pitts, W. (1943). A logical calculus of ideas immanent in nervous activity. </span><span class="font64" style="font-style:italic;">Bulletin of Mathematical Biophysics</span><span class="font64">, 5, 115-133. 14, 15</span></p>
<p><span class="font64">Mead, C. and Ismail, M. (2012). </span><span class="font64" style="font-style:italic;">Analog VLSI implementation of neural systems</span><span class="font64">, volume 80. Springer Science &amp; Business Media. 453</span></p>
<p><span class="font64">Melchior, J., Fischer, A., and Wiskott, L. (2013). How to center binary deep Boltzmann machines. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1311.1354</span><span class="font64"> . 675</span></p>
<p><span class="font64">Memisevic, R. and Hinton, G. E. (2007). Unsupervised learning of image transformations.</span></p>
<p><span class="font64">In </span><span class="font64" style="font-style:italic;">Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR’07). </span><span class="font64">688</span></p>
<p><span class="font64">Memisevic, R. and Hinton, G. E. (2010). Learning to represent spatial transformations with factored higher-order Boltzmann machines. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">,22(6), 1473-1492.&#160;688</span></p>
<p><span class="font64">Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra,&#160;J. (2011). Unsupervised and transfer learning challenge: a deep learning approach. In&#160;</span><span class="font64" style="font-style:italic;">JMLR W&amp;CP: Proc. Unsupervised and Transfer Learning</span><span class="font64">, volume 7. 200, 534, 540</span></p>
<p><span class="font64">Mesnil, G., Rifai, S., Dauphin, Y., Bengio, Y., and Vincent, P. (2012). Surfing on the manifold. Learning Workshop, Snowbird. 713</span></p>
<p><span class="font64">Miikkulainen, R. and Dyer, M. G. (1991). Natural language processing with modular PDP networks and distributed lexicon. </span><span class="font64" style="font-style:italic;">Cognitive Science</span><span class="font64">, 15, 343-399. 479</span></p>
<p><span class="font64">Mikolov, T. (2012). </span><span class="font64" style="font-style:italic;">Statistical Language Models based on Neural Networks</span><span class="font64">. Ph.D. thesis, Brno University of Technology. 416</span></p>
<p><span class="font64">Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky, J. (2011a). Empirical evaluation and combination of advanced language modeling techniques. In </span><span class="font64" style="font-style:italic;">Proc. 12th annual conference of the international speech communication association (INTERSPEECH&#160;2011).</span><span class="font64"> 474</span></p>
<p><span class="font64">Mikolov, T., Deoras, A., Povey, D., Burget, L., and Cernocky, J. (2011b). Strategies for training large scale neural network language models. In </span><span class="font64" style="font-style:italic;">Proc. ASRU’2011</span><span class="font64">. 328, 474</span></p>
<p><span class="font64">Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Efficient estimation of word representations in vector space. In </span><span class="font64" style="font-style:italic;">International Conference on Learning Representations: Workshops Track</span><span class="font64">. 538</span></p>
<p><span class="font64">Mikolov, T., Le, Q. V., and Sutskever, I. (2013b). Exploiting similarities among languages for machine translation. Technical report, arXiv:1309.4168. 541</span></p>
<p><span class="font64">Minka, T. (2005). Divergence measures and message passing. </span><span class="font64" style="font-style:italic;">Microsoft Research Cambridge UK Tech Rep MSRTR2005173</span><span class="font64"> , 72(TR-2005-173). 626</span></p>
<p><span class="font64">Minsky, M. L. and Papert, S. A. (1969). </span><span class="font64" style="font-style:italic;">Perceptrons</span><span class="font64">. MIT Press, Cambridge. 15</span></p>
<p><span class="font64">Mirza, M. and Osindero, S. (2014). Conditional generative adversarial nets. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1411.1784</span><span class="font64"> . 703</span></p>
<p><span class="font64">Mishkin, D. and Matas, J. (2015). All you need is a good init. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1511.06422</span><span class="font64"> . 305</span></p>
<p><span class="font64">Misra, J. and Saha, I. (2010). Artificial neural networks in hardware: A survey of two decades of progress. </span><span class="font64" style="font-style:italic;">Neurocomputing</span><span class="font64">, 74(1), 239-255. 453</span></p>
<p><span class="font64">Mitchell, T. M. (1997). </span><span class="font64" style="font-style:italic;">Machine Learning</span><span class="font64">. McGraw-Hill, New York. 99</span></p>
<p><span class="font64">Miyato, T., Maeda, S., Koyama, M., Nakae, K., and Ishii, S. (2015). Distributional smoothing with virtual adversarial training. In </span><span class="font64" style="font-style:italic;">ICLR.</span><span class="font64"> Preprint: arXiv:1507.00677. 269</span></p>
<p><span class="font64">Mnih, A. and Gregor, K. (2014). Neural variational inference and learning in belief networks. In </span><span class="font64" style="font-style:italic;">ICML’2014</span><span class="font64">. 693, 695</span></p>
<p><span class="font64">Mnih, A. and Hinton, G. E. (2007). Three new graphical models for statistical language modelling. In Z. Ghahramani, editor, </span><span class="font64" style="font-style:italic;">Proceedings of the Twenty-fourth International&#160;Conference on Machine Learning (ICML’07)</span><span class="font64">, pages 641-648. ACM. 466</span></p>
<p><span class="font64">Mnih, A. and Hinton, G. E. (2009). A scalable hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural&#160;Information Processing Systems 21 (NIPS’08),</span><span class="font64"> pages 1081-1088. 469</span></p>
<p><span class="font64">Mnih, A. and Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noise-contrastive estimation. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 26</span><span class="font64">, pages&#160;2265-2273. Curran Associates, Inc. 474, 624</span></p>
<p><span class="font64">Mnih, A. and Teh, Y. W. (2012). A fast and simple algorithm for training neural probabilistic language models. In </span><span class="font64" style="font-style:italic;">ICML’2012</span><span class="font64">, pages 1751-1758. 474</span></p>
<p><span class="font64">Mnih, V. and Hinton, G. (2010). Learning to detect roads in high-resolution aerial images. In </span><span class="font64" style="font-style:italic;">Proceedings of the 11th European Conference on Computer Vision (ECCV).</span><span class="font64"> 102</span></p>
<p><span class="font64">Mnih, V., Larochelle, H., and Hinton, G. (2011). Conditional restricted Boltzmann machines for structure output prediction. In </span><span class="font64" style="font-style:italic;">Proc. Conf. on Uncertainty in Artificial&#160;Intelligence (UAI)</span><span class="font64">. 687</span></p>
<p><span class="font64">Mnih, V., Kavukcuoglo, K., Silver, D., Graves, A., Antonoglou, I., and Wierstra, D. (2013). Playing Atari with deep reinforcement learning. Technical report, arXiv:1312.5602. 106</span></p>
<p><span class="font64">Mnih, V., Heess, N., Graves, A., and Kavukcuoglu, K. (2014). Recurrent models of visual attention. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger,&#160;editors, </span><span class="font64" style="font-style:italic;">NIPS’2014</span><span class="font64">, pages 2204-2212. 693</span></p>
<p><span class="font64">Mnih, V., Kavukcuoglo, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidgeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A.,&#160;Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015).&#160;Human-level control through deep reinforcement learning. </span><span class="font64" style="font-style:italic;">Nature</span><span class="font64">, 518, 529-533. 25</span></p>
<p><span class="font64">Mobahi, H. and Fisher, III, J. W. (2015). A theoretical analysis of optimization by Gaussian continuation. In </span><span class="font64" style="font-style:italic;">AAAI’2015</span><span class="font64">. 327</span></p>
<p><span class="font64">Mobahi, H., Collobert, R., and Weston, J. (2009). Deep learning from temporal coherence in video. In L. Bottou and M. Littman, editors, </span><span class="font64" style="font-style:italic;">Proceedings of the 26th International&#160;Conference on Machine Learning</span><span class="font64">, pages 737-744, Montreal. Omnipress. 496</span></p>
<p><span class="font64">Mohamed, A., Dahl, G., and Hinton, G. (2009). Deep belief networks for phone recognition. 461</span></p>
<p><span class="font64">Mohamed, A., Sainath, T. N., Dahl, G., Ramabhadran, B., Hinton, G. E., and Picheny, M. A. (2011). Deep belief networks using discriminative features for phone recognition. In&#160;</span><span class="font64" style="font-style:italic;">Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference&#160;on</span><span class="font64">, pages 5060-5063. IEEE. 461</span></p>
<p><span class="font64">Mohamed, A., Dahl, G., and Hinton, G. (2012a). Acoustic modeling using deep belief networks. </span><span class="font64" style="font-style:italic;">IEEE Trans. on Audio, Speech and Language Processing</span><span class="font64">, 20(1), 14-22. 461</span></p>
<p><span class="font64">Mohamed, A., Hinton, G., and Penn, G. (2012b). Understanding how deep belief networks perform acoustic modelling. In </span><span class="font64" style="font-style:italic;">Acoustics, Speech and Signal Processing (ICASSP),&#160;2012 IEEE International Conference on</span><span class="font64">, pages 4273-4276. IEEE. 461</span></p>
<p><span class="font64">Moller, M. F. (1993). A scaled conjugate gradient algorithm for fast supervised learning. </span><span class="font64" style="font-style:italic;">Neural Networks</span><span class="font64">, 6, 525-533. 316</span></p>
<p><span class="font64">Montavon, G. and Muller, K.-R. (2012). Deep Boltzmann machines and the centering trick. In G. Montavon, G. Orr, and K.-R. Muller, editors, </span><span class="font64" style="font-style:italic;">Neural Networks: Tricks of&#160;the Trade</span><span class="font64">, volume 7700 of </span><span class="font64" style="font-style:italic;">Lecture Notes in Computer Science</span><span class="font64">, pages 621-637. Preprint:&#160;</span><a href="http://arxiv.org/abs/1203.3783"><span class="font64">http://arxiv.org/abs/1203.3783</span></a><span class="font64">. 675</span></p>
<p><span class="font64">Montufar, G. (2014). Universal approximation depth and errors of narrow belief networks with discrete units. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 26. 555</span></p>
<p><span class="font64">Montufar, G. and Ay, N. (2011). Refinements of universal approximation results for deep belief networks and restricted Boltzmann machines. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 23(5),&#160;1306-1319. 555</span></p>
<p><span class="font64">Montufar, G. F., Pascanu, R., Cho, K., and Bengio, Y. (2014). On the number of linear regions of deep neural networks. In </span><span class="font64" style="font-style:italic;">NIPS’2014</span><span class="font64">. 19, 199</span></p>
<p><span class="font64">Mor-Yosef, S., Samueloff, A., Modan, B., Navot, D., and Schenker, J. G. (1990). Ranking the risk factors for cesarean: logistic regression analysis of a nationwide study. </span><span class="font64" style="font-style:italic;">Obstet&#160;Gynecol</span><span class="font64">, 75(6), 944-7. 3</span></p>
<p><span class="font64">Morin, F. and Bengio, Y. (2005). Hierarchical probabilistic neural network language model. In </span><span class="font64" style="font-style:italic;">AISTATS’2005</span><span class="font64">. 469, 471</span></p>
<p><span class="font64">Mozer, M. C. (1992). The induction of multiscale temporal structure. In J. M. S. Hanson and R. Lippmann, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 4&#160;(NIPS’91),</span><span class="font64"> pages 275-282, San Mateo, CA. Morgan Kaufmann. 409</span></p>
<p><span class="font64">Murphy, K. P. (2012). </span><span class="font64" style="font-style:italic;">Machine Learning: a Probabilistic Perspective</span><span class="font64">. MIT Press, Cambridge, MA, USA. 62, 98, 145</span></p>
<p><span class="font64">Murray, B. U. I. and Larochelle, H. (2014). A deep and tractable density estimator. In </span><span class="font64" style="font-style:italic;">ICML’2014</span><span class="font64">. 189, 712</span></p>
<p><span class="font64">Nair, V. and Hinton, G. (2010). Rectified linear units improve restricted Boltzmann machines. In </span><span class="font64" style="font-style:italic;">ICML’2010</span><span class="font64">. 16, 173, 196</span></p>
<p><span class="font64">Nair, V. and Hinton, G. E. (2009). 3d object recognition with deep belief nets. In Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, editors, </span><span class="font64" style="font-style:italic;">Advances in&#160;Neural Information Processing Systems 22</span><span class="font64">, pages 1339-1347. Curran Associates, Inc.&#160;688</span></p>
<p><span class="font64">Narayanan, H. and Mitter, S. (2010). Sample complexity of testing the manifold hypothesis. In </span><span class="font64" style="font-style:italic;">NIPS’2010</span><span class="font64">. 163</span></p>
<p><span class="font64">Naumann, U. (2008). Optimal Jacobian accumulation is NP-complete. </span><span class="font64" style="font-style:italic;">Mathematical Programming</span><span class="font64">, 112(2), 427-441. 221</span></p>
<p><span class="font64">Navigli, R. and Velardi, P. (2005). Structural semantic interconnections: a knowledge-based approach to word sense disambiguation. </span><span class="font64" style="font-style:italic;">IEEE Trans. Pattern Analysis and Machine Intelligence</span><span class="font64">, 27(7), 1075—1086. 486</span></p>
<p><span class="font64">Neal, R. and Hinton, G. (1999). A view of the EM algorithm that justifies incremental, sparse, and other variants. In M. I. Jordan, editor, </span><span class="font64" style="font-style:italic;">Learning in Graphical Models</span><span class="font64">. MIT&#160;Press, Cambridge, MA. 636</span></p>
<p><span class="font64">Neal, R. M. (1990). Learning stochastic feedforward networks. Technical report. 694</span></p>
<p><span class="font64">Neal, R. M. (1993). Probabilistic inference using Markov chain Monte-Carlo methods. Technical Report CRG-TR-93-1, Dept. of Computer Science, University of Toronto. 682</span></p>
<p><span class="font64">Neal, R. M. (1994). Sampling from multimodal distributions using tempered transitions. Technical Report 9421, Dept. of Statistics, University of Toronto. 605</span></p>
<p><span class="font64">Neal, R. M. (1996). </span><span class="font64" style="font-style:italic;">Bayesian Learning for Neural Networks</span><span class="font64">. Lecture Notes in Statistics. Springer. 265</span></p>
<p><span class="font64">Neal, R. M. (2001). Annealed importance sampling. </span><span class="font64" style="font-style:italic;">Statistics and Computing</span><span class="font64">, 11(2), 125-139. 627, 629, 630</span></p>
<p><span class="font64">Neal, R. M. (2005). Estimating ratios of normalizing constants using linked importance sampling. 631</span></p>
<p><span class="font64">Nesterov, Y. (1983). A method of solving a convex programming problem with convergence rate O(1/k<sup>2</sup>). </span><span class="font64" style="font-style:italic;">Soviet Mathematics Doklady</span><span class="font64">, 27, 372-376. 300</span></p>
<p><span class="font64">Nesterov, Y. (2004). </span><span class="font64" style="font-style:italic;">Introductory lectures on convex optimization : a basic course</span><span class="font64">. Applied optimization. Kluwer Academic Publ., Boston, Dordrecht, London. 300</span></p>
<p><span class="font64">Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. (2011). Reading digits in natural images with unsupervised feature learning. Deep Learning and&#160;Unsupervised Feature Learning Workshop, NIPS. 21</span></p>
<p><span class="font64">Ney, H. and Kneser, R. (1993). Improved clustering techniques for class-based statistical language modelling. In </span><span class="font64" style="font-style:italic;">European Conference on Speech Communication and Technology&#160;(Eurospeech),</span><span class="font64"> pages 973-976, Berlin. 465</span></p>
<p><span class="font64">Ng, A. (2015). &#160;&#160;&#160;Advice for applying machine learning.</span></p>
<p><a href="https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf"><span class="font64">https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf</span></a><span class="font64">. 423</span></p>
<p><span class="font64">Niesler, T. R., Whittaker, E. W. D., and Woodland, P. C. (1998). Comparison of part-of-speech and automatically derived category-based language models for speech recognition. In </span><span class="font64" style="font-style:italic;">International Conference on Acoustics, Speech and Signal Processing (ICASSP),&#160;</span><span class="font64">pages 177-180. 465</span></p>
<p><span class="font64">Ning, F., Delhomme, D., LeCun, Y., Piano, F., Bottou, L., and Barbano, P. E. (2005). Toward automatic phenotyping of developing embryos from videos. </span><span class="font64" style="font-style:italic;">Image Processing,&#160;IEEE Transactions on</span><span class="font64">, 14(9), 1360-1371. 360</span></p>
<p><span class="font64">Nocedal, J. and Wright, S. (2006). </span><span class="font64" style="font-style:italic;">Numerical Optimization</span><span class="font64">. Springer. 92, 95</span></p>
<p><span class="font64">Norouzi, M. and Fleet, D. J. (2011). Minimal loss hashing for compact binary codes. In </span><span class="font64" style="font-style:italic;">ICML ’2011</span><span class="font64">. 527</span></p>
<p><span class="font64">Nowlan, S. J. (1990). Competing experts: An experimental investigation of associative mixture models. Technical Report CRG-TR-90-5, University of Toronto. 452</span></p>
<p><span class="font64">Nowlan, S. J. and Hinton, G. E. (1992). Simplifying neural networks by soft weight-sharing. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 4(4), 473-493. 139</span></p>
<p><span class="font64">Olshausen, B. and Field, D. J. (2005). How close are we to understanding V1? </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 17, 1665-1699. 16</span></p>
<p><span class="font64">Olshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. </span><span class="font64" style="font-style:italic;">Nature</span><span class="font64">, 381, 607-609. 146, 255, 370, 498</span></p>
<p><span class="font64">Olshausen, B. A., Anderson, C. H., and Van Essen, D. C. (1993). A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing&#160;of information. </span><span class="font64" style="font-style:italic;">J. Neurosci.,</span><span class="font64"> 13(11), 4700-4719. 452</span></p>
<p><span class="font64">Opper, M. and Archambeau, C. (2009). The variational Gaussian approximation revisited. </span><span class="font64" style="font-style:italic;">Neural computation</span><span class="font64">, 21(3), 786-792. 691</span></p>
<p><span class="font64">Oquab, M., Bottou, L., Laptev, I., and Sivic, J. (2014). Learning and transferring mid-level image representations using convolutional neural networks. In </span><span class="font64" style="font-style:italic;">Computer Vision and&#160;Pattern Recognition (CVPR), 2014 IEEE Conference on</span><span class="font64">, pages 1717-1724. IEEE. 538</span></p>
<p><span class="font64">Osindero, S. and Hinton, G. E. (2008). Modeling image patches with a directed hierarchy of Markov random fields. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors,&#160;</span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 20 (NIPS’07),</span><span class="font64"> pages 1121-1128,&#160;Cambridge, MA. MIT Press. 634</span></p>
<p><span class="font64">Ovid and Martin, C. (2004). </span><span class="font64" style="font-style:italic;">Metamorphoses</span><span class="font64">. W.W. Norton. 1</span></p>
<p><span class="font64">Paccanaro, A. and Hinton, G. E. (2000). Extracting distributed representations of concepts and relations from positive and negative propositions. In </span><span class="font64" style="font-style:italic;">International Joint Conference&#160;on Neural Networks (IJCNN),</span><span class="font64"> Como, Italy. IEEE, New York. 486</span></p>
<p><span class="font64">Paine, T. L., Khorrami, P., Han, W., and Huang, T. S. (2014). An analysis of unsupervised pre-training in light of recent advances. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1412.6597</span><span class="font64">. 534</span></p>
<p><span class="font64">Palatucci, M., Pomerleau, D., Hinton, G. E., and Mitchell, T. M. (2009). Zero-shot learning with semantic output codes. In Y. Bengio, D. Schuurmans, J. D. Lafferty,&#160;C. K. I. Williams, and A. Culotta, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing&#160;Systems 22</span><span class="font64">, pages 1410-1418. Curran Associates, Inc. 541</span></p>
<p><span class="font64">Parker, D. B. (1985). Learning-logic. Technical Report TR-47, Center for Comp. Research in Economics and Management Sci., MIT. 224</span></p>
<p><span class="font64">Pascanu, R., Mikolov, T., and Bengio, Y. (2013a). On the difficulty of training recurrent neural networks. In </span><span class="font64" style="font-style:italic;">ICML’2013</span><span class="font64">. 289, 402, 405, 409, 416, 418</span></p>
<p><span class="font64">Pascanu, R., Montufar, G., and Bengio, Y. (2013b). On the number of inference regions of deep feed forward networks with piece-wise linear activations. Technical report, U.&#160;Montreal, arXiv:1312.6098. 198</span></p>
<p><span class="font64">Pascanu, R., Gulgehre, Q., Cho, K., and Bengio, Y. (2014a). How to construct deep recurrent neural networks. In </span><span class="font64" style="font-style:italic;">ICLR’2014</span><span class="font64">. 19, 199, 265, 398, 399, 400, 412, 462</span></p>
<p><span class="font64">Pascanu, R., Montufar, G., and Bengio, Y. (2014b). On the number of inference regions of deep feed forward networks with piece-wise linear activations. In </span><span class="font64" style="font-style:italic;">ICLR’2014</span><span class="font64">. 552</span></p>
<p><span class="font64">Pati, Y., Rezaiifar, R., and Krishnaprasad, P. (1993). Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. In </span><span class="font64" style="font-style:italic;">Proceedings of the 27 th Annual Asilomar Conference on Signals, Systems, and Computers</span><span class="font64">,&#160;pages 40-44. 255</span></p>
<p><span class="font64">Pearl, J. (1985). Bayesian networks: A model of self-activated memory for evidential reasoning. In </span><span class="font64" style="font-style:italic;">Proceedings of the 7th Conference of the Cognitive Science Society,&#160;University of California, Irvine</span><span class="font64">, pages 329-334. 565</span></p>
<p><span class="font64">Pearl, J. (1988). </span><span class="font64" style="font-style:italic;">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</span><span class="font64">. Morgan Kaufmann. 54</span></p>
<p><span class="font64">Perron, O. (1907). Zur theorie der matrices. </span><span class="font64" style="font-style:italic;">Mathematische Annalen</span><span class="font64">,64(2), 248-263. 599</span></p>
<p><span class="font64">Petersen, K. B. and Pedersen, M. S. (2006). The matrix cookbook. Version 20051003. 31</span></p>
<p><span class="font64">Peterson, G. B. (2004). A day of great illumination: B. F. Skinner’s discovery of shaping. </span><span class="font64" style="font-style:italic;">Journal of the Experimental Analysis of Behavior</span><span class="font64">, 82(3), 317-328. 328</span></p>
<p><span class="font64">Pham, D.-T., Garat, P., and Jutten, C. (1992). Separation of a mixture of independent sources through a maximum likelihood approach. In </span><span class="font64" style="font-style:italic;">EUSIPCO</span><span class="font64">, pages 771-774. 493</span></p>
<p><span class="font64">Pham, P.-H., Jelaca, D., Farabet, C., Martini, B., LeCun, Y., and Culurcieho, E. (2012). NeuFlow: dataflow vision processing system-on-a-chip. In </span><span class="font64" style="font-style:italic;">Circuits and Systems (MWS-CAS), 2012 IEEE 55th International Midwest Symposium on</span><span class="font64">, pages 1044-1047. IEEE.&#160;453</span></p>
<p><span class="font64">Pinheiro, P. H. O. and Collobert, R. (2014). Recurrent convolutional neural networks for scene labeling. In </span><span class="font64" style="font-style:italic;">ICML’2014</span><span class="font64">. 359</span></p>
<p><span class="font64">Pinheiro, P. H. O. and Collobert, R. (2015). From image-level to pixel-level labeling with convolutional networks. In </span><span class="font64" style="font-style:italic;">Conference on Computer Vision and Pattern Recognition&#160;(CVPR).</span><span class="font64"> 359</span></p>
<p><span class="font64">Pinto, N., Cox, D. D., and DiCarlo, J. J. (2008). Why is real-world visual object recognition hard? </span><span class="font64" style="font-style:italic;">PLoS Comput Biol</span><span class="font64">, 4. 458</span></p>
<p><span class="font64">Pinto, N., Stone, Z., Zickler, T., and Cox, D. (2011). Scaling up biologically-inspired computer vision: A case study in unconstrained face recognition on facebook. In&#160;</span><span class="font64" style="font-style:italic;">Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer&#160;Society Conference on</span><span class="font64">, pages 35-42. IEEE. 363</span></p>
<p><span class="font64">Pollack, J. B. (1990). Recursive distributed representations. </span><span class="font64" style="font-style:italic;">Artificial Intelligence</span><span class="font64">,46(1), 77-105. 400</span></p>
<p><span class="font64">Polyak, B. and Juditsky, A. (1992). Acceleration of stochastic approximation by averaging. </span><span class="font64" style="font-style:italic;">SIAM J. Control and Optimization</span><span class="font64">, 30(4), 838-855. 322</span></p>
<p><span class="font64">Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. </span><span class="font64" style="font-style:italic;">USSR Computational Mathematics and Mathematical Physics</span><span class="font64">, 4(5), 1-17. 296</span></p>
<p><span class="font64">Poole, B., Sohl-Dickstein, J., and Ganguli, S. (2014). Analyzing noise in autoencoders and deep networks. </span><span class="font64" style="font-style:italic;">CoRR,</span><span class="font64"> abs/1406.1831. 241</span></p>
<p><span class="font64">Poon, H. and Domingos, P. (2011). Sum-product networks: A new deep architecture. In</span></p>
<p><span class="font64" style="font-style:italic;">Proceedings of the Twenty-seventh Conference in Uncertainty in Artificial Intelligence (UAI),</span><span class="font64"> Barcelona, Spain. 556</span></p>
<p><span class="font64">Presley, R. K. and Haggard, R. L. (1994). A fixed point implementation of the backpropa-gation learning algorithm. In </span><span class="font64" style="font-style:italic;">Southeastcon’94■ Creative Technology Transfer-A Global Affair., Proceedings of the 1994 IEEE</span><span class="font64">, pages 136-138. IEEE. 453</span></p>
<p><span class="font64">Price, R. (1958). A useful theorem for nonlinear devices having Gaussian inputs. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Information Theory</span><span class="font64">, 4(2), 69-72. 691</span></p>
<p><span class="font64">Quiroga, R. Q., Reddy, L., Kreiman, G., Koch, C., and Fried, I. (2005). Invariant visual representation by single neurons in the human brain. </span><span class="font64" style="font-style:italic;">Nature</span><span class="font64">, 435(7045), 1102-1107.&#160;366</span></p>
<p><span class="font64">Radford, A., Metz, L., and Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1511.06434</span><span class="font64">.&#160;554, 703, 704</span></p>
<p><span class="font64">Raiko, T., Yao, L., Cho, K., and Bengio, Y. (2014). Iterative neural autoregressive distribution estimator (NADE-k). Technical report, arXiv:1406.1485. 678, 711</span></p>
<p><span class="font64">Raina, R., Madhavan, A., and Ng, A. Y. (2009). Large-scale deep unsupervised learning using graphics processors. In L. Bottou and M. Littman, editors, </span><span class="font64" style="font-style:italic;">Proceedings of the&#160;Twenty-sixth International Conference on Machine Learning (ICML’09),</span><span class="font64"> pages 873-880,&#160;New York, NY, USA. ACM. 27, 448</span></p>
<p><span class="font64">Ramsey, F. P. (1926). Truth and probability. In R. B. Braithwaite, editor, </span><span class="font64" style="font-style:italic;">The Foundations of Mathematics and other Logical Essays</span><span class="font64">, chapter 7, pages 156-198. McMaster University&#160;Archive for the History of Economic Thought. 56</span></p>
<p><span class="font64">Ranzato, M. and Hinton, G. H. (2010). Modeling pixel means and covariances using factorized third-order Boltzmann machines. In </span><span class="font64" style="font-style:italic;">CVPR’2010</span><span class="font64">, pages 2551-2558. 682</span></p>
<p><span class="font64">Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007a). Efficient learning of sparse representations with an energy-based model. In </span><span class="font64" style="font-style:italic;">NIPS’2006</span><span class="font64">. 14, 19, 509, 530, 532</span></p>
<p><span class="font64">Ranzato, M., Huang, F., Boureau, Y., and LeCun, Y. (2007b). Unsupervised learning of invariant feature hierarchies with applications to object recognition. In </span><span class="font64" style="font-style:italic;">Proceedings of&#160;the Computer Vision and Pattern Recognition Conference (CVPR’07).</span><span class="font64"> IEEE Press. 364</span></p>
<p><span class="font64">Ranzato, M., Boureau, Y., and LeCun, Y. (2008). Sparse feature learning for deep belief networks. In </span><span class="font64" style="font-style:italic;">NIPS’2007</span><span class="font64">. 509</span></p>
<p><span class="font64">Ranzato, M., Krizhevsky, A., and Hinton, G. E. (2010a). Factored 3-way restricted Boltzmann machines for modeling natural images. In </span><span class="font64" style="font-style:italic;">Proceedings of AISTATS 2010</span><span class="font64">.&#160;680, 681</span></p>
<p><span class="font64">Ranzato, M., Mnih, V., and Hinton, G. (2010b). Generating more realistic images using gated MRFs. In </span><span class="font64" style="font-style:italic;">NIPS’2010</span><span class="font64">. 682</span></p>
<p><span class="font64">Rao, C. (1945). Information and the accuracy attainable in the estimation of statistical parameters. </span><span class="font64" style="font-style:italic;">Bulletin of the Calcutta Mathematical Society</span><span class="font64">, 37, 81-89. 135, 295</span></p>
<p><span class="font64">Rasmus, A., Valpola, H., Honkala, M., Berglund, M., and Raiko, T. (2015). Semi-supervised learning with ladder network. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1507.02672</span><span class="font64">. 428, 532</span></p>
<p><span class="font64">Recht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In </span><span class="font64" style="font-style:italic;">NIPS’2011</span><span class="font64">. 449</span></p>
<p><span class="font64">Reichert, D. P., Series, P., and Storkey, A. J. (2011). Neuronal adaptation for sampling-based probabilistic inference in perceptual bistability. In </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems</span><span class="font64">, pages 2357-2365. 668</span></p>
<p><span class="font64">Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In </span><span class="font64" style="font-style:italic;">ICML’2014</span><span class="font64">. Preprint:&#160;arXiv:1401.4082. 654, 691, 698</span></p>
<p><span class="font64">Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a). Contractive auto-encoders: Explicit invariance during feature extraction. In </span><span class="font64" style="font-style:italic;">ICML’2011</span><span class="font64">. 523, 524,&#160;525</span></p>
<p><span class="font64">Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., and Glorot, X. (2011b). Higher order contractive auto-encoder. In </span><span class="font64" style="font-style:italic;">ECML PKDD</span><span class="font64">. 523, 524</span></p>
<p><span class="font64">Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X. (2011c). The manifold tangent classifier. In </span><span class="font64" style="font-style:italic;">NIPS’2011</span><span class="font64">. 271, 272</span></p>
<p><span class="font64">Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012). A generative process for sampling contractive auto-encoders. In </span><span class="font64" style="font-style:italic;">ICML’2012</span><span class="font64">. 713</span></p>
<p><span class="font64">Ringach, D. and Shapley, R. (2004). Reverse correlation in neurophysiology. </span><span class="font64" style="font-style:italic;">Cognitive Science</span><span class="font64">, 28(2), 147-166. 368</span></p>
<p><span class="font64">Roberts, S. and Everson, R. (2001). </span><span class="font64" style="font-style:italic;">Independent component analysis: principles and practice</span><span class="font64">. Cambridge University Press. 495</span></p>
<p><span class="font64">Robinson, A. J. and Fallside, F. (1991). A recurrent error propagation network speech recognition system. </span><span class="font64" style="font-style:italic;">Computer Speech and Language</span><span class="font64">, 5(3), 259-274. 27, 461</span></p>
<p><span class="font64">Rockafellar, R. T. (1997). Convex analysis. princeton landmarks in mathematics. 93</span></p>
<p><span class="font64">Romero, A., Ballas, N., Ebrahimi Kahou, S., Chassang, A., Gatta, C., and Bengio, Y. (2015). Fitnets: Hints for thin deep nets. In </span><span class="font64" style="font-style:italic;">ICLR’2015, arXiv:1412.6550</span><span class="font64">. 325</span></p>
<p><span class="font64">Rosen, J. B. (1960). The gradient projection method for nonlinear programming. part i. linear constraints. </span><span class="font64" style="font-style:italic;">Journal of the Society for Industrial and Applied Mathematics</span><span class="font64">, 8(1),&#160;pp. 181-217. 93</span></p>
<p><span class="font64">Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. </span><span class="font64" style="font-style:italic;">Psychological Review</span><span class="font64">, 65, 386-408. 14, 15, 27</span></p>
<p><span class="font64">Rosenblatt, F. (1962). </span><span class="font64" style="font-style:italic;">Principles of Neurodynamics</span><span class="font64">. Spartan, New York. 15, 27</span></p>
<p><span class="font64">Roweis, S. and Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear embedding. </span><span class="font64" style="font-style:italic;">Science</span><span class="font64">, 290(5500). 163, 520</span></p>
<p><span class="font64">Roweis, S., Saul, L., and Hinton, G. (2002). Global coordination of local linear models. In T. Dietterich, S. Becker, and Z. Ghahramani, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information&#160;Processing Systems 14 (NIPS’01</span><span class="font64">), Cambridge, MA. MIT Press. 491</span></p>
<p><span class="font64">Rubin, D. B. </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (1984). Bayesianly justifiable and relevant frequency calculations for the applied statistician. </span><span class="font64" style="font-style:italic;">The Annals of Statistics</span><span class="font64">, 12(4), 1151-1172. 718</span></p>
<p><span class="font64">Rumelhart, D., Hinton, G., and Williams, R. (1986a). Learning representations by back-propagating errors. </span><span class="font64" style="font-style:italic;">Nature</span><span class="font64">, 323, 533-536. 14, 18, 23, 203, 225, 373, 478, 484</span></p>
<p><span class="font64">Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986b). Learning internal representations by error propagation. In D. E. Rumelhart and J. L. McClelland, editors, </span><span class="font64" style="font-style:italic;">Parallel Distributed Processing</span><span class="font64">, volume 1, chapter 8, pages 318-362. MIT Press, Cambridge. 21,&#160;27, 225</span></p>
<p><span class="font64">Rumelhart, D. E., McClelland, J. L., and the PDP Research Group (1986c). </span><span class="font64" style="font-style:italic;">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</span><span class="font64">. MIT Press,&#160;Cambridge. 17</span></p>
<p><span class="font64">Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2014a). ImageNet Large&#160;Scale Visual Recognition Challenge. 21</span></p>
<p><span class="font64">Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2014b). Imagenet large scale visual recognition&#160;challenge. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv: 1409.0575</span><span class="font64">. 28</span></p>
<p><span class="font64">Russel, S. J. and Norvig, P. (2003). </span><span class="font64" style="font-style:italic;">Artificial Intelligence: a Modern Approach</span><span class="font64">. Prentice Hall. 86</span></p>
<p><span class="font64">Rust, N., Schwartz, O., Movshon, J. A., and Simoncelli, E. (2005). Spatiotemporal elements of macaque V1 receptive fields. </span><span class="font64" style="font-style:italic;">Neuron</span><span class="font64">, 46(6), 945-956. 367</span></p>
<p><span class="font64">Sainath, T., Mohamed, A., Kingsbury, B., and Ramabhadran, B. (2013). Deep convolutional neural networks for LVCSR. In </span><span class="font64" style="font-style:italic;">ICASSP 2013</span><span class="font64">. 462</span></p>
<p><span class="font64">Salakhutdinov, R. (2010). Learning in Markov random fields using tempered transitions. In Y. Bengio, D. Schuurmans, C. Williams, J. Lafferty, and A. Culotta, editors, </span><span class="font64" style="font-style:italic;">Advances&#160;in Neural Information Processing Systems 22 (NIPS’09)</span><span class="font64">. 605</span></p>
<p><span class="font64">Salakhutdinov, R. and Hinton, G. (2009a). Deep Boltzmann machines. In </span><span class="font64" style="font-style:italic;">Proceedings of the International Conference on Artificial Intelligence and Statistics</span><span class="font64">, volume 5, pages&#160;448-455. 24, 27, 531, 665, 668, 673, 674</span></p>
<p><span class="font64">Salakhutdinov, R. and Hinton, G. (2009b). Semantic hashing. In </span><span class="font64" style="font-style:italic;">International Journal of Approximate Reasoning</span><span class="font64">. 527</span></p>
<p><span class="font64">Salakhutdinov, R. and Hinton, G. E. (2007a). Learning a nonlinear embedding by preserving class neighbourhood structure. In </span><span class="font64" style="font-style:italic;">Proceedings of the Eleventh International&#160;Conference on Artificial Intelligence and Statistics (AISTATS’07),</span><span class="font64"> San Juan, Porto&#160;Rico. Omnipress. 529</span></p>
<p><span class="font64">Salakhutdinov, R. and Hinton, G. E. (2007b). Semantic hashing. In </span><span class="font64" style="font-style:italic;">SIGIR’2007</span><span class="font64">. 527</span></p>
<p><span class="font64">Salakhutdinov, R. and Hinton, G. E. (2008). Using deep belief nets to learn covariance kernels for Gaussian processes. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors,&#160;</span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 20 (NIPS’07),</span><span class="font64"> pages 1249-1256,&#160;Cambridge, MA. MIT Press. 244</span></p>
<p><span class="font64">Salakhutdinov, R. and Larochelle, H. (2010). Efficient learning of deep Boltzmann machines. In </span><span class="font64" style="font-style:italic;">Proceedings of the Thirteenth International Conference on Artificial Intelligence and&#160;Statistics (AISTATS 2010), JMLR W&amp;CP</span><span class="font64">, volume 9, pages 693-700. 654</span></p>
<p><span class="font64">Salakhutdinov, R. and Mnih, A. (2008). Probabilistic matrix factorization. In </span><span class="font64" style="font-style:italic;">NIPS’2008</span><span class="font64">. 481</span></p>
<p><span class="font64">Salakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of deep belief networks. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, </span><span class="font64" style="font-style:italic;">Proceedings of&#160;the Twenty-fifth International Conference on Machine Learning (ICML’08),</span><span class="font64"> volume 25,&#160;pages 872-879. ACM. 630, 664</span></p>
<p><span class="font64">Salakhutdinov, R., Mnih, A., and Hinton, G. (2007). Restricted Boltzmann machines for collaborative filtering. In </span><span class="font64" style="font-style:italic;">ICML.</span><span class="font64"> 481</span></p>
<p><span class="font64">Sanger, T. D. (1994). Neural network learning control of robot manipulators using gradually increasing task difficulty. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Robotics and Automation,&#160;</span><span class="font64">10(3). 328</span></p>
<p><span class="font64">Saul, L. K. and Jordan, M. I. (1996). Exploiting tractable substructures in intractable networks. In D. Touretzky, M. Mozer, and M. Hasselmo, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural&#160;Information Processing Systems 8 (NIPS’95).</span><span class="font64"> MIT Press, Cambridge, MA. 640</span></p>
<p><span class="font64">Saul, L. K., Jaakkola, T., and Jordan, M. I. (1996). Mean field theory for sigmoid belief networks. </span><span class="font64" style="font-style:italic;">Journal of Artificial Intelligence Research</span><span class="font64">, 4, 61-76. 27, 695</span></p>
<p><span class="font64">Savich, A. W., Moussa, M., and Areibi, S. (2007). The impact of arithmetic representation on implementing mlp-bp on fpgas: A study. </span><span class="font64" style="font-style:italic;">Neural Networks, IEEE Transactions on</span><span class="font64">,&#160;18(1), 240-252. 453</span></p>
<p><span class="font64">Saxe, A. M., Koh, P. W., Chen, Z., Bhand, M., Suresh, B., and Ng, A. (2011). On random weights and unsupervised feature learning. In </span><span class="font64" style="font-style:italic;">Proc. ICML’2011</span><span class="font64">. ACM. 363</span></p>
<p><span class="font64">Saxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In </span><span class="font64" style="font-style:italic;">ICLR.</span><span class="font64"> 285, 286, 303</span></p>
<p><span class="font64">Schaul, T., Antonoglou, I., and Silver, D. (2014). Unit tests for stochastic optimization. In </span><span class="font64" style="font-style:italic;">International Conference on Learning Representations</span><span class="font64">. 309</span></p>
<p><span class="font64">Schmidhuber, J. (1992). Learning complex, extended sequences using the principle of history compression. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 4(2), 234-242. 400</span></p>
<p><span class="font64">Schmidhuber, J. (1996). Sequential neural text compression. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Neural Networks</span><span class="font64">, 7(1), 142-146. 479</span></p>
<p><span class="font64">Schmidhuber, J. (2012). Self-delimiting neural networks. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1210.0118</span><span class="font64">. 390</span></p>
<p><span class="font64">Scholkopf, B. and Smola, A. J. (2002). </span><span class="font64" style="font-style:italic;">Learning with kernels: Support vector machines, regularization, optimization, and beyond</span><span class="font64">. MIT press. 705</span></p>
<p><span class="font64">Scholkopf, B., Smola, A., and Muller, K.-R. (1998). Nonlinear component analysis as a kernel eigenvalue problem. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 10, 1299-1319. 163, 520</span></p>
<p><span class="font64">Scholkopf, B., Burges, C. J. C., and Smola, A. J. (1999). </span><span class="font64" style="font-style:italic;">Advances in Kernel Methods</span><span class="font64"> — </span><span class="font64" style="font-style:italic;">Support Vector Learning</span><span class="font64">. MIT Press, Cambridge, MA. 18, 142</span></p>
<p><span class="font64">Scholkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., and Mooij, J. (2012). On causal and anticausal learning. In </span><span class="font64" style="font-style:italic;">ICML’2012</span><span class="font64">, pages 1255-1262. 547</span></p>
<p><span class="font64">Schuster, M. (1999). On supervised learning from sequential data with applications for speech recognition. 189</span></p>
<p><span class="font64">Schuster, M. and Paliwal, K. (1997). Bidirectional recurrent neural networks. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Signal Processing</span><span class="font64">, 45(11), 2673-2681. 395</span></p>
<p><span class="font64">Schwenk, H. (2007). Continuous space language models. </span><span class="font64" style="font-style:italic;">Computer speech and language</span><span class="font64">, 21, 492-518. 468</span></p>
<p><span class="font64">Schwenk, H. (2010). Continuous space language models for statistical machine translation. </span><span class="font64" style="font-style:italic;">The Prague Bulletin of Mathematical Linguistics</span><span class="font64">, 93, 137-146. 475</span></p>
<p><span class="font64">Schwenk, H. (2014). Cleaned subset of WMT ’14 dataset. 21</span></p>
<p><span class="font64">Schwenk, H. and Bengio, Y. (1998). Training methods for adaptive boosting of neural networks. In M. Jordan, M. Kearns, and S. Solla, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 10 (NIPS’97)</span><span class="font64">, pages 647-653. MIT Press. 258</span></p>
<p><span class="font64">Schwenk, H. and Gauvain, J.-L. (2002). Connectionist language modeling for large vocabulary continuous speech recognition. In </span><span class="font64" style="font-style:italic;">International Conference on Acoustics,&#160;Speech and Signal Processing (ICASSP)</span><span class="font64">, pages 765-768, Orlando, Florida. 468</span></p>
<p><span class="font64">Schwenk, H., Costa-jussa, M. R., and Fonollosa, J. A. R. (2006). Continuous space language models for the IWSLT 2006 task. In </span><span class="font64" style="font-style:italic;">International Workshop on Spoken&#160;Language Translation</span><span class="font64">, pages 166-173. 475</span></p>
<p><span class="font64">Seide, F., Li, G., and Yu, D. (2011). Conversational speech transcription using context-dependent deep neural networks. In </span><span class="font64" style="font-style:italic;">Interspeech 2011</span><span class="font64">, pages 437-440. 23</span></p>
<p><span class="font64">Sejnowski, T. (1987). Higher-order Boltzmann machines. In </span><span class="font64" style="font-style:italic;">AIP Conference Proceedings 151 on Neural Networks for Computing</span><span class="font64">, pages 398-403. American Institute of Physics&#160;Inc. 688</span></p>
<p><span class="font64">Series, P., Reichert, D. P., and Storkey, A. J. (2010). Hallucinations in Charles Bonnet syndrome induced by homeostasis: a deep Boltzmann machine model. In </span><span class="font64" style="font-style:italic;">Advances in&#160;Neural Information Processing Systems</span><span class="font64">, pages 2020-2028. 668</span></p>
<p><span class="font64">Sermanet, P., Chintala, S., and LeCun, Y. (2012). Convolutional neural networks applied to house numbers digit classification. </span><span class="font64" style="font-style:italic;">CoRR,</span><span class="font64"> abs/1204.3968. 458</span></p>
<p><span class="font64">Sermanet, P., Kavukcuoglu, K., Chintala, S., and LeCun, Y. (2013). Pedestrian detection with unsupervised multi-stage feature learning. In </span><span class="font64" style="font-style:italic;">Proc. International Conference on&#160;Computer Vision and Pattern Recognition (CVPR’13).</span><span class="font64"> IEEE. 23, 200</span></p>
<p><span class="font64">Shilov, G. (1977). </span><span class="font64" style="font-style:italic;">Linear Algebra</span><span class="font64">. Dover Books on Mathematics Series. Dover Publications. 31</span></p>
<p><span class="font64">Siegelmann, H. (1995). Computation beyond the Turing limit. </span><span class="font64" style="font-style:italic;">Science</span><span class="font64">, 268(5210), 545-548. 379</span></p>
<p><span class="font64">Siegelmann, H. and Sontag, E. (1991). Turing computability with neural nets. </span><span class="font64" style="font-style:italic;">Applied Mathematics Letters</span><span class="font64">, 4(6), 77-80. 379</span></p>
<p><span class="font64">Siegelmann, H. T. and Sontag, E. D. (1995). On the computational power of neural nets. </span><span class="font64" style="font-style:italic;">Journal of Computer and Systems Sciences</span><span class="font64">, 50(1), 132-150. 379, 405</span></p>
<p><span class="font64">Sietsma, J. and Dow, R. (1991). Creating artificial neural networks that generalize. </span><span class="font64" style="font-style:italic;">Neural Networks</span><span class="font64">, 4(1), 67-79. 241</span></p>
<p><span class="font64">Simard, D., Steinkraus, P. Y., and Platt, J. C. (2003). Best practices for convolutional neural networks. In </span><span class="font64" style="font-style:italic;">ICDAR’2003</span><span class="font64">. 371</span></p>
<p><span class="font64">Simard, P. and Graf, H. P. (1994). Backpropagation without multiplication. In </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems</span><span class="font64">, pages 232-239. 453</span></p>
<p><span class="font64">Simard, P., Victorri, B., LeCun, Y., and Denker, J. (1992). Tangent prop - A formalism for specifying selected invariances in an adaptive network. In </span><span class="font64" style="font-style:italic;">NIPS’1991</span><span class="font64">. 270, 271, 272,&#160;356</span></p>
<p><span class="font64">Simard, P. Y., LeCun, Y., and Denker, J. (1993). Efficient pattern recognition using a new transformation distance. In </span><span class="font64" style="font-style:italic;">NIPS’92</span><span class="font64">. 270</span></p>
<p><span class="font64">Simard, P. Y., LeCun, Y. A., Denker, J. S., and Victorri, B. (1998). Transformation invariance in pattern recognition — tangent distance and tangent propagation. </span><span class="font64" style="font-style:italic;">Lecture&#160;Notes in Computer Science</span><span class="font64">, 1524. 270</span></p>
<p><span class="font64">Simons, D. J. and Levin, D. T. (1998). Failure to detect changes to people during a real-world interaction. </span><span class="font64" style="font-style:italic;">Psychonomic Bulletin &amp; Review</span><span class="font64">, 5(4), 644-649. 545</span></p>
<p><span class="font64">Simonyan, K. and Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. In </span><span class="font64" style="font-style:italic;">ICLR</span><span class="font64">. 323</span></p>
<p><span class="font64">Sjoberg, J. and Ljung, L. (1995). Overtraining, regularization and searching for a minimum, with application to neural networks. </span><span class="font64" style="font-style:italic;">International Journal of Control</span><span class="font64">,62(6), 1391-1407.&#160;250</span></p>
<p><span class="font64">Skinner, B. F. (1958). Reinforcement today. </span><span class="font64" style="font-style:italic;">American Psychologist</span><span class="font64">, 13, 94-99. 328</span></p>
<p><span class="font64">Smolensky, P. (1986). Information processing in dynamical systems: Foundations of harmony theory. In D. E. Rumelhart and J. L. McClelland, editors, </span><span class="font64" style="font-style:italic;">Parallel Distributed&#160;Processing</span><span class="font64">, volume 1, chapter 6, pages 194-281. MIT Press, Cambridge. 573, 589, 658</span></p>
<p><span class="font64">Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. In </span><span class="font64" style="font-style:italic;">NIPS’2012</span><span class="font64">. 438</span></p>
<p><span class="font64">Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning, C. D. (2011a). Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In </span><span class="font64" style="font-style:italic;">NIPS’2011</span><span class="font64">.&#160;400, 402</span></p>
<p><span class="font64">Socher, R., Manning, C., and Ng, A. Y. (2011b). Parsing natural scenes and natural language with recursive neural networks. In </span><span class="font64" style="font-style:italic;">Proceedings of the Twenty-Eighth International Conference on Machine Learning (ICML’2011)</span><span class="font64">. 400</span></p>
<p><span class="font64">Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011c). Semi-supervised recursive autoencoders for predicting sentiment distributions. In&#160;</span><span class="font64" style="font-style:italic;">EMNLP’2011</span><span class="font64">. 400</span></p>
<p><span class="font64">Socher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. (2013a). Recursive deep models for semantic compositionality over a sentiment&#160;treebank. In </span><span class="font64" style="font-style:italic;">EMNLP’2013</span><span class="font64">. 400, 402</span></p>
<p><span class="font64">Socher, R., Ganjoo, M., Manning, C. D., and Ng, A. Y. (2013b). Zero-shot learning through cross-modal transfer. In </span><span class="font64" style="font-style:italic;">27th Annual Conference on Neural Information Processing&#160;Systems (NIPS 2013).</span><span class="font64"> 541</span></p>
<p><span class="font64">Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. 717, 718</span></p>
<p><span class="font64">Sohn, K., Zhou, G., and Lee, H. (2013). Learning and selecting features jointly with point-wise gated Boltzmann machines. In </span><span class="font64" style="font-style:italic;">ICML’2013</span><span class="font64">. 689</span></p>
<p><span class="font64">Solomonoff, R. J. (1989). A system for incremental learning based on algorithmic probability. 328</span></p>
<p><span class="font64">Sontag, E. D. (1998). VC dimension of neural networks. </span><span class="font64" style="font-style:italic;">NATO ASI Series F Computer and Systems Sciences</span><span class="font64">, 168, 69-96. 549, 553</span></p>
<p><span class="font64">Sontag, E. D. and Sussman, H. J. (1989). Backpropagation can give rise to spurious local minima even for networks without hidden layers. </span><span class="font64" style="font-style:italic;">Complex Systems</span><span class="font64">, 3, 91-106. 284</span></p>
<p><span class="font64">Sparkes, B. (1996). </span><span class="font64" style="font-style:italic;">The Red and the Black: Studies in Greek Pottery</span><span class="font64">. Routledge. 1</span></p>
<p><span class="font64">Spitkovsky, V. I., Alshawi, H., and Jurafsky, D. (2010). From baby steps to leapfrog: how “less is more” in unsupervised dependency parsing. In </span><span class="font64" style="font-style:italic;">HLT’10</span><span class="font64">. 328</span></p>
<p><span class="font64">Squire, W. and Trapp, G. (1998). Using complex variables to estimate derivatives of real functions. </span><span class="font64" style="font-style:italic;">SIAM Rev.,</span><span class="font64"> 40(1), 110—112. 441</span></p>
<p><span class="font64">Srebro, N. and Shraibman, A. (2005). Rank, trace-norm and max-norm. In </span><span class="font64" style="font-style:italic;">Proceedings of the 18th Annual Conference on Learning Theory</span><span class="font64">, pages 545-560. Springer-Verlag. 238</span></p>
<p><span class="font64">Srivastava, N. (2013). </span><span class="font64" style="font-style:italic;">Improving Neural Networks With Dropout</span><span class="font64">. Master’s thesis, U. Toronto. 537</span></p>
<p><span class="font64">Srivastava, N. and Salakhutdinov, R. (2012). Multimodal learning with deep Boltzmann machines. In </span><span class="font64" style="font-style:italic;">NIPS’2012</span><span class="font64">. 543</span></p>
<p><span class="font64">Srivastava, N., Salakhutdinov, R. R., and Hinton, G. E. (2013). Modeling documents with deep Boltzmann machines. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1309.6865</span><span class="font64">. 665</span></p>
<p><span class="font64">Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. </span><span class="font64" style="font-style:italic;">Journal of Machine&#160;Learning Research</span><span class="font64">, 15, 1929-1958. 258, 265, 267, 674</span></p>
<p><span class="font64">Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway networks. </span><span class="font64" style="font-style:italic;">arXiv:1505.00387</span><span class="font64">. 326</span></p>
<p><span class="font64">Steinkrau, D., Simard, P. Y., and Buck, I. (2005). Using GPUs for machine learning algorithms. </span><span class="font64" style="font-style:italic;">2013 12th International Conference on Document Analysis and Recognition</span><span class="font64">,&#160;0, 1115-1119. 447</span></p>
<p><span class="font64">Stoyanov, V., Ropson, A., and Eisner, J. (2011). Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In&#160;</span><span class="font64" style="font-style:italic;">Proceedings of the 1fth International Conference on Artificial Intelligence and Statistics&#160;(AISTATS),</span><span class="font64"> volume 15 of </span><span class="font64" style="font-style:italic;">JMLR Workshop and Conference Proceedings</span><span class="font64">, pages 725-733,&#160;Fort Lauderdale. Supplementary material (4 pages) also available. 676, 700</span></p>
<p><span class="font64">Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. (2015). Weakly supervised memory networks. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1503.08895</span><span class="font64">. 420</span></p>
<p><span class="font64">Supancic, J. and Ramanan, D. (2013). Self-paced learning for long-term tracking. In </span><span class="font64" style="font-style:italic;">CVPR’2013</span><span class="font64">. 328</span></p>
<p><span class="font64">Sussillo, D. (2014). Random walks: Training very deep nonlinear feed-forward networks with smart initialization. </span><span class="font64" style="font-style:italic;">CoRR,</span><span class="font64"> abs/1412.6558. 290, 303, 305, 404</span></p>
<p><span class="font64">Sutskever, I. (2012). </span><span class="font64" style="font-style:italic;">Training Recurrent Neural Networks</span><span class="font64">. Ph.D. thesis, Department of computer science, University of Toronto. 407, 414</span></p>
<p><span class="font64">Sutskever, I. and Hinton, G. E. (2008). Deep narrow sigmoid belief networks are universal approximators. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 20(11), 2629-2636. 695</span></p>
<p><span class="font64">Sutskever, I. and Tieleman, T. (2010). On the Convergence Properties of Contrastive Divergence. In Y. W. Teh and M. Titterington, editors, </span><span class="font64" style="font-style:italic;">Proc. of the International&#160;Conference on Artificial Intelligence and Statistics (AISTATS)</span><span class="font64">, volume 9, pages 789-795.&#160;614</span></p>
<p><span class="font64">Sutskever, I., Hinton, G., and Taylor, G. (2009). The recurrent temporal restricted Boltzmann machine. In </span><span class="font64" style="font-style:italic;">NIPS’2008</span><span class="font64">. 687</span></p>
<p><span class="font64">Sutskever, I., Martens, J., and Hinton, G. E. (2011). Generating text with recurrent neural networks. In </span><span class="font64" style="font-style:italic;">ICML’2011</span><span class="font64">, pages 1017-1024. 479</span></p>
<p><span class="font64">Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In </span><span class="font64" style="font-style:italic;">ICML.</span><span class="font64"> 300, 407, 414</span></p>
<p><span class="font64">Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. In </span><span class="font64" style="font-style:italic;">NIPS’2014, arXiv:1409.3215</span><span class="font64">. 25, 101, 396, 410, 413, 476, 477</span></p>
<p><span class="font64">Sutton, R. and Barto, A. (1998). </span><span class="font64" style="font-style:italic;">Reinforcement Learning: An Introduction</span><span class="font64">. MIT Press. 106</span></p>
<p><span class="font64">Sutton, R. S., Mcallester, D., Singh, S., and Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. In </span><span class="font64" style="font-style:italic;">NIPS’1999</span><span class="font64">, pages 1057-1063. MIT Press. 693</span></p>
<p><span class="font64">Swersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas, N. (2011). On autoencoders and score matching for energy based models. In </span><span class="font64" style="font-style:italic;">ICML’2011</span><span class="font64">. ACM. 515</span></p>
<p><span class="font64">Swersky, K., Snoek, J., and Adams, R. P. (2014). Freeze-thaw Bayesian optimization. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1406.3896</span><span class="font64">. 438</span></p>
<p><span class="font64">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014a). Going deeper with convolutions. Technical report,&#160;arXiv:1409.4842. 24, 27, 200, 258, 269, 326, 347</span></p>
<p><span class="font64">Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and Fergus, R. (2014b). Intriguing properties of neural networks. </span><span class="font64" style="font-style:italic;">ICLR,</span><span class="font64"> abs/1312.6199.&#160;268, 271</span></p>
<p><span class="font64">Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2015). Rethinking the Inception Architecture for Computer Vision. </span><span class="font64" style="font-style:italic;">ArXiv e-prints</span><span class="font64">. 243, 322</span></p>
<p><span class="font64">Taigman, Y., Yang, M., Ranzato, M., and Wolf, L. (2014). DeepFace: Closing the gap to human-level performance in face verification. In </span><span class="font64" style="font-style:italic;">CVPR ’2014</span><span class="font64">. 100</span></p>
<p><span class="font64">Tandy, D. W. (1997). </span><span class="font64" style="font-style:italic;">Works and Days: A Translation and Commentary for the Social Sciences</span><span class="font64">. University of California Press. 1</span></p>
<p><span class="font64">Tang, Y. and Eliasmith, C. (2010). Deep networks for robust visual recognition. In</span></p>
<p><span class="font64" style="font-style:italic;">Proceedings of the 27th International Conference on Machine Learning, June 21-24, 2010, Haifa, Israel</span><span class="font64">. 241</span></p>
<p><span class="font64">Tang, Y., Salakhutdinov, R., and Hinton, G. (2012). Deep mixtures of factor analysers. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1206.4635</span><span class="font64">. 491</span></p>
<p><span class="font64">Taylor, G. and Hinton, G. (2009). Factored conditional restricted Boltzmann machines for modeling motion style. In L. Bottou and M. Littman, editors, </span><span class="font64" style="font-style:italic;">Proceedings of&#160;the Twenty-sixth International Conference on Machine Learning (ICML’09),</span><span class="font64"> pages&#160;1025-1032, Montreal, Quebec, Canada. ACM. 687</span></p>
<p><span class="font64">Taylor, G., Hinton, G. E., and Roweis, S. (2007). Modeling human motion using binary latent variables. In B. Scholkopf, J. Platt, and T. Hoffman, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural&#160;Information Processing Systems 19 (NIPS’06),</span><span class="font64"> pages 1345-1352. MIT Press, Cambridge,&#160;MA. 687</span></p>
<p><span class="font64">Teh, Y., Welling, M., Osindero, S., and Hinton, G. E. (2003). Energy-based models for sparse overcomplete representations. </span><span class="font64" style="font-style:italic;">Journal of Machine Learning Research</span><span class="font64">, 4,&#160;1235-1260. 493</span></p>
<p><span class="font64">Tenenbaum, J., de Silva, V., and Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. </span><span class="font64" style="font-style:italic;">Science</span><span class="font64">, 290(5500), 2319-2323. 163, 520, 535</span></p>
<p><span class="font64">Theis, L., van den Oord, A., and Bethge, M. (2015). A note on the evaluation of generative models. arXiv:1511.01844. 699, 721</span></p>
<p><span class="font64">Thompson, J., Jain, A., LeCun, Y., and Bregler, C. (2014). Joint training of a convolutional network and a graphical model for human pose estimation. In </span><span class="font64" style="font-style:italic;">NIPS’2014</span><span class="font64">. 360</span></p>
<p><span class="font64">Thrun, S. (1995). Learning to play the game of chess. In </span><span class="font64" style="font-style:italic;">NIPS’1994</span><span class="font64"> . 271</span></p>
<p><span class="font64">Tibshirani, R. J. (1995). Regression shrinkage and selection via the lasso. </span><span class="font64" style="font-style:italic;">Journal of the Royal Statistical Society B</span><span class="font64">, 58, 267-288. 236</span></p>
<p><span class="font64">Tieleman, T. (2008). Training restricted Boltzmann machines using approximations to the likelihood gradient. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, </span><span class="font64" style="font-style:italic;">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML’08),&#160;</span><span class="font64">pages 1064-1071. ACM. 614</span></p>
<p><span class="font64">Tieleman, T. and Hinton, G. (2009). Using fast weights to improve persistent contrastive divergence. In L. Bottou and M. Littman, editors, </span><span class="font64" style="font-style:italic;">Proceedings of the Twenty-sixth&#160;International Conference on Machine Learning (ICML’09),</span><span class="font64"> pages 1033-1040. ACM.&#160;616</span></p>
<p><span class="font64">Tipping, M. E. and Bishop, C. M. (1999). Probabilistic principal components analysis. </span><span class="font64" style="font-style:italic;">Journal of the Royal Statistical Society B</span><span class="font64">, 61(3), 611-622. 493</span></p>
<p><span class="font64">Torralba, A., Fergus, R., and Weiss, Y. (2008). Small codes and large databases for recognition. In </span><span class="font64" style="font-style:italic;">Proceedings of the Computer Vision and Pattern Recognition Conference&#160;(CVPR’08),</span><span class="font64"> pages 1-8. 527</span></p>
<p><span class="font64">Touretzky, D. S. and Minton, G. E. (1985). Symbols among the neurons: Details of a connectionist inference architecture. In </span><span class="font64" style="font-style:italic;">Proceedings of the 9th International Joint&#160;Conference on Artificial Intelligence - Volume 1</span><span class="font64">, IJCAI’85, pages 238-243, San Francisco,&#160;CA, USA. Morgan Kaufmann Publishers Inc. 17</span></p>
<p><span class="font64">Tu, K. and Honavar, V. (2011). On the utility of curricula in unsupervised learning of probabilistic grammars. In </span><span class="font64" style="font-style:italic;">IJCAI’2011</span><span class="font64">. 328</span></p>
<p><span class="font64">Turaga, S. C., Murray, J. F., Jain, V., Roth, F., Helmstaedter, M., Briggman, K., Denk, W., and Seung, H. S. (2010). Convolutional networks can learn to generate affinity&#160;graphs for image segmentation. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 22(2), 511-538. 359</span></p>
<p><span class="font64">Turian, J., Ratinov, L., and Bengio, Y. (2010). Word representations: A simple and general method for semi-supervised learning. In </span><span class="font64" style="font-style:italic;">Proc. ACL’2010</span><span class="font64">, pages 384-394. 537</span></p>
<p><span class="font64">Toscher, A., Jahrer, M., and Bell, R. M. (2009). The BigChaos solution to the Netflix grand prize. 481</span></p>
<p><span class="font64">Uria, B., Murray, I., and Larochelle, H. (2013). Rnade: The real-valued neural autoregressive density-estimator. In </span><span class="font64" style="font-style:italic;">NIPS’2013</span><span class="font64">. 711</span></p>
<p><span class="font64">van den Oord, A., Dieleman, S., and Schrauwen, B. (2013). Deep content-based music recommendation. In </span><span class="font64" style="font-style:italic;">NIPS’2013</span><span class="font64">. 482</span></p>
<p><span class="font64">van der Maaten, L. and Hinton, G. E. (2008). Visualizing data using t-SNE. </span><span class="font64" style="font-style:italic;">J. Machine Learning Res.,</span><span class="font64"> 9. 479, 521</span></p>
<p><span class="font64">Vanhoucke, V., Senior, A., and Mao, M. Z. (2011). Improving the speed of neural networks on CPUs. In </span><span class="font64" style="font-style:italic;">Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop</span><span class="font64">.&#160;446, 454</span></p>
<p><span class="font64">Vapnik, V. N. (1982). </span><span class="font64" style="font-style:italic;">Estimation of Dependences Based on Empirical Data</span><span class="font64">. Springer-Verlag, Berlin. 114</span></p>
<p><span class="font64">Vapnik, V. N. (1995). </span><span class="font64" style="font-style:italic;">The Nature of Statistical Learning Theory</span><span class="font64">. Springer, New York. 114</span></p>
<p><span class="font64">Vapnik, V. N. and Chervonenkis, A. Y. (1971). On the uniform convergence of relative frequencies of events to their probabilities. </span><span class="font64" style="font-style:italic;">Theory of Probability and Its Applications</span><span class="font64">,&#160;16, 264-280. 114</span></p>
<p><span class="font64">Vincent, P. (2011). A connection between score matching and denoising autoencoders. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 23(7). 515, 517, 714</span></p>
<p><span class="font64">Vincent, P. and Bengio, Y. (2003). Manifold Parzen windows. In </span><span class="font64" style="font-style:italic;">NIPS’2002</span><span class="font64">. MIT Press. 522</span></p>
<p><span class="font64">Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and composing robust features with denoising autoencoders. In </span><span class="font64" style="font-style:italic;">ICML 2008</span><span class="font64">. 241, 517</span></p>
<p><span class="font64">Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local&#160;denoising criterion. </span><span class="font64" style="font-style:italic;">J. Machine Learning Res.,</span><span class="font64"> 11. 517</span></p>
<p><span class="font64">Vincent, P., de Brebisson, A., and Bouthillier, X. (2015). Efficient exact gradient update for training deep networks with very large sparse targets. In C. Cortes, N. D. Lawrence,&#160;D. D. Lee, M. Sugiyama, and R. Garnett, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information&#160;Processing Systems 28</span><span class="font64">, pages 1108-1116. Curran Associates, Inc. 467</span></p>
<p><span class="font64">Vinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., and Hinton, G. (2014a). Grammar as a foreign language. Technical report, arXiv:1412.7449. 410</span></p>
<p><span class="font64">Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2014b). Show and tell: a neural image caption generator. arXiv 1411.4555. 410</span></p>
<p><span class="font64">Vinyals, O., Fortunato, M., and Jaitly, N. (2015a). Pointer networks. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1506.03134</span><span class="font64"> . 420</span></p>
<p><span class="font64">Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015b). Show and tell: a neural image caption generator. In </span><span class="font64" style="font-style:italic;">CVPR’2015</span><span class="font64">. arXiv:1411.4555. 102</span></p>
<p><span class="font64">Viola, P. and Jones, M. (2001). Robust real-time object detection. In </span><span class="font64" style="font-style:italic;">International Journal of Computer Vision</span><span class="font64">. 451</span></p>
<p><span class="font64">Visin, F., Kastner, K., Cho, K., Matteucci, M., Courville, A., and Bengio, Y. (2015). ReNet: A recurrent neural network based alternative to convolutional networks. </span><span class="font64" style="font-style:italic;">arXiv&#160;preprint arXiv:1505.00393</span><span class="font64">. 396</span></p>
<p><span class="font64">Von Melchner, L., Pallas, S. L., and Sur, M. (2000). Visual behaviour mediated by retinal projections directed to the auditory pathway. </span><span class="font64" style="font-style:italic;">Nature</span><span class="font64">, 404(6780), 871-876. 16</span></p>
<p><span class="font64">Wager, S., Wang, S., and Liang, P. (2013). Dropout training as adaptive regularization.</span></p>
<p><span class="font64">In </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 26</span><span class="font64">, pages 351-359. 265</span></p>
<p><span class="font64">Waibel, A., Hanazawa, T., Hinton, G. E., Shikano, K., and Lang, K. (1989). Phoneme recognition using time-delay neural networks. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Acoustics, Speech,&#160;and Signal Processing</span><span class="font64">, 37, 328-339. 374, 455, 461</span></p>
<p><span class="font64">Wan, L., Zeiler, M., Zhang, S., LeCun, Y., and Fergus, R. (2013). Regularization of neural networks using dropconnect. In </span><span class="font64" style="font-style:italic;">ICML’2013</span><span class="font64">. 266</span></p>
<p><span class="font64">Wang, S. and Manning, C. (2013). Fast dropout training. In </span><span class="font64" style="font-style:italic;">ICML’2013</span><span class="font64">. 266</span></p>
<p><span class="font64">Wang, Z., Zhang, J., Feng, J., and Chen, Z. (2014a). Knowledge graph and text jointly embedding. In </span><span class="font64" style="font-style:italic;">Proc. EMNLP’2014</span><span class="font64">. 486</span></p>
<p><span class="font64">Wang, Z., Zhang, J., Feng, J., and Chen, Z. (2014b). Knowledge graph embedding by translating on hyperplanes. In </span><span class="font64" style="font-style:italic;">Proc. AAAI’2014</span><span class="font64">. 486</span></p>
<p><span class="font64">Warde-Farley, D., Goodfellow, I. J., Courville, A., and Bengio, Y. (2014). An empirical analysis of dropout in piecewise linear networks. In </span><span class="font64" style="font-style:italic;">ICLR’2014</span><span class="font64">. 262, 266, 267</span></p>
<p><span class="font64">Wawrzynek, J., Asanovic, K., Kingsbury, B., Johnson, D., Beck, J., and Morgan, N. (1996). Spert-II: A vector microprocessor system. </span><span class="font64" style="font-style:italic;">Computer</span><span class="font64">, 29(3), 79-86. 453</span></p>
<p><span class="font64">Weaver, L. and Tao, N. (2001). The optimal reward baseline for gradient-based reinforcement learning. In </span><span class="font64" style="font-style:italic;">Proc. UAI’2001</span><span class="font64">, pages 538-545. 693</span></p>
<p><span class="font64">Weinberger, K. Q. and Saul, L. K. (2004). Unsupervised learning of image manifolds by semidefinite programming. In </span><span class="font64" style="font-style:italic;">CVPR’2004</span><span class="font64">, pages 988-995. 163, 521</span></p>
<p><span class="font64">Weiss, Y., Torralba, A., and Fergus, R. (2008). Spectral hashing. In </span><span class="font64" style="font-style:italic;">NIPS</span><span class="font64">, pages 1753-1760. 527</span></p>
<p><span class="font64">Welling, M., Zemel, R. S., and Hinton, G. E. (2002). Self supervised boosting. In </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems</span><span class="font64">, pages 665-672. 705</span></p>
<p><span class="font64">Welling, M., Hinton, G. E., and Osindero, S. (2003a). Learning sparse topographic representations with products of Student-t distributions. In </span><span class="font64" style="font-style:italic;">NIPS’2002</span><span class="font64">. 682</span></p>
<p><span class="font64">Welling, M., Zemel, R., and Hinton, G. E. (2003b). Self-supervised boosting. In S. Becker, S. Thrun, and K. Obermayer, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing&#160;Systems 15 (NIPS’02),</span><span class="font64"> pages 665-672. MIT Press. 624</span></p>
<p><span class="font64">Welling, M., Rosen-Zvi, M., and Hinton, G. E. (2005). Exponential family harmoniums with an application to information retrieval. In L. Saul, Y. Weiss, and L. Bottou,&#160;editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information Processing Systems 17 (NIPS’04),</span><span class="font64"> volume 17,&#160;Cambridge, MA. MIT Press. 678</span></p>
<p><span class="font64">Werbos, P. J. (1981). Applications of advances in nonlinear sensitivity analysis. In </span><span class="font64" style="font-style:italic;">Proceedings of the 10th IFIP Conference, 31.8 - 4-9, NYC</span><span class="font64">, pages 762-770. 224</span></p>
<p><span class="font64">Weston, J., Bengio, S., and Usunier, N. (2010). Large scale image annotation: learning to rank with joint word-image embeddings. </span><span class="font64" style="font-style:italic;">Machine Learning</span><span class="font64">, 81(1), 21-35. 402</span></p>
<p><span class="font64">Weston, J., Chopra, S., and Bordes, A. (2014). Memory networks. </span><span class="font64" style="font-style:italic;">arXiv preprint arXiv:1410.3916</span><span class="font64">. 420, 487</span></p>
<p><span class="font64">Widrow, B. and Hoff, M. E. (1960). Adaptive switching circuits. In </span><span class="font64" style="font-style:italic;">1960 IRE WESCON Convention Record</span><span class="font64">, volume 4, pages 96-104. IRE, New York. 15, 21, 24, 27</span></p>
<p><span class="font64">Wikipedia (2015). List of animals by number of neurons — Wikipedia, the free encyclopedia. [Online; accessed 4-March-2015]. 24, 27</span></p>
<p><span class="font64">Williams, C. K. I. and Agakov, F. V. (2002). Products of Gaussians and Probabilistic Minor Component Analysis. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 14(5), 1169-1182. 684</span></p>
<p><span class="font64">Williams, C. K. I. and Rasmussen, C. E. (1996). Gaussian processes for regression. In D. Touretzky, M. Mozer, and M. Hasselmo, editors, </span><span class="font64" style="font-style:italic;">Advances in Neural Information&#160;Processing Systems 8 (NIPS’95)</span><span class="font64">, pages 514-520. MIT Press, Cambridge, MA. 142</span></p>
<p><span class="font64">Williams, R. J. (1992). Simple statistical gradient-following algorithms connectionist reinforcement learning. </span><span class="font64" style="font-style:italic;">Machine Learning</span><span class="font64">, 8, 229-256. 690, 691</span></p>
<p><span class="font64">Williams, R. J. and Zipser, D. (1989). A learning algorithm for continually running fully recurrent neural networks. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 1, 270-280. 222</span></p>
<p><span class="font64">Wilson, D. R. and Martinez, T. R. (2003). The general inefficiency of batch training for gradient descent learning. </span><span class="font64" style="font-style:italic;">Neural Networks</span><span class="font64">, 16(10), 1429-1451. 279</span></p>
<p><span class="font64">Wilson, J. R. (1984). Variance reduction techniques for digital simulation. </span><span class="font64" style="font-style:italic;">American Journal of Mathematical and Management Sciences</span><span class="font64">, 4(3), 277—312. 692</span></p>
<p><span class="font64">Wiskott, L. and Sejnowski, T. J. (2002). Slow feature analysis: Unsupervised learning of invariances. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 14(4), 715-770. 496</span></p>
<p><span class="font64">Wolpert, D. and MacReady, W. (1997). No free lunch theorems for optimization. </span><span class="font64" style="font-style:italic;">IEEE Transactions on Evolutionary Computation</span><span class="font64">, 1, 67-82. 293</span></p>
<p><span class="font64">Wolpert, D. H. (1996). The lack of a priori distinction between learning algorithms. </span><span class="font64" style="font-style:italic;">Neural Computation</span><span class="font64">, 8(7), 1341-1390. 116</span></p>
<p><span class="font64">Wu, R., Yan, S., Shan, Y., Dang, Q., and Sun, G. (2015). Deep image: Scaling up image recognition. arXiv:1501.02876. 449</span></p>
<p><span class="font64">Wu, Z. (1997). Global continuation for distance geometry problems. </span><span class="font64" style="font-style:italic;">SIAM Journal of Optimization</span><span class="font64">, 7, 814-836. 327</span></p>
<p><span class="font64">Xiong, H. Y., Barash, Y., and Frey, B. J. (2011). Bayesian prediction of tissue-regulated splicing using RNA sequence and cellular context. </span><span class="font64" style="font-style:italic;">Bioinformatics</span><span class="font64">, 27(18), 2554-2562.&#160;265</span></p>
<p><span class="font64">Xu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R. S., and Bengio, Y. (2015). Show, attend and tell: Neural image caption generation with visual&#160;attention. In </span><span class="font64" style="font-style:italic;">ICML’2015, arXiv:1502.03044</span><span class="font64">. 102 , 410, 693</span></p>
<p><span class="font64">Yildiz, I. B., Jaeger, H., and Kiebel, S. J. (2012). Re-visiting the echo state property. </span><span class="font64" style="font-style:italic;">Neural networks</span><span class="font64">, 35, 1-9. 406</span></p>
<p><span class="font64">Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features in deep neural networks? In </span><span class="font64" style="font-style:italic;">NIPS’2014</span><span class="font64">. 323, 538</span></p>
<p><span class="font64">Younes, L. (1998). On the convergence of Markovian stochastic algorithms with rapidly decreasing ergodicity rates. In </span><span class="font64" style="font-style:italic;">Stochastics and Stochastics Models</span><span class="font64">, pages 177-228. 614</span></p>
<p><span class="font64">Yu, D., Wang, S., and Deng, L. (2010). Sequential labeling using deep-structured conditional random fields. </span><span class="font64" style="font-style:italic;">IEEE Journal of Selected Topics in Signal Processing</span><span class="font64">. 323</span></p>
<p><span class="font64">Zaremba, W. and Sutskever, I. (2014). Learning to execute. arXiv 1410.4615. 329</span></p>
<p><span class="font64">Zaremba, W. and Sutskever, I. (2015). Reinforcement learning neural Turing machines. </span><span class="font64" style="font-style:italic;">arXiv:1505.00521</span><span class="font64">. 421</span></p>
<p><span class="font64">Zaslavsky, T. (1975). </span><span class="font64" style="font-style:italic;">Facing Up to Arrangements: Face-Count Formulas for Partitions of Space by Hyperplanes</span><span class="font64">. Number no. 154 in Memoirs of the American Mathematical&#160;Society. American Mathematical Society. 552</span></p>
<p><span class="font64">Zeiler, M. D. and Fergus, R. (2014). Visualizing and understanding convolutional networks. In </span><span class="font64" style="font-style:italic;">ECCV’14</span><span class="font64">. 6</span></p>
<p><span class="font64">Zeiler, M. D., Ranzato, M., Monga, R., Mao, M., Yang, K., Le, Q., Nguyen, P., Senior, A., Vanhoucke, V., Dean, J., and Hinton, G. E. (2013). On rectified linear units for&#160;speech processing. In </span><span class="font64" style="font-style:italic;">ICASSP 2013</span><span class="font64">. 461</span></p>
<p><span class="font64">Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. (2015). Object detectors emerge in deep scene CNNs. ICLR’2015, arXiv:1412.6856. 553</span></p>
<p><span class="font64">Zhou, J. and Troyanskaya, O. G. (2014). Deep supervised and convolutional generative stochastic network for protein secondary structure prediction. In </span><span class="font64" style="font-style:italic;">ICML’2014</span><span class="font64">. 717</span></p>
<p><span class="font64">Zhou, Y. and Chellappa, R. (1988). Computation of optical flow using a neural network. In </span><span class="font64" style="font-style:italic;">Neural Networks, 1988., IEEE International Conference on</span><span class="font64">, pages 71-78. IEEE. 339</span></p>
<p><span class="font64">Zohrer, M. and Pernkopf, F. (2014). General stochastic networks for classification. In </span><span class="font64" style="font-style:italic;">NIPS’2014</span><span class="font64">. 717</span></p>
</body>
</html>