<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h4><a id="bookmark0"></a><span class="font65" style="font-weight:bold;">9.7 Data Types</span></h4>
<p><span class="font64">The data used with a convolutional network usually consists of several channels, each channel being the observation of a different quantity at some point in space&#160;or time. See Table 9.1 for examples of data types with different dimensionalities&#160;and number of channels.</span></p>
<p><span class="font64">For an example of convolutional networks applied to video, see Chen </span><span class="font64" style="font-weight:bold;font-style:italic;">et al. </span><span class="font64">(</span><span class="font18">2010</span><span class="font64">).</span></p>
<p><span class="font64">So far we have discussed only the case where every example in the train and test data has the same spatial dimensions. One advantage to convolutional networks&#160;is that they can also process inputs with varying spatial extents. These kinds of&#160;input simply cannot be represented by traditional, matrix multiplication-based&#160;neural networks. This provides a compelling reason to use convolutional networks&#160;even when computational cost and overfitting are not significant issues.</span></p>
<p><span class="font64">For example, consider a collection of images, where each image has a different width and height. It is unclear how to model such inputs with a weight matrix of&#160;fixed size. Convolution is straightforward to apply; the kernel is simply applied a&#160;different number of times depending on the size of the input, and the output of the&#160;convolution operation scales accordingly. Convolution may be viewed as matrix&#160;multiplication; the same convolution kernel induces a different size of doubly block&#160;circulant matrix for each size of input. Sometimes the output of the network is&#160;allowed to have variable size as well as the input, for example if we want to assign&#160;a class label to each pixel of the input. In this case, no further design work is&#160;necessary. In other cases, the network must produce some fixed-size output, for&#160;example if we want to assign a single class label to the entire image. In this case&#160;we must make some additional design steps, like inserting a pooling layer whose&#160;pooling regions scale in size proportional to the size of the input, in order to&#160;maintain a fixed number of pooled outputs. Some examples of this kind of strategy&#160;are shown in Fig. 9.11.</span></p>
<p><span class="font64">Note that the use of convolution for processing variable sized inputs only makes sense for inputs that have variable size because they contain varying amounts</span></p>
<table border="1">
<tr><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font64">Single channel</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">Multi-channel</span></p></td></tr>
<tr><td>
<p><span class="font64">1-D</span></p></td><td>
<p><span class="font64">Audio waveform: The axis we convolve over corresponds to&#160;time. We discretize time and&#160;measure the amplitude of the&#160;waveform once per time step.</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">Skeleton animation data: Animations of 3-D computer-rendered characters are generated by altering the pose of a “skeleton” over&#160;time. At each point in time, the&#160;pose of the character is described&#160;by a specification of the angles of&#160;each of the joints in the character’s skeleton. Each channel in&#160;the data we feed to the convolutional model represents the angle&#160;about one axis of one joint.</span></p></td></tr>
<tr><td>
<p><span class="font64">2-D</span></p></td><td>
<p><span class="font64">Audio data that has been preprocessed with a Fourier transform: We can transform the audio waveform into a 2D tensor with different rows corresponding to different frequencies and different&#160;columns corresponding to different points in time. Using convolution in the time makes the model&#160;equivariant to shifts in time. Using convolution across the frequency axis makes the model&#160;equivariant to frequency, so that&#160;the same melody played in a different octave produces the same&#160;representation but at a different&#160;height in the network’s output.</span></p></td><td>
<p><span class="font64">Color image data: One channel contains the red pixels, one the&#160;green pixels, and one the blue&#160;pixels. The convolution kernel&#160;moves over both the horizontal&#160;and vertical axes of the image,&#160;conferring translation equivari-ance in both directions.</span></p></td></tr>
<tr><td>
<p><span class="font64">3-D</span></p></td><td>
<p><span class="font64">Volumetric data: A common source of this kind of data is medical imaging technology, such as&#160;CT scans.</span></p></td><td>
<p><span class="font64">Color video data: One axis corresponds to time, one to the height of the video frame, and one to&#160;the width of the video frame.</span></p></td></tr>
</table>
<p><span class="font64">Table 9.1: Examples of different formats of data that can be used with convolutional networks.</span></p>
<p><span class="font64">of observation of the same kind of thing—different lengths of recordings over time, different widths of observations over space, etc. Convolution does not make&#160;sense if the input has variable size because it can optionally include different&#160;kinds of observations. For example, if we are processing college applications, and&#160;our features consist of both grades and standardized test scores, but not every&#160;applicant took the standardized test, then it does not make sense to convolve the&#160;same weights over both the features corresponding to the grades and the features&#160;corresponding to the test scores.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">9.8 Efficient Convolution Algorithms</span></h4>
<p><span class="font64">Modern convolutional network applications often involve networks containing more than one million units. Powerful implementations exploiting parallel computation&#160;resources, as discussed in Sec. 12.1, are essential. However, in many cases it is also&#160;possible to speed up convolution by selecting an appropriate convolution algorithm.</span></p>
<p><span class="font64">Convolution is equivalent to converting both the input and the kernel to the frequency domain using a Fourier transform, performing point-wise multiplication&#160;of the two signals, and converting back to the time domain using an inverse&#160;Fourier transform. For some problem sizes, this can be faster than the naive&#160;implementation of discrete convolution.</span></p>
<p><span class="font64">When a d-dimensional kernel can be expressed as the outer product of d vectors, one vector per dimension, the kernel is called </span><span class="font64" style="font-weight:bold;font-style:italic;">separable.</span><span class="font64"> When the kernel&#160;is separable, naive convolution is inefficient. It is equivalent to compose d onedimensional convolutions with each of these vectors. The composed approach&#160;is significantly faster than performing one d-dimensional convolution with their&#160;outer product. The kernel also takes fewer parameters to represent as vectors.&#160;If the kernel is w elements wide in each dimension, then naive multidimensional&#160;convolution requires O (w<sup>d</sup>) runtime and parameter storage space, while separable&#160;convolution requires O(w x d) runtime and parameter storage space. Of course,&#160;not every convolution can be represented in this way.</span></p>
<p><span class="font64">Devising faster ways of performing convolution or approximate convolution without harming the accuracy of the model is an active area of research. Even techniques that improve the efficiency of only forward propagation are useful because&#160;in the commercial setting, it is typical to devote more resources to deployment of&#160;a network than to its training.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">9.9 Random or Unsupervised Features</span></h4>
<p><span class="font64">Typically, the most expensive part of convolutional network training is learning the features. The output layer is usually relatively inexpensive due to the small number&#160;of features provided as input to this layer after passing through several layers of&#160;pooling. When performing supervised training with gradient descent, every gradient&#160;step requires a complete run of forward propagation and backward propagation&#160;through the entire network. One way to reduce the cost of convolutional network&#160;training is to use features that are not trained in a supervised fashion.</span></p>
<p><span class="font64">There are three basic strategies for obtaining convolution kernels without supervised training. One is to simply initialize them randomly. Another is to&#160;design them by hand, for example by setting each kernel to detect edges at a&#160;certain orientation or scale. Finally, one can learn the kernels with an unsupervised&#160;criterion. For example, Coates </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011) apply k-means clustering to small&#160;image patches, then use each learned centroid as a convolution kernel. Part III&#160;describes many more unsupervised learning approaches. Learning the features&#160;with an unsupervised criterion allows them to be determined separately from the&#160;classifier layer at the top of the architecture. One can then extract the features for&#160;the entire training set just once, essentially constructing a new training set for the&#160;last layer. Learning the last layer is then typically a convex optimization problem,&#160;assuming the last layer is something like logistic regression or an SVM.</span></p>
<p><span class="font64">Random filters often work surprisingly well in convolutional networks (Jarrett </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009; Saxe </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011; Pinto </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011; Cox and Pinto, 2011). Saxe </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(</span><span class="font18">2011</span><span class="font64">) showed that layers consisting of convolution following by pooling naturally&#160;become frequency selective and translation invariant when assigned random weights.&#160;They argue that this provides an inexpensive way to choose the architecture of&#160;a convolutional network: first evaluate the performance of several convolutional&#160;network architectures by training only the last layer, then take the best of these&#160;architectures and train the entire architecture using a more expensive approach.</span></p>
<p><span class="font64">An intermediate approach is to learn the features, but using methods that do not require full forward and back-propagation at every gradient step. As with&#160;multilayer perceptrons, we use greedy layer-wise pretraining, to train the first layer&#160;in isolation, then extract all features from the first layer only once, then train the&#160;second layer in isolation given those features, and so on. Chapter </span><span class="font18">8</span><span class="font64"> has described&#160;how to perform supervised greedy layer-wise pretraining, and Part III extends this&#160;to greedy layer-wise pretraining using an unsupervised criterion at each layer. The&#160;canonical example of greedy layer-wise pretraining of a convolutional model is the&#160;convolutional deep belief network (Lee </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009). Convolutional networks offer&#160;us the opportunity to take the pretraining strategy one step further than is possible&#160;with multilayer perceptrons. Instead of training an entire convolutional layer at a&#160;time, we can train a model of a small patch, as Coates </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011) do with k-means.&#160;We can then use the parameters from this patch-based model to define the kernels&#160;of a convolutional layer. This means that it is possible to use unsupervised learning&#160;to train a convolutional network </span><span class="font64" style="font-weight:bold;">without ever using convolution during the&#160;training process</span><span class="font64">. Using this approach, we can train very large models and incur a&#160;high computational cost only at inference time (Ranzato </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2007b; Jarrett </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2009; Kavukcuoglu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010; Coates </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013). This approach was popular&#160;from roughly 2007-2013, when labeled datasets were small and computational&#160;power was more limited. Today, most convolutional networks are trained in a&#160;purely supervised fashion, using full forward and back-propagation through the&#160;entire network on each training iteration.</span></p>
<p><span class="font64">As with other approaches to unsupervised pretraining, it remains difficult to tease apart the cause of some of the benefits seen with this approach. Unsupervised&#160;pretraining may offer some regularization relative to supervised training, or it may&#160;simply allow us to train much larger architectures due to the reduced computational&#160;cost of the learning rule.</span></p><h4><a id="bookmark3"></a><span class="font65" style="font-weight:bold;">9.10 The Neuroscientific Basis for Convolutional Networks</span></h4>
<p><span class="font64">Convolutional networks are perhaps the greatest success story of biologically inspired artificial intelligence. Though convolutional networks have been guided&#160;by many other fields, some of the key design principles of neural networks were&#160;drawn from neuroscience.</span></p>
<p><span class="font64">The history of convolutional networks begins with neuroscientific experiments long before the relevant computational models were developed. Neurophysiologists&#160;David Hubel and Torsten Wiesel collaborated for several years to determine many&#160;of the most basic facts about how the mammalian vision system works (Hubel and&#160;Wiesel, 1959, 1962, 1968). Their accomplishments were eventually recognized with&#160;a Nobel prize. Their findings that have had the greatest influence on contemporary&#160;deep learning models were based on recording the activity of individual neurons in&#160;cats. They observed how neurons in the cat’s brain responded to images projected&#160;in precise locations on a screen in front of the cat. Their great discovery was&#160;that neurons in the early visual system responded most strongly to very specific&#160;patterns of light, such as precisely oriented bars, but responded hardly at all to&#160;other patterns.</span></p>
<p><span class="font64">Their work helped to characterize many aspects of brain function that are beyond the scope of this book. From the point of view of deep learning, we can&#160;focus on a simplified, cartoon view of brain function.</span></p>
<p><span class="font64">In this simplified view, we focus on a part of the brain called </span><span class="font64" style="font-weight:bold;font-style:italic;">V1,</span><span class="font64"> also known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">primary visual cortex.</span><span class="font64"> V1 is the first area of the brain that begins to perform&#160;significantly advanced processing of visual input. In this cartoon view, images are&#160;formed by light arriving in the eye and stimulating the retina, the light-sensitive&#160;tissue in the back of the eye. The neurons in the retina perform some simple&#160;preprocessing of the image but do not substantially alter the way it is represented.&#160;The image then passes through the optic nerve and a brain region called the lateral&#160;geniculate nucleus. The main role, as far as we are concerned here, of both of these&#160;anatomical regions is primarily just to carry the signal from the eye to V1, which&#160;is located at the back of the head.</span></p>
<p><span class="font64">A convolutional network layer is designed to capture three properties of V1:</span></p>
<p><span class="font64">1. &#160;&#160;&#160;V1 is arranged in a spatial map. It actually has a two-dimensional structure&#160;mirroring the structure of the image in the retina. For example, light&#160;arriving at the lower half of the retina affects only the corresponding half of&#160;V1. Convolutional networks capture this property by having their features&#160;defined in terms of two dimensional maps.</span></p>
<p><span class="font64">2. &#160;&#160;&#160;V1 contains many </span><span class="font64" style="font-weight:bold;font-style:italic;">simple cells.</span><span class="font64"> A simple cell’s activity can to some extent be&#160;characterized by a linear function of the image in a small, spatially localized&#160;receptive field. The detector units of a convolutional network are designed&#160;to emulate these properties of simple cells.</span></p>
<p><span class="font64">3. &#160;&#160;&#160;V1 also contains many </span><span class="font64" style="font-weight:bold;font-style:italic;">complex cells.</span><span class="font64"> These cells respond to features that&#160;are similar to those detected by simple cells, but complex cells are invariant&#160;to small shifts in the position of the feature. This inspires the pooling units&#160;of convolutional networks. Complex cells are also invariant to some changes&#160;in lighting that cannot be captured simply by pooling over spatial locations.&#160;These invariances have inspired some of the cross-channel pooling strategies&#160;in convolutional networks, such as maxout units (Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013a).</span></p>
<p><span class="font64">Though we know the most about V1, it is generally believed that the same basic principles apply to other areas of the visual system. In our cartoon view of&#160;the visual system, the basic strategy of detection followed by pooling is repeatedly&#160;applied as we move deeper into the brain. As we pass through multiple anatomical&#160;layers of the brain, we eventually find cells that respond to some specific concept&#160;and are invariant to many transformations of the input. These cells have been&#160;nicknamed “grandmother cells”—the idea is that a person could have a neuron that&#160;activates when seeing an image of their grandmother, regardless of whether she&#160;appears in the left or right side of the image, whether the image is a close-up of&#160;her face or zoomed out shot of her entire body, whether she is brightly lit, or in&#160;shadow, etc.</span></p>
<p><span class="font64">These grandmother cells have been shown to actually exist in the human brain, in a region called the medial temporal lobe (Quiroga </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2005). Researchers&#160;tested whether individual neurons would respond to photos of famous individuals.&#160;They found what has come to be called the “Halle Berry neuron”: an individual&#160;neuron that is activated by the concept of Halle Berry. This neuron fires when a&#160;person sees a photo of Halle Berry, a drawing of Halle Berry, or even text containing&#160;the words “Halle Berry.” Of course, this has nothing to do with Halle Berry herself;&#160;other neurons responded to the presence of Bill Clinton, Jennifer Aniston, etc.</span></p>
<p><span class="font64">These medial temporal lobe neurons are somewhat more general than modern convolutional networks, which would not automatically generalize to identifying&#160;a person or object when reading its name. The closest analog to a convolutional&#160;network’s last layer of features is a brain area called the inferotemporal cortex&#160;(IT). When viewing an object, information flows from the retina, through the&#160;LGN, to V1, then onward to V2, then V4, then IT. This happens within the first&#160;100ms of glimpsing an object. If a person is allowed to continue looking at the&#160;object for more time, then information will begin to flow backwards as the brain&#160;uses top-down feedback to update the activations in the lower level brain areas.&#160;However, if we interrupt the person’s gaze, and observe only the firing rates that&#160;result from the first 100ms of mostly feedforward activation, then IT proves to be&#160;very similar to a convolutional network. Convolutional networks can predict IT&#160;firing rates, and also perform very similarly to (time limited) humans on object&#160;recognition tasks (DiCarlo, 2013).</span></p>
<p><span class="font64">That being said, there are many differences between convolutional networks and the mammalian vision system. Some of these differences are well known&#160;to computational neuroscientists, but outside the scope of this book. Some of&#160;these differences are not yet known, because many basic questions about how the&#160;mammalian vision system works remain unanswered. As a brief list:</span></p>
<p><span class="font64">• The human eye is mostly very low resolution, except for a tiny patch called the </span><span class="font64" style="font-weight:bold;font-style:italic;">fovea.</span><span class="font64"> The fovea only observes an area about the size of a thumbnail held at&#160;arms length. Though we feel as if we can see an entire scene in high resolution,&#160;this is an illusion created by the subconscious part of our brain, as it stitches&#160;together several glimpses of small areas. Most convolutional networks actually&#160;receive large full resolution photographs as input. The human brain makes&#160;several eye movements called </span><span class="font64" style="font-weight:bold;font-style:italic;">saccades</span><span class="font64"> to glimpse the most visually salient or&#160;task-relevant parts of a scene. Incorporating similar attention mechanisms&#160;into deep learning models is an active research direction. In the context of&#160;deep learning, attention mechanisms have been most successful for natural&#160;language processing, as described in Sec. 12.4.5.1. Several visual models&#160;with foveation mechanisms have been developed but so far have not become&#160;the dominant approach (Larochelle and Hinton, 2010; Denil </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012).</span></p>
<p><span class="font64">• &#160;&#160;&#160;The human visual system is integrated with many other senses, such as&#160;hearing, and factors like our moods and thoughts. Convolutional networks&#160;so far are purely visual.</span></p>
<p><span class="font64">• &#160;&#160;&#160;The human visual system does much more than just recognize objects. It is&#160;able to understand entire scenes including many objects and relationships&#160;between objects, and processes rich 3-D geometric information needed for&#160;our bodies to interface with the world. Convolutional networks have been&#160;applied to some of these problems but these applications are in their infancy.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Even simple brain areas like V1 are heavily impacted by feedback from higher&#160;levels. Feedback has been explored extensively in neural network models but&#160;has not yet been shown to offer a compelling improvement.</span></p>
<p><span class="font64">• &#160;&#160;&#160;While feedforward IT firing rates capture much of the same information as&#160;convolutional network features, it is not clear how similar the intermediate&#160;computations are. The brain probably uses very different activation and&#160;pooling functions. An individual neuron’s activation probably is not well-characterized by a single linear filter response. A recent model of V1 involves&#160;multiple quadratic filters for each neuron (Rust </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2005). Indeed our&#160;cartoon picture of “simple cells” and “complex cells” might create a nonexistent distinction; simple cells and complex cells might both be the same&#160;kind of cell but with their “parameters” enabling a continuum of behaviors&#160;ranging from what we call “simple” to what we call “complex.”</span></p>
<p><span class="font64">It is also worth mentioning that neuroscience has told us relatively little about how to </span><span class="font64" style="font-weight:bold;font-style:italic;">train</span><span class="font64"> convolutional networks. Model structures with parameter&#160;sharing across multiple spatial locations date back to early connectionist models&#160;of vision (Marr and Poggio, 1976), but these models did not use the modern&#160;back-propagation algorithm and gradient descent. For example, the Neocognitron&#160;(Fukushima, 1980) incorporated most of the model architecture design elements of&#160;the modern convolutional network but relied on a layer-wise unsupervised clustering&#160;algorithm.</span></p>
<p><span class="font64">Lang and Hinton (1988) introduced the use of back-propagation to train </span><span class="font64" style="font-weight:bold;font-style:italic;">time-delay neural networks</span><span class="font64"> (TDNNs). To use contemporary terminology, TDNNs are one-dimensional convolutional networks applied to time series. Back-propagation&#160;applied to these models was not inspired by any neuroscientific observation and&#160;is considered by some to be biologically implausible. Following the success of&#160;back-propagation-based training of TDNNs, (LeCun </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1989) developed the&#160;modern convolutional network by applying the same training algorithm to 2-D&#160;convolution applied to images.</span></p>
<p><span class="font64">So far we have described how simple cells are roughly linear and selective for certain features, complex cells are more nonlinear and become invariant to some&#160;transformations of these simple cell features, and stacks of layers that alternate&#160;between selectivity and invariance can yield grandmother cells for very specific&#160;phenomena. We have not yet described precisely what these individual cells detect.&#160;In a deep, nonlinear network, it can be difficult to understand the function of&#160;individual cells. Simple cells in the first layer are easier to analyze, because their&#160;responses are driven by a linear function. In an artificial neural network, we can&#160;just display an image of the convolution kernel to see what the corresponding&#160;channel of a convolutional layer responds to. In a biological neural network, we&#160;do not have access to the weights themselves. Instead, we put an electrode in the&#160;neuron itself, display several samples of white noise images in front of the animal’s&#160;retina, and record how each of these samples causes the neuron to activate. We&#160;can then fit a linear model to these responses in order to obtain an approximation&#160;of the neuron’s weights. This approach is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">reverse correlation</span><span class="font64"> (Ringach&#160;and Shapley, 2004).</span></p>
<p><span class="font64">Reverse correlation shows us that most V1 cells have weights that are described by </span><span class="font64" style="font-weight:bold;font-style:italic;">Gabor functions.</span><span class="font64"> The Gabor function describes the weight at a 2-D point in the&#160;image. We can think of an image as being a function of 2-D coordinates, I(x, y).&#160;Likewise, we can think of a simple cell as sampling the image at a set of locations,&#160;defined by a set of x coordinates X and a set of y coordinates, Y, and applying&#160;weights that are also a function of the location, </span><span class="font64" style="font-weight:bold;font-style:italic;">w(x, y</span><span class="font64">). From this point of view,&#160;the response of a simple cell to an image is given by</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">s<sup>(I</sup></span><span class="font64"><sup>)</sup> = &#160;&#160;&#160;<sup>w(x,</sup>y<sup>)I (x,</sup>y)•&#160;&#160;&#160;&#160;<sup>(9</sup>.<sup>15)</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x£Xy£</span><span class="font64">Y</span></p>
<p><span class="font64">Specifically, w(x, y) takes the form of a Gabor function:</span></p>
<p><span class="font64">w(x, y; </span><span class="font64" style="font-weight:bold;font-style:italic;">a,fix,Py</span><span class="font64">,f,</span><span class="font18">0</span><span class="font64">,xo,y</span><span class="font18">0</span><span class="font64" style="font-variant:small-caps;">, t) = a exp (־ArX</span><span class="font18"><sup>/2</sup></span><span class="font64"> - <sup>£</sup>yy'<sup>2</sup>) cos(fx<sup>/</sup> + </span><span class="font64" style="font-weight:bold;font-style:italic;">^)<sup>,</sup></span><span class="font64"><sup> &#160;&#160;&#160;(9</sup>.<sup>16)</sup></span></p>
<p><span class="font64">where</span></p>
<p><span class="font64">x<sup>/</sup> = (x - xo) cos(t) + (y - yo) sin(T) &#160;&#160;&#160;(9.17)</span></p>
<p><span class="font64">and</span></p>
<p><span class="font64">y' = - (x - x</span><span class="font18">0</span><span class="font64">) sin(r) + (y - y0) cos(r). &#160;&#160;&#160;(9.18)</span></p>
<p><span class="font64">Here, a, 3<sub>x</sub>, 3<sub>y</sub>, f, 3, x</span><span class="font18">0</span><span class="font64">, y</span><span class="font18">0</span><span class="font64">, and t are parameters that control the properties of the Gabor function. Fig. 9.18 shows some examples of Gabor functions with&#160;different settings of these parameters.</span></p>
<p><span class="font64">The parameters x</span><span class="font18">0</span><span class="font64">, yo, and t define a coordinate system. We translate and rotate x and y to form </span><span class="font64" style="font-weight:bold;font-style:italic;">x<sup>!</sup></span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64">'. Specifically, the simple cell will respond to image&#160;features centered at the point (x </span><span class="font18">0</span><span class="font64">, y </span><span class="font18">0</span><span class="font64">), and it will respond to changes in brightness&#160;as we move along a line rotated t radians from the horizontal.</span></p>
<p><span class="font64">Viewed as a function of </span><span class="font64" style="font-weight:bold;font-style:italic;">x'</span><span class="font64"> and y', the function w then responds to changes in brightness as we move along the x' axis. It has two important factors: one is a&#160;Gaussian function and the other is a cosine function.</span></p>
<p><span class="font64">The Gaussian factor a exp (—3<sub>x</sub> x'<sup>2</sup> — 3</span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64">3<sup>2</sup>) can be seen as a gating term that ensures the simple cell will only respond to values near where x' and </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"> are both&#160;zero, in other words, near the center of the cell’s receptive field. The scaling factor&#160;a adjusts the total magnitude of the simple cell’s response, while </span><span class="font64" style="font-weight:bold;font-style:italic;">3<sub>x</sub></span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">3<sub>y</sub></span><span class="font64"> control&#160;how quickly its receptive field falls off.</span></p>
<p><span class="font64">The cosine factor cos (fx' + </span><span class="font64" style="font-weight:bold;font-style:italic;">3</span><span class="font64">) controls how the simple cell responds to changing brightness along the x' axis. The parameter f controls the frequency of the cosine&#160;and </span><span class="font64" style="font-weight:bold;font-style:italic;">3</span><span class="font64"> controls its phase offset.</span></p>
<p><span class="font64">Altogether, this cartoon view of simple cells means that a simple cell responds to a specific spatial frequency of brightness in a specific direction at a specific&#160;location. Simple cells are most excited when the wave of brightness in the image&#160;has the same phase as the weights. This occurs when the image is bright where the&#160;weights are positive and dark where the weights are negative. Simple cells are most&#160;inhibited when the wave of brightness is fully out of phase with the weights—when&#160;the image is dark where the weights are positive and bright where the weights are&#160;negative.</span></p>
<p><span class="font64">The cartoon view of a complex cell is that it computes the </span><span class="font64" style="font-weight:bold;font-style:italic;">L</span><span class="font64"> norm of the 2-D vector containing two simple cells’ responses: c(I) </span><span class="font64" style="font-weight:bold;font-style:italic;">= y</span><span class="font64">/s</span><span class="font18">0</span><span class="font64">(I)<sup>2</sup> + </span><span class="font64" style="font-weight:bold;font-style:italic;">s<sub>1</sub>(I</span><span class="font64">) <sup>2</sup>. An&#160;important special case occurs when s </span><span class="font18">1</span><span class="font64"> has all of the same parameters as s</span><span class="font18">0</span><span class="font64"> except&#160;for </span><span class="font64" style="font-weight:bold;font-style:italic;">3</span><span class="font64">, and </span><span class="font64" style="font-weight:bold;font-style:italic;">3</span><span class="font64"> is set such that si is one quarter cycle out of phase with s</span><span class="font18">0</span><span class="font64">. In&#160;this case, s</span><span class="font18">0</span><span class="font64"> and s! form a </span><span class="font64" style="font-weight:bold;font-style:italic;">quadrature pair.</span><span class="font64"> A complex cell defined in this way&#160;responds when the Gaussian reweighted image I(x, y) exp(—</span><span class="font64" style="font-weight:bold;font-style:italic;">3</span><span class="font64"><sub>x</sub>x<sup>2</sup> — </span><span class="font64" style="font-weight:bold;font-style:italic;">3<sub>y</sub>y'<sup>2</sup>)</span><span class="font64"> contains&#160;a high amplitude sinusoidal wave with frequency f in direction t near (x</span><span class="font18">0</span><span class="font64">, y</span><span class="font18">0</span><span class="font64">),&#160;</span><span class="font64" style="font-weight:bold;">regardless of the phase offset of this wave</span><span class="font64">. In other words, the complex cell&#160;is invariant to small translations of the image in direction t , or to negating the</span></p>
<p><span class="font20" style="font-weight:bold;">aaSBBBBB nnODDDDDD DDDDDDDin</span></p><div><div><img src="main-112.jpg" alt=""/>
<p><span class="font64">Figure 9.18: Gabor functions with a variety of parameter settings. White indicates large positive weight, black indicates large negative weight, and the background gray&#160;corresponds to zero weight. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> Gabor functions with different values of the parameters&#160;that control the coordinate system: x </span><span class="font19">0</span><span class="font64">, y</span><span class="font19">0</span><span class="font64">, and t. Each Gabor function in this grid is&#160;assigned a value of X</span><span class="font19">0</span><span class="font64"> and </span><span class="font64" style="font-style:italic;">y</span><span class="font19">0</span><span class="font64"> proportional to its position in its grid, and t is chosen so&#160;that each Gabor filter is sensitive to the direction radiating out from the center of the grid.&#160;For the other two plots, X</span><span class="font19">0</span><span class="font64">, </span><span class="font64" style="font-style:italic;">yo</span><span class="font64">, and t are fixed to zero. </span><span class="font64" style="font-style:italic;">(Center)</span><span class="font64"> Gabor functions with&#160;different Gaussian scale parameters fl<sub>x</sub> and fl<sub>y</sub>. Gabor functions are arranged in increasing&#160;width (decreasing fl<sub>x</sub>) as we move left to right through the grid, and increasing height&#160;(decreasing fl<sub>y</sub>) as we move top to bottom. For the other two plots, the values are fixed&#160;to 1.5x the image width. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> Gabor functions with different sinusoid parameters f&#160;and As we move top to bottom, </span><span class="font64" style="font-style:italic;">f</span><span class="font64"> increases, and as we move left to right, </span><span class="font64" style="font-style:italic;">$</span><span class="font64"> increases.&#160;For the other two plots, </span><span class="font64" style="font-style:italic;">$</span><span class="font64"> is fixed to 0 and f is fixed to 5x the image width.</span></p></div></div>
<p><span class="font64">image (replacing black with white and vice versa).</span></p>
<p><span class="font64">Some of the most striking correspondences between neuroscience and machine learning come from visually comparing the features learned by machine learning&#160;models with those employed by V1. Olshausen and Field (1996) showed that&#160;a simple unsupervised learning algorithm, sparse coding, learns features with&#160;receptive fields similar to those of simple cells. Since then, we have found that&#160;an extremely wide variety of statistical learning algorithms learn features with&#160;Gabor-like functions when applied to natural images. This includes most deep&#160;learning algorithms, which learn these features in their first layer. Fig. 9.19 shows&#160;some examples. Because so many different learning algorithms learn edge detectors,&#160;it is difficult to conclude that any specific learning algorithm is the “right” model&#160;of the brain just based on the features that it learns (though it can certainly be a&#160;bad sign if an algorithm does </span><span class="font64" style="font-weight:bold;font-style:italic;">not</span><span class="font64"> learn some sort of edge detector when applied to&#160;natural images). These features are an important part of the statistical structure&#160;of natural images and can be recovered by many different approaches to statistical&#160;modeling. See Hyvarinen </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2009) for a review of the field of natural image&#160;statistics.</span></p>
<p><span class="font64">= = ^ /</span></p><div><div><img src="main-113.jpg" alt=""/></div></div><div>
<p><span class="font64">V I</span></p><img src="main-114.jpg" alt=""/></div>
<p><span class="font64">Figure 9.19: Many machine learning algorithms learn features that detect edges or specific colors of edges when applied to natural images. These feature detectors are reminiscent of&#160;the Gabor functions known to be present in primary visual cortex. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> Weights learned&#160;by an unsupervised learning algorithm (spike and slab sparse coding) applied to small&#160;image patches. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> Convolution kernels learned by the first layer of a fully supervised&#160;convolutional maxout network. Neighboring pairs of filters drive the same maxout unit.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">9.11 Convolutional Networks and the History of Deep&#160;Learning</span></h4>
<p><span class="font64">Convolutional networks have played an important role in the history of deep learning. They are a key example of a successful application of insights obtained&#160;by studying the brain to machine learning applications. They were also some of&#160;the first deep models to perform well, long before arbitrary deep models were&#160;considered viable. Convolutional networks were also some of the first neural&#160;networks to solve important commercial applications and remain at the forefront&#160;of commercial applications of deep learning today. For example, in the 1990s, the&#160;neural network research group at AT&amp;T developed a convolutional network for&#160;reading checks (LeCun </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1998b). By the end of the 1990s, this system deployed&#160;by NEC was reading over 10% of all the checks in the US. Later, several OCR&#160;and handwriting recognition systems based on convolutional nets were deployed&#160;by Microsoft (Simard </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2003). See Chapter 12 for more details on such&#160;applications and more modern applications of convolutional networks. See LeCun&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (</span><span class="font18">2010</span><span class="font64">) for a more in-depth history of convolutional networks up to </span><span class="font18">2010</span><span class="font64">.</span></p>
<p><span class="font64">Convolutional networks were also used to win many contests. The current intensity of commercial interest in deep learning began when Krizhevsky </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(2012) won the ImageNet object recognition challenge, but convolutional networks&#160;had been used to win other machine learning and computer vision contests with&#160;less impact for years earlier.</span></p>
<p><span class="font64">Convolutional nets were some of the first working deep networks trained with back-propagation. It is not entirely clear why convolutional networks succeeded&#160;when general back-propagation networks were considered to have failed. It may&#160;simply be that convolutional networks were more computationally efficient than&#160;fully connected networks, so it was easier to run multiple experiments with them&#160;and tune their implementation and hyperparameters. Larger networks also seem&#160;to be easier to train. With modern hardware, large fully connected networks&#160;appear to perform reasonably on many tasks, even when using datasets that were&#160;available and activation functions that were popular during the times when fully&#160;connected networks were believed not to work well. It may be that the primary&#160;barriers to the success of neural networks were psychological (practitioners did&#160;not expect neural networks to work, so they did not make a serious effort to use&#160;neural networks). Whatever the case, it is fortunate that convolutional networks&#160;performed well decades ago. In many ways, they carried the torch for the rest of&#160;deep learning and paved the way to the acceptance of neural networks in general.</span></p>
<p><span class="font64">Convolutional networks provide a way to specialize neural networks to work with data that has a clear grid-structured topology and to scale such models to&#160;very large size. This approach has been the most successful on a two-dimensional,&#160;image topology. To process one-dimensional, sequential data, we turn next to&#160;another powerful specialization of the neural networks framework: recurrent neural&#160;networks.</span></p><div><div><img src="main-115.jpg" alt=""/>
<p><span class="font64">Figure 9.16: A comparison of locally connected layers, tiled convolution, and standard convolution. All three have the same sets of connections between units, when the same&#160;size of kernel is used. This diagram illustrates the use of a kernel that is two pixels wide.&#160;The differences between the methods lies in how they share parameters. </span><span class="font64" style="font-style:italic;">(Top)</span><span class="font64"> A locally&#160;connected layer has no sharing at all. We indicate that each connection has its own weight&#160;by labeling each connection with a unique letter. </span><span class="font64" style="font-style:italic;">(Center)</span><span class="font64"> Tiled convolution has a set of&#160;</span><span class="font64" style="font-style:italic;">t</span><span class="font64"> different kernels. Here we illustrate the case of </span><span class="font64" style="font-style:italic;">t =</span><span class="font64"> 2. One of these kernels has edges&#160;labeled “a” and “b,” while the other has edges labeled “c” and “d.” Each time we move one&#160;pixel to the right in the output, we move on to using a different kernel. This means that,&#160;like the locally connected layer, neighboring units in the output have different parameters.&#160;Unlike the locally connected layer, after we have gone through all t available kernels,&#160;we cycle back to the first kernel. If two output units are separated by a multiple oft&#160;steps, then they share parameters. </span><span class="font64" style="font-style:italic;">(Bottom)</span><span class="font64"> Traditional convolution is equivalent to tiled&#160;convolution with t = 1. There is only one kernel and it is applied everywhere, as indicated&#160;in the diagram by using the kernel with weights labeled “a” and “b” everywhere.</span></p></div></div>
</body>
</html>