<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 8</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Optimization for Training Deep Models</span></h2>
<p><span class="font64">Deep learning algorithms involve optimization in many contexts. For example, performing inference in models such as PCA involves solving an optimization&#160;problem. We often use analytical optimization to write proofs or design algorithms.&#160;Of all of the many optimization problems involved in deep learning, the most&#160;difficult is neural network training. It is quite common to invest days to months of&#160;time on hundreds of machines in order to solve even a single instance of the neural&#160;network training problem. Because this problem is so important and so expensive,&#160;a specialized set of optimization techniques have been developed for solving it.&#160;This chapter presents these optimization techniques for neural network training.</span></p>
<p><span class="font64">If you are unfamiliar with the basic principles of gradient-based optimization, we suggest reviewing Chapter 4. That chapter includes a brief overview of numerical&#160;optimization in general.</span></p>
<p><span class="font64">This chapter focuses on one particular case of optimization: finding the parameters </span><span class="font64" style="font-weight:bold;">0 </span><span class="font64">of a neural network that significantly reduce a cost function J(</span><span class="font64" style="font-weight:bold;">0</span><span class="font64">), which typically includes a performance measure evaluated on the entire training set as&#160;well as additional regularization terms.</span></p>
<p><span class="font64">We begin with a description of how optimization used as a training algorithm for a machine learning task differs from pure optimization. Next, we present several&#160;of the concrete challenges that make optimization of neural networks difficult. We&#160;then define several practical algorithms, including both optimization algorithms&#160;themselves and strategies for initializing the parameters. More advanced algorithms&#160;adapt their learning rates during training or leverage information contained in&#160;the second derivatives of the cost function. Finally, we conclude with a review of&#160;several optimization strategies that are formed by combining simple optimization&#160;algorithms into higher-level procedures.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">8.1 &#160;&#160;&#160;How Learning Differs from Pure Optimization</span></h4>
<p><span class="font64">Optimization algorithms used for training of deep models differ from traditional optimization algorithms in several ways. Machine learning usually acts indirectly.&#160;In most machine learning scenarios, we care about some performance measure&#160;P, that is defined with respect to the test set and may also be intractable. We&#160;therefore optimize P only indirectly. We reduce a different cost function J(</span><span class="font64" style="font-weight:bold;">0</span><span class="font64">) in&#160;the hope that doing so will improve P. This is in contrast to pure optimization,&#160;where minimizing J is a goal in and of itself. Optimization algorithms for training&#160;deep models also typically include some specialization on the specific structure of&#160;machine learning objective functions.</span></p>
<p><span class="font64">Typically, the cost function can be written as an average over the training set, such as</span></p>
<p><span class="font64"><sup>J (</sup></span><span class="font64" style="font-weight:bold;"><sup>0</sup></span><span class="font64"><sup>)</sup> = <sup>E</sup>(*,yWPdata</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>L(f </sup></span><span class="font64"><sup>(X</sup>i </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>0</sup>)^)<sup>,</sup></span><span class="font64"> &#160;&#160;&#160;I<sup>8</sup>.<sup>1</sup>)</span></p>
<p><span class="font64">where L is the per-example loss function, f (</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">0</span><span class="font64">) is the predicted output when the input is </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">, pd<sub>ata</sub> is the empirical distribution. In the supervised learning case,&#160;y is the target output. Throughout this chapter, we develop the unregularized&#160;supervised case, where the arguments to L are f (</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">0</span><span class="font64">) and y. However, it is trivial&#160;to extend this development, for example, to include </span><span class="font64" style="font-weight:bold;">0 </span><span class="font64">or </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">as arguments, or to&#160;exclude y as arguments, in order to develop various forms of regularization or&#160;unsupervised learning.</span></p>
<p><span class="font64">Eq. 8.1 defines an objective function with respect to the training set. We would usually prefer to minimize the corresponding objective function where the&#160;expectation is taken across </span><span class="font64" style="font-weight:bold;">the data generating distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">pd<sub>ata</sub></span><span class="font64"> rather than&#160;just over the finite training set:</span></p>
<p><span class="font64"><sup>J</sup>*(</span><span class="font64" style="font-weight:bold;"><sup>0</sup></span><span class="font64">) = <sup>E</sup>(x,y)~p<sub>d</sub>ata <sup>L</sup>(f ^ </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>0</sup>)<sup>,</sup>V)<sup>-</sup></span><span class="font64"> &#160;&#160;&#160;(<sup>8</sup>■<sup>2</sup>)</span></p><h5><a id="bookmark2"></a><span class="font64" style="font-weight:bold;">8.1.1 &#160;&#160;&#160;Empirical Risk Minimization</span></h5>
<p><span class="font64">The goal of a machine learning algorithm is to reduce the expected generalization error given by Eq. 8.2. This quantity is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">risk.</span><span class="font64"> We emphasize here that&#160;the expectation is taken over the true underlying distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">pd<sub>ata</sub></span><span class="font64">. If we knew&#160;the true distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">pd<sub>ata</sub></span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">,y), risk minimization would be an optimization task&#160;solvable by an optimization algorithm. However, when we do not knowpd<sub>ata</sub>(x,y)&#160;but only have a training set of samples, we have a machine learning problem.</span></p>
<p><span class="font64">The simplest way to convert a machine learning problem back into an optimization problem is to minimize the expected loss on the training set. This means replacing the true distribution p(x, </span><span class="font64" style="font-weight:bold;font-style:italic;">y)</span><span class="font64"> with the empirical distribution p(x, </span><span class="font64" style="font-weight:bold;font-style:italic;">y)&#160;</span><span class="font64">defined by the training set. We now minimize the </span><span class="font64" style="font-weight:bold;font-style:italic;">empirical risk</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">m</span></p>
<p><span class="font62"><sup>E</sup>x,y~Pdata(*,y)<sup>[L(</sup>/<sup>(x</sup>; &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">= -J2</span><span class="font64"> <sup>L(/ </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(x(i)</sup>;</span><span class="font64"> </span><span class="font18"><sup>0</sup></span><span class="font64"><sup>)</sup>, y<sup>(i))&#160;&#160;&#160;&#160;(</sup></span><span class="font18"><sup>0</sup></span><span class="font64"><sup>)</sup></span></p>
<p><span class="font64"><sup>m</sup> i</span><span class="font18">=1</span></p>
<p><span class="font64">where m is the number of training examples.</span></p>
<p><span class="font64">The training process based on minimizing this average training error is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">empirical risk minimization.</span><span class="font64"> In this setting, machine learning is still very similar&#160;to straightforward optimization. Rather than optimizing the risk directly, we&#160;optimize the empirical risk, and hope that the risk decreases significantly as well.&#160;A variety of theoretical results establish conditions under which the true risk can&#160;be expected to decrease by various amounts.</span></p>
<p><span class="font64">However, empirical risk minimization is prone to overfitting. Models with high capacity can simply memorize the training set. In many cases, empirical&#160;risk minimization is not really feasible. The most effective modern optimization&#160;algorithms are based on gradient descent, but many useful loss functions, such&#160;as </span><span class="font18">0-1</span><span class="font64"> loss, have no useful derivatives (the derivative is either zero or undefined&#160;everywhere). These two problems mean that, in the context of deep learning, we&#160;rarely use empirical risk minimization. Instead, we must use a slightly different&#160;approach, in which the quantity that we actually optimize is even more different&#160;from the quantity that we truly want to optimize.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">8.1.2 Surrogate Loss Functions and Early Stopping</span></h5>
<p><span class="font64">Sometimes, the loss function we actually care about (say classification error) is not one that can be optimized efficiently. For example, exactly minimizing expected 0-1&#160;loss is typically intractable (exponential in the input dimension), even for a linear&#160;classifier (Marcotte and Savard, 1992). In such situations, one typically optimizes&#160;a </span><span class="font64" style="font-weight:bold;font-style:italic;">surrogate loss function</span><span class="font64"> instead, which acts as a proxy but has advantages. For&#160;example, the negative log-likelihood of the correct class is typically used as a&#160;surrogate for the 0-1 loss. The negative log-likelihood allows the model to estimate&#160;the conditional probability of the classes, given the input, and if the model can&#160;do that well, then it can pick the classes that yield the least classification error in&#160;expectation.</span></p>
<p><span class="font64">In some cases, a surrogate loss function actually results in being able to learn more. For example, the test set 0-1 loss often continues to decrease for a long&#160;time after the training set </span><span class="font18">0-1</span><span class="font64"> loss has reached zero, when training using the&#160;log-likelihood surrogate. This is because even when the expected 0-1 loss is zero,&#160;one can improve the robustness of the classifier by further pushing the classes apart&#160;from each other, obtaining a more confident and reliable classifier, thus extracting&#160;more information from the training data than would have been possible by simply&#160;minimizing the average </span><span class="font18">0-1</span><span class="font64"> loss on the training set.</span></p>
<p><span class="font64">A very important difference between optimization in general and optimization as we use it for training algorithms is that training algorithms do not usually halt&#160;at a local minimum. Instead, a machine learning algorithm usually minimizes&#160;a surrogate loss function but halts when a convergence criterion based on early&#160;stopping (Sec. 7.8) is satisfied. Typically the early stopping criterion is based on&#160;the true underlying loss function, such as </span><span class="font18">0-1</span><span class="font64"> loss measured on a validation set,&#160;and is designed to cause the algorithm to halt whenever overfitting begins to occur.&#160;Training often halts while the surrogate loss function still has large derivatives,&#160;which is very different from the pure optimization setting, where an optimization&#160;algorithm is considered to have converged when the gradient becomes very small.</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">8.1.3 Batch and Minibatch Algorithms</span></h5>
<p><span class="font64">One aspect of machine learning algorithms that separates them from general optimization algorithms is that the objective function usually decomposes as a sum&#160;over the training examples. Optimization algorithms for machine learning typically&#160;compute each update to the parameters based on an expected value of the cost&#160;function estimated using only a subset of the terms of the full cost function.</span></p>
<p><span class="font64">For example, maximum likelihood estimation problems, when viewed in log space, decompose into a sum over each example:</span></p><div>
<p><span class="font64">(8.4)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">Oml</span><span class="font64"> = arg max ^ log Pmode!(*<sup>W</sup>, y<sup>(i)</sup>; #)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p>
<p><span class="font64">Maximizing this sum is equivalent to maximizing the expectation over the empirical distribution defined by the training set:</span></p>
<p><span class="font64"><sup>J(</sup>^<sup>) &#160;&#160;&#160;E</sup>x,y~j5d<sub>ata</sub> </span><span class="font18"><sup>10</sup></span><span class="font64"><sup>gp</sup>model<sup>(x</sup>, </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>y</sup>; •</span><span class="font64">&#160;&#160;&#160;&#160;<sup>(8</sup>.<sup>5)</sup></span></p>
<p><span class="font64">Most of the properties of the objective function J used by most of our optimization algorithms are also expectations over the training set. For example, the</span></p>
<p><span class="font64">most commonly used property is the gradient:</span></p>
<p><span class="font64"><sup>V</sup></span><span class="font18">0</span><span class="font64"> <sup>J(</sup></span><span class="font18"><sup>0</sup></span><span class="font64"><sup>)</sup> = <sup>E</sup>x,y~j5d<sub>ata</sub><sup>V</sup># <sup>1ogp</sup>model </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(x,y;</sup></span><span class="font64"><sup> </sup></span><span class="font18"><sup>0</sup></span><span class="font64"><sup>)</sup>• &#160;&#160;&#160;(<sup>86</sup>־)</span></p>
<p><span class="font64">Computing this expectation exactly is very expensive because it requires evaluating the model on every example in the entire dataset. In practice, we can&#160;compute these expectations by randomly sampling a small number of examples&#160;from the dataset, then taking the average over only those examples.</span></p>
<p><span class="font64">Recall that the standard error of the mean (Eq. 5.46) estimated from n samples is given by </span><span class="font64" style="font-weight:bold;font-style:italic;">a^Jn,</span><span class="font64"> where a is the true standard deviation of the value of the samples.&#160;The denominator of </span><span class="font64" style="font-weight:bold;font-style:italic;">\/n</span><span class="font64"> shows that there are less than linear returns to using&#160;more examples to estimate the gradient. Compare two hypothetical estimates of&#160;the gradient, one based on </span><span class="font18">100</span><span class="font64"> examples and another based on </span><span class="font18">10,000</span><span class="font64"> examples.&#160;The latter requires 100 times more computation than the former, but reduces the&#160;standard error of the mean only by a factor of 10. Most optimization algorithms&#160;converge much faster (in terms of total computation, not in terms of number of&#160;updates) if they are allowed to rapidly compute approximate estimates of the&#160;gradient rather than slowly computing the exact gradient.</span></p>
<p><span class="font64">Another consideration motivating statistical estimation of the gradient from a small number of samples is redundancy in the training set. In the worst case, all&#160;m samples in the training set could be identical copies of each other. A sampling-based estimate of the gradient could compute the correct gradient with a single&#160;sample, using m times less computation than the naive approach. In practice, we&#160;are unlikely to truly encounter this worst-case situation, but we may find large&#160;numbers of examples that all make very similar contributions to the gradient.</span></p>
<p><span class="font64">Optimization algorithms that use the entire training set are called </span><span class="font64" style="font-weight:bold;font-style:italic;">batch</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">deterministic</span><span class="font64"> gradient methods, because they process all of the training examples&#160;simultaneously in a large batch. This terminology can be somewhat confusing&#160;because the word “batch” is also often used to describe the minibatch used by&#160;minibatch stochastic gradient descent. Typically the term “batch gradient descent”&#160;implies the use of the full training set, while the use of the term “batch” to describe&#160;a group of examples does not. For example, it is very common to use the term&#160;“batch size” to describe the size of a minibatch.</span></p>
<p><span class="font64">Optimization algorithms that use only a single example at a time are sometimes called </span><span class="font64" style="font-weight:bold;font-style:italic;">stochastic</span><span class="font64"> or sometimes </span><span class="font64" style="font-weight:bold;font-style:italic;">online</span><span class="font64"> methods. The term online is usually reserved&#160;for the case where the examples are drawn from a stream of continually created&#160;examples rather than from a fixed-size training set over which several passes are&#160;made.</span></p>
<p><span class="font64">Most algorithms used for deep learning fall somewhere in between, using more than one but less than all of the training examples. These were traditionally called&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">minibatch</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">minibatch stochastic</span><span class="font64"> methods and it is now common to simply call&#160;them </span><span class="font64" style="font-weight:bold;font-style:italic;">stochastic</span><span class="font64"> methods.</span></p>
<p><span class="font64">The canonical example of a stochastic method is stochastic gradient descent, presented in detail in Sec. 8.3.1.</span></p>
<p><span class="font64">Minibatch sizes are generally driven by the following factors:</span></p>
<p><span class="font64">• &#160;&#160;&#160;Larger batches provide a more accurate estimate of the gradient, but with&#160;less than linear returns.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Multicore architectures are usually underutilized by extremely small batches.&#160;This motivates using some absolute minimum batch size, below which there&#160;is no reduction in the time to process a minibatch.</span></p>
<p><span class="font64">• &#160;&#160;&#160;If all examples in the batch are to be processed in parallel (as is typically&#160;the case), then the amount of memory scales with the batch size. For many&#160;hardware setups this is the limiting factor in batch size.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Some kinds of hardware achieve better runtime with specific sizes of arrays.&#160;Especially when using GPUs, it is common for power of 2 batch sizes to offer&#160;better runtime. Typical power of 2 batch sizes range from 32 to 256, with 16&#160;sometimes being attempted for large models.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Small batches can offer a regularizing effect (Wilson and Martinez, 2003),&#160;perhaps due to the noise they add to the learning process. Generalization&#160;error is often best for a batch size of 1. Training with such a small batch&#160;size might require a small learning rate to maintain stability due to the high&#160;variance in the estimate of the gradient. The total runtime can be very high&#160;due to the need to make more steps, both because of the reduced learning&#160;rate and because it takes more steps to observe the entire training set.</span></p>
<p><span class="font64">Different kinds of algorithms use different kinds of information from the minibatch in different ways. Some algorithms are more sensitive to sampling error than others, either because they use information that is difficult to estimate accurately&#160;with few samples, or because they use information in ways that amplify sampling&#160;errors more. Methods that compute updates based only on the gradient </span><span class="font64" style="font-weight:bold;">g </span><span class="font64">are&#160;usually relatively robust and can handle smaller batch sizes like 100. Second-order&#160;methods, which use also the Hessian matrix </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">and compute updates such as&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">H<sup>-1</sup>g</span><span class="font64">, typically require much larger batch sizes like 10,000. These large batch&#160;sizes are required to minimize fluctuations in the estimates of </span><span class="font64" style="font-weight:bold;">H<sup>-1</sup>g</span><span class="font64">. Suppose&#160;that </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">is estimated perfectly but has a poor condition number. Multiplication by</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">H</span><span class="font64"> or its inverse amplifies pre-existing errors, in this case, estimation errors in </span><span class="font64" style="font-weight:bold;">g</span><span class="font64">. Very small changes in the estimate of </span><span class="font64" style="font-weight:bold;">g </span><span class="font64">can thus cause large changes in the update&#160;</span><span class="font64" style="font-weight:bold;">H<sup>-1</sup>g</span><span class="font64">, even if </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">were estimated perfectly. Of course, </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">will be estimated only&#160;approximately, so the update </span><span class="font64" style="font-weight:bold;">H<sup>-1</sup>g </span><span class="font64">will contain even more error than we would&#160;predict from applying a poorly conditioned operation to the estimate of </span><span class="font64" style="font-weight:bold;">g</span><span class="font64">.</span></p>
<p><span class="font64">It is also crucial that the minibatches be selected randomly. Computing an unbiased estimate of the expected gradient from a set of samples requires that those&#160;samples be independent. We also wish for two subsequent gradient estimates to be&#160;independent from each other, so two subsequent minibatches of examples should&#160;also be independent from each other. Many datasets are most naturally arranged&#160;in a way where successive examples are highly correlated. For example, we might&#160;have a dataset of medical data with a long list of blood sample test results. This&#160;list might be arranged so that first we have five blood samples taken at different&#160;times from the first patient, then we have three blood samples taken from the&#160;second patient, then the blood samples from the third patient, and so on. If we&#160;were to draw examples in order from this list, then each of our minibatches would&#160;be extremely biased, because it would represent primarily one patient out of the&#160;many patients in the dataset. In cases such as these where the order of the dataset&#160;holds some significance, it is necessary to shuffle the examples before selecting&#160;minibatches. For very large datasets, for example datasets containing billions of&#160;examples in a data center, it can be impractical to sample examples truly uniformly&#160;at random every time we want to construct a minibatch. Fortunately, in practice&#160;it is usually sufficient to shuffle the order of the dataset once and then store it in&#160;shuffled fashion. This will impose a fixed set of possible minibatches of consecutive&#160;examples that all models trained thereafter will use, and each individual model&#160;will be forced to reuse this ordering every time it passes through the training&#160;data. However, this deviation from true random selection does not seem to have a&#160;significant detrimental effect. Failing to ever shuffle the examples in any way can&#160;seriously reduce the effectiveness of the algorithm.</span></p>
<p><span class="font64">Many optimization problems in machine learning decompose over examples well enough that we can compute entire separate updates over different examples&#160;in parallel. In other words, we can compute the update that minimizes J(</span><span class="font64" style="font-weight:bold;">X</span><span class="font64">) for&#160;one minibatch of examples </span><span class="font64" style="font-weight:bold;">X </span><span class="font64">at the same time that we compute the update for&#160;several other minibatches. Such asynchronous parallel distributed approaches are&#160;discussed further in Sec. 12.1.3.</span></p>
<p><span class="font64">An interesting motivation for minibatch stochastic gradient descent is that it follows the gradient of the true </span><span class="font64" style="font-weight:bold;">generalization error </span><span class="font64">(Eq. 8.2) so long as no&#160;examples are repeated. Most implementations of minibatch stochastic gradient&#160;descent shuffle the dataset once and then pass through it multiple times. On the&#160;first pass, each minibatch is used to compute an unbiased estimate of the true&#160;generalization error. On the second pass, the estimate becomes biased because it is&#160;formed by re-sampling values that have already been used, rather than obtaining&#160;new fair samples from the data generating distribution.</span></p>
<p><span class="font64">The fact that stochastic gradient descent minimizes generalization error is easiest to see in the online learning case, where examples or minibatches are drawn&#160;from a </span><span class="font64" style="font-weight:bold;font-style:italic;">stream</span><span class="font64"> of data. In other words, instead of receiving a fixed-size training&#160;set, the learner is similar to a living being who sees a new example at each instant,&#160;with every example (x, y) coming from the data generating distributionpd<sub>ata</sub>(x, </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64">).&#160;In this scenario, examples are never repeated; every experience is a fair sample&#160;from pdata.</span></p>
<p><span class="font64">The equivalence is easiest to derive when both </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> and y are discrete. In this case, the generalization error (Eq. 8.2) can be written as a sum</span></p>
<p><span class="font64"><sup>J</sup>*<sup>(Q)</sup> = ^ ^Pdata<sup>(</sup>X y<sup>)L(f (x</sup>; </span><span class="font64" style="font-weight:bold;font-style:italic;">Q)<sup>,</sup>y)<sup>,</sup></span><span class="font64"><sup> &#160;&#160;&#160;(8</sup>.<sup>7)</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x y</span></p>
<p><span class="font64">with the exact gradient</span></p>
<p><span class="font64">g = <sup>V</sup></span><span class="font18">0</span><span class="font64"> J*(Q) = &#160;&#160;&#160;Pdata</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(x,</sup>y)<sup>V</sup>eL(f (x;</span><span class="font64"> Q), y).&#160;&#160;&#160;&#160;(</span><span class="font18">8</span><span class="font64">.</span><span class="font18">8</span><span class="font64">)</span></p>
<p><span class="font64">xy</span></p>
<p><span class="font64">We have already seen the same fact demonstrated for the log-likelihood in Eq. 8.5 and Eq. </span><span class="font18">8</span><span class="font64">.</span><span class="font18">6</span><span class="font64">; we observe now that this holds for other functions L besides the&#160;likelihood. A similar result can be derived when x and </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"> are continuous, under&#160;mild assumptions regarding pdata and L.</span></p>
<p><span class="font64">Hence, we can obtain an unbiased estimator of the exact gradient of the generalization error by sampling a minibatch of examples {x<sup>(1)</sup>,... x<sup>(m)</sup>} with corresponding targets y<sup>(i)</sup> from the data generating distribution pd<sub>ata</sub>, and computing&#160;the gradient of the loss with respect to the parameters for that minibatch:</span></p>
<p><span class="font18">1</span></p>
<p><span class="font64">g = </span><span class="font64" style="font-weight:bold;font-style:italic;">-VeY, L(f</span><span class="font64">(x«;Q),y«). &#160;&#160;&#160;(8.9)</span></p>
<p><span class="font64">m</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">%</span></p>
<p><span class="font64">Updating Q in the direction of g performs SGD on the generalization error.</span></p>
<p><span class="font64">Of course, this interpretation only applies when examples are not reused. Nonetheless, it is usually best to make several passes through the training set,&#160;unless the training set is extremely large. When multiple such epochs are used,&#160;only the first epoch follows the unbiased gradient of the generalization error, but&#160;of course, the additional epochs usually provide enough benefit due to decreased&#160;training error to offset the harm they cause by increasing the gap between training&#160;error and test error.</span></p>
<p><span class="font64">With some datasets growing rapidly in size, faster than computing power, it is becoming more common for machine learning applications to use each training&#160;example only once or even to make an incomplete pass through the training&#160;set. When using an extremely large training set, overfitting is not an issue, so&#160;underfitting and computational efficiency become the predominant concerns. See&#160;also Bottou and Bousquet (2008) for a discussion of the effect of computational&#160;bottlenecks on generalization error, as the number of training examples grows.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">8.2 Challenges in Neural Network Optimization</span></h4>
<p><span class="font64">Optimization in general is an extremely difficult task. Traditionally, machine learning has avoided the difficulty of general optimization by carefully designing&#160;the objective function and constraints to ensure that the optimization problem is&#160;convex. When training neural networks, we must confront the general non-convex&#160;case. Even convex optimization is not without its complications. In this section,&#160;we summarize several of the most prominent challenges involved in optimization&#160;for training deep models.</span></p><h5><a id="bookmark6"></a><span class="font64" style="font-weight:bold;">8.2.1 Ill-Conditioning</span></h5>
<p><span class="font64">Some challenges arise even when optimizing convex functions. Of these, the most prominent is ill-conditioning of the Hessian matrix </span><span class="font64" style="font-weight:bold;">H</span><span class="font64">. This is a very general&#160;problem in most numerical optimization, convex or otherwise, and is described in&#160;more detail in Sec. 4.3.1.</span></p>
<p><span class="font64">The ill-conditioning problem is generally believed to be present in neural network training problems. Ill-conditioning can manifest by causing SGD to get&#160;“stuck” in the sense that even very small steps increase the cost function.</span></p>
<p><span class="font64">Recall from Eq. 4.9 that a second-order Taylor series expansion of the cost function predicts that a gradient descent step of —e</span><span class="font64" style="font-weight:bold;">g </span><span class="font64">will add</span></p>
<p><span class="font64">2</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">e</span><span class="font18"><sup>2</sup></span><span class="font64" style="font-weight:bold;">g<sup>T</sup> Hg </span><span class="font64">— e</span><span class="font64" style="font-weight:bold;">g g &#160;&#160;&#160;</span><span class="font64">(8.10)</span></p>
<p><span class="font64">to the cost. Ill-conditioning of the gradient becomes a problem when 1־ e</span><span class="font18"><sup>2</sup></span><span class="font64" style="font-weight:bold;">g<sup>T</sup>Hg </span><span class="font64">exceeds e</span><span class="font64" style="font-weight:bold;">g<sup>T</sup>g</span><span class="font64">. To determine whether ill-conditioning is detrimental to a neural&#160;network training task, one can monitor the squared gradient norm </span><span class="font64" style="font-weight:bold;">g <sup>T</sup>g </span><span class="font64">and the</span></p><div><div>
<p><span class="font64">a</span></p>
<p><span class="font63">o</span></p>
<p><span class="font23">a</span></p><img src="main-79.png" alt=""/>
<p dir="rtl"><span class="font0">כד</span></p>
<p><span class="font10">c6</span></p>
<p><span class="font6" style="font-weight:bold;font-style:italic;">u</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">O</span></p></div></div><div><div><img src="main-80.jpg" alt=""/>
<p><span class="font63">Training time (epochs)</span></p></div></div><div><div><img src="main-81.jpg" alt=""/>
<p><span class="font63">0 &#160;&#160;&#160;50&#160;&#160;&#160;&#160;100&#160;&#160;&#160;&#160;150&#160;&#160;&#160;&#160;200&#160;&#160;&#160;&#160;250</span></p>
<p><span class="font63">Training time (epochs)</span></p></div></div>
<p><span class="font64">Figure 8.1: Gradient descent often does not arrive at a critical point of any kind. In this example, the gradient norm increases throughout training of a convolutional network&#160;used for object detection. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> A scatterplot showing how the norms of individual&#160;gradient evaluations are distributed over time. To improve legibility, only one gradient&#160;norm is plotted per epoch. The running average of all gradient norms is plotted as a solid&#160;curve. The gradient norm clearly increases over time, rather than decreasing as we would&#160;expect if the training process converged to a critical point. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> Despite the increasing&#160;gradient, the training process is reasonably successful. The validation set classification&#160;error decreases to a low level.</span></p>
<p><span class="font64" style="font-weight:bold;">g<sup>T</sup>Hg </span><span class="font64">term. In many cases, the gradient norm does not shrink significantly throughout learning, but the </span><span class="font64" style="font-weight:bold;">g<sup>T</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">Hg</span><span class="font64"> term grows by more than order of magnitude.&#160;The result is that learning becomes very slow despite the presence of a strong&#160;gradient because the learning rate must be shrunk to compensate for even stronger&#160;curvature. Fig. 8.1 shows an example of the gradient increasing significantly during&#160;the successful training of a neural network.</span></p>
<p><span class="font64">Though ill-conditioning is present in other settings besides neural network training, some of the techniques used to combat it in other contexts are less&#160;applicable to neural networks. For example, Newton’s method is an excellent tool&#160;for minimizing convex functions with poorly conditioned Hessian matrices, but in&#160;the subsequent sections we will argue that Newton’s method requires significant&#160;modification before it can be applied to neural networks.</span></p><h5><a id="bookmark7"></a><span class="font64" style="font-weight:bold;">8.2.2 Local Minima</span></h5>
<p><span class="font64">One of the most prominent features of a convex optimization problem is that it can be reduced to the problem of finding a local minimum. Any local minimum is&#160;guaranteed to be a global minimum. Some convex functions have a flat region at&#160;the bottom rather than a single global minimum point, but any point within such&#160;a flat region is an acceptable solution. When optimizing a convex function, we&#160;know that we have reached a good solution if we find a critical point of any kind.</span></p>
<p><span class="font64">With non-convex functions, such as neural nets, it is possible to have many local minima. Indeed, nearly any deep model is essentially guaranteed to have&#160;an extremely large number of local minima. However, as we will see, this is not&#160;necessarily a major problem.</span></p>
<p><span class="font64">Neural networks and any models with multiple equivalently parametrized latent variables all have multiple local minima because of the </span><span class="font64" style="font-weight:bold;font-style:italic;">model identifiability</span><span class="font64"> problem.&#160;A model is said to be identifiable if a sufficiently large training set can rule out all&#160;but one setting of the model’s parameters. Models with latent variables are often&#160;not identifiable because we can obtain equivalent models by exchanging latent&#160;variables with each other. For example, we could take a neural network and modify&#160;layer </span><span class="font18">1</span><span class="font64"> by swapping the incoming weight vector for unit i with the incoming weight&#160;vector for unit j, then doing the same for the outgoing weight vectors. If we have&#160;m layers with n units each, then there are </span><span class="font64" style="font-weight:bold;font-style:italic;">n!<sup>m</sup></span><span class="font64"> ways of arranging the hidden units.&#160;This kind of non-identifiability is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">weight space symmetry.</span></p>
<p><span class="font64">In addition to weight space symmetry, many kinds of neural networks have additional causes of non-identifiability. For example, in any rectified linear or&#160;maxout network, we can scale all of the incoming weights and biases of a unit by&#160;a if we also scale all of its outgoing weights by ־a. This means that—if the cost&#160;function does not include terms such as weight decay that depend directly on the&#160;weights rather than the models’ outputs—every local minimum of a rectified linear&#160;or maxout network lies on an (m x n)-dimensional hyperbola of equivalent local&#160;minima.</span></p>
<p><span class="font64">These model identifiability issues mean that there can be an extremely large or even uncountably infinite amount of local minima in a neural network cost&#160;function. However, all of these local minima arising from non-identifiability are&#160;equivalent to each other in cost function value. As a result, these local minima are&#160;not a problematic form of non-convexity.</span></p>
<p><span class="font64">Local minima can be problematic if they have high cost in comparison to the global minimum. One can construct small neural networks, even without hidden&#160;units, that have local minima with higher cost than the global minimum (Sontag&#160;and Sussman, 1989; Brady </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1989; Gori and Tesi, 1992). If local minima&#160;with high cost are common, this could pose a serious problem for gradient-based&#160;optimization algorithms.</span></p>
<p><span class="font64">It remains an open question whether there are many local minima of high cost for networks of practical interest and whether optimization algorithms encounter&#160;them. For many years, most practitioners believed that local minima were a&#160;common problem plaguing neural network optimization. Today, that does not&#160;appear to be the case. The problem remains an active area of research, but experts&#160;now suspect that, for sufficiently large neural networks, most local minima have a&#160;low cost function value, and that it is not important to find a true global minimum&#160;rather than to find a point in parameter space that has low but not minimal cost&#160;(Saxe </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013; Dauphin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014; Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015; Choromanska&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014).</span></p>
<p><span class="font64">Many practitioners attribute nearly all difficulty with neural network optimization to local minima. We encourage practitioners to carefully test for specific problems. A test that can rule out local minima as the problem is to plot the&#160;norm of the gradient over time. If the norm of the gradient does not shrink to&#160;insignificant size, the problem is neither local minima nor any other kind of critical&#160;point. This kind of negative test can rule out local minima. In high dimensional&#160;spaces, it can be very difficult to positively establish that local minima are the&#160;problem. Many structures other than local minima also have small gradients.</span></p><h5><a id="bookmark8"></a><span class="font64" style="font-weight:bold;">8.2.3 Plateaus, Saddle Points and Other Flat Regions</span></h5>
<p><span class="font64">For many high-dimensional non-convex functions, local minima (and maxima) are in fact rare compared to another kind of point with zero gradient: a saddle&#160;point. Some points around a saddle point have greater cost than the saddle point,&#160;while others have a lower cost. At a saddle point, the Hessian matrix has both&#160;positive and negative eigenvalues. Points lying along eigenvectors associated with&#160;positive eigenvalues have greater cost than the saddle point, while points lying&#160;along negative eigenvalues have lower value. We can think of a saddle point as&#160;being a local minimum along one cross-section of the cost function and a local&#160;maximum along another cross-section. See Fig. 4.5 for an illustration.</span></p>
<p><span class="font64">Many classes of random functions exhibit the following behavior: in lowdimensional spaces, local minima are common. In higher dimensional spaces, local minima are rare and saddle points are more common. For a function f : R<sup>n</sup> ^ R of&#160;this type, the expected ratio of the number of saddle points to local minima grows&#160;exponentially with n. To understand the intuition behind this behavior, observe&#160;that the Hessian matrix at a local minimum has only positive eigenvalues. The&#160;Hessian matrix at a saddle point has a mixture of positive and negative eigenvalues.&#160;Imagine that the sign of each eigenvalue is generated by flipping a coin. In a single&#160;dimension, it is easy to obtain a local minimum by tossing a coin and getting heads&#160;once. In n-dimensional space, it is exponentially unlikely that all n coin tosses will</span></p>
<p><span class="font64">be heads. See Dauphin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) for a review of the relevant theoretical work.</span></p>
<p><span class="font64">An amazing property of many random functions is that the eigenvalues of the Hessian become more likely to be positive as we reach regions of lower cost. In&#160;our coin tossing analogy, this means we are more likely to have our coin come up&#160;heads n times if we are at a critical point with low cost. This means that local&#160;minima are much more likely to have low cost than high cost. Critical points with&#160;high cost are far more likely to be saddle points. Critical points with extremely&#160;high cost are more likely to be local maxima.</span></p>
<p><span class="font64">This happens for many classes of random functions. Does it happen for neural networks? Baldi and Hornik (1989) showed theoretically that shallow autoencoders&#160;(feedforward networks trained to copy their input to their output, described in&#160;Chapter 14) with no nonlinearities have global minima and saddle points but no&#160;local minima with higher cost than the global minimum. They observed without&#160;proof that these results extend to deeper networks without nonlinearities. The&#160;output of such networks is a linear function of their input, but they are useful&#160;to study as a model of nonlinear neural networks because their loss function is&#160;a non-convex function of their parameters. Such networks are essentially just&#160;multiple matrices composed together. Saxe </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013) provided exact solutions&#160;to the complete learning dynamics in such networks and showed that learning in&#160;these models captures many of the qualitative features observed in the training of&#160;deep models with nonlinear activation functions. Dauphin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) showed&#160;experimentally that real neural networks also have loss functions that contain very&#160;many high-cost saddle points. Choromanska </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) provided additional&#160;theoretical arguments, showing that another class of high-dimensional random&#160;functions related to neural networks does so as well.</span></p>
<p><span class="font64">What are the implications of the proliferation of saddle points for training algorithms? For first-order optimization algorithms that use only gradient information, the situation is unclear. The gradient can often become very small near a saddle&#160;point. On the other hand, gradient descent empirically seems to be able to escape&#160;saddle points in many cases. Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) provided visualizations of&#160;several learning trajectories of state-of-the-art neural networks, with an example&#160;given in Fig. 8.2. These visualizations show a flattening of the cost function near&#160;a prominent saddle point where the weights are all zero, but they also show the&#160;gradient descent trajectory rapidly escaping this region. Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015)&#160;also argue that continuous-time gradient descent may be shown analytically to be&#160;repelled from, rather than attracted to, a nearby saddle point, but the situation&#160;may be different for more realistic uses of gradient descent.</span></p>
<p><span class="font64">For Newton’s method, it is clear that saddle points constitute a problem.</span></p><div><img src="main-82.jpg" alt=""/>
<p><span class="font64">Figure 8.2: A visualization of the cost function of a neural network. Image adapted with permission from Goodfellow </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2015). These visualizations appear similar for&#160;feedforward neural networks, convolutional networks, and recurrent networks applied&#160;to real object recognition and natural language processing tasks. Surprisingly, these&#160;visualizations usually do not show many conspicuous obstacles. Prior to the success of&#160;stochastic gradient descent for training very large models beginning in roughly 2012,&#160;neural net cost function surfaces were generally believed to have much more non-convex&#160;structure than is revealed by these projections. The primary obstacle revealed by this&#160;projection is a saddle point of high cost near where the parameters are initialized, but, as&#160;indicated by the blue path, the SGD training trajectory escapes this saddle point readily.&#160;Most of training time is spent traversing the relatively flat valley of the cost function,&#160;which may be due to high noise in the gradient, poor conditioning of the Hessian matrix&#160;in this region, or simply the need to circumnavigate the tall “mountain” visible in the&#160;figure via an indirect arcing path.</span></p></div>
<p><span class="font64">Gradient descent is designed to move “downhill” and is not explicitly designed to seek a critical point. Newton’s method, however, is designed to solve for a&#160;point where the gradient is zero. Without appropriate modification, it can jump&#160;to a saddle point. The proliferation of saddle points in high dimensional spaces&#160;presumably explains why second-order methods have not succeeded in replacing&#160;gradient descent for neural network training. Dauphin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) introduced&#160;a </span><span class="font64" style="font-weight:bold;font-style:italic;">saddle-free Newton method</span><span class="font64"> for second-order optimization and showed that it&#160;improves significantly over the traditional version. Second-order methods remain&#160;difficult to scale to large neural networks, but this saddle-free approach holds&#160;promise if it could be scaled.</span></p>
<p><span class="font64">There are other kinds of points with zero gradient besides minima and saddle points. There are also maxima, which are much like saddle points from the&#160;perspective of optimization—many algorithms are not attracted to them, but&#160;unmodified Newton’s method is. Maxima become exponentially rare in high&#160;dimensional space, just like minima do.</span></p>
<p><span class="font64">There may also be wide, flat regions of constant value. In these locations, the gradient and also the Hessian are all zero. Such degenerate locations pose major&#160;problems for all numerical optimization algorithms. In a convex problem, a wide,&#160;flat region must consist entirely of global minima, but in a general optimization&#160;problem, such a region could correspond to a high value of the objective function.</span></p><h5><a id="bookmark9"></a><span class="font64" style="font-weight:bold;">8.2.4 Cliffs and Exploding Gradients</span></h5>
<p><span class="font64">Neural networks with many layers often have extremely steep regions resembling cliffs, as illustrated in Fig. 8.3. These result from the multiplication of several large&#160;weights together. On the face of an extremely steep cliff structure, the gradient&#160;update step can move the parameters extremely far, usually jumping off of the&#160;cliff structure altogether.</span></p><div><img src="main-83.jpg" alt=""/>
<p><span class="font64" style="font-style:italic;">b</span></p>
<p><span class="font64">Figure 8.3: The objective function for highly nonlinear deep neural networks or for recurrent neural networks often contains sharp nonlinearities in parameter space resulting&#160;from the multiplication of several parameters. These nonlinearities give rise to very&#160;high derivatives in some places. When the parameters get close to such a cliff region, a&#160;gradient descent update can catapult the parameters very far, possibly losing most of the&#160;optimization work that had been done. Figure adapted with permission from Pascanu&#160;</span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2013a).</span></p></div>
<p><span class="font64">The cliff can be dangerous whether we approach it from above or from below, but fortunately its most serious consequences can be avoided using the </span><span class="font64" style="font-weight:bold;font-style:italic;">gradient&#160;clipping</span><span class="font64"> heuristic described in Sec. 10.11.1. The basic idea is to recall that the&#160;gradient does not specify the optimal step size, but only the optimal direction&#160;within an infinitesimal region. When the traditional gradient descent algorithm&#160;proposes to make a very large step, the gradient clipping heuristic intervenes to&#160;reduce the step size to be small enough that it is less likely to go outside the region&#160;where the gradient indicates the direction of approximately steepest descent. Cliff&#160;structures are most common in the cost functions for recurrent neural networks,&#160;because such models involve a multiplication of many factors, with one factor&#160;for each time step. Long temporal sequences thus incur an extreme amount of&#160;multiplication.</span></p><h5><a id="bookmark10"></a><span class="font64" style="font-weight:bold;">8.2.5 Long-Term Dependencies</span></h5>
<p><span class="font64">Another difficulty that neural network optimization algorithms must overcome arises when the computational graph becomes extremely deep. Feedforward networks&#160;with many layers have such deep computational graphs. So do recurrent networks,&#160;described in Chapter 10, which construct very deep computational graphs by&#160;repeatedly applying the same operation at each time step of a long temporal&#160;sequence. Repeated application of the same parameters gives rise to especially&#160;pronounced difficulties.</span></p>
<p><span class="font64">For example, suppose that a computational graph contains a path that consists of repeatedly multiplying by a matrix </span><span class="font64" style="font-weight:bold;">W</span><span class="font64">. After t steps, this is equivalent to multiplying by </span><span class="font64" style="font-weight:bold;font-style:italic;">W</span><span class="font64"><sup>l</sup>. Suppose that </span><span class="font64" style="font-weight:bold;">W </span><span class="font64">has an eigendecomposition </span><span class="font64" style="font-weight:bold;">W </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">V</span><span class="font64">diag( </span><span class="font64" style="font-weight:bold;">A</span><span class="font64">)</span><span class="font64" style="font-weight:bold;">V</span><span class="font18"><sup>1</sup></span><span class="font64">־.&#160;In this simple case, it is straightforward to see that</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">W<sup>t</sup></span><span class="font64"> = (</span><span class="font64" style="font-weight:bold;">V </span><span class="font64">diag( </span><span class="font64" style="font-weight:bold;">A</span><span class="font64">) </span><span class="font64" style="font-weight:bold;">V </span><span class="font18"><sup>1</sup></span><span class="font64">־</span><span class="font64" style="font-weight:bold;">^ </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">V</span><span class="font64">diag(</span><span class="font64" style="font-weight:bold;">A</span><span class="font64">) V <sup>-</sup></span><span class="font18"><sup>1</sup></span><span class="font64">. &#160;&#160;&#160;(8.11)</span></p>
<p><span class="font64">Any eigenvalues A<sub>i</sub> that are not near an absolute value of 1 will either explode if they are greater than </span><span class="font18">1</span><span class="font64"> in magnitude or vanish if they are less than </span><span class="font18">1</span><span class="font64"> in magnitude.&#160;The </span><span class="font64" style="font-weight:bold;font-style:italic;">vanishing and exploding gradient problem</span><span class="font64"> refers to the fact that gradients&#160;through such a graph are also scaled according to diag( </span><span class="font64" style="font-weight:bold;">A</span><span class="font64">) h Vanishing gradients&#160;make it difficult to know which direction the parameters should move to improve&#160;the cost function, while exploding gradients can make learning unstable. The cliff&#160;structures described earlier that motivate gradient clipping are an example of the&#160;exploding gradient phenomenon.</span></p>
<p><span class="font64">The repeated multiplication by </span><span class="font64" style="font-weight:bold;">W </span><span class="font64">at each time step described here is very similar to the </span><span class="font64" style="font-weight:bold;font-style:italic;">power method</span><span class="font64"> algorithm used to find the largest eigenvalue of a matrix&#160;</span><span class="font64" style="font-weight:bold;">W </span><span class="font64">and the corresponding eigenvector. From this point of view it is not surprising&#160;that </span><span class="font64" style="font-weight:bold;">x </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>T</sup>W<sup>t</sup></span><span class="font64"> will eventually discard all components of </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">that are orthogonal to the&#160;principal eigenvector of </span><span class="font64" style="font-weight:bold;">W</span><span class="font64">.</span></p>
<p><span class="font64">Recurrent networks use the same matrix </span><span class="font64" style="font-weight:bold;">W </span><span class="font64">at each time step, but feedforward networks do not, so even very deep feedforward networks can largely avoid the&#160;vanishing and exploding gradient problem (Sussillo, 2014).</span></p>
<p><span class="font64">We defer a further discussion of the challenges of training recurrent networks until Sec. 10.7, after recurrent networks have been described in more detail.</span></p><h5><a id="bookmark11"></a><span class="font64" style="font-weight:bold;">8.2.6 Inexact Gradients</span></h5>
<p><span class="font64">Most optimization algorithms are primarily motivated by the case where we have exact knowledge of the gradient or Hessian matrix. In practice, we usually only&#160;have a noisy or even biased estimate of these quantities. Nearly every deep learning&#160;algorithm relies on sampling-based estimates at least insofar as using a minibatch&#160;of training examples to compute the gradient.</span></p>
<p><span class="font64">In other cases, the objective function we want to minimize is actually intractable. When the objective function is intractable, typically its gradient is intractable as&#160;well. In such cases we can only approximate the gradient. These issues mostly arise&#160;with the more advanced models in Part III. For example, contrastive divergence&#160;gives a technique for approximating the gradient of the intractable log-likelihood&#160;of a Boltzmann machine.</span></p>
<p><span class="font64">Various neural network optimization algorithms are designed to account for imperfections in the gradient estimate. One can also avoid the problem by choosing&#160;a surrogate loss function that is easier to approximate than the true loss.</span></p><h5><a id="bookmark12"></a><span class="font64" style="font-weight:bold;">8.2.7 Poor Correspondence between Local and Global Structure</span></h5>
<p><span class="font64">Many of the problems we have discussed so far correspond to properties of the loss function at a single point—it can be difficult to make a single step if J(</span><span class="font64" style="font-weight:bold;">0</span><span class="font64">) is&#160;poorly conditioned at the current point </span><span class="font64" style="font-weight:bold;">0</span><span class="font64">, or if </span><span class="font64" style="font-weight:bold;">0 </span><span class="font64">lies on a cliff, or if </span><span class="font64" style="font-weight:bold;">0 </span><span class="font64">is a saddle&#160;point hiding the opportunity to make progress downhill from the gradient.</span></p>
<p><span class="font64">It is possible to overcome all of these problems at a single point and still perform poorly if the direction that results in the most improvement locally does&#160;not point toward distant regions of much lower cost.</span></p>
<p><span class="font64">Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) argue that much of the runtime of training is due to the length of the trajectory needed to arrive at the solution. Fig. 8.2 shows that&#160;the learning trajectory spends most of its time tracing out a wide arc around a&#160;mountain-shaped structure.</span></p>
<p><span class="font64">Much of research into the difficulties of optimization has focused on whether training arrives at a global minimum, a local minimum, or a saddle point, but in&#160;practice neural networks do not arrive at a critical point of any kind. Fig. 8.1&#160;shows that neural networks often do not arrive at a region of small gradient. Indeed,&#160;such critical points do not even necessarily exist. For example, the loss function&#160;— log p( </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;font-style:italic;">x;</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">0</span><span class="font64">) can lack a global minimum point and instead asymptotically&#160;approach some value as the model becomes more confident. For a classifier with&#160;discrete y and p (y | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) provided by a softmax, the negative log-likelihood can&#160;become arbitrarily close to zero if the model is able to correctly classify every&#160;example in the training set, but it is impossible to actually reach the value of&#160;zero. Likewise, a model of real values p(y | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) = N(y;f (</span><span class="font64" style="font-weight:bold;">0</span><span class="font64">),3<sup>-</sup></span><span class="font18"><sup>1</sup></span><span class="font64">) can have negative&#160;log-likelihood that asymptotes to negative infinity—if f (</span><span class="font64" style="font-weight:bold;">0</span><span class="font64">) is able to correctly&#160;predict the value of all training set y targets, the learning algorithm will increase&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">3</span><span class="font64"> without bound. See Fig. 8.4 for an example of a failure of local optimization to&#160;find a good cost function value even in the absence of any local minima or saddle&#160;points.</span></p>
<p><span class="font64">Future research will need to develop further understanding of the factors that influence the length of the learning trajectory and better characterize the outcome</span></p><div><img src="main-84.jpg" alt=""/>
<p><span class="font64">Figure 8.4: Optimization based on local downhill moves can fail if the local surface does not point toward the global solution. Here we provide an example of how this can occur,&#160;even if there are no saddle points and no local minima. This example cost function&#160;contains only asymptotes toward low values, not minima. The main cause of difficulty in&#160;this case is being initialized on the wrong side of the “mountain” and not being able to&#160;traverse it. In higher dimensional space, learning algorithms can often circumnavigate&#160;such mountains but the trajectory associated with doing so may be long and result in&#160;excessive training time, as illustrated in Fig. 8.2.</span></p></div>
<p><span class="font64">of the process.</span></p>
<p><span class="font64">Many existing research directions are aimed at finding good initial points for problems that have difficult global structure, rather than developing algorithms&#160;that use non-local moves.</span></p>
<p><span class="font64">Gradient descent and essentially all learning algorithms that are effective for training neural networks are based on making small, local moves. The previous&#160;sections have primarily focused on how the correct direction of these local moves&#160;can be difficult to compute. We may be able to compute some properties of the&#160;objective function, such as its gradient, only approximately, with bias or variance&#160;in our estimate of the correct direction. In these cases, local descent may or may&#160;not define a reasonably short path to a valid solution, but we are not actually&#160;able to follow the local descent path. The objective function may have issues&#160;such as poor conditioning or discontinuous gradients, causing the region where&#160;the gradient provides a good model of the objective function to be very small. In&#160;these cases, local descent with steps of size e may define a reasonably short path&#160;to the solution, but we are only able to compute the local descent direction with&#160;steps of size 5 ^ e. In these cases, local descent may or may not define a path&#160;to the solution, but the path contains many steps, so following the path incurs a&#160;high computational cost. Sometimes local information provides us no guide, when&#160;the function has a wide flat region, or if we manage to land exactly on a critical&#160;point (usually this latter scenario only happens to methods that solve explicitly&#160;for critical points, such as Newton’s method). In these cases, local descent does&#160;not define a path to a solution at all. In other cases, local moves can be too greedy&#160;and lead us along a path that moves downhill but away from any solution, as in&#160;Fig. 8.4, or along an unnecessarily long trajectory to the solution, as in Fig. 8.2.&#160;Currently, we do not understand which of these problems are most relevant to&#160;making neural network optimization difficult, and this is an active area of research.</span></p>
<p><span class="font64">Regardless of which of these problems are most significant, all of them might be avoided if there exists a region of space connected reasonably directly to a solution&#160;by a path that local descent can follow, and if we are able to initialize learning&#160;within that well-behaved region. This last view suggests research into choosing&#160;good initial points for traditional optimization algorithms to use.</span></p><h5><a id="bookmark13"></a><span class="font64" style="font-weight:bold;">8.2.8 Theoretical Limits of Optimization</span></h5>
<p><span class="font64">Several theoretical results show that there are limits on the performance of any optimization algorithm we might design for neural networks (Blum and Rivest,&#160;1992; Judd, 1989; Wolpert and MacReady, 1997). Typically these results have&#160;little bearing on the use of neural networks in practice.</span></p>
<p><span class="font64">Some theoretical results apply only to the case where the units of a neural network output discrete values. However, most neural network units output&#160;smoothly increasing values that make optimization via local search feasible. Some&#160;theoretical results show that there exist problem classes that are intractable, but&#160;it can be difficult to tell whether a particular problem falls into that class. Other&#160;results show that finding a solution for a network of a given size is intractable, but&#160;in practice we can find a solution easily by using a larger network for which many&#160;more parameter settings correspond to an acceptable solution. Moreover, in the&#160;context of neural network training, we usually do not care about finding the exact&#160;minimum of a function, but only in reducing its value sufficiently to obtain good&#160;generalization error. Theoretical analysis of whether an optimization algorithm&#160;can accomplish this goal is extremely difficult. Developing more realistic bounds&#160;on the performance of optimization algorithms therefore remains an important&#160;goal for machine learning research.</span></p><h4><a id="bookmark14"></a><span class="font65" style="font-weight:bold;">8.3 Basic Algorithms</span></h4>
<p><span class="font64">We have previously introduced the gradient descent (Sec. 4.3) algorithm that follows the gradient of an entire training set downhill. This may be accelerated&#160;considerably by using stochastic gradient descent to follow the gradient of randomly&#160;selected minibatches downhill, as discussed in Sec. 5.9 and Sec. 8.1.3.</span></p><h5><a id="bookmark15"></a><span class="font64" style="font-weight:bold;">8.3.1 Stochastic Gradient Descent</span></h5>
<p><span class="font64">Stochastic gradient descent (SGD) and its variants are probably the most used optimization algorithms for machine learning in general and for deep learning in&#160;particular. As discussed in Sec. 8.1.3, it is possible to obtain an unbiased estimate&#160;of the gradient by taking the average gradient on a minibatch of m examples drawn</span></p>
<p><span class="font64">i.i.d from the data generating distribution.</span></p>
<p><span class="font64">Algorithm 8.1 shows how to follow this estimate of the gradient downhill.</span></p>
<p><span class="font64" style="font-weight:bold;">Algorithm 8.1 </span><span class="font64">Stochastic gradient descent (SGD) update at training iteration k</span></p>
<p><span class="font64" style="font-weight:bold;">Require: </span><span class="font64">Learning rate ek.</span></p>
<p><span class="font64" style="font-weight:bold;">Require: </span><span class="font64">Initial parameter </span><span class="font18">6</span></p>
<p><span class="font64" style="font-weight:bold;">while </span><span class="font64">stopping criterion not met </span><span class="font64" style="font-weight:bold;">do</span></p>
<p><span class="font64">Sample a minibatch of m examples from the training set {x<sup>(1)</sup>,..., x with corresponding targets y<sup>(i)</sup>.</span></p>
<p><span class="font64">Compute gradient estimate: </span><span class="font64" style="font-weight:bold;font-style:italic;">g </span><span class="font64">&lt;—+ ^V</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>i</sub></span><span class="font64"> L(f (x<sup>(i)</sup>; </span><span class="font18">6</span><span class="font64">), y<sup>(i)</sup>)</span></p>
<p><span class="font64">Apply update: </span><span class="font18">6</span><span class="font64"> ^ </span><span class="font18">6</span><span class="font64"> — eg </span><span class="font64" style="font-weight:bold;">end while</span></p>
<p><span class="font64">A crucial parameter for the SGD algorithm is the learning rate. Previously, we have described SGD as using a fixed learning rate e. In practice, it is necessary to&#160;gradually decrease the learning rate over time, so we now denote the learning rate&#160;at iteration k as ek.</span></p>
<p><span class="font64">This is because the SGD gradient estimator introduces a source of noise (the random sampling of m training examples) that does not vanish even when we arrive&#160;at a minimum. By comparison, the true gradient of the total cost function becomes&#160;small and then </span><span class="font18">0</span><span class="font64"> when we approach and reach a minimum using batch gradient&#160;descent, so batch gradient descent can use a fixed learning rate. A sufficient&#160;condition to guarantee convergence of SGD is that</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">ek =</span><span class="font64"> ro, &#160;&#160;&#160;and&#160;&#160;&#160;&#160;(</span><span class="font18">8</span><span class="font64">.</span><span class="font18">12</span><span class="font64">)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">J24 &lt;</span><span class="font64"> &#160;&#160;&#160;(s•<sup>13</sup>)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">k= 1</span></p>
<p><span class="font64">In practice, it is common to decay the learning rate linearly until iteration t:</span></p>
<p><span class="font64">£</span><span class="font7">k </span><span class="font64">= (1 <sup>- a)e</sup></span><span class="font18">0</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">ae<sub>T</sub></span><span class="font64"> &#160;&#160;&#160;(8.14)</span></p>
<p><span class="font64">with a </span><span class="font64" style="font-weight:bold;font-style:italic;">= <sup>k</sup></span><span class="font64">. After iteration t, it is common to leave e constant.</span></p>
<p><span class="font7">T &#160;&#160;&#160;</span><span class="font64"><sup>5</sup></span></p>
<p><span class="font64">The learning rate may be chosen by trial and error, but it is usually best to choose it by monitoring learning curves that plot the objective function as a&#160;function of time. This is more of an art than a science, and most guidance on this&#160;subject should be regarded with some skepticism. When using the linear schedule,&#160;the parameters to choose are eo, e</span><span class="font7"><sub>T</sub></span><span class="font64">, and t. Usually t may be set to the number of&#160;iterations required to make a few hundred passes through the training set. Usually&#160;e</span><span class="font7"><sub>T</sub> </span><span class="font64">should be set to roughly 1% the value of eo. The main question is how to set eo.&#160;If it is too large, the learning curve will show violent oscillations, with the cost&#160;function often increasing significantly. Gentle oscillations are fine, especially if&#160;training with a stochastic cost function such as the cost function arising from the&#160;use of dropout. If the learning rate is too low, learning proceeds slowly, and if the&#160;initial learning rate is too low, learning may become stuck with a high cost value.&#160;Typically, the optimal initial learning rate, in terms of total training time and the&#160;final cost value, is higher than the learning rate that yields the best performance&#160;after the first 100 iterations or so. Therefore, it is usually best to monitor the first&#160;several iterations and use a learning rate that is higher than the best-performing&#160;learning rate at this time, but not so high that it causes severe instability.</span></p>
<p><span class="font64">The most important property of SGD and related minibatch or online gradient-based optimization is that computation time per update does not grow with the number of training examples. This allows convergence even when the number&#160;of training examples becomes very large. For a large enough dataset, SGD may&#160;converge to within some fixed tolerance of its final test set error before it has&#160;processed the entire training set.</span></p>
<p><span class="font64">To study the convergence rate of an optimization algorithm it is common to measure the </span><span class="font64" style="font-weight:bold;font-style:italic;">excess error J(6) —</span><span class="font64"> min# J(</span><span class="font18">6</span><span class="font64">), which is the amount that the current&#160;cost function exceeds the minimum possible cost. When SGD is applied to a convex&#160;problem, the excess error is O () after k iterations, while in the strongly convex</span></p>
<p><span class="font64">case it is O( 1). These bounds cannot be improved unless extra conditions are assumed. Batch gradient descent enjoys better convergence rates than stochastic&#160;gradient descent in theory. However, the Cramer-Rao bound (Cramer, 1946; Rao,&#160;1945) states that generalization error cannot decrease faster than O (־!). Bottou&#160;and Bousquet (2008) argue that it therefore may not be worthwhile to pursue&#160;an optimization algorithm that converges faster than O( -j) for machine learning&#160;tasks—faster convergence presumably corresponds to overfitting. Moreover, the&#160;asymptotic analysis obscures many advantages that stochastic gradient descent&#160;has after a small number of steps. With large datasets, the ability of SGD to make&#160;rapid initial progress while evaluating the gradient for only very few examples&#160;outweighs its slow asymptotic convergence. Most of the algorithms described in&#160;the remainder of this chapter achieve benefits that matter in practice but are lost&#160;in the constant factors obscured by the O(-j) asymptotic analysis. One can also&#160;trade off the benefits of both batch and stochastic gradient descent by gradually&#160;increasing the minibatch size during the course of learning.</span></p>
<p><span class="font64">For more information on SGD, see Bottou (1998).</span></p><h5><a id="bookmark16"></a><span class="font64" style="font-weight:bold;">8.3.2 Momentum</span></h5>
<p><span class="font64">While stochastic gradient descent remains a very popular optimization strategy, learning with it can sometimes be slow. The method of momentum (Polyak, 1964)&#160;is designed to accelerate learning, especially in the face of high curvature, small but&#160;consistent gradients, or noisy gradients. The momentum algorithm accumulates&#160;an exponentially decaying moving average of past gradients and continues to move&#160;in their direction. The effect of momentum is illustrated in Fig. 8.5.</span></p>
<p><span class="font64">Formally, the momentum algorithm introduces a variable v that plays the role of velocity—it is the direction and speed at which the parameters move through&#160;parameter space. The velocity is set to an exponentially decaying average of&#160;the negative gradient. The name </span><span class="font64" style="font-weight:bold;font-style:italic;">momentum</span><span class="font64"> derives from a physical analogy, in&#160;which the negative gradient is a force moving a particle through parameter space,&#160;according to Newton’s laws of motion. Momentum in physics is mass times velocity.&#160;In the momentum learning algorithm, we assume unit mass, so the velocity vector v&#160;may also be regarded as the momentum of the particle. A hyperparameter a E [0,1)&#160;determines how quickly the contributions of previous gradients exponentially decay.&#160;The update rule is given by:</span></p><div>
<p><span class="font64"><sup>,</sup></span></p></div><div>
<p><span class="font64">(8.15)</span></p></div>
<p><span class="font18"><sub>1</sub></span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>m</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">v ^ av - eV</span><span class="font18">0</span><span class="font64"> &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">L(f</span><span class="font64"> (x<sup>(</sup></span><span class="font7"><sup>i</sup></span><span class="font64"><sup>)</sup>; </span><span class="font18">6</span><span class="font64">), y<sup>(</sup></span><span class="font7"><sup>i</sup></span><span class="font64"><sup>)</sup>)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">m</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">1=1</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">6 ^ 6</span><span class="font64"> + v. &#160;&#160;&#160;(8.16)</span></p>
<p><span class="font64">The velocity v accumulates the gradient elements V</span><span class="font18">0</span><span class="font64"> (</span><span class="font7">m </span><span class="font64" style="font-weight:bold;font-style:italic;">Y^m=<sub>1</sub> L(f</span><span class="font64"> (x<sup>(</sup></span><span class="font7"><sup>i</sup></span><span class="font64"><sup>)</sup>; </span><span class="font18">6</span><span class="font64">), y<sup>(</sup></span><span class="font7"><sup>i</sup></span><span class="font64"><sup>)</sup>)). The larger a is relative to e, the more previous gradients affect the current direction.&#160;The SGD algorithm with momentum is given in Algorithm 8.2.</span></p><div><div><img src="main-85.jpg" alt=""/>
<p><span class="font64">Figure 8.5: Momentum aims primarily to solve two problems: poor conditioning of the Hessian matrix and variance in the stochastic gradient. Here, we illustrate how momentum&#160;overcomes the first of these two problems. The contour lines depict a quadratic loss&#160;function with a poorly conditioned Hessian matrix. The red path cutting across the&#160;contours indicates the path followed by the momentum learning rule as it minimizes this&#160;function. At each step along the way, we draw an arrow indicating the step that gradient&#160;descent would take at that point. We can see that a poorly conditioned quadratic objective&#160;looks like a long, narrow valley or canyon with steep sides. Momentum correctly traverses&#160;the canyon lengthwise, while gradient steps waste time moving back and forth across the&#160;narrow axis of the canyon. Compare also Fig. 4.6, which shows the behavior of gradient&#160;descent without momentum.</span></p></div></div>
<p><span class="font64">Previously, the size of the step was simply the norm of the gradient multiplied by the learning rate. Now, the size of the step depends on how large and how&#160;aligned a </span><span class="font64" style="font-weight:bold;">sequence </span><span class="font64">of gradients are. The step size is largest when many successive&#160;gradients point in exactly the same direction. If the momentum algorithm always&#160;observes gradient g, then it will accelerate in the direction of — </span><span class="font64" style="font-weight:bold;font-style:italic;">g</span><span class="font64">, until reaching a&#160;terminal velocity where the size of each step is</span></p><div>
<p><span class="font64">(8.17)</span></p></div>
<p><span class="font64" style="text-decoration:underline;">e||</span><span class="font64" style="font-weight:bold;text-decoration:underline;">g</span><span class="font64" style="text-decoration:underline;">||</span></p>
<p><span class="font18">1</span><span class="font64"> a</span></p>
<p><span class="font64">It is thus helpful to think of the momentum hyperparameter in terms of . For example, a = .9 corresponds to multiplying the maximum speed by 10 relative to&#160;the gradient descent algorithm.</span></p>
<p><span class="font64">Common values of a used in practice include .5, .9, and .99. Like the learning rate, a may also be adapted over time. Typically it begins with a small value and&#160;is later raised. It is less important to adapt a over time than to shrink e over time.</span></p>
<p><span class="font64" style="font-weight:bold;">Algorithm 8.2 </span><span class="font64">Stochastic gradient descent (SGD) with momentum</span></p>
<p><span class="font64" style="font-weight:bold;">Require: </span><span class="font64">Learning rate e, momentum parameter a.</span></p>
<p><span class="font64" style="font-weight:bold;">Require: </span><span class="font64">Initial parameter </span><span class="font64" style="font-weight:bold;">6</span><span class="font64">, initial velocity </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">. </span><span class="font64" style="font-weight:bold;">while </span><span class="font64">stopping criterion not met </span><span class="font64" style="font-weight:bold;">do</span></p>
<p><span class="font64">Sample a minibatch of m examples from the training set {</span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(</sup></span><span class="font7"><sup>m</sup></span><span class="font64"><sup>)</sup>} with corresponding targets </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(</sup></span><span class="font7"><sup>i</sup></span><span class="font64"><sup>)</sup>.</span></p>
<p><span class="font64">Compute gradient estimate: </span><span class="font64" style="font-weight:bold;">g </span><span class="font64">^ </span><span class="font7">m </span><span class="font64">V</span><span class="font18">0</span><span class="font64"> ^ L(f (</span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(</sup></span><span class="font7"><sup>i</sup></span><span class="font64"><sup>)</sup>; </span><span class="font64" style="font-weight:bold;">6</span><span class="font64">), &#160;&#160;&#160;)</span></p>
<p><span class="font64">Compute velocity update: </span><span class="font64" style="font-weight:bold;">v </span><span class="font64">^ a</span><span class="font64" style="font-weight:bold;">v </span><span class="font64">— e</span><span class="font64" style="font-weight:bold;">g </span><span class="font64">Apply update: </span><span class="font64" style="font-weight:bold;">6 </span><span class="font64">^ </span><span class="font64" style="font-weight:bold;">6 </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;">v&#160;end while</span></p>
<p><span class="font64">We can view the momentum algorithm as simulating a particle subject to continuous-time Newtonian dynamics. The physical analogy can help to build&#160;intuition for how the momentum and gradient descent algorithms behave.</span></p>
<p><span class="font64">The position of the particle at any point in time is given by </span><span class="font64" style="font-weight:bold;">6 </span><span class="font64">(t). The particle experiences net force </span><span class="font64" style="font-weight:bold;">f </span><span class="font64">(t). This force causes the particle to accelerate:</span></p><div>
<p><span class="font64">(8.18)</span></p></div>
<p><span class="font64"><sup>f (t)</sup> = </span><span class="font8" style="font-style:italic;">l<sub>t</sub></span><span class="font64" style="font-weight:bold;font-style:italic;">2 <sup>6(t)</sup>.</span></p>
<p><span class="font64">Rather than viewing this as a second-order differential equation of the position, we can introduce the variable </span><span class="font64" style="font-weight:bold;">v </span><span class="font64">(t) representing the velocity of the particle at time&#160;t and rewrite the Newtonian dynamics as a first-order differential equation:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>v(t)</sup> =</span><span class="font64"> dt </span><span class="font64" style="font-weight:bold;"><sup>6</sup></span><span class="font64"><sup>(t) &#160;&#160;&#160;</sup></span><span class="font18"><sup>(8</sup></span><span class="font64"><sup>A9)</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">f</span><span class="font64"> (t) = &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;">v</span><span class="font64">(t).&#160;&#160;&#160;&#160;(</span><span class="font18">8</span><span class="font64">.</span><span class="font18">20</span><span class="font64">)</span></p>
<p><span class="font64">The momentum algorithm then consists of solving the differential equations via numerical simulation. A simple numerical method for solving differential equations&#160;is Euler’s method, which simply consists of simulating the dynamics defined by&#160;the equation by taking small, finite steps in the direction of each gradient.</span></p>
<p><span class="font64">This explains the basic form of the momentum update, but what specifically are the forces? One force is proportional to the negative gradient of the cost function:&#160;—V# J (</span><span class="font64" style="font-weight:bold;">6</span><span class="font64">). This force pushes the particle downhill along the cost function surface.&#160;The gradient descent algorithm would simply take a single step based on each&#160;gradient, but the Newtonian scenario used by the momentum algorithm instead&#160;uses this force to alter the velocity of the particle. We can think of the particle&#160;as being like a hockey puck sliding down an icy surface. Whenever it descends a&#160;steep part of the surface, it gathers speed and continues sliding in that direction&#160;until it begins to go uphill again.</span></p>
<p><span class="font64">One other force is necessary. If the only force is the gradient of the cost function, then the particle might never come to rest. Imagine a hockey puck sliding down&#160;one side of a valley and straight up the other side, oscillating back and forth forever,&#160;assuming the ice is perfectly frictionless. To resolve this problem, we add one&#160;other force, proportional to </span><span class="font64" style="font-weight:bold;font-style:italic;">—v(t).</span><span class="font64"> In physics terminology, this force corresponds&#160;to viscous drag, as if the particle must push through a resistant medium such as&#160;syrup. This causes the particle to gradually lose energy over time and eventually&#160;converge to a local minimum.</span></p>
<p><span class="font64">Why do we use — </span><span class="font64" style="font-weight:bold;">v </span><span class="font64">(t) and viscous drag in particular? Part of the reason to use — </span><span class="font64" style="font-weight:bold;">v </span><span class="font64">(t) is mathematical convenience—an integer power of the velocity is easy&#160;to work with. However, other physical systems have other kinds of drag based&#160;on other integer powers of the velocity. For example, a particle traveling through&#160;the air experiences turbulent drag, with force proportional to the square of the&#160;velocity, while a particle moving along the ground experiences dry friction, with a&#160;force of constant magnitude. We can reject each of these options. Turbulent drag,&#160;proportional to the square of the velocity, becomes very weak when the velocity is&#160;small. It is not powerful enough to force the particle to come to rest. A particle&#160;with a non-zero initial velocity that experiences only the force of turbulent drag&#160;will move away from its initial position forever, with the distance from the starting&#160;point growing like O(log t). We must therefore use a lower power of the velocity.&#160;If we use a power of zero, representing dry friction, then the force is too strong.&#160;When the force due to the gradient of the cost function is small but non-zero, the&#160;constant force due to friction can cause the particle to come to rest before reaching&#160;a local minimum. Viscous drag avoids both of these problems—it is weak enough&#160;that the gradient can continue to cause motion until a minimum is reached, but&#160;strong enough to prevent motion if the gradient does not justify moving.</span></p>
</body>
</html>