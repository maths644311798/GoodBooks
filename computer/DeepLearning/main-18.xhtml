<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h5><a id="bookmark0"></a><span class="font64" style="font-weight:bold;">8.7.2 Coordinate Descent</span></h5>
<p><span class="font64">In some cases, it may be possible to solve an optimization problem quickly by breaking it into separate pieces. If we minimize f (x) with respect to a single variable&#160;x<sub>i</sub>, then minimize it with respect to another variable </span><span class="font64" style="font-weight:bold;font-style:italic;">xj</span><span class="font64"> and so on, repeatedly&#160;cycling through all variables, we are guaranteed to arrive at a (local) minimum.&#160;This practice is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">coordinate descent,</span><span class="font64"> because we optimize one coordinate&#160;at a time. More generally, </span><span class="font64" style="font-weight:bold;font-style:italic;">block coordinate descent</span><span class="font64"> refers to minimizing with&#160;respect to a subset of the variables simultaneously. The term “coordinate descent”&#160;is often used to refer to block coordinate descent as well as the strictly individual&#160;coordinate descent.</span></p>
<p><span class="font64">Coordinate descent makes the most sense when the different variables in the optimization problem can be clearly separated into groups that play relatively&#160;isolated roles, or when optimization with respect to one group of variables is&#160;significantly more efficient than optimization with respect to all of the variables.&#160;For example, consider the cost function</span></p>
<p><span class="font64">2</span></p>
<p><span class="font64">J(H, </span><span class="font64" style="font-weight:bold;font-style:italic;">W</span><span class="font64">) = £ |Hi,j| + £ (x - </span><span class="font64" style="font-weight:bold;font-style:italic;">W</span><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">H) <sub>ij</sub>.</span><span class="font64"> &#160;&#160;&#160;(8.39)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sub>i,j i,j</sub> <sup>i,j</sup></span></p>
<p><span class="font64">This function describes a learning problem called sparse coding, where the goal is to find a weight matrix W that can linearly decode a matrix of activation values&#160;H to reconstruct the training set X. Most applications of sparse coding also&#160;involve weight decay or a constraint on the norms of the columns of W, in order&#160;to prevent the pathological solution with extremely small H and large W.</span></p>
<p><span class="font64">The function J is not convex. However, we can divide the inputs to the training algorithm into two sets: the dictionary parameters W and the code&#160;representations H. Minimizing the objective function with respect to either one of&#160;these sets of variables is a convex problem. Block coordinate descent thus gives&#160;us an optimization strategy that allows us to use efficient convex optimization&#160;algorithms, by alternating between optimizing W with H fixed, then optimizing&#160;H with W fixed.</span></p>
<p><span class="font64">Coordinate descent is not a very good strategy when the value of one variable strongly influences the optimal value of another variable, as in the function f </span><span class="font64" style="font-weight:bold;font-style:italic;">(x) =&#160;</span><span class="font64">(x</span><span class="font18">1</span><span class="font64"> — x</span><span class="font18">2)<sup>2</sup></span><span class="font64"> + a </span><span class="font64" style="font-weight:bold;font-style:italic;">(x</span><span class="font18"><sup>2</sup></span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"><sup>2</sup>) where a is a positive constant. The first term encourages&#160;the two variables to have similar value, while the second term encourages them&#160;to be near zero. The solution is to set both to zero. Newton’s method can solve&#160;the problem in a single step because it is a positive definite quadratic problem.&#160;However, for small a, coordinate descent will make very slow progress because the&#160;first term does not allow a single variable to be changed to a value that differs&#160;significantly from the current value of the other variable.</span></p><h5><a id="bookmark1"></a><span class="font64" style="font-weight:bold;">8.7.3 &#160;&#160;&#160;Polyak Averaging</span></h5>
<p><span class="font64">Polyak averaging (Polyak and Juditsky, 1992) consists of averaging together several points in the trajectory through parameter space visited by an optimization&#160;algorithm. If t iterations of gradient descent visit points </span><span class="font18">6</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font18">6</span><span class="font64"><sup>(t)</sup>, then the&#160;output of the Polyak averaging algorithm is </span><span class="font18">6</span><span class="font64">'<sup>(t)</sup> = -1 </span><span class="font64" style="font-weight:bold;font-style:italic;">^ ^</span><span class="font64"> </span><span class="font18">6</span><span class="font64"><sup>(i)</sup>. On some problem&#160;classes, such as gradient descent applied to convex problems, this approach has&#160;strong convergence guarantees. When applied to neural networks, its justification&#160;is more heuristic, but it performs well in practice. The basic idea is that the&#160;optimization algorithm may leap back and forth across a valley several times&#160;without ever visiting a point near the bottom of the valley. The average of all of&#160;the locations on either side should be close to the bottom of the valley though.</span></p>
<p><span class="font64">In non-convex problems, the path taken by the optimization trajectory can be very complicated and visit many different regions. Including points in parameter&#160;space from the distant past that may be separated from the current point by large&#160;barriers in the cost function does not seem like a useful behavior. As a result,&#160;when applying Polyak averaging to non-convex problems, it is typical to use an&#160;exponentially decaying running average:</span></p>
<p><span class="font18">6</span><span class="font64"><sup>(t)</sup> = a</span><span class="font18">6</span><span class="font64"><sup>(t-1)</sup> + (1 — a)</span><span class="font18">6</span><span class="font64"><sup>(t)</sup>. &#160;&#160;&#160;(8.40)</span></p>
<p><span class="font64">The running average approach is used in numerous applications. See Szegedy </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) for a recent example.</span></p><h5><a id="bookmark2"></a><span class="font64" style="font-weight:bold;">8.7.4 &#160;&#160;&#160;Supervised Pretraining</span></h5>
<p><span class="font64">Sometimes, directly training a model to solve a specific task can be too ambitious if the model is complex and hard to optimize or if the task is very difficult. It is&#160;sometimes more effective to train a simpler model to solve the task, then make the&#160;model more complex. It can also be more effective to train the model to solve a&#160;simpler task, then move on to confront the final task. These strategies that involve&#160;training simple models on simple tasks before confronting the challenge of training&#160;the desired model to perform the desired task are collectively known as </span><span class="font64" style="font-weight:bold;font-style:italic;">pretraining.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Greedy</span><span class="font64"> algorithms break a problem into many components, then solve for the optimal version of each component in isolation. Unfortunately, combining the&#160;individually optimal components is not guaranteed to yield an optimal complete&#160;solution. However, greedy algorithms can be computationally much cheaper than&#160;algorithms that solve for the best joint solution, and the quality of a greedy solution&#160;is often acceptable if not optimal. Greedy algorithms may also be followed by a&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">fine-tuning</span><span class="font64"> stage in which a joint optimization algorithm searches for an optimal&#160;solution to the full problem. Initializing the joint optimization algorithm with a&#160;greedy solution can greatly speed it up and improve the quality of the solution it&#160;finds.</span></p>
<p><span class="font64">Pretraining, and especially greedy pretraining, algorithms are ubiquitous in deep learning. In this section, we describe specifically those pretraining algorithms&#160;that break supervised learning problems into other simpler supervised learning&#160;problems. This approach is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">greedy supervised pretraining.</span></p>
<p><span class="font64">In the original (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2007) version of greedy supervised pretraining, each stage consists of a supervised learning training task involving only a subset of&#160;the layers in the final neural network. An example of greedy supervised pretraining&#160;is illustrated in Fig. 8.7, in which each added hidden layer is pretrained as part of&#160;a shallow supervised MLP, taking as input the output of the previously trained&#160;hidden layer. Instead of pretraining one layer at a time, Simonyan and Zisserman&#160;(2015) pretrain a deep convolutional network (eleven weight layers) and then use&#160;the first four and last three layers from this network to initialize even deeper&#160;networks (with up to nineteen layers of weights). The middle layers of the new,&#160;very deep network are initialized randomly. The new network is then jointly trained.&#160;Another option, explored by Yu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2010) is to use the </span><span class="font64" style="font-weight:bold;font-style:italic;">outputs</span><span class="font64"> of the previously&#160;trained MLPs, as well as the raw input, as inputs for each added stage.</span></p>
<p><span class="font64">Why would greedy supervised pretraining help? The hypothesis initially discussed by Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2007) is that it helps to provide better guidance to the&#160;intermediate levels of a deep hierarchy. In general, pretraining may help both in&#160;terms of optimization and in terms of generalization.</span></p>
<p><span class="font64">An approach related to supervised pretraining extends the idea to the context of transfer learning: Yosinski </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) pretrain a deep convolutional net with </span><span class="font18">8&#160;</span><span class="font64">layers of weights on a set of tasks (a subset of the 1000 ImageNet object categories)&#160;and then initialize a same-size network with the first k layers of the first net. All&#160;the layers of the second network (with the upper layers initialized randomly) are</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">fit =</span></p></div><div>
<p><span class="font64">(VJ (Ot) - Ve </span><span class="font64" style="font-weight:bold;font-style:italic;">J (O t-</span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;">))</span><span class="font64"><sup>T</sup> Ve J </span><span class="font64" style="font-weight:bold;font-style:italic;">(Ot) </span><span class="font64">VJ (Ot-</span><span class="font18">1</span><span class="font64"> )<sup>T</sup> VeJ (O t-</span><span class="font18">1</span><span class="font64">)</span></p></div><div>
<p><span class="font64">(8.32)</span></p></div><div><div><img src="main-87.jpg" alt=""/></div></div><div>
<p><span class="font64">(a)</span></p></div><div><div><img src="main-88.jpg" alt=""/></div></div><div><div><img src="main-89.jpg" alt=""/>
<p><span class="font64">Figure 8.7: Illustration of one form of greedy supervised pretraining (Bengio </span><span class="font64" style="font-style:italic;">et al</span><span class="font64">2007). </span><span class="font64" style="font-style:italic;">(a)</span><span class="font64"> We start by training a sufficiently shallow architecture. </span><span class="font64" style="font-style:italic;">(b)</span><span class="font64"> Another drawing of the&#160;same architecture. </span><span class="font64" style="font-style:italic;">(c)</span><span class="font64"> We keep only the input-to-hidden layer of the original network and&#160;discard the hidden-to-output layer. We send the output of the first hidden layer as input&#160;to another supervised single hidden layer MLP that is trained with the same objective&#160;as the first network was, thus adding a second hidden layer. This can be repeated for&#160;as many layers as desired. </span><span class="font64" style="font-style:italic;">(d)</span><span class="font64"> Another drawing of the result, viewed as a feedforward&#160;network. To further improve the optimization, we can jointly fine-tune all the layers,&#160;either only at the end or at each stage of this process.</span></p></div></div>
<p><span class="font64">then jointly trained to perform a different set of tasks (another subset of the </span><span class="font18">1000 </span><span class="font64">ImageNet object categories), with fewer training examples than for the first set of&#160;tasks. Other approaches to transfer learning with neural networks are discussed in&#160;Sec. 15.2.</span></p>
<p><span class="font64">Another related line of work is the </span><span class="font64" style="font-weight:bold;font-style:italic;">FitNets</span><span class="font64"> (Romero </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015) approach. This approach begins by training a network that has low enough depth and great enough&#160;width (number of units per layer) to be easy to train. This network then becomes&#160;a </span><span class="font64" style="font-weight:bold;font-style:italic;">teacher</span><span class="font64"> for a second network, designated the </span><span class="font64" style="font-weight:bold;font-style:italic;">student.</span><span class="font64"> The student network is&#160;much deeper and thinner (eleven to nineteen layers) and would be difficult to train&#160;with SGD under normal circumstances. The training of the student network is&#160;made easier by training the student network not only to predict the output for&#160;the original task, but also to predict the value of the middle layer of the teacher&#160;network. This extra task provides a set of hints about how the hidden layers&#160;should be used and can simplify the optimization problem. Additional parameters&#160;are introduced to regress the middle layer of the 5-layer teacher network from&#160;the middle layer of the deeper student network. However, instead of predicting&#160;the final classification target, the objective is to predict the middle hidden layer&#160;of the teacher network. The lower layers of the student networks thus have two&#160;objectives: to help the outputs of the student network accomplish their task, as&#160;well as to predict the intermediate layer of the teacher network. Although a thin&#160;and deep network appears to be more difficult to train than a wide and shallow&#160;network, the thin and deep network may generalize better and certainly has lower&#160;computational cost if it is thin enough to have far fewer parameters. Without&#160;the hints on the hidden layer, the student network performs very poorly in the&#160;experiments, both on the training and test set. Hints on middle layers may thus&#160;be one of the tools to help train neural networks that otherwise seem difficult to&#160;train, but other optimization techniques or changes in the architecture may also&#160;solve the problem.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">8.7.5 Designing Models to Aid Optimization</span></h5>
<p><span class="font64">To improve optimization, the best strategy is not always to improve the optimization algorithm. Instead, many improvements in the optimization of deep models have&#160;come from designing the models to be easier to optimize.</span></p>
<p><span class="font64">In principle, we could use activation functions that increase and decrease in jagged non-monotonic patterns. However, this would make optimization extremely&#160;difficult. In practice, it is more important to choose a model family that&#160;is easy to optimize than to use a powerful optimization algorithm. Most&#160;of the advances in neural network learning over the past 30 years have been&#160;obtained by changing the model family rather than changing the optimization&#160;procedure. Stochastic gradient descent with momentum, which was used to train&#160;neural networks in the 1980s, remains in use in modern state of the art neural&#160;network applications.</span></p>
<p><span class="font64">Specifically, modern neural networks reflect a </span><span class="font64" style="font-weight:bold;font-style:italic;">design choice</span><span class="font64"> to use linear transformations between layers and activation functions that are differentiable almost everywhere and have significant slope in large portions of their domain. In particular, model innovations like the LSTM, rectified linear units and maxout units&#160;have all moved toward using more linear functions than previous models like deep&#160;networks based on sigmoidal units. These models have nice properties that make&#160;optimization easier. The gradient flows through many layers provided that the&#160;Jacobian of the linear transformation has reasonable singular values. Moreover,&#160;linear functions consistently increase in a single direction, so even if the model’s&#160;output is very far from correct, it is clear simply from computing the gradient&#160;which direction its output should move to reduce the loss function. In other words,&#160;modern neural nets have been designed so that their </span><span class="font64" style="font-weight:bold;font-style:italic;">local</span><span class="font64"> gradient information&#160;corresponds reasonably well to moving toward a distant solution.</span></p>
<p><span class="font64">Other model design strategies can help to make optimization easier. For example, linear paths or skip connections between layers reduce the length of&#160;the shortest path from the lower layer’s parameters to the output, and thus&#160;mitigate the vanishing gradient problem (Srivastava </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015). A related idea&#160;to skip connections is adding extra copies of the output that are attached to the&#160;intermediate hidden layers of the network, as in GoogLeNet (Szegedy </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014a)&#160;and deeply-supervised nets (Lee </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014). These “auxiliary heads” are trained&#160;to perform the same task as the primary output at the top of the network in order&#160;to ensure that the lower layers receive a large gradient. When training is complete&#160;the auxiliary heads may be discarded. This is an alternative to the pretraining&#160;strategies, which were introduced in the previous section. In this way, one can&#160;train jointly all the layers in a single phase but change the architecture, so that&#160;intermediate layers (especially the lower ones) can get some hints about what they&#160;should do, via a shorter path. These hints provide an error signal to lower layers.</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">8.7.6 Continuation Methods and Curriculum Learning</span></h5>
<p><span class="font64">As argued in Sec. 8.2.7, many of the challenges in optimization arise from the global structure of the cost function and cannot be resolved merely by making better&#160;estimates of local update directions. The predominant strategy for overcoming this&#160;problem is to attempt to initialize the parameters in a region that is connected&#160;to the solution by a short path through parameter space that local descent can&#160;discover.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Continuation methods</span><span class="font64"> are a family of strategies that can make optimization easier by choosing initial points to ensure that local optimization spends most of&#160;its time in well-behaved regions of space. The idea behind continuation methods is&#160;to construct a series of objective functions over the same parameters. In order to&#160;minimize a cost function </span><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64"> (</span><span class="font18">6</span><span class="font64">), we will construct new cost functions { J<sup>(0)</sup>,..., J<sup>(n)</sup>}.&#160;These cost functions are designed to be increasingly difficult, with J<sup>(0)</sup> being fairly&#160;easy to minimize, and J<sup>(n)</sup>, the most difficult, being J(</span><span class="font18">6</span><span class="font64">), the true cost function&#160;motivating the entire process. When we say that J<sup>(i)</sup> is easier than J <sup>(i</sup>+<sup>x)</sup>, we&#160;mean that it is well behaved over more of </span><span class="font18">6</span><span class="font64"> space. A random initialization is more&#160;likely to land in the region where local descent can minimize the cost function&#160;successfully because this region is larger. The series of cost functions are designed&#160;so that a solution to one is a good initial point of the next. We thus begin by&#160;solving an easy problem then refine the solution to solve incrementally harder&#160;problems until we arrive at a solution to the true underlying problem.</span></p>
<p><span class="font64">Traditional continuation methods (predating the use of continuation methods for neural network training) are usually based on smoothing the objective function.&#160;See Wu (1997) for an example of such a method and a review of some related&#160;methods. Continuation methods are also closely related to simulated annealing,&#160;which adds noise to the parameters (Kirkpatrick </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1983). Continuation&#160;methods have been extremely successful in recent years. See Mobahi and Fisher&#160;(2015) for an overview of recent literature, especially for AI applications.</span></p>
<p><span class="font64">Continuation methods traditionally were mostly designed with the goal of overcoming the challenge of local minima. Specifically, they were designed to&#160;reach a global minimum despite the presence of many local minima. To do so,&#160;these continuation methods would construct easier cost functions by “blurring” the&#160;original cost function. This blurring operation can be done by approximating</span></p>
<p><span class="font64">J<sup>(</sup></span><span class="font18">6</span><span class="font64">)&lt;<sup>־</sup>) = </span><span class="font64" style="font-weight:bold;font-style:italic;">E<sub>e</sub>,</span><span class="font64"> &#160;&#160;&#160;;</span><span class="font18">8</span><span class="font64">,</span><span class="font18">02</span><span class="font64">&gt;״ )J </span><span class="font18">(6</span><span class="font64"> 8.41)&#160;&#160;&#160;&#160;(׳)</span></p>
<p><span class="font64">via sampling. The intuition for this approach is that some non-convex functions become approximately convex when blurred. In many cases, this blurring preserves&#160;enough information about the location of a global minimum that we can find the&#160;global minimum by solving progressively less blurred versions of the problem. This&#160;approach can break down in three different ways. First, it might successfully define&#160;a series of cost functions where the first is convex and the optimum tracks from&#160;one function to the next arriving at the global minimum, but it might require so&#160;many incremental cost functions that the cost of the entire procedure remains high.&#160;NP-hard optimization problems remain NP-hard, even when continuation methods&#160;are applicable. The other two ways that continuation methods fail both correspond&#160;to the method not being applicable. First, the function might not become convex,&#160;no matter how much it is blurred. Consider for example the function </span><span class="font64" style="font-weight:bold;font-style:italic;">J(</span><span class="font64"> ff) </span><span class="font64" style="font-weight:bold;font-style:italic;">= -9</span><span class="font64"><sup>T</sup>ff.&#160;Second, the function may become convex as a result of blurring, but the minimum&#160;of this blurred function may track to a local rather than a global minimum of the&#160;original cost function.</span></p>
<p><span class="font64">Though continuation methods were mostly originally designed to deal with the problem of local minima, local minima are no longer believed to be the primary&#160;problem for neural network optimization. Fortunately, continuation methods can&#160;still help. The easier objective functions introduced by the continuation method can&#160;eliminate flat regions, decrease variance in gradient estimates, improve conditioning&#160;of the Hessian matrix, or do anything else that will either make local updates&#160;easier to compute or improve the correspondence between local update directions&#160;and progress toward a global solution.</span></p>
<p><span class="font64">Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2009) observed that an approach called </span><span class="font64" style="font-weight:bold;font-style:italic;">curriculum learning</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">shaping</span><span class="font64"> can be interpreted as a continuation method. Curriculum learning is based&#160;on the idea of planning a learning process to begin by learning simple concepts&#160;and progress to learning more complex concepts that depend on these simpler&#160;concepts. This basic strategy was previously known to accelerate progress in animal&#160;training (Skinner, 1958; Peterson, 2004; Krueger and Dayan, 2009) and machine&#160;learning (Solomonoff, 1989; Elman, 1993; Sanger, 1994). Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2009)&#160;justified this strategy as a continuation method, where earlier J<sup>(i)</sup> are made easier by&#160;increasing the influence of simpler examples (either by assigning their contributions&#160;to the cost function larger coefficients, or by sampling them more frequently), and&#160;experimentally demonstrated that better results could be obtained by following a&#160;curriculum on a large-scale neural language modeling task. Curriculum learning&#160;has been successful on a wide range of natural language (Spitkovsky </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010;&#160;Collobert </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011a; Mikolov </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011b; Tu and Honavar, 2011) and computer&#160;vision (Kumar </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010; Lee and Grauman, 2011; Supancic and Ramanan, 2013)&#160;tasks. Curriculum learning was also verified as being consistent with the way in&#160;which humans </span><span class="font64" style="font-weight:bold;font-style:italic;">teach</span><span class="font64"> (Khan </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011): teachers start by showing easier and&#160;more prototypical examples and then help the learner refine the decision surface&#160;with the less obvious cases. Curriculum-based strategies are </span><span class="font64" style="font-weight:bold;font-style:italic;">more effective</span><span class="font64"> for&#160;teaching humans than strategies based on uniform sampling of examples, and can&#160;also increase the effectiveness of other teaching strategies (Basu and Christensen,&#160;2013).</span></p>
<p><span class="font64">Another important contribution to research on curriculum learning arose in the context of training recurrent neural networks to capture long-term dependencies:</span></p>
<p><span class="font64">Zaremba and Sutskever (2014) found that much better results were obtained with a </span><span class="font64" style="font-weight:bold;font-style:italic;">stochastic curriculum</span><span class="font64">, in which a random mix of easy and difficult examples is always&#160;presented to the learner, but where the average proportion of the more difficult&#160;examples (here, those with longer-term dependencies) is gradually increased. With&#160;a deterministic curriculum, no improvement over the baseline (ordinary training&#160;from the full training set) was observed.</span></p>
<p><span class="font64">We have now described the basic family of neural network models and how to regularize and optimize them. In the chapters ahead, we turn to specializations of&#160;the neural network family, that allow neural networks to scale to very large sizes and&#160;process input data that has special structure. The optimization methods discussed&#160;in this chapter are often directly applicable to these specialized architectures with&#160;little or no modification.</span></p>
</body>
</html>