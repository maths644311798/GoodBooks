<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 19</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Approximate Inference</span></h2>
<p><span class="font64">Many probabilistic models are difficult to train because it is difficult to perform inference in them. In the context of deep learning, we usually have a set of visible&#160;variables v and a set of latent variables </span><span class="font64" style="font-weight:bold;font-style:italic;">h.</span><span class="font64"> The challenge of inference usually&#160;refers to the difficult problem of computing </span><span class="font64" style="font-weight:bold;font-style:italic;">p(h</span><span class="font64"> | v) or taking expectations with&#160;respect to it. Such operations are often necessary for tasks like maximum likelihood&#160;learning.</span></p>
<p><span class="font64">Many simple graphical models with only one hidden layer, such as restricted Boltzmann machines and probabilistic PCA, are defined in a way that makes&#160;inference operations like computing</span><span class="font64" style="font-weight:bold;font-style:italic;">p(h</span><span class="font64"> | v), or taking expectations with respect&#160;to it, simple. Unfortunately, most graphical models with multiple layers of hidden&#160;variables have intractable posterior distributions. Exact inference requires an&#160;exponential amount of time in these models. Even some models with only a single&#160;layer, such as sparse coding, have this problem.</span></p>
<p><span class="font64">In this chapter, we introduce several of the techniques for confronting these intractable inference problems. Later, in Chapter 20, we will describe how to use&#160;these techniques to train probabilistic models that would otherwise be intractable,&#160;such as deep belief networks and deep Boltzmann machines.</span></p>
<p><span class="font64">Intractable inference problems in deep learning usually arise from interactions between latent variables in a structured graphical model. See Fig. 19.1 for some&#160;examples. These interactions may be due to direct interactions in undirected&#160;models or “explaining away” interactions between mutual ancestors of the same&#160;visible unit in directed models.</span></p><div><img src="main-184.jpg" alt=""/></div>
<p><span class="font64">Figure 19.1: Intractable inference problems in deep learning are usually the result of interactions between latent variables in a structured graphical model. These can be due to&#160;edges directly connecting one latent variable to another, or due to longer paths that are&#160;activated when the child of a V-structure is observed. </span><span class="font64" style="font-weight:bold;font-style:italic;">(Left)</span><span class="font64"> A </span><span class="font64" style="font-weight:bold;font-style:italic;">semi-restricted Boltzmann&#160;machine</span><span class="font64"> (Osindero and Hinton, 2008) with connections between hidden units. These&#160;direct connections between latent variables make the posterior distribution intractable&#160;due to large cliques of latent variables. </span><span class="font64" style="font-weight:bold;font-style:italic;">(Center)</span><span class="font64"> A deep Boltzmann machine, organized&#160;into layers of variables without intra-layer connections, still has an intractable posterior&#160;distribution due to the connections between layers. </span><span class="font64" style="font-weight:bold;font-style:italic;">(Right)</span><span class="font64"> This directed model has&#160;interactions between latent variables when the visible variables are observed, because&#160;every two latent variables are co-parents. Some probabilistic models are able to provide&#160;tractable inference over the latent variables despite having one of the graph structures&#160;depicted above. This is possible if the conditional probability distributions are chosen to&#160;introduce additional independences beyond those described by the graph. For example,&#160;probabilistic PCA has the graph structure shown in the right, yet still has simple inference&#160;due to special properties of the specific conditional distributions it uses (linear-Gaussian&#160;conditionals with mutually orthogonal basis vectors).</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">19.1 Inference as Optimization</span></h4>
<p><span class="font64">Many approaches to confronting the problem of difficult inference make use of the observation that exact inference can be described as an optimization problem.&#160;Approximate inference algorithms may then be derived by approximating the&#160;underlying optimization problem.</span></p>
<p><span class="font64">To construct the optimization problem, assume we have a probabilistic model consisting of observed variables v and latent variables h. We would like to compute&#160;the log probability of the observed data, logp(v; 6). Sometimes it is too difficult&#160;to compute log</span><span class="font62" style="font-style:italic;">p(v; 6</span><span class="font62">) if it is costly to marginalize out h. Instead, we can compute&#160;a lower bound L(v, 6, q) on logp(v; </span><span class="font62" style="font-style:italic;">6</span><span class="font64">). This bound is called the </span><span class="font64" style="font-weight:bold;font-style:italic;">evidence lower&#160;bound</span><span class="font64"> (ELBO). Another commonly used name for this lower bound is the negative&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">variational free energy.</span><span class="font64"> Specifically, the evidence lower bound is defined to be</span></p>
<p><span class="font64">L(v, 6, q) = log</span><span class="font62" style="font-style:italic;">p</span><span class="font64" style="font-weight:bold;font-style:italic;">(</span><span class="font62" style="font-style:italic;">v</span><span class="font64">; 6) - </span><span class="font62" style="font-style:italic;font-variant:small-caps;">Dkl </span><span class="font64" style="font-weight:bold;font-style:italic;">(</span><span class="font62" style="font-style:italic;">q</span><span class="font64" style="font-weight:bold;font-style:italic;">(</span><span class="font62" style="font-style:italic;">h</span><span class="font62"> | v)||p(h | v; 6)) &#160;&#160;&#160;(19.1)</span></p>
<p><span class="font64">where q is an arbitrary probability distribution over h.</span></p>
<p><span class="font64">Because the difference between logp(v) and L(v, 6, q) is given by the KL divergence and because the KL divergence is always non-negative, we can see that&#160;L always has at most the same value as the desired log probability. The two are&#160;equal if and only if q is the same distribution as </span><span class="font62" style="font-style:italic;">p</span><span class="font64" style="font-weight:bold;font-style:italic;">(</span><span class="font62" style="font-style:italic;">h</span><span class="font62"> | v).</span></p>
<p><span class="font64">Surprisingly, L can be considerably easier to compute for some distributions q. Simple algebra shows that we can rearrange L into a much more convenient form:</span></p>
<p><span class="font64">L(v, 6, q) = logp(v; 6) - DKL(q(h | v) ||p(h | v; 6)) &#160;&#160;&#160;(19.2)</span></p>
<p><span class="font64">= <sup>lo</sup>gp<sup>(v</sup>; <sup>6) - E</sup>h^q <sup>l</sup>og p^^j^) &#160;&#160;&#160;<sup>(19</sup>.<sup>3)</sup></span></p>
<p><span class="font64">= ^g</span><span class="font62" style="font-variant:small-caps;">p<sup>(v</sup>; </span><span class="font64"><sup>6) - E</sup>h-q ^g &#160;&#160;&#160;<sup>(1</sup>M</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(v;0)</span></p>
<p><span class="font64">= logp(v; 6) - Eh-q [log </span><span class="font62" style="font-style:italic;">q</span><span class="font64" style="font-weight:bold;font-style:italic;">(</span><span class="font62" style="font-style:italic;">h |</span><span class="font62"> v) - log</span><span class="font62" style="font-style:italic;">p</span><span class="font64" style="font-weight:bold;font-style:italic;">(</span><span class="font62" style="font-style:italic;">h,</span><span class="font62"> v; 6) +logp(v; 6)] (19.5) = - Eh~q [log q(h | v) - logp(h, v; 6)] .&#160;&#160;&#160;&#160;(19.6)</span></p>
<p><span class="font64">This yields the more canonical definition of the evidence lower bound,</span></p>
<p><span class="font64">L(v, 6, q) = Eh-q [logp(h, v)] + H(q). &#160;&#160;&#160;(19.7)</span></p>
<p><span class="font64">For an appropriate choice of q, L is tractable to compute. For any choice of q, L provides a lower bound on the likelihood. For q(h | v) that are better&#160;approximations of </span><span class="font64" style="font-weight:bold;font-style:italic;">p(h |</span><span class="font64"> v), the lower bound L will be tighter, in other words,&#160;closer to logp( v). When q(h | v) = </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> (h | v), the approximation is perfect, and&#160;L(v, </span><span class="font64" style="font-weight:bold;font-style:italic;">0,q) =</span><span class="font64"> logp(v; </span><span class="font18">6</span><span class="font64">).</span></p>
<p><span class="font64">We can thus think of inference as the procedure for finding the q that maximizes L. Exact inference maximizes L perfectly by searching over a family of functions&#160;q that includes p(h | v). Throughout this chapter, we will show how to derive&#160;different forms of approximate inference by using approximate optimization to&#160;find q. We can make the optimization procedure less expensive but approximate&#160;by restricting the family of distributions q the optimization is allowed to search&#160;over or by using an imperfect optimization procedure that may not completely&#160;maximize L but merely increase it by a significant amount.</span></p>
<p><span class="font64">No matter what choice of q we use, L is a lower bound. We can get tighter or looser bounds that are cheaper or more expensive to compute depending on&#160;how we choose to approach this optimization problem. We can obtain a poorly&#160;matched q but reduce the computational cost by using an imperfect optimization&#160;procedure, or by using a perfect optimization procedure over a restricted family of&#160;q distributions.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">19.2 Expectation Maximization</span></h4>
<p><span class="font64">The first algorithm we introduce based on maximizing a lower bound L is the </span><span class="font64" style="font-weight:bold;font-style:italic;">expectation maximization</span><span class="font64"> (EM) algorithm, a popular training algorithm for models&#160;with latent variables. We describe here a view on the EM algorithm developed by&#160;Neal and Hinton (1999). Unlike most of the other algorithms we describe in this&#160;chapter, EM is not an approach to approximate inference, but rather an approach&#160;to learning with an approximate posterior.</span></p>
<p><span class="font64">The EM algorithm consists of alternating between two steps until convergence:</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">E-step</span><span class="font64"> (Expectation step): Let 6<sup>(0)</sup> denote the value of the parameters at the beginning of the step. Set q(h<sup>(i)</sup> | v) = p(h<sup>(i)</sup> | v<sup>(i)</sup>; 6<sup>(0)</sup>) for all indices&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> of the training examples v<sup>(i)</sup> we want to train on (both batch and minibatch&#160;variants are valid). By this we mean q is defined in terms of the current&#160;parameter value of 6^<sup>0)</sup>; if we vary 6 thenp(h | v; 6) will change but q(h | v)&#160;will remain equal to p(h | v; 6<sup>(0)</sup>).</span></p>
<p><span class="font64">The M-step (Maximization step): Completely or partially maximize</span></p><div>
<p><span class="font64">(19.8)</span></p></div>
<p><span class="font64">E L(v<sup>(i)</sup>. 6.4)</span></p>
<p><span class="font64">with respect to 0 using your optimization algorithm of choice.</span></p>
<p><span class="font64">This can be viewed as a coordinate ascent algorithm to maximize L. On one step, we maximize L with respect to q, and on the other, we maximize L with&#160;respect to 0.</span></p>
<p><span class="font64">Stochastic gradient ascent on latent variable models can be seen as a special case of the EM algorithm where the M step consists of taking a single gradient&#160;step. Other variants of the EM algorithm can make much larger steps. For some&#160;model families, the M step can even be performed analytically, jumping all the&#160;way to the optimal solution for 0 given the current q.</span></p>
<p><span class="font64">Even though the E-step involves exact inference, we can think of the EM algorithm as using approximate inference in some sense. Specifically, the M-step&#160;assumes that the same value of q can be used for all values of 0. This will introduce&#160;a gap between L and the true log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(</span><span class="font64"> v) as the M-step moves further and further&#160;away from the value 0<sup>(0)</sup> used in the E-step. Fortunately, the E-step reduces the&#160;gap to zero again as we enter the loop for the next time.</span></p>
<p><span class="font64">The EM algorithm contains a few different insights. First, there is the basic structure of the learning process, in which we update the model parameters to&#160;improve the likelihood of a completed dataset, where all missing variables have&#160;their values provided by an estimate of the posterior distribution. This particular&#160;insight is not unique to the EM algorithm. For example, using gradient descent to&#160;maximize the log-likelihood also has this same property; the log-likelihood gradient&#160;computations require taking expectations with respect to the posterior distribution&#160;over the hidden units. Another key insight in the EM algorithm is that we can&#160;continue to use one value of q even after we have moved to a different value of 0.&#160;This particular insight is used throughout classical machine learning to derive large&#160;M-step updates. In the context of deep learning, most models are too complex&#160;to admit a tractable solution for an optimal large M-step update, so this second&#160;insight which is more unique to the EM algorithm is rarely used.</span></p><h4><a id="bookmark3"></a><span class="font65" style="font-weight:bold;">19.3 MAP Inference and Sparse Coding</span></h4>
<p><span class="font64">We usually use the term inference to refer to computing the probability distribution over one set of variables given another. When training probabilistic models with&#160;latent variables, we are usually interested in computing </span><span class="font64" style="font-weight:bold;font-style:italic;">p(h</span><span class="font64"> | v). An alternative&#160;form of inference is to compute the single most likely value of the missing variables,&#160;rather than to infer the entire distribution over their possible values. In the context&#160;of latent variable models, this means computing</span></p>
<p><span class="font64">h* = argmax</span><span class="font64" style="font-weight:bold;font-style:italic;">p(</span><span class="font65" style="font-style:italic;">h</span><span class="font63"> | v). &#160;&#160;&#160;(19.9)</span></p>
<p><span class="font64">h</span></p>
<p><span class="font64">This is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">maximum a posteriori</span><span class="font64"> inference, abbreviated MAP inference.</span></p>
<p><span class="font64">MAP inference is usually not thought of as approximate inference—it does compute the exact most likely value of h*. However, if we wish to develop a&#160;learning process based on maximizing L( v, h, q), then it is helpful to think of MAP&#160;inference as a procedure that provides a value of q. In this sense, we can think of&#160;MAP inference as approximate inference, because it does not provide the optimal</span></p>
<p><span class="font64">q.</span></p>
<p><span class="font64">Recall from Sec. 19.1 that exact inference consists of maximizing</span></p>
<p><span class="font64">L(v, Q, q) = </span><span class="font64" style="font-weight:bold;font-style:italic;">Eh^q</span><span class="font64"> [log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(</span><span class="font65" style="font-style:italic;">h</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> v)] + H(q) &#160;&#160;&#160;(19.10)</span></p>
<p><span class="font64">with respect to q over an unrestricted family of probability distributions, using an exact optimization algorithm. We can derive MAP inference as a form of&#160;approximate inference by restricting the family of distributions q may be drawn&#160;from. Specifically, we require q to take on a Dirac distribution:</span></p>
<p><span class="font64">q(h | v) = 5</span><span class="font64" style="font-weight:bold;font-style:italic;">(</span><span class="font65" style="font-style:italic;">h — p</span><span class="font64" style="font-weight:bold;font-style:italic;">).</span><span class="font64"> &#160;&#160;&#160;(19.11)</span></p>
<p><span class="font64">This means that we can now control q entirely via p. Dropping terms of L that do not vary with p, we are left with the optimization problem</span></p>
<p><span class="font64">p* = argmaxlogp(h = p, v), &#160;&#160;&#160;(19.12)</span></p>
<p><span class="font64">which is equivalent to the MAP inference problem</span></p>
<p><span class="font64">h* = argmax</span><span class="font64" style="font-weight:bold;font-style:italic;">p(</span><span class="font65" style="font-style:italic;">h</span><span class="font63"> | v). &#160;&#160;&#160;(19.13)</span></p>
<p><span class="font64">h</span></p>
<p><span class="font64">We can thus justify a learning procedure similar to EM, in which we alternate between performing MAP inference to infer h * and then update Q to increase&#160;log</span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font65" style="font-style:italic;">(h</span><span class="font64" style="font-weight:bold;font-style:italic;">*,</span><span class="font64"> v). As with EM, this is a form of coordinate ascent on L, where we&#160;alternate between using inference to optimize L with respect to q and using&#160;parameter updates to optimize L with respect to Q. The procedure as a whole can&#160;be justified by the fact that L is a lower bound on logp(v). In the case of MAP&#160;inference, this justification is rather vacuous, because the bound is infinitely loose,&#160;due to the Dirac distribution’s differential entropy of negative infinity. However,&#160;adding noise to p would make the bound meaningful again.</span></p>
<p><span class="font62">MAP inference is commonly used in deep learning as both a feature extractor and a learning mechanism. It is primarily used for sparse coding models.</span></p>
<p><span class="font62">Recall from Sec. 13.4 that sparse coding is a linear factor model that imposes a sparsity-inducing prior on its hidden units. A common choice is a factorial Laplace&#160;prior, with</span></p>
<p><span class="font62" style="font-style:italic;">p(hi)</span><span class="font62"> = ^e<sup>-A|h 1</sup>. &#160;&#160;&#160;(1R14)</span></p>
<p><span class="font62">The visible units are then generated by performing a linear transformation and adding noise:</span></p>
<p><span class="font62">p(x | h) </span><span class="font62" style="font-style:italic;">= N</span><span class="font62">(v; Wh + b, </span><span class="font62" style="font-style:italic;">0<sup>-1</sup>I</span><span class="font62">). &#160;&#160;&#160;(19.15)</span></p>
<p><span class="font62">Computing or even representing p(h | v) is difficult. Every pair of variables h<sub>i </sub>and </span><span class="font62" style="font-style:italic;">hj</span><span class="font62"> are both parents of v. This means that when v is observed, the graphical&#160;model contains an active path connecting h<sub>i</sub> and hj. All of the hidden units thus&#160;participate in one massive clique in p (h | v). If the model were Gaussian then&#160;these interactions could be modeled efficiently via the covariance matrix, but the&#160;sparse prior makes these interactions non-Gaussian.</span></p>
<p><span class="font62">Because p(h | v) is intractable, so is the computation of the log-likelihood and its gradient. We thus cannot use exact maximum likelihood learning. Instead, we&#160;use MAP inference and learn the parameters by maximizing the ELBO defined by&#160;the Dirac distribution around the MAP estimate of h.</span></p>
<p><span class="font62">If we concatenate all of the h vectors in the training set into a matrix H, and concatenate all of the v vectors into a matrix V, then the sparse coding learning&#160;process consists of minimizing</span></p><div>
<p><span class="font64">J(H, W) = £ |Hi,j | +£ (V</span></p>
<p><span class="font62">i,j &#160;&#160;&#160;i,j</span></p></div><div>
<p><span class="font64">HW</span></p></div><div>
<p><span class="font64">T</span></p></div><div>
<p><span class="font62">)<sup>2</sup>.</span></p>
<p><span class="font62">i,j</span></p></div><div>
<p><span class="font62">(19.16)</span></p></div>
<p><span class="font62">Most applications of sparse coding also involve weight decay or a constraint on the norms of the columns of W, in order to prevent the pathological solution with&#160;extremely small H and large W.</span></p>
<p><span class="font62">We can minimize J by alternating between minimization with respect to H and minimization with respect to W. Both sub-problems are convex. In fact,&#160;the minimization with respect to W is just a linear regression problem. However,&#160;minimization of J with respect to both arguments is usually not a convex problem.</span></p>
<p><span class="font62">Minimization with respect to H requires specialized algorithms such as the feature-sign search algorithm (Lee </span><span class="font62" style="font-style:italic;">et al.,</span><span class="font62"> 2007).</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">19.4 Variational Inference and Learning</span></h4>
<p><span class="font64">We have seen how the evidence lower bound L (v, </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>0,</sup>q)</span><span class="font64"> is a lower bound on log </span><span class="font64" style="font-weight:bold;font-style:italic;">p(v;</span><span class="font64"> 0), how inference can be viewed as maximizing L with respect to q, and&#160;how learning can be viewed as maximizing L with respect to 0. We have seen&#160;that the EM algorithm allows us to make large learning steps with a fixed q and&#160;that learning algorithms based on MAP inference allow us to learn using a point&#160;estimate of </span><span class="font64" style="font-weight:bold;font-style:italic;">p(h |</span><span class="font64"> v) rather than inferring the entire distribution. Now we develop&#160;the more general approach to variational learning.</span></p>
<p><span class="font64">The core idea behind variational learning is that we can maximize L over a restricted family of distributions q. This family should be chosen so that it is easy&#160;to compute E<sub>q</sub> log p(h, v). A typical way to do this is to introduce assumptions&#160;about how q factorizes.</span></p>
<p><span class="font64">A common approach to variational learning is to impose the restriction that q is a factorial distribution:</span></p>
<p><span class="font64">q<sup>(h 1 v)</sup> = IJ </span><span class="font64" style="font-weight:bold;font-style:italic;">q<sup>(h</sup>i<sup>1</sup></span><span class="font64"><sup> v)</sup>. &#160;&#160;&#160;</span><span class="font18">(<sup>19</sup></span><span class="font64">T</span><span class="font18">7</span><span class="font64">)</span></p>
<p><span class="font62">i</span></p>
<p><span class="font64">This is called the </span><span class="font64" style="font-weight:bold;font-style:italic;">mean field</span><span class="font64"> approach. More generally, we can impose any graphical model structure we choose on q, to flexibly determine how many interactions we&#160;want our approximation to capture. This fully general graphical model approach&#160;is called </span><span class="font64" style="font-weight:bold;font-style:italic;">structured variational inference</span><span class="font64"> (Saul and Jordan, 1996).</span></p>
<p><span class="font64">The beauty of the variational approach is that we do not need to specify a specific parametric form for q. We specify how it should factorize, but then the&#160;optimization problem determines the optimal probability distribution within those&#160;factorization constraints. For discrete latent variables, this just means that we&#160;use traditional optimization techniques to optimize a finite number of variables&#160;describing the q distribution. For continuous latent variables, this means that we&#160;use a branch of mathematics called calculus of variations to perform optimization&#160;over a space of functions, and actually determine which function should be used&#160;to represent q. Calculus of variations is the origin of the names “variational&#160;learning” and “variational inference,” though these names apply even when the&#160;latent variables are discrete and calculus of variations is not needed. In the case&#160;of continuous latent variables, calculus of variations is a powerful technique that&#160;removes much of the responsibility from the human designer of the model, who&#160;now must specify only how q factorizes, rather than needing to guess how to design&#160;a specific q that can accurately approximate the posterior.</span></p>
<p><span class="font64">Because L(v, 0,q) is defined to be log</span><span class="font64" style="font-weight:bold;font-style:italic;">p( v; 0</span><span class="font64">) — </span><span class="font64" style="font-weight:bold;font-style:italic;">D<sub>KL</sub> (q(h</span><span class="font64"> | v)||p(h | v; 0)), we can think of maximizing L with respect to q as minimizing DKL(q(h | v)||p(h | v)).</span></p>
<p><span class="font64">In this sense, we are fitting q to p. However, we are doing so with the opposite direction of the KL divergence than we are used to using for fitting an approximation.&#160;When we use maximum likelihood learning to fit a model to data, we minimize&#160;DKL(pd<sub>ata</sub> ||p<sub>mo</sub>d<sub>e</sub>i)• As illustrated in Fig. 3.6, this means that maximum likelihood&#160;encourages the model to have high probability everywhere that the data has high&#160;probability, while our optimization-based inference procedure encourages q to&#160;have low probability everywhere the true posterior has low probability. Both&#160;directions of the KL divergence can have desirable and undesirable properties. The&#160;choice of which to use depends on which properties are the highest priority for&#160;each application. In the case of the inference optimization problem, we choose&#160;to use Dkl(q(h | v)||p(h | v)) for computational reasons. Specifically, computing&#160;DKL(q(h | v) ||p(h | v)) involves evaluating expectations with respect to q, so by&#160;designing q to be simple, we can simplify the required expectations. The opposite&#160;direction of the KL divergence would require computing expectations with respect&#160;to the true posterior. Because the form of the true posterior is determined by&#160;the choice of model, we cannot design a reduced-cost approach to computing&#160;DKL(p(h | v)||q(h | v)) exactly.</span></p><h5><a id="bookmark5"></a><span class="font64" style="font-weight:bold;">19.4.1 Discrete Latent Variables</span></h5>
<p><span class="font64">Variational inference with discrete latent variables is relatively straightforward. We define a distribution q, typically one where each factor of q is just defined&#160;by a lookup table over discrete states. In the simplest case, h is binary and we&#160;make the mean field assumption that q factorizes over each individual hi. In this&#160;case we can parametrize q with a vector h whose entries are probabilities. Then</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">q<sup>(h</sup></span><span class="font62" style="font-style:italic;">i </span><span class="font64" style="font-weight:bold;font-style:italic;">= 1</span><span class="font64"> | v) = </span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font62" style="font-style:italic;">j</span><span class="font64" style="font-weight:bold;font-style:italic;">.</span></p>
<p><span class="font64">After determining how to represent q, we simply optimize its parameters. In the case of discrete latent variables, this is just a standard optimization problem.&#160;In principle the selection of q could be done with any optimization algorithm, such&#160;as gradient descent.</span></p>
<p><span class="font64">Because this optimization must occur in the inner loop of a learning algorithm, it must be very fast. To achieve this speed, we typically use special optimization&#160;algorithms that are designed to solve comparatively small and simple problems in&#160;very few iterations. A popular choice is to iterate fixed point equations, in other&#160;words, to solve</span></p>
<p><span class="font62">d</span></p>
<p><span class="font64">L = 0 &#160;&#160;&#160;(19.18)</span></p>
<p><span class="font62">dhj</span></p>
<p><span class="font64">for hj. We repeatedly update different elements of h until we satisfy a convergence</span></p>
<p><span class="font64">criterion.</span></p>
<p><span class="font64">To make this more concrete, we show how to apply variational inference to the </span><span class="font64" style="font-weight:bold;font-style:italic;">binary sparse coding</span><span class="font64"> model (we present here the model developed by Henniges </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(</span><span class="font18">2010</span><span class="font64">) but demonstrate traditional, generic mean field applied to the model, while&#160;they introduce a specialized algorithm). This derivation goes into considerable&#160;mathematical detail and is intended for the reader who wishes to fully resolve&#160;any ambiguity in the high-level conceptual description of variational inference and&#160;learning we have presented so far. Readers who do not plan to derive or implement&#160;variational learning algorithms may safely skip to the next section without missing&#160;any new high-level concepts. Readers who proceed with the binary sparse coding&#160;example are encouraged to review the list of useful properties of functions that&#160;commonly arise in probabilistic models in Sec. 3.10. We use these properties&#160;liberally throughout the following derivations without highlighting exactly where&#160;we use each one.</span></p>
<p><span class="font64">In the binary sparse coding model, the input </span><span class="font64" style="font-weight:bold;font-style:italic;">v</span><span class="font64"> £ R<sup>n</sup> is generated from the model by adding Gaussian noise to the sum of m different components which&#160;can each be present or absent. Each component is switched on or off by the&#160;corresponding hidden unit in h £ {</span><span class="font18">0</span><span class="font64">,</span><span class="font18">1</span><span class="font64">}<sup>m</sup>:</span></p><div>
<p><span class="font64">(19.19)</span></p>
<p><span class="font64">(19.20)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">P<sup>(h</sup>i =</span><span class="font64"> <sup>1</sup>) = ^<sup>(b</sup>i)</span></p>
<p><span class="font64">p(v | h) = </span><span class="font64" style="font-weight:bold;font-style:italic;">N(v; Wh, 3</span><span class="font64"> *)</span></p>
<p><span class="font64">where b is a learnable set of biases, W is a learnable weight matrix, and 3 is a learnable, diagonal precision matrix.</span></p>
<p><span class="font64">Training this model with maximum likelihood requires taking the derivative with respect to the parameters. Consider the derivative with respect to one of the&#160;biases:</span></p><div>
<p><span class="font64">(19.21)</span></p>
<p><span class="font64">(19.22)</span></p>
<p><span class="font64">(19.23)</span></p>
<p><span class="font64">(19.24)</span></p></div>
<p><span class="font64">di <sup>lQ</sup>g p<sup>(v)</sup></span></p>
<p><span class="font62">. </span><span class="font62" style="font-style:italic;">a</span><span class="font3" style="font-style:italic;">t</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">i</span><span class="font64">P<sup>(v) </sup>p(v)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Wi &#160;&#160;&#160;h p<sup>(h </sup></span><span class="font64"><sup>v)</sup></span></p>
<p><span class="font64">p(v)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Wi</span><span class="font64"> ^h </span><span class="font64" style="font-weight:bold;font-style:italic;">p<sup>(h)</sup>p<sup>(v</sup></span><span class="font64"> I <sup>h) </sup>p(v)</span></p><div><div><img src="main-185.jpg" alt=""/>
<p><span class="font64">Figure 19.2: The graph structure of a binary sparse coding model with four hidden units. </span><span class="font64" style="font-weight:bold;font-style:italic;">(Left)</span><span class="font64"> The graph structure of </span><span class="font64" style="font-weight:bold;font-style:italic;">p(</span><span class="font64"> h, v). Note that the edges are directed, and that every two&#160;hidden units are co-parents of every visible unit. </span><span class="font64" style="font-weight:bold;font-style:italic;">(Right)</span><span class="font64"> The graph structure ofp(h | v).&#160;In order to account for the active paths between co-parents, the posterior distribution&#160;needs an edge between all of the hidden units.</span></p></div></div><div><div><img src="main-186.jpg" alt=""/></div></div><div>
<p><span class="font64">Eh P<sup>(v 1 h)</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">6b i</span><span class="font64"> P<sup>(h)</sup></span></p></div><div>
<p><span class="font64">P<sup>(v)</sup></span></p></div><div>
<p><span class="font64">£ p(h </span><span class="font18">1</span><span class="font64"> ״) *<sup>p(h)</sup></span></p>
<p><span class="font64">h</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span></p></div><div>
<p><span class="font64">P<sup>(h)</sup></span></p></div><div>
<p><span class="font64"><sup>E</sup>h~p(h|v) </span><span class="font64" style="font-weight:bold;font-style:italic;">db</span><span class="font62" style="font-style:italic;">-</span><span class="font62"> <sup>10gp(h)</sup>־</span></p></div><div>
<p><span class="font64">(19.25)</span></p>
<p><span class="font64">(19.26)</span></p>
<p><span class="font64">(19.27)</span></p></div>
<p><span class="font64">This requires computing expectations with respect to p(h | v). Unfortunately, p(h | v) is a complicated distribution. See Fig. 19.2 for the graph structure of&#160;p(h, v) andp(h | v). The posterior distribution corresponds to the complete graph&#160;over the hidden units, so variable elimination algorithms do not help us to compute&#160;the required expectations any faster than brute force.</span></p>
<p><span class="font64">We can resolve this difficulty by using variational inference and variational learning instead.</span></p>
<p><span class="font64">We can make a mean field approximation:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">q(h</span><span class="font64"> | v) = </span><span class="font64" style="font-weight:bold;font-style:italic;">Y[q(hi</span><span class="font64"> | v). &#160;&#160;&#160;(19.28)</span></p>
<p><span class="font62">i</span></p>
<p><span class="font64">The latent variables of the binary sparse coding model are binary, so to represent a factorial q we simply need to model m Bernoulli distributions q( h<sub>i</sub> | v). A natural&#160;way to represent the means of the Bernoulli distributions is with a vector h of&#160;probabilities, with q(h<sub>i</sub> =1 | v) = h. We impose a restriction that h<sub>i</sub> is never&#160;equal to 0 or to 1, in order to avoid errors when computing, for example, log h<sub>i</sub>.</span></p>
<p><span class="font64">We will see that the variational inference equations never assign 0 or 1 to h<sub>i</sub> analytically. However, in a software implementation, machine rounding error could&#160;result in 0 or 1 values. In software, we may wish to implement binary sparse&#160;coding using an unrestricted vector of variational parameters z and obtain h via&#160;the relation h = a </span><span class="font64" style="font-weight:bold;font-style:italic;">(z</span><span class="font64">). We can thus safely compute log h on a computer by using&#160;the identity log </span><span class="font64" style="font-weight:bold;font-style:italic;">a(z) =</span><span class="font64"> —Z(—</span><span class="font64" style="font-weight:bold;font-style:italic;">z</span><span class="font64"><sub>i</sub>) relating the sigmoid and the softplus.</span></p>
<p><span class="font64">To begin our derivation of variational learning in the binary sparse coding model, we show that the use of this mean field approximation makes learning&#160;tractable.</span></p>
<p><span class="font64">The evidence lower bound is given by</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">L(v, 0,q</span><span class="font64">)</span></p>
<p><span class="font64">=Eh</span><span class="font62" style="font-style:italic;">~q</span><span class="font64">[log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(h, v</span><span class="font64">)] + H(q)</span></p>
<p><span class="font64">=Eh[logp(h) + log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(v</span><span class="font64"> | h) — log q(h | v)]</span></p></div><div>
<p><span class="font62" style="font-style:italic;">Y</span><span class="font62"> log </span><span class="font62" style="font-style:italic;">p(hi</span><span class="font62">) </span><span class="font62" style="font-style:italic;">+Y</span><span class="font62"> logp(vi I h) </span><span class="font62" style="font-style:italic;">— Y</span><span class="font62"> log </span><span class="font62" style="font-style:italic;">q(hi</span><span class="font62"> I v)</span></p>
<p><span class="font62">.i=1 &#160;&#160;&#160;i=1&#160;&#160;&#160;&#160;i=1</span></p>
<p><span class="font62" style="font-style:italic;">= Y</span><span class="font62"> [hi(log</span><span class="font62" style="font-style:italic;">a(bi) <sup>—</sup></span><span class="font62"><sup> lo</sup>g </span><span class="font62" style="font-style:italic;">hi)</span><span class="font62"> + <sup>(</sup>1 <sup>—</sup> hi<sup>)(lo</sup>g<sup>a(—b</sup>i<sup>) — lo</sup>g<sup>(</sup>l <sup>—</sup> h))</span></p>
<p><span class="font62"><sup>—</sup>^ <sup>(vi — Wi</sup>'<sup>h)2</sup>)</span></p>
<p><span class="font62" style="font-style:italic;">Y</span><span class="font62"> [hi(log </span><span class="font62" style="font-style:italic;">a(bi) <sup>—</sup></span><span class="font62"><sup> lo</sup>g </span><span class="font62" style="font-style:italic;">hi)</span><span class="font62"> + <sup>(1 —</sup> hi<sup>)(lo</sup>g <sup>a(—b</sup>i<sup>) — lo</sup>g<sup>(1 —</sup> </span><span class="font62" style="font-style:italic;">hi))</span></p></div><div>
<p><span class="font64">=Eh</span></p></div><div>
<p><span class="font62" style="font-style:italic;">i</span><span class="font62">=1</span></p>
<p><span class="font64">+ Eh</span></p></div><div>
<p><span class="font64">(19.29)</span></p>
<p><span class="font64">(19.30)</span></p>
<p><span class="font64">(19.31)</span></p>
<p><span class="font64">(19.32)</span></p>
<p><span class="font64">(19.33)</span></p>
<p><span class="font64">(19.34)</span></p>
<p><span class="font64">(19.35)</span></p></div><div>
<p><span class="font62" style="font-style:italic;">i</span><span class="font62">=1</span></p></div><div><h3><a id="bookmark6"></a><span class="font64">+2E</span></h3></div><div>
<p><span class="font62">i</span><span class="font18">=1</span></p></div><div>
<p><span class="font64">log־^ <sup>—</sup> &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">(</span><span class="font64">v<sup>2 —</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">2viWi: h</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">Y</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">^ <sup>j</sup></span></p></div><div>
<p><span class="font62" style="font-style:italic;">wZfy</span><span class="font62"> + &#160;&#160;&#160;<sup>W</sup>i,j <sup>W</sup>i,k </span><span class="font62" style="font-style:italic;"><sup>h</sup>j<sup>h</sup>k</span></p>
<p><span class="font62" style="font-style:italic;"><sup>k</sup>=<sup>j</sup></span></p></div><div>
<p><span class="font64">J </span><span class="font67" style="font-weight:bold;font-style:italic;">J</span></p>
<p><span class="font64">(19.36)</span></p></div>
<p><span class="font64">While these equations are somewhat unappealing aesthetically, they show that L can be expressed in a small number of simple arithmetic operations. The evidence&#160;lower bound L is therefore tractable. We can use L as a replacement for the&#160;intractable log-likelihood.</span></p>
<p><span class="font64">In principle, we could simply run gradient ascent on both v and h and this would make a perfectly acceptable combined inference and training algorithm.&#160;Usually, however, we do not do this, for two reasons. First, this would require&#160;storing h for each v. We typically prefer algorithms that do not require per-example memory. It is difficult to scale learning algorithms to billions of examples&#160;if we must remember a dynamically updated vector associated with each example.</span></p>
<p><span class="font64">Second, we would like to be able to extract the features h very quickly, in order to recognize the content of v. In a realistic deployed setting, we would need to be&#160;able to compute h in real time.</span></p>
<p><span class="font64">For both these reasons, we typically do not use gradient descent to compute the mean field parameters h. Instead, we rapidly estimate them with fixed point&#160;equations.</span></p>
<p><span class="font64">The idea behind fixed point equations is that we are seeking a local maximum with respect to h, where </span><span class="font64" style="font-weight:bold;font-style:italic;">VhL</span><span class="font64">(v, 6, h) = 0. We cannot efficiently solve this&#160;equation with respect to all of h simultaneously. However, we can solve for a single&#160;variable:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d hi</span></p></div><div>
<p><span class="font64">(19.37)</span></p></div>
<p><span class="font64">L(v, 6, h) = 0.</span></p>
<p><span class="font64">We can then iteratively apply the solution to the equation for </span><span class="font64" style="font-weight:bold;font-style:italic;">i =</span><span class="font64"> 1,..., m, and repeat the cycle until we satisfy a converge criterion. Common convergence&#160;criteria include stopping when a full cycle of updates does not improve L by more&#160;than some tolerance amount, or when the cycle does not change h by more than&#160;some amount.</span></p>
<p><span class="font64">Iterating mean field fixed point equations is a general technique that can provide fast variational inference in a broad variety of models. To make this more&#160;concrete, we show how to derive the updates for the binary sparse coding model in&#160;particular.</span></p>
<p><span class="font64">First, we must write an expression for the derivatives with respect to </span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font64"><sub>i</sub>. To do so, we substitute Eq. 19.36 into the left side of Eq. 19.37:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dhi</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dhi</span></p></div><div>
<p><span class="font64">(19.38)</span></p></div>
<p><span class="font64">L(v, 6, h)</span></p>
<p><span class="font64">m</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Y</span><span class="font64"> [hj (log </span><span class="font64" style="font-weight:bold;font-style:italic;">a(bj</span><span class="font64">) - loghj) + (1 - </span><span class="font64" style="font-weight:bold;font-style:italic;">hj</span><span class="font64"> )(log a(-bj) - log(1 - hj)) &#160;&#160;&#160;(19.39)</span></p>
<p><span class="font18"><sup>j</sup>=1</span></p><div><h3><a id="bookmark7"></a><span class="font64">+2</span></h3></div><div>
<p><span class="font64"><sup>j</sup></span><span class="font18">=1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font62" style="font-style:italic;">3</span></p></div><div>
<p><span class="font62">^g &#160;&#160;&#160;<sup>-</sup> </span><span class="font65" style="font-style:italic;font-variant:small-caps;">p </span><span class="font62" style="font-style:italic;">j</span><span class="font62"> </span><span class="font18">1</span><span class="font64"> </span><span class="font62" style="font-style:italic;"><sup>y</sup>j</span><span class="font64" style="font-weight:bold;font-style:italic;">j<sup>-</sup></span><span class="font64"><sup> 2v</sup> </span><span class="font62" style="font-style:italic;">3<sup>W</sup>3</span><span class="font62"><sup>h</sup></span><span class="font62" style="font-style:italic;">+</span><span class="font64" style="font-weight:bold;font-style:italic;">Y</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>W</sup>j,k<sup>h</sup>k</span><span class="font64"> + &#160;&#160;&#160;<sup>W</sup>j,k<sup>W</sup>j,l </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>h</sup>k<sup>h</sup>l</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">l=k</span></p></div><div>
<p><span class="font64">= log </span><span class="font64" style="font-weight:bold;font-style:italic;">a</span><span class="font64">(bi) - log</span><span class="font64" style="font-weight:bold;font-style:italic;">hi -</span><span class="font64"> </span><span class="font18">1</span><span class="font64"> +log</span><span class="font18">(1</span><span class="font64"> - </span><span class="font64" style="font-weight:bold;font-style:italic;">hi)</span><span class="font64"> + </span><span class="font18">1</span><span class="font64"> - log a(-bi)</span></p>
<p><span class="font18">1</span></p></div><div>
<p><span class="font64"><sup>+</sup></span></p>
<p><span class="font64"><sup>j</sup> = l</span></p></div><div>
<p><span class="font64">Pj I </span><span class="font64" style="font-weight:bold;font-style:italic;">VjWji -</span><span class="font64"> </span><span class="font18"><sub>2</sub></span><span class="font64"> Wji - £ </span><span class="font64" style="font-weight:bold;font-style:italic;">WjkWji hk</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">k=i</span></p></div><div>
<p><span class="font64">(19.40)</span></p>
<p><span class="font64">(19.41)</span></p>
<p><span class="font64">(19.42)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">b -</span><span class="font64"> log </span><span class="font64" style="font-weight:bold;font-style:italic;">hi +</span><span class="font64"> log(1 - </span><span class="font64" style="font-weight:bold;font-style:italic;">hi</span><span class="font64">) + </span><span class="font64" style="font-weight:bold;font-style:italic;">v<sup>T</sup>pW,i -</span><span class="font64"> ״WT</span><span class="font64" style="font-weight:bold;font-style:italic;">0W:,i -Y,</span><span class="font64"> W.T</span><span class="font64" style="font-weight:bold;font-style:italic;">0W<sub>:J</sub>f1<sub>j</sub>.</span><span class="font64"> (19.43)</span></p>
<p><span class="font62">To apply the fixed point update inference rule, we solve for the h<sub>i</sub> that sets Eq. 19.43 to 0:</span></p><div>
<p><span class="font62" style="font-style:italic;">i</span></p></div><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64">= a I bi + v</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>T</sup>pW:,i -</span><span class="font64"> &#160;&#160;&#160;W.TpW:,i -&#160;&#160;&#160;&#160;WTPW:</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">T</span></p></div><div>
<p><span class="font62" style="font-style:italic;">i</span></p></div><div>
<p><span class="font62" style="font-style:italic;">i</span></p></div><div>
<p><span class="font64">2</span></p></div><div>
<p><span class="font62" style="font-style:italic;">i</span></p></div><div>
<p><span class="font62" style="font-style:italic;">,i j</span></p></div><div>
<p><span class="font62">(19.44)</span></p></div>
<p><span class="font62">At this point, we can see that there is a close connection between recurrent neural networks and inference in graphical models. Specifically, the mean field&#160;fixed point equations defined a recurrent neural network. The task of this network&#160;is to perform inference. We have described how to derive this network from a&#160;model description, but it is also possible to train the inference network directly.&#160;Several ideas based on this theme are described in Chapter 20.</span></p>
<p><span class="font62">In the case of binary sparse coding, we can see that the recurrent network connection specified by Eq. 19.44 consists of repeatedly updating the hidden&#160;units based on the changing values of the neighboring hidden units. The input&#160;always sends a fixed message of v<sup>T</sup>^ W to the hidden units, but the hidden units&#160;constantly update the message they send to each other. Specifically, two units h<sub>i&#160;</sub>and </span><span class="font62" style="font-style:italic;">hj</span><span class="font62"> inhibit each other when their weight vectors are aligned. This is a form of&#160;competition—between two hidden units that both explain the input, only the one&#160;that explains the input best will be allowed to remain active. This competition is&#160;the mean field approximation’s attempt to capture the explaining away interactions&#160;in the binary sparse coding posterior. The explaining away effect actually should&#160;cause a multi-modal posterior, so that if we draw samples from the posterior,&#160;some samples will have one unit active, other samples will have the other unit&#160;active, but very few samples have both active. Unfortunately, explaining away&#160;interactions cannot be modeled by the factorial q used for mean field, so the mean&#160;field approximation is forced to choose one mode to model. This is an instance of&#160;the behavior illustrated in Fig. 3.6.</span></p>
<p><span class="font62">We can rewrite Eq. 19.44 into an equivalent form that reveals some further insights:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">hi = a bi +</span><span class="font64"> | v </span><span class="font64" style="font-weight:bold;font-style:italic;">-Y,</span><span class="font64"> W: </span><span class="font64" style="font-weight:bold;font-style:italic;">j hj</span><span class="font64"> | PW:,i - 1</span><span class="font64" style="font-weight:bold;font-style:italic;">W.,i PW:,i</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>j</sup>=<sup>i</sup></span></p></div>
<p><span class="font62">/ \<sup>T</sup></span></p>
<p><span class="font62">(19.45)</span></p>
<p><span class="font62">In this reformulation, we see the input at each step as consisting of v - ^j= W<sub>:j</sub>hj rather than v. We can thus think of unit i as attempting to encode the residual&#160;error in v given the code of the other units. We can thus think of sparse coding as&#160;an iterative autoencoder, that repeatedly encodes and decodes its input, attempting&#160;to fix mistakes in the reconstruction after each iteration.</span></p>
<p><span class="font62">In this example, we have derived an update rule that updates a single unit at a time. It would be advantageous to be able to update more units simultaneously.&#160;Some graphical models, such as deep Boltzmann machines, are structured in such a&#160;way that we can solve for many entries of h simultaneously. Unfortunately, binary&#160;sparse coding does not admit such block updates. Instead, we can use a heuristic&#160;technique called </span><span class="font64" style="font-weight:bold;font-style:italic;">damping</span><span class="font64"> to perform block updates. In the damping approach, we&#160;solve for the individually optimal values of every element of h, then move all of&#160;the values in a small step in that direction. This approach is no longer guaranteed&#160;to increase L at each step, but works well in practice for many models. See Koller&#160;and Friedman (2009) for more information about choosing the degree of synchrony&#160;and damping strategies in message passing algorithms.</span></p><h5><a id="bookmark8"></a><span class="font64">19.4.2 Calculus of Variations</span></h5>
<p><span class="font62">Before continuing with our presentation of variational learning, we must briefly introduce an important set of mathematical tools used in variational learning:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">calculus of variations.</span></p>
<p><span class="font62">Many machine learning techniques are based on minimizing a function J(6) by finding the input vector 6 E R<sup>n</sup> for which it takes on its minimal value. This can&#160;be accomplished with multivariate calculus and linear algebra, by solving for the&#160;critical points where </span><span class="font65" style="font-style:italic;">V</span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">q</span><span class="font65" style="font-style:italic;">J</span><span class="font64">(6) = 0. In some cases, we actually want to solve for a&#160;function f (x), such as when we want to find the probability density function over&#160;some random variable. This is what calculus of variations enables us to do.</span></p><div>
<p><span class="font62" style="font-style:italic;"><sup>Sf</sup></span><span class="font62"> (<sup>x</sup>)</span></p>
<p><span class="font62">A complete formal development of functional derivatives is beyond the scope of this book. For our purposes, it is sufficient to state that for differentiable functions&#160;f (x) and differentiable functions g(y, x) with continuous derivatives, that</span></p></div>
<p><span class="font62">A function of a function f is known as a </span><span class="font64" style="font-weight:bold;font-style:italic;">functional J</span><span class="font64"> [f]. Much as we can take partial derivatives of a function with respect to elements of its vector-valued&#160;argument, we can take </span><span class="font64" style="font-weight:bold;font-style:italic;">functional derivatives</span><span class="font62" style="font-style:italic;">,</span><span class="font62"> also known as </span><span class="font64" style="font-weight:bold;font-style:italic;">variational derivatives</span><span class="font62" style="font-style:italic;">,&#160;</span><span class="font62">of a functional J[f ] with respect to individual values of the function f (x) at any&#160;specific value of x. The functional derivative of the functional J with respect to&#160;the value of the function f at point x is denoted</span></p><div>
<p><span class="font62">(19.46)</span></p></div>
<p><span class="font64">$ &#160;&#160;&#160;d</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">spx)</span><span class="font64"> g<sup>(f(x)</sup>׳<sup>x) dx</sup>=<sub>dy</sub>g<sup>(f(x)x)</sup>׳</span></p>
<p><span class="font64">To gain some intuition for this identity, one can think of f (x) as being a vector with uncountably many elements, indexed by a real vector x. In this (somewhat&#160;incomplete view), the identity providing the functional derivatives is the same as&#160;we would obtain for a vector </span><span class="font18">6</span><span class="font64"> </span><span class="font65" style="font-style:italic;">E</span><span class="font63"> R<sup>n</sup> indexed by positive integers:</span></p>
<p><span class="font64">J^ </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>g</sup></span><span class="font4" style="font-style:italic;">A A■</span><span class="font64"> &#160;&#160;&#160;<sup>(</sup></span><span class="font18"><sup>19</sup></span><span class="font64">-47)</span></p>
<p><span class="font64">Many results in other machine learning publications are presented using the more general </span><span class="font64" style="font-weight:bold;font-style:italic;">Euler-Lagrange equation</span><span class="font64"> which allows g to depend on the derivatives of f&#160;as well as the value of f, but we do not need this fully general form for the results&#160;presented in this book.</span></p>
<p><span class="font64">To optimize a function with respect to a vector, we take the gradient of the function with respect to the vector and solve for the point where every element of&#160;the gradient is equal to zero. Likewise, we can optimize a functional by solving for&#160;the function where the functional derivative at every point is equal to zero.</span></p>
<p><span class="font64">As an example of how this process works, consider the problem of finding the probability distribution function over </span><span class="font64" style="font-weight:bold;font-style:italic;">x </span><span class="font65" style="font-style:italic;">E</span><span class="font63"> R that has maximal differential entropy.&#160;Recall that the entropy of a probability distribution p(x) is defined as</span></p><div>
<p><span class="font64">(19.48)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">H [p]</span><span class="font64"> = -Ex log p(x).</span></p>
<p><span class="font64">For continuous values, the expectation is an integral:</span></p>
<p><span class="font64"><sup>H [p]</sup> = </span><span class="font64" style="font-weight:bold;font-style:italic;">~!</span><span class="font64"><sup>p(x)log </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>p(x)dx</sup>■</span><span class="font64"> &#160;&#160;&#160;<sup>(1949</sup>׳<sup>)</sup></span></p>
<p><span class="font64">We cannot simply maximize H</span><span class="font64" style="font-weight:bold;font-style:italic;">[p</span><span class="font64">] with respect to the function </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x</span><span class="font64">), because the result might not be a probability distribution. Instead, we need to use Lagrange&#160;multipliers to add a constraint that p(x) integrates to 1. Also, the entropy&#160;increases without bound as the variance increases. This makes the question of&#160;which distribution has the greatest entropy uninteresting. Instead, we ask which&#160;distribution has maximal entropy for fixed variance a <sup>2</sup>. Finally, the problem&#160;is underdetermined because the distribution can be shifted arbitrarily without&#160;changing the entropy. To impose a unique solution, we add a constraint that the&#160;mean of the distribution be p. The Lagrangian functional for this optimization&#160;problem is</span></p><div>
<p><span class="font64">p(x)dx — 1 + A</span><span class="font18"><sub>2</sub></span><span class="font64"> (E[x] — p) + A</span><span class="font18"><sub>3</sub></span><span class="font64"> (E[(x — p)<sup>2</sup>] — a<sup>2</sup>) + H[p] (19.50)</span></p></div>
<p><span class="font64"><sup>L[</sup>P<sup>]</sup> = A! ^</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">= &#160;&#160;&#160;A!</span><span class="font64"> p(x) + A</span><span class="font18">2</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x)x</span><span class="font64"> + A</span><span class="font18">3</span><span class="font64" style="font-weight:bold;font-style:italic;">p(x)(x -</span><span class="font64"> p</span><span class="font18">)<sup>2</sup></span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">- p(x)</span><span class="font64"> log p(x)) </span><span class="font64" style="font-weight:bold;font-style:italic;">dx -</span><span class="font64"> A! - pA</span><span class="font18">2</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">-</span><span class="font64"> a <sup>2</sup>A </span><span class="font18">3</span><span class="font64">.</span></p>
<p><span class="font64">(19.51)</span></p>
<p><span class="font64">To minimize the Lagrangian with respect to p, we set the functional derivatives equal to </span><span class="font18">0</span><span class="font64">:</span></p><div>
<p><span class="font64">Vx,</span></p></div><div>
<p><span class="font64">S</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Sp(x)</span></p></div><div>
<p><span class="font64">L = A<sub>!</sub> + A<sub>2</sub>x + A<sub>3</sub>(x - p</span><span class="font18">)<sup>2</sup></span><span class="font64"> - 1 - logp(x) = 0.</span></p></div><div>
<p><span class="font64">(19.52)</span></p></div>
<p><span class="font64">This condition now tells us the functional form of p (x). By algebraically re-arranging the equation, we obtain</span></p><div>
<p><span class="font64">(19.53)</span></p></div>
<p><span class="font64">p(x) = exp (A! + A</span><span class="font18">2</span><span class="font64">x + A</span><span class="font18">3</span><span class="font64">(x - p</span><span class="font18">)<sup>2</sup></span><span class="font64"> - 1) .</span></p>
<p><span class="font64">We never assumed directly that p(x) would take this functional form; we obtained the expression itself by analytically minimizing a functional. To finish&#160;the minimization problem, we must choose the A values to ensure that all of our&#160;constraints are satisfied. We are free to choose any A values, because the gradient&#160;of the Lagrangian with respect to the A variables is zero so long as the constraints&#160;are satisfied. To satisfy all of the constraints, we may set A! = 1 - log a 2/ץn,&#160;A</span><span class="font18"><sub>2</sub></span><span class="font64"> = 0, and A</span><span class="font18">3</span><span class="font64"> = -^ to obtain</span></p><div>
<p><span class="font64">(19.54)</span></p></div>
<p><span class="font64">p(x) = </span><span class="font64" style="font-weight:bold;font-style:italic;">N</span><span class="font64">(x; p, </span><span class="font64" style="font-weight:bold;font-style:italic;">a</span><span class="font64"><sup>2</sup>).</span></p>
<p><span class="font64">This is one reason for using the normal distribution when we do not know the true distribution. Because the normal distribution has the maximum entropy, we&#160;impose the least possible amount of structure by making this assumption.</span></p>
<p><span class="font64">While examining the critical points of the Lagrangian functional for the entropy, we found only one critical point, corresponding to maximizing the entropy for&#160;fixed variance. What about the probability distribution function that </span><span class="font64" style="font-weight:bold;">minimizes&#160;</span><span class="font64">the entropy? Why did we not find a second critical point corresponding to the&#160;minimum? The reason is that there is no specific function that achieves minimal&#160;entropy. As functions place more probability density on the two points x = p + a&#160;and x = p - a, and place less probability density on all other values of x, they lose&#160;entropy while maintaining the desired variance. However, any function placing&#160;exactly zero mass on all but two points does not integrate to one, and is not a&#160;valid probability distribution. There thus is no single minimal entropy probability&#160;distribution function, much as there is no single minimal positive real number.&#160;Instead, we can say that there is a sequence of probability distributions converging&#160;toward putting mass only on these two points. This degenerate scenario may be&#160;described as a mixture of Dirac distributions. Because Dirac distributions are&#160;not described by a single probability distribution function, no Dirac or mixture of&#160;Dirac distribution corresponds to a single specific point in function space. These&#160;distributions are thus invisible to our method of solving for a specific point where&#160;the functional derivatives are zero. This is a limitation of the method. Distributions&#160;such as the Dirac must be found by other methods, such as guessing the solution&#160;and then proving that it is correct.</span></p><h5><a id="bookmark9"></a><span class="font64">19.4.3 Continuous Latent Variables</span></h5>
<p><span class="font62">When our graphical model contains continuous latent variables, we may still perform variational inference and learning by maximizing L. However, we must&#160;now use calculus of variations when maximizing L with respect to q(h | v).</span></p>
<p><span class="font62">In most cases, practitioners need not solve any calculus of variations problems themselves. Instead, there is a general equation for the mean field fixed point&#160;updates. If we make the mean field approximation</span></p>
<p><span class="font62" style="font-style:italic;">q(h</span><span class="font62"> | v) = &#160;&#160;&#160;</span><span class="font62" style="font-style:italic;">q(h</span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> | v),&#160;&#160;&#160;&#160;(19.55)</span></p>
<p><span class="font62">i</span></p><div>
<p><span class="font62" style="font-style:italic;">v</span><span class="font62">) may be obtained by</span></p></div>
<p><span class="font62">and fix q(hj | v) for all </span><span class="font62" style="font-style:italic;">j = i,</span><span class="font62"> then the optimal q(h<sub>i</sub> | normalizing the unnormalized distribution</span></p>
<p><span class="font62"><sup>q(h</sup>i <sup>1 v)</sup> = <sup>ex</sup>p (Eh_^q(h_» <sup>10</sup>g </span><span class="font62" style="font-style:italic;">^<sup>,</sup></span><span class="font62"><sup> h)</sup>) &#160;&#160;&#160;<sup>(19</sup>.<sup>56)</sup></span></p>
<p><span class="font62">so long as p does not assign 0 probability to any joint configuration of variables. Carrying out the expectation inside the equation will yield the correct functional&#160;form of q (h<sub>i</sub> | v). It is only necessary to derive functional forms of q directly using&#160;calculus of variations if one wishes to develop a new form of variational learning;&#160;Eq. 19.56 yields the mean field approximation for any probabilistic model.</span></p>
<p><span class="font62">Eq. 19.56 is a fixed point equation, designed to be iteratively applied for each value of i repeatedly until convergence. However, it also tells us more than that. It&#160;tells us the functional form that the optimal solution will take, whether we arrive&#160;there by fixed point equations or not. This means we can take the functional form&#160;from that equation but regard some of the values that appear in it as parameters,&#160;that we can optimize with any optimization algorithm we like.</span></p>
<p><span class="font62">As an example, consider a very simple probabilistic model, with latent variables h e R<sup>2</sup> and just one visible variable, v. Suppose that p(h) = N(h; 0, I) and&#160;</span><span class="font62" style="font-style:italic;">p(v</span><span class="font62"> | h) = N </span><span class="font62" style="font-style:italic;">(v; w<sup>T</sup>h;</span><span class="font62"> 1). We could actually simplify this model by integrating&#160;out h; the result is just a Gaussian distribution over v. The model itself is not&#160;interesting; we have constructed it only to provide a simple demonstration of how&#160;calculus of variations may be applied to probabilistic modeling.</span></p>
<p><span class="font64">The true posterior is given, up to a normalizing constant, by</span></p><div>
<p><span class="font64">(19.57)</span></p>
<p><span class="font64">(19.58)</span></p>
<p><span class="font64">(19.59)</span></p>
<p><span class="font64">(19.60)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(h |</span><span class="font64"> v)</span></p>
<p><span class="font64">(</span><span class="font64" style="font-weight:bold;font-style:italic;">xp(h</span><span class="font64">, v)</span></p>
<p><span class="font64">=p(h</span><span class="font18">1</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">)p(h</span><span class="font62" style="font-style:italic;">2</span><span class="font64" style="font-weight:bold;font-style:italic;">)p(v</span><span class="font64"> | h)</span></p><div>
<p><span class="font64">( exp</span></p></div>
<p><span class="font64">^- <sup>1</sup> [h? + h</span><span class="font18"><sup>2</sup></span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">(v — h </span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;"> w </span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;"> — h</span><span class="font62" style="font-style:italic;">2</span><span class="font64" style="font-weight:bold;font-style:italic;"> w</span><span class="font62" style="font-style:italic;">2</span><span class="font64"> )<sup>2</sup>]</span></p>
<p><span class="font64">exp ^ ^ [h? + h| + v</span><span class="font18"><sup>2</sup></span><span class="font64"> + h?w? + </span><span class="font64" style="font-weight:bold;font-style:italic;">W —</span><span class="font64"> 2vh <sub>?</sub>w<sub>?</sub> — 2vh</span><span class="font18"><sub>2</sub></span><span class="font64">w</span><span class="font18">2</span><span class="font64"> + 2h<sub>?</sub> w<sub>?</sub> h</span><span class="font18"><sub>2</sub></span><span class="font64">w<sub>2</sub>] ^ .</span></p>
<p><span class="font64">(19.61)</span></p>
<p><span class="font64">Due to the presence of the terms multiplying h? and h</span><span class="font18">2</span><span class="font64"> together, we can see that the true posterior does not factorize over h</span><span class="font18"><sub>1</sub></span><span class="font64"> and h<sub>2</sub>.</span></p>
<p><span class="font64">Applying Eq. 19.56, we find that</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">q(h?</span><span class="font64"> </span><span class="font18"><sup>1</sup></span><span class="font64"><sup> v)</sup></span></p>
<p><span class="font64">:<sup>ex</sup>P (<sup>E</sup>h</span><span class="font18">2</span><span class="font64">~q(h</span><span class="font18">2</span><span class="font64"> |v) <sup>10</sup>g <sup>p(v, h)</sup>) </span><span class="font18">1</span></p>
<p><span class="font64"><sub>2</sub>E</span></p></div><div>
<p><span class="font64">exp ( — </span><span class="font18">-<sup>1</sup></span><span class="font64"> Eh</span><span class="font18"><sub>2</sub></span><span class="font64">^q(<sub>h</sub></span><span class="font18"><sub>2</sub></span><span class="font64">|v) [h</span><span class="font18"><sup>2</sup></span><span class="font64"> + h</span><span class="font18"><sup>2</sup></span><span class="font64"> + v</span><span class="font18"><sup>2</sup></span><span class="font64"> + h</span><span class="font18"><sup>2</sup></span><span class="font64">w</span><span class="font18"><sup>2</sup></span><span class="font64"> + h</span><span class="font18">2</span><span class="font64">w</span><span class="font18"><sup>2 </sup></span><span class="font64">—</span><span class="font18">2</span><span class="font64">vh<sub>?</sub>w<sub>?</sub> — </span><span class="font18">2</span><span class="font64">vh</span><span class="font18"><sub>2</sub></span><span class="font64">w</span><span class="font18"><sub>2</sub></span><span class="font64"> + </span><span class="font18">2</span><span class="font64">h <sub>?</sub>w<sub>?</sub>h</span><span class="font18"><sub>2</sub></span><span class="font64">w<sub>2</sub>]^ .</span></p></div><div>
<p><span class="font64">(19.62)</span></p>
<p><span class="font64">(19.63)</span></p>
<p><span class="font64">(19.64)</span></p>
<p><span class="font64">(19.65)</span></p></div>
<p><span class="font64">From this, we can see that there are effectively only two values we need to obtain from q(h2 | v): E<sub>h</sub></span><span class="font18"><sub>2</sub></span><span class="font62">^<sub>g</sub>(h|v)[h2] and E<sub>h</sub></span><span class="font18"><sub>2</sub></span><span class="font62">^q(h|v)[h|]. Writing these as (h2) and (h2),&#160;we obtain</span></p>
<p><span class="font64">q׳(h</span><span class="font18">1</span><span class="font64"> | v) = exp^ — 2־ [h? + (h2) + v</span><span class="font18"><sup>2</sup></span><span class="font64"> + h</span><span class="font18"><sup>2</sup></span><span class="font64">w</span><span class="font18"><sup>2</sup></span><span class="font64"> + (h2)w2 &#160;&#160;&#160;(19.66)</span></p>
<p><span class="font64">—2vh<sub>?</sub>w<sub>?</sub> — 2v(h</span><span class="font18"><sub>2</sub></span><span class="font64">)w</span><span class="font18"><sub>2</sub></span><span class="font64"> + 2h<sub>?</sub>w<sub>?</sub>(h</span><span class="font18"><sub>2</sub></span><span class="font64">)w</span><span class="font18">2</span><span class="font64">]^ . &#160;&#160;&#160;(19.67)</span></p>
<p><span class="font64">From this, we can see that g has the functional form of a Gaussian. We can thus conclude </span><span class="font64" style="font-weight:bold;font-style:italic;">q</span><span class="font64"> (h | v) = N(h; </span><span class="font64" style="font-weight:bold;font-style:italic;">p, ft</span><span class="font64"><sup>-</sup></span><span class="font18"><sup>1</sup></span><span class="font64">) where </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> and diagonal </span><span class="font64" style="font-weight:bold;font-style:italic;">ft</span><span class="font64"> are variational&#160;parameters that we can optimize using any technique we choose. It is important&#160;to recall that we did not ever assume that q would be Gaussian; its Gaussian&#160;form was derived automatically by using calculus of variations to maximize q with&#160;respect to L. Using the same approach on a different model could yield a different&#160;functional form of q.</span></p>
<p><span class="font64">This was of course, just a small case constructed for demonstration purposes. For examples of real applications of variational learning with continuous variables&#160;in the context of deep learning, see Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013d).</span></p><h5><a id="bookmark10"></a><span class="font64">19.4.4 Interactions between Learning and Inference</span></h5>
<p><span class="font64">Using approximate inference as part of a learning algorithm affects the learning process, and this in turn affects the accuracy of the inference algorithm.</span></p>
<p><span class="font64">Specifically, the training algorithm tends to adapt the model in a way that makes the approximating assumptions underlying the approximate inference algorithm&#160;become more true. When training the parameters, variational learning increases</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Eh~q</span><span class="font64"> logp(v, h). &#160;&#160;&#160;(19.68)</span></p>
<p><span class="font64">For a specific v, this increases </span><span class="font64" style="font-weight:bold;font-style:italic;">p(h |</span><span class="font64"> v) for values of h that have high probability under q(h | v) and decreases p(h | v) for values of h that have low probability&#160;under q(h | v).</span></p>
<p><span class="font64">This behavior causes our approximating assumptions to become self-fulfilling prophecies. If we train the model with a unimodal approximate posterior, we will&#160;obtain a model with a true posterior that is far closer to unimodal than we would&#160;have obtained by training the model with exact inference.</span></p>
<p><span class="font64">Computing the true amount of harm imposed on a model by a variational approximation is thus very difficult. There exist several methods for estimating&#160;log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(v).</span><span class="font64"> We often estimate log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(v</span><span class="font64">; 0) after training the model, and find that&#160;the gap with L(v, 0, q) is small. From this, we can conclude that our variational&#160;approximation is accurate for the specific value of 0 that we obtained from the&#160;learning process. We should not conclude that our variational approximation is&#160;accurate in general or that the variational approximation did little harm to the&#160;learning process. To measure the true amount of harm induced by the variational&#160;approximation, we would need to know 0* = max# logp( v; 0). It is possible for&#160;L(v, 0, q)&#160;&#160;&#160;&#160;log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(v</span><span class="font64">; 0) and logp (v; 0) ^ logp(v; 0*) to hold simultaneously. If</span></p>
<p><span class="font64">max<sub>q</sub>L(v, 0* ,q) ^ logp(v; 0*), because 0* induces too complicated of a posterior distribution for our q family to capture, then the learning process will never&#160;approach 0*. Such a problem is very difficult to detect, because we can only know&#160;for sure that it happened if we have a superior learning algorithm that can find 0*&#160;for comparison.</span></p>
</body>
</html>