<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2 dir="rtl"><a id="bookmark0"></a><span class="font64">0ס000000000נ;&amp;</span></h2>
<p><span class="font67" style="font-weight:bold;">r </span><span class="font52" style="font-style:italic;">trrtrrrrrrtr </span><span class="font67" style="font-weight:bold;">lamuum</span></p>
<p dir="rtl"><span class="font66" style="font-weight:bold;font-style:italic;">בבבבבבבבבבב</span><span class="font60">(p</span></p>
<p dir="rtl"><span class="font64">0</span><span class="font67" style="font-weight:bold;"> (f (j (» (» u to ל ל ל Ir i</span></p>
<p dir="rtl"><span class="font43">7 </span><span class="font43" style="font-style:italic;">77777777777 7 W</span></p>
<p dir="rtl"><span class="font65" style="font-weight:bold;">חחחחחהן</span></p>
<p><span class="font43">I mnn 3 3 3 3 3</span></p>
<p><span class="font64">Figure 20.12: Illustration of clamping the right half of the image and running the Markov Chain by resampling only the left half at each step. These samples come from a GSN&#160;trained to reconstruct MNIST digits at each time step using the walkback procedure.</span></p><h5><a id="bookmark1"></a><span class="font64" style="font-weight:bold;">20.11.3 Walk-Back Training Procedure</span></h5>
<p><span class="font64">The walk-back training procedure was proposed by Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013c) as a way to accelerate the convergence of generative training of denoising autoencoders.&#160;Instead of performing a one-step encode-decode reconstruction, this procedure&#160;consists in alternative multiple stochastic encode-decode steps (as in the generative</span></p>
<p><span class="font64">Markov chain) initialized at a training example (just like with the contrastive divergence algorithm, described in Sec. 18.2) and penalizing the last probabilistic&#160;reconstructions (or all of the reconstructions along the way).</span></p>
<p><span class="font64">Training with k steps is equivalent (in the sense of achieving the same stationary distribution) as training with one step, but practically has the advantage that&#160;spurious modes farther from the data can be removed more efficiently.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">20.12 Generative Stochastic Networks</span></h4>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Generative stochastic networks</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">GSNs</span><span class="font64"> (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014) are generalizations of denoising autoencoders that include latent variables h in the generative Markov&#160;chain, in addition to the visible variables (usually denoted x).</span></p>
<p><span class="font64">A GSN is parametrized by two conditional probability distributions which specify one step of the Markov chain:</span></p>
<p><span class="font64">1. &#160;&#160;&#160;p (x<sup>(k)</sup> | h<sup>(k)</sup>) tells how to generate the next visible variable given the current&#160;latent state. Such a “reconstruction distribution” is also found in denoising&#160;autoencoders, RBMs, DBNs and DBMs.</span></p>
<p><span class="font64">2. &#160;&#160;&#160;p(h<sup>(k)</sup> | h<sup>(k-1)</sup>, x<sup>(k-1)</sup>) tells how to update the latent state variable, given&#160;the previous latent state and visible variable.</span></p>
<p><span class="font64">Denoising autoencoders and GSNs differ from classical probabilistic models (directed or undirected) in that they parametrize the generative process itself rather&#160;than the mathematical specification of the joint distribution of visible and latent&#160;variables. Instead, the latter is defined </span><span class="font64" style="font-weight:bold;">implicitly</span><span class="font64">, </span><span class="font64" style="font-weight:bold;font-style:italic;">if it exists,</span><span class="font64"> as the stationary&#160;distribution of the generative Markov chain. The conditions for existence of the&#160;stationary distribution are mild and are the same conditions required by standard&#160;MCMC methods (see Sec. 17.3). These conditions are necessary to guarantee&#160;that the chain mixes, but they can be violated by some choices of the transition&#160;distributions (for example, if they were deterministic).</span></p>
<p><span class="font64">One could imagine different training criteria for GSNs. The one proposed and evaluated by Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) is simply reconstruction log-probability on the&#160;visible units, just like for denoising autoencoders. This is achieved by clamping&#160;x<sup>(0)</sup> = x to the observed example and maximizing the probability of generating </span><span class="font64" style="font-style:italic;">x&#160;</span><span class="font64">at some subsequent time steps, i.e., maximizing logp(x<sup>(k)</sup> = x | h<sup>(k)</sup>), where h<sup>(k)&#160;</sup>is sampled from the chain, given x<sup>(0)</sup> = x. In order to estimate the gradient of&#160;logp(x<sup>(k)</sup> = x | h<sup>(k)</sup>) with respect to the other pieces of the model, Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(2014) use the reparametrization trick, introduced in Sec. 20.9.</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">walk-back training</span><span class="font64"> protocol (described in Sec. 20.11.3) was used (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014) to improve training convergence of GSNs.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">20.12.1 Discriminant GSNs</span></h5>
<p><span class="font64">The original formulation of GSNs (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al</span><span class="font64">., 2014) was meant for unsupervised learning and implicitly modeling p(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) for observed data </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">, but it is possible to&#160;modify the framework to optimize p(</span><span class="font64" style="font-weight:bold;">y </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">).</span></p>
<p><span class="font64">For example, Zhou and Troyanskaya (2014) generalize GSNs in this way, by only back-propagating the reconstruction log-probability over the output variables, keeping the input variables fixed. They applied this successfully to model </span><span class="font64" style="font-weight:bold;">sequences&#160;</span><span class="font64">(protein secondary structure) and introduced a (one-dimensional) </span><span class="font64" style="font-weight:bold;">convolutional&#160;</span><span class="font64">structure in the transition operator of the Markov chain. It is important to remember that, for each step of the Markov chain, one generates a new sequence for&#160;each layer, and that sequence is the input for computing other layer values (say&#160;the one below and the one above) at the next time step.</span></p>
<p><span class="font64">Hence the Markov chain is really over the </span><span class="font64" style="font-weight:bold;">output variable </span><span class="font64">(and associated higher-level hidden layers), and the input sequence only serves to condition that&#160;chain, with back-propagation allowing to learn how the input sequence can condition&#160;the output distribution implicitly represented by the Markov chain. It is therefore&#160;a case of using the GSN in the context of </span><span class="font64" style="font-weight:bold;">structured outputs</span><span class="font64">, where p(</span><span class="font64" style="font-weight:bold;">y </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">)&#160;does not have a simple parametric form but instead the components of </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">are&#160;statistically dependent of each other, given </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">, in complicated ways.</span></p>
<p><span class="font64">Zohrer and Pernkopf (2014) introduced a hybrid model that combines a supervised objective (as in the above work) and an unsupervised objective (as in the original GSN work), by simply adding (with a different weight) the supervised and&#160;unsupervised costs i.e., the reconstruction log-probabilities of </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">and </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">respectively.&#160;Such a hybrid criterion had previously been introduced for RBMs by Larochelle&#160;and Bengio (2008). They show improved classification performance using this&#160;scheme.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">20.13 Other Generation Schemes</span></h4>
<p><span class="font64">The methods we have described so far use either MCMC sampling, ancestral sampling, or some mixture of the two to generate samples. While these are the&#160;most popular approaches to generative modeling, they are by no means the only&#160;approaches.</span></p>
<p><span class="font64">Sohl-Dickstein </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) developed a </span><span class="font64" style="font-weight:bold;font-style:italic;">diffusion inversion</span><span class="font64"> training scheme for learning a generative model, based on non-equilibrium thermodynamics. The&#160;approach is based on the idea that the probability distributions we wish to sample&#160;from have structure. This structure can gradually be destroyed by a diffusion&#160;process that incrementally changes the probability distribution to have more&#160;entropy. To form a generative model, we can run the process in reverse, by training&#160;a model that gradually restores the structure to an unstructured distribution. By&#160;iteratively applying a process that brings a distribution closer to the target one, we&#160;can gradually approach that target distribution. This approach resembles MCMC&#160;methods in the sense that it involves many iterations to produce a sample. However,&#160;the model is defined to be the probability distribution produced by the final step&#160;of the chain. In this sense, there is no approximation induced by the iterative&#160;procedure. The approach introduced by Sohl-Dickstein </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) is also very&#160;close to the generative interpretation of the denoising autoencoder (Sec. 20.11.1).&#160;Like with the denoising autoencoder, the training objective trains a transition&#160;operator which attempts to probabilistically undo the effect of adding some noise,&#160;trying to undo one step of the diffusion process. If we compare with the walkback&#160;training procedure (Sec. 20.11.3) for denoising autoencoders and GSNs, the main&#160;difference is that instead of reconstructing only towards the observed training point&#160;x, the objective function only tries to reconstruct towards the previous point in&#160;the diffusion trajectory that started at x (which should be easier). This addresses&#160;the following dilemma present with the ordinary reconstruction log-likelihood&#160;objective of denoising autoencoders: with small levels of noise the learner only sees&#160;configurations near the data points, while with large levels of noise it is asked to do&#160;an almost impossible job (because the denoising distribution is going to be highly&#160;complex and multi-modal). With the diffusion inversion objective, the learner can&#160;learn more precisely the shape of the density around the data points as well as&#160;remove spurious modes that could show up far from the data points.</span></p>
<p><span class="font64">Another approach to sample generation is the </span><span class="font64" style="font-weight:bold;font-style:italic;">approximate Bayesian computation</span><span class="font64"> (ABC) framework (Rubin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1984). In this approach, samples are rejected or modified in order to make the moments of selected functions of the samples&#160;match those of the desired distribution. While this idea uses the moments of the&#160;samples like in moment matching, it is different from moment matching because it&#160;modifies the samples themselves, rather than training the model to automatically&#160;emit samples with the correct moments. Bachman and Precup (2015) showed how&#160;to use ideas from ABC in the context of deep learning, by using ABC to shape the&#160;MCMC trajectories of GSNs.</span></p>
<p><span class="font64">We expect that many other possible approaches to generative modeling await discovery.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">20.14 Evaluating Generative Models</span></h4>
<p><span class="font64">Researchers studying generative models often need to compare one generative model to another, usually in order to demonstrate that a newly invented generative&#160;model is better at capturing some distribution than the pre-existing models.</span></p>
<p><span class="font64">This can be a difficult and subtle task. In many cases, we can not actually evaluate the log probability of the data under the model, but only an approximation.&#160;In these cases, it is important to think and communicate clearly about exactly what&#160;is being measured. For example, suppose we can evaluate a stochastic estimate of&#160;the log-likelihood for model A, and a deterministic lower bound on the log-likelihood&#160;for model B. If model A gets a higher score than model B, which is better? If we&#160;care about determining which model has a better internal representation of the&#160;distribution, we actually cannot tell, unless we have some way of determining how&#160;loose the bound for model B is. However, if we care about how well we can use&#160;the model in practice, for example to perform anomaly detection, then it is fair to&#160;say that a model is preferable based on a criterion specific to the practical task of&#160;interest, e.g., based on ranking test examples and ranking criteria such as precision&#160;and recall.</span></p>
<p><span class="font64">Another subtlety of evaluating generative models is that the evaluation metrics are often hard research problems in and of themselves. It can be very difficult&#160;to establish that models are being compared fairly. For example, suppose we use&#160;AIS to estimate log Z in order to compute log </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x) —</span><span class="font64"> log </span><span class="font64" style="font-weight:bold;font-style:italic;">Z</span><span class="font64"> for a new model we&#160;have just invented. A computationally economical implementation of AIS may fail&#160;to find several modes of the model distribution and underestimate Z, which will&#160;result in us overestimating log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(x).</span><span class="font64"> It can thus be difficult to tell whether a high&#160;likelihood estimate is due to a good model or a bad AIS implementation.</span></p>
<p><span class="font64">Other fields of machine learning usually allow for some variation in the preprocessing of the data. For example, when comparing the accuracy of object recognition algorithms, it is usually acceptable to preprocess the input images&#160;slightly differently for each algorithm based on what kind of input requirements&#160;it has. Generative modeling is different because changes in preprocessing, even&#160;very small and subtle ones, are completely unacceptable. Any change to the input&#160;data changes the distribution to be captured and fundamentally alters the task.&#160;For example, multiplying the input by 0.1 will artificially increase likelihood by a&#160;factor of 10.</span></p>
<p><span class="font64">Issues with preprocessing commonly arise when benchmarking generative models on the MNIST dataset, one of the more popular generative modeling benchmarks.&#160;MNIST consists of grayscale images. Some models treat MNIST images as points&#160;in a real vector space, while others treat them as binary. Yet others treat the&#160;grayscale values as probabilities for a binary samples. It is essential to compare&#160;real-valued models only to other real-valued models and binary-valued models only&#160;to other binary-valued models. Otherwise the likelihoods measured are not on the&#160;same space. For binary-valued models, the log-likelihood can be at most zero, while&#160;for real-valued models it can be arbitrarily high, since it is the measurement of a&#160;density. Among binary models, it is important to compare models using exactly&#160;the same kind of binarization. For example, we might binarize a gray pixel to 0 or 1&#160;by thresholding at 0.5, or by drawing a random sample whose probability of being&#160;1 is given by the gray pixel intensity. If we use the random binarization, we might&#160;binarize the whole dataset once, or we might draw a different random example for&#160;each step of training and then draw multiple samples for evaluation. Each of these&#160;three schemes yields wildly different likelihood numbers, and when comparing&#160;different models it is important that both models use the same binarization scheme&#160;for training and for evaluation. In fact, researchers who apply a single random&#160;binarization step share a file containing the results of the random binarization, so&#160;that there is no difference in results based on different outcomes of the binarization&#160;step.</span></p>
<p><span class="font64">Because being able to generate realistic samples from the data distribution is one of the goals of a generative model, practitioners often evaluate generative&#160;models by visually inspecting the samples. In the best case, this is done not by the&#160;researchers themselves, but by experimental subjects who do not know the source&#160;of the samples (Denton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015). Unfortunately, it is possible for a very poor&#160;probabilistic model to produce very good samples. A common practice to verify&#160;if the model only copies some of the training examples is illustrated in Fig. 16.1.&#160;The idea is to show for some of the generated samples their nearest neighbor in&#160;the training set, according to Euclidean distance in the space of x. This test is&#160;intended to detect the case where the model overfits the training set and just&#160;reproduces training instances. It is even possible to simultaneously underfit and&#160;overfit yet still produce samples that individually look good. Imagine a generative&#160;model trained on images of dogs and cats that simply learns to reproduce the&#160;training images of dogs. Such a model has clearly overfit, because it does not&#160;produces images that were not in the training set, but it has also underfit, because&#160;it assigns no probability to the training images of cats. Yet a human observer&#160;would judge each individual image of a dog to be high quality. In this simple&#160;example, it would be easy for a human observer who can inspect many samples to&#160;determine that the cats are absent. In more realistic settings, a generative model&#160;trained on data with tens of thousands of modes may ignore a small number of&#160;modes, and a human observer would not easily be able to inspect or remember&#160;enough images to detect the missing variation.</span></p>
<p><span class="font64">Since the visual quality of samples is not a reliable guide, we often also evaluate the log-likelihood that the model assigns to the test data, when this is&#160;computationally feasible. Unfortunately, in some cases the likelihood seems not&#160;to measure any attribute of the model that we really care about. For example,&#160;real-valued models of MNIST can obtain arbitrarily high likelihood by assigning&#160;arbitrarily low variance to background pixels that never change. Models and&#160;algorithms that detect these constant features can reap unlimited rewards, even&#160;though this is not a very useful thing to do. The potential to achieve a cost&#160;approaching negative infinity is present for any kind of maximum likelihood&#160;problem with real values, but it is especially problematic for generative models of&#160;MNIST because so many of the output values are trivial to predict. This strongly&#160;suggests a need for developing other ways of evaluating generative models.</span></p>
<p><span class="font64">Theis </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) review many of the issues involved in evaluating generative models, including many of the ideas described above. They highlight the fact&#160;that there are many different uses of generative models and that the choice of&#160;metric must match the intended use of the model. For example, some generative&#160;models are better at assigning high probability to most realistic points while other&#160;generative models are better at rarely assigning high probability to unrealistic&#160;points. These differences can result from whether a generative model is designed&#160;to minimize </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">Dkl</span><span class="font64" style="font-style:italic;">(</span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">p</span><span class="font64" style="font-weight:bold;">data</span><span class="font64">||</span><span class="font64" style="font-weight:bold;">pmode1</span><span class="font64">) or </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">Dkl</span><span class="font64" style="font-style:italic;">(</span><span class="font64" style="font-weight:bold;">Pmodei </span><span class="font64">ll</span><span class="font64" style="font-weight:bold;">pdata</span><span class="font64">), as illustrated in Fig. 3.6.&#160;Unfortunately, even when we restrict the use of each metric to the task it is most&#160;suited for, all of the metrics currently in use continue to have serious weaknesses.&#160;One of the most important research topics in generative modeling is therefore not&#160;just how to improve generative models, but in fact, designing new techniques to&#160;measure our progress.</span></p><h4><a id="bookmark6"></a><span class="font65" style="font-weight:bold;">20.15 Conclusion</span></h4>
<p><span class="font64">Training generative models with hidden units is a powerful way to make models understand the world represented in the given training data. By learning a model&#160;</span><span class="font64" style="font-weight:bold;">pm<sub>o</sub>d</span><span class="font20"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">(x) and a representation </span><span class="font64" style="font-weight:bold;">p<sub>mo</sub>d</span><span class="font20"><sub>e</sub>1</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">(h | </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> ),a generative model can provide&#160;answers to many inference problems about the relationships between input variables&#160;in x and can provide many different ways of representing x by taking expectations&#160;of h at different layers of the hierarchy. Generative models hold the promise to&#160;provide AI systems with a framework for all of the many different intuitive concepts&#160;they need to understand, and the ability to reason about these concepts in the&#160;face of uncertainty. We hope that our readers will find new ways to make these</span></p>
<p><span class="font64">approaches more powerful and continue the journey to understanding the principles that underlie learning and intelligence.</span></p>
</body>
</html>