<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Index</span></h2>
<p><span class="font64">0-1 loss, </span><span class="font26" style="font-weight:bold;">104</span><span class="font64">, 276</span></p>
<p><span class="font64">Absolute value rectification, 192 Accuracy, 425&#160;Activation function, 170&#160;Active constraint, 95&#160;AdaGrad, 307</span></p>
<p><span class="font64">ADALINE, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> adaptive linear element Adam, 308, 427</span></p>
<p><span class="font64">Adaptive linear element, 15, 24, 27 Adversarial example, 268&#160;Adversarial training, 268, 271, 532&#160;Affine, 110</span></p>
<p><span class="font64">AIS, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> annealed importance sampling Almost everywhere, 71&#160;Almost sure convergence, 130&#160;Ancestral sampling, 582, 597&#160;ANN, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> Artificial neural network&#160;Annealed importance sampling, 627, 670,&#160;719</span></p>
<p><span class="font64">Approximate Bayesian computation, 718 Approximate inference, 585&#160;Artificial intelligence, 1</span></p>
<p><span class="font64">Artificial neural network, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> Neural network</span></p>
<p><span class="font64">ASR, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> automatic speech recognition Asymptotically unbiased, 124&#160;Audio, 102, 360, 460&#160;Autoencoder, 4, 356, </span><span class="font26" style="font-weight:bold;">504&#160;</span><span class="font64">Automatic speech recognition, 460</span></p>
<p><span class="font64">Back-propagation, 203 Back-propagation through time, </span><span class="font26" style="font-weight:bold;">384&#160;</span><span class="font64">Backprop, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> back-propagation</span></p>
<p><span class="font64">Bag of words, 473 Bagging, 256</span></p>
<p><span class="font64">Batch normalization, 268, 427 Bayes error, </span><span class="font26" style="font-weight:bold;">117&#160;</span><span class="font64">Bayes’ rule, 70</span></p>
<p><span class="font64">Bayesian hyperparameter optimization, 438 Bayesian network, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> directed graphical&#160;model</span></p>
<p><span class="font64">Bayesian probability, 55</span></p>
<p><span class="font64">Bayesian statistics, </span><span class="font26" style="font-weight:bold;">135</span></p>
<p><span class="font64">Belief network, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> directed graphical model</span></p>
<p><span class="font64">Bernoulli distribution, 62</span></p>
<p><span class="font64">BFGS, 316</span></p>
<p><span class="font64">Bias, 124, 229</span></p>
<p><span class="font64">Bias parameter, 110</span></p>
<p><span class="font64">Biased importance sampling, 595</span></p>
<p><span class="font64">Bigram, 464</span></p>
<p><span class="font64">Binary relation, 484</span></p>
<p><span class="font64">Block Gibbs sampling, 601</span></p>
<p><span class="font64">Boltzmann distribution, 572</span></p>
<p><span class="font64">Boltzmann machine, 572, 656</span></p>
<p><span class="font64">BPTT, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> back-propagation through time</span></p>
<p><span class="font64">Broadcasting, 34</span></p>
<p><span class="font64">Burn-in, 599</span></p>
<p><span class="font64">CAE, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> contractive autoencoder Calculus of variations, 179&#160;Categorical distribution, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> multinoulli distribution</span></p>
<p><span class="font64">CD, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> contrastive divergence Centering trick (DBM), 675&#160;Central limit theorem, 63&#160;Chain rule (calculus), 206&#160;Chain rule of probability, 59</span></p>
<p><span class="font64">Chess, 2</span></p>
<p><span class="font64">Chord, 581</span></p>
<p><span class="font64">Chordal graph, 581</span></p>
<p><span class="font64">Class-based language models, 465</span></p>
<p><span class="font64">Classical dynamical system, 375</span></p>
<p><span class="font64">Classification, 100</span></p>
<p><span class="font64">Clique potential, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> factor (graphical model)</span></p>
<p><span class="font64">CNN, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> convolutional neural network</span></p>
<p><span class="font64">Collaborative Filtering, 480</span></p>
<p><span class="font64">Collider, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> explaining away</span></p>
<p><span class="font64">Color images, 360</span></p>
<p><span class="font64">Complex cell, 365</span></p>
<p><span class="font64">Computational graph, 204</span></p>
<p><span class="font64">Computer vision, 454</span></p>
<p><span class="font64">Concept drift, 540</span></p>
<p><span class="font64">Condition number, 279</span></p>
<p><span class="font64">Conditional computation, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> dynamic structure</span></p>
<p><span class="font64">Conditional independence, xiii, 60 Conditional probability, 59&#160;Conditional RBM, 687&#160;Connectionism, 17, 445&#160;Connectionist temporal classification, 462&#160;Consistency, 130, 515&#160;Constrained optimization, 93, 237&#160;Content-based addressing, 421&#160;Content-based recommender systems, 482&#160;Context-specific independence, 575&#160;Contextual bandits, 482&#160;Continuation methods, 327&#160;Contractive autoencoder, 523&#160;Contrast, 456</span></p>
<p><span class="font64">Contrastive divergence, 291, 612, 674 Convex optimization, 141&#160;Convolution, 330, 685&#160;Convolutional network, 16&#160;Convolutional neural network, 254, </span><span class="font26" style="font-weight:bold;">330</span><span class="font64">, 427,&#160;462</span></p>
<p><span class="font64">Coordinate descent, 321, 673 Correlation, 61</span></p>
<p><span class="font64">Cost function, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> objective function Covariance, xiii, 61&#160;Covariance matrix, 62&#160;Coverage, 426</span></p>
<p><span class="font64">Critical temperature, 605 Cross-correlation, 332&#160;Cross-entropy, </span><span class="font26" style="font-weight:bold;">75</span><span class="font64">, 132&#160;Cross-validation, 122</span></p>
<p><span class="font64">CTC, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> connectionist temporal classification</span></p>
<p><span class="font64">Curriculum learning, 328 Curse of dimensionality, 154&#160;Cyc, 2</span></p>
<p><span class="font64">D-separation, 574</span></p>
<p><span class="font64">DAE, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> denoising autoencoder</span></p>
<p><span class="font64">Data generating distribution, </span><span class="font26" style="font-weight:bold;">111</span><span class="font64">, 131</span></p>
<p><span class="font64">Data generating process, 111</span></p>
<p><span class="font64">Data parallelism, 449</span></p>
<p><span class="font64">Dataset, 105</span></p>
<p><span class="font64">Dataset augmentation, 271, 459 DBM, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> deep Boltzmann machine&#160;DCGAN, 553, 554, 703&#160;Decision tree, </span><span class="font26" style="font-weight:bold;">145</span><span class="font64">, 550&#160;Decoder, 4</span></p>
<p><span class="font64">Deep belief network, 27, 531, 633, 659, 662, 686, 694&#160;Deep Blue, 2</span></p>
<p><span class="font64">Deep Boltzmann machine, 24, 27, 531, 633, 654, 659, 665, 674, 686&#160;Deep feedforward network, 167, 427&#160;Deep learning, 2, 5&#160;Denoising autoencoder, 512, 691&#160;Denoising score matching, 621&#160;Density estimation, 103&#160;Derivative, xiii, 83&#160;Design matrix, </span><span class="font26" style="font-weight:bold;">106&#160;</span><span class="font64">Detector layer, 339&#160;Determinant, xii&#160;Diagonal matrix, 41&#160;Differential entropy, 74, 648&#160;Dirac delta function, 65&#160;Directed graphical model, 77, 509, 565, 694&#160;Directional derivative, 85&#160;Discriminative fine-tuning, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> supervised&#160;fine-tuning</span></p>
<p><span class="font64">Discriminative RBM, 688 Distributed representation, 17, 150, 548&#160;Domain adaptation, 538</span></p>
<p><span class="font64">Dot product, 34, 141 Double backprop, 271&#160;Doubly block circulant matrix, 333&#160;Dream sleep, 611, 654&#160;DropConnect, 266</span></p>
<p><span class="font64">Dropout, </span><span class="font26" style="font-weight:bold;">258</span><span class="font64">, 427, 432, 433, 674, 691 Dynamic structure, 450, 451</span></p>
<p><span class="font64">E-step, 636</span></p>
<p><span class="font64">Early stopping, 246, 247, 249, 250, 427 EBM, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> energy-based model&#160;Echo state network, 24, 27, 405&#160;Effective capacity, 114&#160;Eigendecomposition, 42&#160;Eigenvalue, 42&#160;Eigenvector, 42</span></p>
<p><span class="font64">ELBO, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> evidence lower bound Element-wise product, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> Hadamard product, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> Hadamard product&#160;EM, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> expectation maximization&#160;Embedding, 518&#160;Empirical distribution, 66&#160;Empirical risk, 276&#160;Empirical risk minimization, 276&#160;Encoder, 4&#160;Energy function, 571&#160;Energy-based model, 571, 597, 656, 665&#160;Ensemble methods, 256&#160;Epoch, 246</span></p>
<p><span class="font64">Equality constraint, 94 Equivariance, 338</span></p>
<p><span class="font64">Error function, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> objective function ESN, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> echo state network&#160;Euclidean norm, 39&#160;Euler-Lagrange equation, 648&#160;Evidence lower bound, 635, 663&#160;Example, 99&#160;Expectation, 60&#160;Expectation maximization, 636&#160;Expected value, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> expectation&#160;Explaining away, 576, 633, 646&#160;Exploitation, 483&#160;Exploration, 483&#160;Exponential distribution, </span><span class="font26" style="font-weight:bold;">65</span></p>
<p><span class="font64">F-score, 425</span></p>
<p><span class="font64">Factor (graphical model), 569</span></p>
<p><span class="font64">Factor analysis, 492</span></p>
<p><span class="font64">Factor graph, 581</span></p>
<p><span class="font64">Factors of variation, 4</span></p>
<p><span class="font64">Feature, 99</span></p>
<p><span class="font64">Feature selection, 236</span></p>
<p><span class="font64">Feedforward neural network, 167</span></p>
<p><span class="font64">Fine-tuning, 323</span></p>
<p><span class="font64">Finite differences, 441</span></p>
<p><span class="font64">Forget gate, 306</span></p>
<p><span class="font64">Forward propagation, 203</span></p>
<p><span class="font64">Fourier transform, 360, 362</span></p>
<p><span class="font64">Fovea, 366</span></p>
<p><span class="font64">FPCD, 616</span></p>
<p><span class="font64">Free energy, </span><span class="font26" style="font-weight:bold;">573</span><span class="font64">, 682</span></p>
<p><span class="font64">Freebase, 485</span></p>
<p><span class="font64">Frequentist probability, 55</span></p>
<p><span class="font64">Frequentist statistics, </span><span class="font26" style="font-weight:bold;">135</span></p>
<p><span class="font64">Frobenius norm, 46</span></p>
<p><span class="font64">Fully-visible Bayes network, 707</span></p>
<p><span class="font64">Functional derivatives, 647</span></p>
<p><span class="font64">FVBN, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> fully-visible Bayes network</span></p>
<p><span class="font64">Gabor function, 368</span></p>
<p><span class="font64">GANs, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> generative adversarial networks Gated recurrent unit, 427&#160;Gaussian distribution, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> normal distribution</span></p>
<p><span class="font64">Gaussian kernel, 142</span></p>
<p><span class="font64">Gaussian mixture, 67, 188</span></p>
<p><span class="font64">GCN, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> global contrast normalization</span></p>
<p><span class="font64">GeneOntology, 485</span></p>
<p><span class="font64">Generalization, 110</span></p>
<p><span class="font64">Generalized Lagrange function, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> generalized Lagrangian Generalized Lagrangian, 94&#160;Generative adversarial networks, 691, 702&#160;Generative moment matching networks, 705&#160;Generator network, 695&#160;Gibbs distribution, 570&#160;Gibbs sampling, 583, 601&#160;Global contrast normalization, 456&#160;GPU, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> graphics processing unit&#160;Gradient, 84</span></p>
<p><span class="font64">Gradient clipping, 289, 416 Gradient descent, 83, 85&#160;Graph, xii</span></p>
<p><span class="font64">Graphical model, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> structured probabilistic model</span></p>
<p><span class="font64">Graphics processing unit, 446 Greedy algorithm, 323</span></p>
<p><span class="font64">Greedy layer-wise unsupervised pretraining, 530</span></p>
<p><span class="font64">Greedy supervised pretraining, 323 Grid search, 434</span></p>
<p><span class="font64">Hadamard product, xii, 34 Hard tanh, 196</span></p>
<p><span class="font64">Harmonium, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> restricted Boltzmann machine</span></p>
<p><span class="font64">Harmony theory, 573</span></p>
<p><span class="font64">Helmholtz free energy, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> evidence lower bound&#160;Hessian, 223&#160;Hessian matrix, xiii, 87&#160;Heteroscedastic, 187&#160;Hidden layer, 6, 167&#160;Hill climbing, 86</span></p>
<p><span class="font64">Hyperparameter optimization, 434 Hyperparameters, 120, 432&#160;Hypothesis space, 112, 118</span></p>
<p><span class="font64">i.i.d. assumptions, 111, 122, 268 Identity matrix, 36</span></p>
<p><span class="font64">ILSVRC, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> ImageNet Large-Scale Visual Recognition Challenge&#160;ImageNet Large-Scale Visual Recognition&#160;Challenge, 23&#160;Immorality, 579</span></p>
<p><span class="font64">Importance sampling, 594, 626, 700 Importance weighted autoencoder, 700&#160;Independence, xiii, 60</span></p>
<p><span class="font64">Independent and identically distributed, </span><span class="font64" style="font-style:italic;">see </span><span class="font64">i.i.d. assumptions</span></p>
<p><span class="font64">Independent component analysis, 493 Independent subspace analysis, 495&#160;Inequality constraint, 94&#160;Inference, 564, 585, 633, 635, 637, 640, 650,&#160;653</span></p>
<p><span class="font64">Information retrieval, 527 Initialization, 301&#160;Integral, xiii&#160;Invariance, 342&#160;Isotropic, 65</span></p>
<p><span class="font64">Jacobian matrix, xiii, 72, 86 Joint probability, 57</span></p>
<p><span class="font64">k-means, 364, 548 k-nearest neighbors, </span><span class="font26" style="font-weight:bold;">143</span><span class="font64">, 550&#160;Karush-Kuhn-Tucker conditions, 95, 237&#160;Karush-Kuhn-Tucker, 94&#160;Kernel (convolution), 331, 332&#160;Kernel machine, 550&#160;Kernel trick, 141&#160;KKT, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> Karush-Kuhn-Tucker&#160;KKT conditions, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> Karush-Kuhn-Tucker&#160;conditions</span></p>
<p><span class="font64">KL divergence, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> Kullback-Leibler divergence</span></p>
<p><span class="font64">Knowledge base, 2, 485 Krylov methods, 223&#160;Kullback-Leibler divergence, xiii, </span><span class="font26" style="font-weight:bold;">74</span></p>
<p><span class="font64">Label smoothing, 243 Lagrange multipliers, 94, 648&#160;Lagrangian, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> generalized Lagrangian&#160;LAPGAN, 704</span></p>
<p><span class="font64">Laplace distribution, </span><span class="font26" style="font-weight:bold;">65</span><span class="font64">, 498, 499</span></p>
<p><span class="font64">Latent variable, 67</span></p>
<p><span class="font64">Layer (neural network), 167</span></p>
<p><span class="font64">LCN, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> local contrast normalization</span></p>
<p><span class="font64">Leaky ReLU, 192</span></p>
<p><span class="font64">Leaky units, 408</span></p>
<p><span class="font64">Learning rate, 85</span></p>
<p><span class="font64">Line search, 85, 86, 93</span></p>
<p><span class="font64">Linear combination, 37</span></p>
<p><span class="font64">Linear dependence, 38</span></p>
<p><span class="font64">Linear factor models, 491</span></p>
<p><span class="font64">Linear regression, </span><span class="font26" style="font-weight:bold;">107</span><span class="font64">, 110, 140</span></p>
<p><span class="font64">Link prediction, 486</span></p>
<p><span class="font64">Lipschitz constant, 92</span></p>
<p><span class="font64">Lipschitz continuous, 92</span></p>
<p><span class="font64">Liquid state machine, 405</span></p>
<p><span class="font64">Local conditional probability distribution, 566</span></p>
<p><span class="font64">Local contrast normalization, 458 Logistic regression, 3, </span><span class="font26" style="font-weight:bold;">140</span><span class="font64">, 140&#160;Logistic sigmoid, 7, 67&#160;Long short-term memory, 18, 25, 306, </span><span class="font26" style="font-weight:bold;">410</span><span class="font64">,&#160;427</span></p>
<p><span class="font64">Loop, 581</span></p>
<p><span class="font64">Loopy belief propagation, 587 Loss function, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> objective function&#160;L<sup>p</sup> norm, 39</span></p>
<p><span class="font64">LSTM, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> long short-term memory</span></p>
<p><span class="font64">M-step, 636</span></p>
<p><span class="font64">Machine learning, 2</span></p>
<p><span class="font64">Machine translation, 101</span></p>
<p><span class="font64">Main diagonal, 33</span></p>
<p><span class="font64">Manifold, 160</span></p>
<p><span class="font64">Manifold hypothesis, 161</span></p>
<p><span class="font64">Manifold learning, 161</span></p>
<p><span class="font64">Manifold tangent classifier, 272</span></p>
<p><span class="font64">MAP approximation, 138, 507</span></p>
<p><span class="font64">Marginal probability, 58</span></p>
<p><span class="font64">Markov chain, 597</span></p>
<p><span class="font64">Markov chain Monte Carlo, 597</span></p>
<p><span class="font64">Markov network, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> undirected model</span></p>
<p><span class="font64">Markov random field, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> undirected model</span></p>
<p><span class="font64">Matrix, xi, xii, 32</span></p>
<p><span class="font64">Matrix inverse, 36</span></p>
<p><span class="font64">Matrix product, 34</span></p>
<p><span class="font64">Max norm, 40</span></p>
<p><span class="font64">Max pooling, 339</span></p>
<p><span class="font64">Maximum likelihood, </span><span class="font26" style="font-weight:bold;">131</span></p>
<p><span class="font64">Maxout, 192, 427</span></p>
<p><span class="font64">MCMC, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> Markov chain Monte Carlo Mean field, 640, 641, 674&#160;Mean squared error, 108&#160;Measure theory, 71&#160;Measure zero, 71&#160;Memory network, 418, 420&#160;Method of steepest descent, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> gradient&#160;descent</span></p>
<p><span class="font64">Minibatch, 279 Missing inputs, 100&#160;Mixing (Markov chain), 603</span></p>
<p><span class="font64">Mixture density networks, 188 Mixture distribution, 66&#160;Mixture model, 188, 512&#160;Mixture of experts, 452, 550&#160;MLP, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> multilayer perception&#160;MNIST, 21, 22, 674&#160;Model averaging, 256&#160;Model compression, 450&#160;Model identifiability, 284&#160;Model parallelism, 449&#160;Moment matching, 705&#160;Moore-Penrose pseudoinverse, 45, 239&#160;Moralized graph, 579&#160;MP-DBM, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> multi-prediction DBM&#160;MRF (Markov Random Field), </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> undi■&#160;rected model</span></p>
<p><span class="font64">MSE, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> mean squared error Multi-modal learning, 541&#160;Multi-prediction DBM, 676&#160;Multi-task learning, 244, 540&#160;Multilayer perception, 5&#160;Multilayer perceptron, 27&#160;Multinomial distribution, 62&#160;Multinoulli distribution, 62</span></p>
<p><span class="font64">n-gram, </span><span class="font26" style="font-weight:bold;">463 </span><span class="font64">NADE, 710&#160;Naive Bayes, 3&#160;Nat, 73</span></p>
<p><span class="font64">Natural image, 561 Natural language processing, 463&#160;Nearest neighbor regression, </span><span class="font26" style="font-weight:bold;">115&#160;</span><span class="font64">Negative definite, 89&#160;Negative phase, 472, 608, 610&#160;Neocognitron, 16, 24, 27, 367&#160;Nesterov momentum, 300&#160;Netflix Grand Prize, 258, 481&#160;Neural language model, 465, 478&#160;Neural network, 13&#160;Neural Turing machine, 420&#160;Neuroscience, 15&#160;Newton’s method, 89, 310&#160;NLM, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> neural language model&#160;NLP, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> natural language processing&#160;No free lunch theorem, 116</span></p>
<p><span class="font64">Noise-contrastive estimation, 622 Non-parametric model, </span><span class="font26" style="font-weight:bold;">114&#160;</span><span class="font64">Norm, xiv, 39</span></p>
<p><span class="font64">Normal distribution, 63, 64, 125 Normal equations, </span><span class="font26" style="font-weight:bold;">109</span><span class="font64">, 109, 112, 234&#160;Normalized initialization, 303&#160;Numerical differentiation, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> finite differences</span></p>
<p><span class="font64">Object detection, 455</span></p>
<p><span class="font64">Object recognition, 455</span></p>
<p><span class="font64">Objective function, 82</span></p>
<p><span class="font64">OMP-k, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> orthogonal matching pursuit</span></p>
<p><span class="font64">One-shot learning, 540</span></p>
<p><span class="font64">Operation, 204</span></p>
<p><span class="font64">Optimization, 80, 82</span></p>
<p><span class="font64">Orthodox statistics, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> frequentist statistics Orthogonal matching pursuit, 27, </span><span class="font26" style="font-weight:bold;">255&#160;</span><span class="font64">Orthogonal matrix, 42&#160;Orthogonality, 41&#160;Output layer, 167</span></p>
<p><span class="font64">Parallel distributed processing, 17</span></p>
<p><span class="font64">Parameter initialization, 301, 407</span></p>
<p><span class="font64">Parameter sharing, 253, 335, 373, 375, 389</span></p>
<p><span class="font64">Parameter tying, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> Parameter sharing</span></p>
<p><span class="font64">Parametric model, </span><span class="font26" style="font-weight:bold;">114</span></p>
<p><span class="font64">Parametric ReLU, 192</span></p>
<p><span class="font64">Partial derivative, 84</span></p>
<p><span class="font64">Partition function, 570, 607, 671</span></p>
<p><span class="font64">PCA, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> principal components analysis</span></p>
<p><span class="font64">PCD, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> stochastic maximum likelihood</span></p>
<p><span class="font64">Perceptron, 15, 27</span></p>
<p><span class="font64">Persistent contrastive divergence, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> stochastic maximum likelihood Perturbation analysis, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> reparametrization&#160;trick</span></p>
<p><span class="font64">Point estimator, 122</span></p>
<p><span class="font64">Policy, 482</span></p>
<p><span class="font64">Pooling, 330, 685</span></p>
<p><span class="font64">Positive definite, 89</span></p>
<p><span class="font64">Positive phase, 472, 608, 610, 658, 670</span></p>
<p><span class="font64">Precision, 425</span></p>
<p><span class="font64">Precision (of a normal distribution), 63, 65 Predictive sparse decomposition, 525</span></p>
<p><span class="font64">Preprocessing, 455 Pretraining, 323, 530&#160;Primary visual cortex, 365&#160;Principal components analysis, 48, 146-148,&#160;492, 633</span></p>
<p><span class="font64">Prior probability distribution, </span><span class="font26" style="font-weight:bold;">135 </span><span class="font64">Probabilistic max pooling, 685&#160;Probabilistic PCA, 492, 493, 634&#160;Probability density function, 58&#160;Probability distribution, 56&#160;Probability mass function, 56&#160;Probability mass function estimation, 103&#160;Product of experts, 572&#160;Product rule of probability, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> chain rule&#160;of probability</span></p>
<p><span class="font64">PSD, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> predictive sparse decomposition Pseudolikelihood, 617</span></p>
<p><span class="font64">Quadrature pair, 369 Quasi-Newton methods, 316</span></p>
<p><span class="font64">Radial basis function, 196 Random search, 436&#160;Random variable, 56&#160;Ratio matching, 620&#160;RBF, 196</span></p>
<p><span class="font64">RBM, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> restricted Boltzmann machine Recall, 425&#160;Receptive field, 337&#160;Recommender Systems, 480&#160;Rectified linear unit, 171, 192, 427, 509&#160;Recurrent network, 27&#160;Recurrent neural network, 378&#160;Regression, 101</span></p>
<p><span class="font64">Regularization, </span><span class="font26" style="font-weight:bold;">120</span><span class="font64">, 120, 177, 228, 432 Regularizer, 119&#160;REINFORCE, 691</span></p>
<p><span class="font64">Reinforcement learning, 25, 106, 482, 691 Relational database, 485&#160;Reparametrization trick, 690&#160;Representation learning, 3&#160;Representational capacity, 114&#160;Restricted Boltzmann machine, 356, 461,&#160;481, 589, 633, 658, 659, 674, 678,&#160;680, 682, 685</span></p>
<p><span class="font64">Ridge regression, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> weight decay Risk, 275&#160;RNN-RBM, 687</span></p>
<p><span class="font64">Saddle points, 285</span></p>
<p><span class="font64">Sample mean, 125</span></p>
<p><span class="font64">Scalar, xi, xii, 31</span></p>
<p><span class="font64">Score matching, 515, 619</span></p>
<p><span class="font64">Second derivative, 86</span></p>
<p><span class="font64">Second derivative test, 89</span></p>
<p><span class="font64">Self-information, 73</span></p>
<p><span class="font64">Semantic hashing, 527</span></p>
<p><span class="font64">Semi-supervised learning, 243</span></p>
<p><span class="font64">Separable convolution, 362</span></p>
<p><span class="font64">Separation (probabilistic modeling), 574</span></p>
<p><span class="font64">Set, xii</span></p>
<p><span class="font64">SGD, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> stochastic gradient descent Shannon entropy, xiii, 73&#160;Shortlist, 468</span></p>
<p><span class="font64">Sigmoid, xiv, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> logistic sigmoid Sigmoid belief network, 27&#160;Simple cell, 365</span></p>
<p><span class="font64">Singular value, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> singular value decomposition</span></p>
<p><span class="font64">Singular value decomposition, 44, 148, 481 Singular vector, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> singular value decomposition</span></p>
<p><span class="font64">Slow feature analysis, 495</span></p>
<p><span class="font64">SML, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> stochastic maximum likelihood</span></p>
<p><span class="font64">Softmax, 183, 420, 452</span></p>
<p><span class="font64">Softplus, xiv, 68, 196</span></p>
<p><span class="font64">Spam detection, 3</span></p>
<p><span class="font64">Sparse coding, 321, 356, 498, 633, 694 Sparse initialization, 304, 407&#160;Sparse representation, 146, 226, 254, 507,&#160;558</span></p>
<p><span class="font64">Spearmint, 438 Spectral radius, 406</span></p>
<p><span class="font64">Speech recognition, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> automatic speech recognition</span></p>
<p><span class="font64">Sphering, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> whitening Spike and slab restricted Boltzmann machine, 682</span></p>
<p><span class="font64">SPN, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> sum-product network Square matrix, 38&#160;ssRBM, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> spike and slab restricted Boltzmann machine&#160;Standard deviation, 61&#160;Standard error, 127&#160;Standard error of the mean, 128, 278&#160;Statistic, 122</span></p>
<p><span class="font64">Statistical learning theory, 110 Steepest descent, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> gradient descent&#160;Stochastic back-propagation, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> reparametriza■&#160;tion trick</span></p>
<p><span class="font64">Stochastic gradient descent, 15, 150, 279, </span><span class="font26" style="font-weight:bold;">294</span><span class="font64">, 674</span></p>
<p><span class="font64">Stochastic maximum likelihood, 614, 674 Stochastic pooling, 266&#160;Structure learning, 584&#160;Structured output, 101, 687&#160;Structured probabilistic model, 77, 560&#160;Sum rule of probability, 58&#160;Sum-product network, 555&#160;Supervised fine-tuning, 531, 664&#160;Supervised learning, </span><span class="font26" style="font-weight:bold;">105&#160;</span><span class="font64">Support vector machine, 140&#160;Surrogate loss function, 276&#160;SVD, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> singular value decomposition&#160;Symmetric matrix, 41, 43</span></p>
<p><span class="font64">Tangent distance, 270 Tangent plane, 518&#160;Tangent prop, 270</span></p>
<p><span class="font64">TDNN, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> time-delay neural network Teacher forcing, 382, 383&#160;Tempering, 605&#160;Template matching, 141&#160;Tensor, xi, xii, 33&#160;Test set, 110</span></p>
<p><span class="font64">Tikhonov regularization, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> weight decay</span></p>
<p><span class="font64">Tiled convolution, 352</span></p>
<p><span class="font64">Time-delay neural network, 368, 374</span></p>
<p><span class="font64">Toeplitz matrix, 333</span></p>
<p><span class="font64">Topographic ICA, 495</span></p>
<p><span class="font64">Trace operator, 46</span></p>
<p><span class="font64">Training error, 110</span></p>
<p><span class="font64">Transcription, 101</span></p>
<p><span class="font64">Transfer learning, 538</span></p>
<p><span class="font64">Transpose, xii, 33</span></p>
<table border="1">
<tr><td>
<p><span class="font64">Triangle inequality, 39 Triangulated graph, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> chordal graph&#160;Trigram, 464</span></p>
<p><span class="font64">Unbiased, 124</span></p>
<p><span class="font64">Undirected graphical model, 77, 509</span></p>
<p><span class="font64">Undirected model, 568</span></p>
<p><span class="font64">Uniform distribution, 57</span></p>
<p><span class="font64">Unigram, 464</span></p>
<p><span class="font64">Unit norm, 41</span></p>
<p><span class="font64">Unit vector, 41</span></p>
<p><span class="font64">Universal approximation theorem, 197 Universal approximator, 555&#160;Unnormalized probability distribution, 569&#160;Unsupervised learning, </span><span class="font26" style="font-weight:bold;">105</span><span class="font64">, 146&#160;Unsupervised pretraining, 461, 530</span></p>
<p><span class="font64">V-structure, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> explaining away V1, 365</span></p>
<p><span class="font64">VAE, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> variational autoencoder Vapnik-Chervonenkis dimension, 114&#160;Variance, xiii, 61, 229&#160;Variational autoencoder, 691, </span><span class="font26" style="font-weight:bold;">698&#160;</span><span class="font64">Variational derivatives, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> functional derivatives</span></p>
<p><span class="font64">Variational free energy, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> evidence lower bound</span></p>
<p><span class="font64">VC dimension, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> Vapnik-Chervonenkis dimension Vector, xi, xii, 32&#160;Virtual adversarial examples, 269&#160;Visible layer, 6&#160;Volumetric data, 360</span></p>
<p><span class="font64">Wake-sleep, 653, 663</span></p>
<p><span class="font64">Weight decay, </span><span class="font26" style="font-weight:bold;">118</span><span class="font64">, 177, 231, 433</span></p>
<p><span class="font64">Weight space symmetry, 284</span></p>
<p><span class="font64">Weights, 15, 107</span></p>
<p><span class="font64">Whitening, 457</span></p>
<p><span class="font64">Wikibase, 485</span></p>
<p><span class="font64">Wikibase, 485</span></p>
<p><span class="font64">Word embedding, 466</span></p>
<p><span class="font64">Word-sense disambiguation, 486</span></p>
<p><span class="font64">WordNet, 485</span></p>
<p><span class="font64">Zero-data learning, </span><span class="font64" style="font-style:italic;">see</span><span class="font64"> zero-shot learning</span></p></td><td>
<p><span class="font64">Zero-shot learning, 540</span></p></td></tr>
</table>
</body>
</html>