<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h5><a id="bookmark0"></a><span class="font64" style="font-weight:bold;">18.7.1 Annealed Importance Sampling</span></h5>
<p><span class="font64">In situations where Dkl(poIIpi) is large (i.e., where there is little overlap between p</span><span class="font64" style="font-weight:bold;">o </span><span class="font64">and p </span><span class="font62" style="font-weight:bold;">1</span><span class="font64">), a strategy called </span><span class="font64" style="font-weight:bold;font-style:italic;">annealed importance sampling</span><span class="font64"> (AIS) attempts to&#160;bridge the gap by introducing intermediate distributions (Jarzynski, 1997; Neal,&#160;2001). Consider a sequence of distributions p^ ,... </span><span class="font64" style="font-weight:bold;font-style:italic;">,p</span><span class="font64"><sub>Vn</sub>, with 0 = n</span><span class="font64" style="font-weight:bold;">o </span><span class="font64">&lt; n</span><span class="font64" style="font-weight:bold;">i </span><span class="font64">&lt; ־ ־ ־ &lt;&#160;n<sub>n-</sub></span><span class="font62" style="font-weight:bold;">1 </span><span class="font64">&lt; n<sub>n</sub> = </span><span class="font18">1</span><span class="font64"> so that the first and last distributions in the sequence are p</span><span class="font64" style="font-weight:bold;">o </span><span class="font64">and p</span><span class="font64" style="font-weight:bold;">i&#160;</span><span class="font64">respectively.</span></p>
<p><span class="font64">This approach allows us to estimate the partition function of a multimodal distribution defined over a high-dimensional space (such as the distribution defined&#160;by a trained RBM). We begin with a simpler model with a known partition function&#160;(such as an RBM with zeroes for weights) and estimate the ratio between the two&#160;model’s partition functions. The estimate of this ratio is based on the estimate&#160;of the ratios of a sequence of many similar distributions, such as the sequence of&#160;RBMs with weights interpolating between zero and the learned weights.</span></p><div><div><img src="main-183.jpg" alt=""/>
<p><span class="font64">sampling and then use these to obtain an estimate of </span><span class="font64" style="font-weight:bold;font-style:italic;">Z .</span></p></div></div>
<p><span class="font64">Where do these intermediate distributions come from? Just as the original proposal distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">po</span><span class="font64"> is a design choice, so is the sequence of distributions&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">Pn</span><span class="font64"> <sub>1</sub>.. •Pn</span><span class="font64" style="font-weight:bold;font-style:italic;">״_<sub>v</sub></span><span class="font64"> That is, it can be specifically constructed to suit the problem domain.&#160;One general-purpose and popular choice for the intermediate distributions is to&#160;use the weighted geometric average of the target distribution pi and the starting&#160;proposal distribution (for which the partition function is known) po:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Pn j</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">nj 1-n j </span><span class="font64">rc </span><span class="font64" style="font-weight:bold;font-style:italic;">pi Po</span></p></div><div>
<p><span class="font64">(18.50)</span></p></div>
<p><span class="font64">In order to sample from these intermediate distributions, we define a series of Markov chain transition functionsTn</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>J</sub>(x<sup>/</sup></span><span class="font64"> | </span><span class="font64" style="font-weight:bold;font-style:italic;">x)</span><span class="font64"> that define the conditional probability&#160;distribution of transitioning to </span><span class="font64" style="font-weight:bold;font-style:italic;">x'</span><span class="font64"> given we are currently at x. The transition&#160;operator </span><span class="font64" style="font-weight:bold;font-style:italic;">T<sub>nj</sub></span><span class="font64"> </span><span class="font64" style="font-weight:bold;">(</span><span class="font64">x</span><span class="font64" style="font-weight:bold;">' </span><span class="font64">| x</span><span class="font64" style="font-weight:bold;">) </span><span class="font64">is defined to leave p^• </span><span class="font64" style="font-weight:bold;">(</span><span class="font64">x</span><span class="font64" style="font-weight:bold;">) </span><span class="font64">invariant:</span></p>
<p><span class="font64">pn<sub>j</sub> </span><span class="font64" style="font-weight:bold;">(</span><span class="font64">x</span><span class="font64" style="font-weight:bold;">) </span><span class="font64" style="font-weight:bold;font-style:italic;">= </span><span class="font12" style="font-weight:bold;font-style:italic;">J</span><span class="font64">pn<sub>j</sub></span><span class="font64" style="font-weight:bold;">(</span><span class="font64">x</span><span class="font64" style="font-weight:bold;">')</span><span class="font64">T</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>r1j</sub> (x</span><span class="font64"> | x</span><span class="font64" style="font-weight:bold;">') </span><span class="font64" style="font-weight:bold;font-style:italic;">dx</span><span class="font12" style="font-weight:bold;font-style:italic;">'</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;</span><span class="font64">(18.51)</span></p>
<p><span class="font64">These transitions may be constructed as any Markov chain Monte Carlo method (e.g., Metropolis-Hastings, Gibbs), including methods involving multiple passes&#160;through all of the random variables or other kinds of iterations.</span></p>
<p><span class="font64">The AIS sampling strategy is then to generate samples from po and then use the transition operators to sequentially generate samples from the intermediate&#160;distributions until we arrive at samples from the target distribution pi:</span></p>
<p><span class="font64">• for k = 1... K</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">-</span><span class="font64"> Sample </span><span class="font64" style="font-weight:bold;font-style:italic;">xtf!</span><span class="font64"> ~ p</span><span class="font18"><sub>0</sub></span><span class="font64"> (x)</span></p>
<p><span class="font64">For sample k, we can derive the importance weight by chaining together the importance weights for the jumps between the intermediate distributions given in&#160;Eq. 18.49:</span></p><div>
<p><span class="font63">(k)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">(k)</span><span class="font64"> _ </span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:underline;">K</span><span class="font64" style="text-decoration:underline;"><sup>(x</sup> 7r) </span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:underline;">Pn2<sup>(x</sup> 772</span><span class="font64" style="text-decoration:underline;">) &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:underline;"><sup>p</sup></span><span class="font64" style="text-decoration:underline;"> (<sup>x</sup>i<sup>fc))</sup></span></p></div><div>
<p><span class="font64">(18.52)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">(k)</span></p><div>
<p><span class="font64">w</span></p></div><div>
<p><span class="font64">— &#160;&#160;&#160;Sample x^-</span></p>
<p><span class="font64">— &#160;&#160;&#160;Sample x7<sup>k)</sup></span></p>
<p><span class="font64">end</span></p></div><div>
<p><span class="font63">(k)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">7n ^ <sup>T</sup>7n</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">^ t</span><span class="font64"> &#160;&#160;&#160;(<sub>x</sub>7<sup>k)</sup> <sub>x</sub>7<sup>k)</sup> )</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>T</sup>7n-2<sup>(X</sup></span><span class="font64"> 7n-</span><span class="font64" style="font-weight:bold;">1 <sup>1 </sup></span><span class="font64"><sup>x</sup> nn-</span><span class="font64" style="font-weight:bold;">2</span><span class="font64"><sup>)</sup></span></p>
<p><span class="font64" style="font-variant:small-caps;">t</span><span class="font18"><sup>1</sup></span><span class="font64"> &#160;&#160;&#160;(<sub>x</sub>7<sup>k)</sup>&#160;&#160;&#160;&#160;<sub>x</sub>7<sup>k)</sup> )</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>T</sup>7n-1</span><span class="font64"> <sup>(X</sup></span><span class="font18">7</span><span class="font64">n </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>1 x</sup>nn- 1 <sup>)</sup></span></p></div>
<p><span class="font64"><sup>p</sup></span><span class="font18">0</span><span class="font64"><sup>(x(k))</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">Pm<sup>(x</sup>m)</span><span class="font64"> &#160;&#160;&#160;<sup>p</sup>n</span><span class="font18">1</span><span class="font64">-״<sup>(x</sup>S<sup>)</sup></span></p>
<p><span class="font64">To avoid numerical issues such as overflow, it is probably best to compute log w<sup>(k)</sup> by adding and subtracting log probabilities, rather than computing w<sup>(k)</sup> by multiplying&#160;and dividing probabilities.</span></p>
<p><span class="font64">With the sampling procedure thus defined and the importance weights given in Eq. 18.52, the estimate of the ratio of partition functions is given by:</span></p><div>
<p><span class="font64" style="font-weight:bold;">Z1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Z0 &#160;&#160;&#160;K</span></p></div><div>
<p><span class="font63">K</span></p><h3><a id="bookmark1"></a><span class="font66">E</span></h3>
<p><span class="font63">k=1</span></p></div><div>
<p><span class="font64">w</span></p></div><div>
<p><span class="font63">(k)</span></p></div><div>
<p><span class="font64">(18.53)</span></p></div>
<p><span class="font64">In order to verify that this procedure defines a valid importance sampling scheme, we can show (Neal, 2001) that the AIS procedure corresponds to simple&#160;importance sampling on an extended state space with points sampled over the&#160;product space [x<sub>r]1</sub>,..., xn<sub>n-1</sub>, x!]. To do this, we define the distribution over the&#160;extended space as:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">P<sup>(x</sup>n1 <sup>,</sup>•••<sup>, x</sup>nn-1 <sup>x</sup>1</span><span class="font64"><sup>) &#160;&#160;&#160;(18</sup>.<sup>54)</sup></span></p>
<p><span class="font64">_Pi<sup>(x</sup></span><span class="font18">1</span><span class="font64"><sup>)T</sup></span><span class="font18">71</span><span class="font64">-״<sup>(x</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">nn</span><span class="font62" style="font-style:italic;">-1</span><span class="font64" style="font-weight:bold;font-style:italic;"> <sup>1</sup></span><span class="font64"><sup> x</sup></span><span class="font18">1</span><span class="font64"><sup>)T</sup>nn-</span><span class="font18">2</span><span class="font64"><sup>(x</sup></span><span class="font18">7</span><span class="font64">n</span><span class="font18">-2</span><span class="font64"> </span><span class="font18"><sup>1</sup></span><span class="font64"><sup> x</sup>nn-</span><span class="font18">1</span><span class="font64"><sup>)</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">• • •<sup>T</sup>n </span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;"> <sup>(x</sup>n</span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;"> <sup>1</sup></span><span class="font64"><sup> x</sup>n</span><span class="font18">2</span><span class="font64"><sup>)</sup>, &#160;&#160;&#160;<sup>(18</sup>.<sup>55)</sup></span></p>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;font-style:italic;">T<sub>a</sub></span><span class="font64"> is the reverse of the transition operator defined by </span><span class="font64" style="font-weight:bold;font-style:italic;">T<sub>a</sub></span><span class="font64"> (via an application of Bayes’ rule):</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">rp &#160;&#160;&#160;Pa <sup>(x )&#160;&#160;&#160;&#160;p</sup>a <sup>(x</sup></span><span class="font64"><sup> )</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Ta (x</span><span class="font64"> | x) _ &#160;&#160;&#160;Ta(x | x ) _&#160;&#160;&#160;&#160;Ta (x | x )•</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Pa</span><span class="font64"> <sup>(x)</sup></span></p></div><div>
<p><span class="font63"><sup>p</sup>a <sup>(x)</sup></span></p></div><div>
<p><span class="font64">(18.56)</span></p></div>
<p><span class="font64">Plugging the above into the expression for the joint distribution on the extended state space given in Eq. 18.55, we get:</span></p><div>
<p><span class="font64">(18.57)</span></p></div>
<p><span class="font64"><sup>P(x</sup>71 , • • • , </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>x</sup>7n-1 ,<sup>x</sup> 1</span><span class="font64"> <sup>)</sup></span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">n</span><span class="font18">—2</span></p></div><div>
<p><span class="font64"><sup>p</sup>r </span><span class="font64" style="font-weight:bold;font-style:italic;">n-1<sup>(x</sup></span><span class="font64"> rn-</span><span class="font18">1</span><span class="font64"><sup>)</sup></span></p>
<p><span class="font64"><sup>p</sup></span><span class="font18">1</span><span class="font64"><sup>(x</sup> </span><span class="font18">1</span><span class="font64"><sup>)</sup> &#160;&#160;&#160;~&#160;&#160;&#160;&#160;(_ ץ&#160;&#160;&#160;&#160;<sup>T</sup>r</span><span class="font18">1</span><span class="font64">-״<sup>(x</sup> </span><span class="font18">1</span><span class="font64"> </span><span class="font18"><sup>1</sup></span><span class="font64"><sup> x</sup>rn-</span><span class="font18">1</span><span class="font64"><sup>)</sup></span></p></div><div>
<p><span class="font18">#7</span><span class="font64"> n-</span><span class="font18">1<sup>(x</sup>1</span><span class="font64"> <sup>)</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Pr/i</span><span class="font64"> <sup>(x</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">ni</span><span class="font64"> <sup>) p</sup>m (<sup>x</sup> ri+</span><span class="font18">1</span><span class="font64"><sup>)</sup></span></p></div><div>
<p><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">i(<sup>x</sup>m+1</span><span class="font64"> </span><span class="font18"><sup>1</sup></span><span class="font64"><sup> </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>x</sup>m</span><span class="font64"><sup>)</sup></span></p></div><div>
<p><span class="font12" style="font-style:italic;">m.</span></p>
<p><span class="font64">(18.58)</span></p></div><div>
<p><span class="font64">p</span><span class="font18">1</span><span class="font64">(x </span><span class="font18">1</span><span class="font64">)</span></p></div><div>
<p><span class="font63">n2</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">7^-1 <sup>(x</sup>1<sup>)</sup></span></p></div><div>
<p><span class="font64"><sub>x</sub> )<sup>T</sup>rn-</span><span class="font18">1</span><span class="font64"><sup>(x</sup> </span><span class="font18">1</span><span class="font64"> </span><span class="font18"><sup>1</sup></span><span class="font64"><sup> x</sup>rn-</span><span class="font18">1</span><span class="font64"><sup>) </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>p</sup>n</span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;"> <sup>(x</sup>n</span><span class="font64"> </span><span class="font18">1</span><span class="font64"><sup>)</sup></span></p></div><div>
<p><span class="font63">.Pri+1 <sup>(x</sup>r;+1<sup>)</sup></span></p></div><div>
<p><span class="font63"><sup>T</sup>r i<sup>(x</sup>r;+1 <sup>1 x</sup>r; )•</span></p></div><div>
<p><span class="font63">i=</span></p></div><div>
<p><span class="font63">L <sub>p</sub> (<sub>x</sub> ) &#160;&#160;&#160;^ri+1 <sup>1</sup>r</span></p>
<p><span class="font64" style="font-weight:bold;">1 </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>p</sup>ni <sup>(x</sup>ri+1<sup>)</sup></span></p>
<p><span class="font64">(18.59)</span></p></div>
<p><span class="font64">We now have means of generating samples from the joint proposal distribution q over the extended sample via a sampling scheme given above, with the joint&#160;distribution given by:</span></p>
<p><span class="font64"><sup>q(x</sup>n </span><span class="font18">1</span><span class="font64">י • • • י <sup>x</sup>rn-</span><span class="font18">1</span><span class="font64">י<sup>x</sup> </span><span class="font18">1</span><span class="font64"><sup>)</sup> _ <sup>p</sup></span><span class="font18">0</span><span class="font64"><sup>(x</sup>r</span><span class="font18">1</span><span class="font64"> <sup>)T</sup>r</span><span class="font18">1</span><span class="font64"><sup>(x</sup> r</span><span class="font18">2</span><span class="font64"> </span><span class="font18"><sup>1</sup></span><span class="font64"><sup> x</sup>n</span><span class="font18">1</span><span class="font64"> <sup>)</sup> •••<sup>T</sup>rn</span><span class="font18">-1</span><span class="font64"> </span><span class="font18"><sup>(x</sup>1</span><span class="font64"> </span><span class="font18"><sup>1</sup></span><span class="font64"><sup> x</sup>rn-</span><span class="font18">1</span><span class="font64"><sup>)</sup>• &#160;&#160;&#160;<sup>(18</sup>-<sup>60)</sup></span></p>
<p><span class="font64">We have a joint distribution on the extended space given by Eq. 18.59. Taking q(x^ , • • •, xr<sub>n-1</sub>, x</span><span class="font18">1</span><span class="font64">) as the proposal distribution on the extended state space from&#160;which we will draw samples, it remains to determine the importance weights:</span></p><div>
<p><span class="font64" style="font-weight:bold;">w<sup>(k)</sup> _</span></p></div><div>
<p><span class="font63" style="text-decoration:underline;"><sup>p(x</sup>m1 י • • • י <sup>x</sup>rn-1,</span><span class="font65" style="font-style:italic;text-decoration:underline;"><sup>x</sup></span><span class="font64" style="font-weight:bold;text-decoration:underline;"> 1</span><span class="font63" style="text-decoration:underline;">)</span><span class="font63"> _ &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;text-decoration:underline;">,</span><span class="font63" style="text-decoration:underline;"><sup>p</sup></span><span class="font64" style="font-weight:bold;text-decoration:underline;">1</span><span class="font63" style="text-decoration:underline;"><sup>(x</sup></span><span class="font64" style="font-weight:bold;text-decoration:underline;">1</span></p>
<p><span class="font63"><sup>q(x</sup>r1 י • • • י <sup>x</sup>nn-1, <sup>x</sup></span><span class="font64" style="font-weight:bold;">1</span><span class="font63"><sup>)</sup></span></p></div><div>
<p><span class="font63"><sup>p</sup>rn</span></p></div><div><h3><a id="bookmark2"></a><span class="font64">_ </span><span class="font64" style="text-decoration:underline;"><sup>p</sup>n</span><span class="font18" style="text-decoration:underline;">2</span><span class="font64" style="text-decoration:underline;"><sup>(x</sup> W) <sup>p</sup>n</span><span class="font18" style="text-decoration:underline;">1</span><span class="font64" style="text-decoration:underline;"> </span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:underline;"><sup>(x</sup>n1</span><span class="font64" style="text-decoration:underline;"> <sup>)</sup></span></h3><h3><a id="bookmark3"></a><span class="font64" style="font-weight:bold;font-style:italic;">1</span><span class="font64"><sup>(x</sup>rn-</span><span class="font18">1</span><span class="font64"><sup>)</sup> ^</span><span class="font18">1</span><span class="font64"><sup>(x</sup> r?) <sup>p</sup> </span><span class="font18">0</span><span class="font64"><sup>(x</sup></span><span class="font18">0</span><span class="font64"><sup>k))</sup></span></h3></div><div>
<p><span class="font63">.(k)</span></p></div><div>
<p><span class="font64">(18.61)</span></p></div>
<p><span class="font64">These weights are the same as proposed for AIS. Thus we can interpret AIS as simple importance sampling applied to an extended state and its validity follows&#160;immediately from the validity of importance sampling.</span></p>
<p><span class="font64">Annealed importance sampling (AIS) was first discovered by Jarzynski (1997) and then again, independently, by Neal (2001). It is currently the most common&#160;way of estimating the partition function for undirected probabilistic models. The&#160;reasons for this may have more to do with the publication of an influential paper&#160;(Salakhutdinov and Murray, 2008) describing its application to estimating the&#160;partition function of restricted Boltzmann machines and deep belief networks than&#160;with any inherent advantage the method has over the other method described&#160;below.</span></p>
<p><span class="font64">A discussion of the properties of the AIS estimator (e.g.. its variance and efficiency) can be found in Neal (2001).</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">18.7.2 Bridge Sampling</span></h5>
<p><span class="font64">Bridge sampling Bennett (1976) is another method that, like AIS, addresses the shortcomings of importance sampling. Rather than chaining together a series of&#160;intermediate distributions, bridge sampling relies on a single distributionp*, known&#160;as the bridge, to interpolate between a distribution with known partition function,&#160;p</span><span class="font18">0</span><span class="font64">, and a distribution pi for which we are trying to estimate the partition function</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Z1</span><span class="font64">.</span></p>
<p><span class="font64">Bridge sampling estimates the ratio Z! </span><span class="font64" style="font-weight:bold;font-style:italic;">/Z</span><span class="font62" style="font-style:italic;">0</span><span class="font64"> as the ratio of the expected importance weights between p and </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64">* and between p and </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64">*:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>K</sup></span><span class="font64"> &#160;&#160;&#160;(k)\</span></p>
<p><span class="font18"><sup>Z</sup>1</span><span class="font64"> &#160;&#160;&#160;<sup>p</sup>* </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(x</sup>0</span><span class="font64"> <sup>)</sup></span></p>
<p><span class="font64">Z</span><span class="font18"><sub>0</sub></span><span class="font64"> </span><span class="font64" style="font-weight:bold;">~ &#160;&#160;&#160;</span><span class="font64">(k)</span><span class="font64" style="font-weight:bold;">x</span></p>
<p><span class="font64"><sup>Z </sup></span><span class="font18"><sup>0</sup></span><span class="font64"> &#160;&#160;&#160;k</span><span class="font18">=1</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">po</span><span class="font64"> </span><span class="font18"><sup>(x</sup>0</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">)</span></p></div><div>
<p><span class="font63">/!:</span></p></div><div>
<p><span class="font64" style="font-weight:bold;"><sup>p</sup>* (<sup>x</sup>!<sup>k)</sup>) =1 <sup>p</sup>1 <sup>(x</sup>!<sup>k))</sup></span></p></div><div>
<p><span class="font64">(18.62)</span></p></div>
<p><span class="font64">If the bridge distribution p* is chosen carefully to have a large overlap of support with both p</span><span class="font18">0</span><span class="font64"> and p</span><span class="font18">1</span><span class="font64">, then bridge sampling can allow the distance between two&#160;distributions (or more formally, </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">Dkl</span><span class="font64">(p</span><span class="font18">0</span><span class="font64">||p </span><span class="font18">1</span><span class="font64">)) to be much larger than with standard&#160;importance sampling.</span></p>
<p><span class="font64">It can be shown that the optimal bridging distribution is given by p*<sup>opt)</sup> (x) x </span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:line-through;">rpo(x)+ftX(x)</span><span class="font64"> where </span><span class="font64" style="font-weight:bold;font-style:italic;">r = Z</span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;">/Z</span><span class="font18">0</span><span class="font64">. At first, this appears to be an unworkable solution&#160;as it would seem to require the very quantity we are trying to estimate, Z</span><span class="font18">1</span><span class="font64"> /Zo.&#160;However, it is possible to start with a coarse estimate of r and use the resulting&#160;bridge distribution to refine our estimate iteratively (Neal, 2005). That is, we&#160;iteratively re-estimate the ratio and use each iteration to update the value of r.</span></p>
<p><span class="font64">Linked importance sampling Both AIS and bridge sampling have their advantages. If D</span><span class="font64" style="font-variant:small-caps;"><sub>kl</sub></span><span class="font64"> (p </span><span class="font18">0</span><span class="font64">|p<sub>1</sub>) is not too large (because po and p </span><span class="font18"><sub>1</sub></span><span class="font64"> are sufficiently close) bridge sampling can be a more effective means of estimating the ratio of partition&#160;functions than AIS. If, however, the two distributions are too far apart for a single&#160;distribution p* to bridge the gap then one can at least use AIS with potentially&#160;many intermediate distributions to span the distance between po and p</span><span class="font18">1</span><span class="font64">. Neal&#160;(2005) showed how his linked importance sampling method leveraged the power of&#160;the bridge sampling strategy to bridge the intermediate distributions used in AIS&#160;to significantly improve the overall partition function estimates.</span></p>
<p><span class="font64">Estimating the partition function while training While AIS has become accepted as the standard method for estimating the partition function for many&#160;undirected models, it is sufficiently computationally intensive that it remains&#160;infeasible to use during training. However, alternative strategies that have been&#160;explored to maintain an estimate of the partition function throughout training</span></p>
<p><span class="font64">Using a combination of bridge sampling, short-chain AIS and parallel tempering, Desjardins </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011) devised a scheme to track the partition function of an</span></p>
<p><span class="font64">RBM throughout the training process. The strategy is based on the maintenance of independent estimates of the partition functions of the RBM at every temperature&#160;operating in the parallel tempering scheme. The authors combined bridge sampling&#160;estimates of the ratios of partition functions of neighboring chains (i.e. from&#160;parallel tempering) with AIS estimates across time to come up with a low variance&#160;estimate of the partition functions at every iteration of learning.</span></p>
<p><span class="font64">The tools described in this chapter provide many different ways of overcoming the problem of intractable partition functions, but there can be several other&#160;difficulties involved in training and using generative models. Foremost among these&#160;is the problem of intractable inference, which we confront next.</span></p>
</body>
</html>