<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h4><a id="bookmark0"></a><span class="font65" style="font-weight:bold;">10.5 Deep Recurrent Networks</span></h4>
<p><span class="font64">The computation in most RNNs can be decomposed into three blocks of parameters and associated transformations:</span></p>
<p><span class="font64">1. &#160;&#160;&#160;from the input to the hidden state,</span></p>
<p><span class="font64">2. &#160;&#160;&#160;from the previous hidden state to the next hidden state, and</span></p>
<p><span class="font64">3. &#160;&#160;&#160;from the hidden state to the output.</span></p>
<p><span class="font64">With the RNN architecture of Fig. 10.3, each of these three blocks is associated with a single weight matrix. In other words, when the network is unfolded, each&#160;of these corresponds to a shallow transformation. By a shallow transformation,&#160;we mean a transformation that would be represented by a single layer within&#160;a deep MLP. Typically this is a transformation represented by a learned affine&#160;transformation followed by a fixed nonlinearity.</span></p>
<p><span class="font64">Would it be advantageous to introduce depth in each of these operations? Experimental evidence (Graves </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013; Pascanu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014a) strongly suggests&#160;so. The experimental evidence is in agreement with the idea that we need enough</span></p><div><img src="main-129.jpg" alt=""/>
<p><span class="font64">Figure 10.13: A recurrent neural network can be made deep in many ways (Pascanu </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2014a). </span><span class="font64" style="font-style:italic;">(a)</span><span class="font64"> The hidden recurrent state can be broken down into groups organized&#160;hierarchically. </span><span class="font64" style="font-style:italic;">(b)</span><span class="font64"> Deeper computation (e.g., an MLP) can be introduced in the input-to-hidden, hidden-to-hidden and hidden-to-output parts. This may lengthen the shortest&#160;path linking different time steps. </span><span class="font64" style="font-style:italic;">(c)</span><span class="font64"> The path-lengthening effect can be mitigated by&#160;introducing skip connections.</span></p></div>
<p><span class="font64">depth in order to perform the required mappings. See also Schmidhuber (1992), El Hihi and Bengio (1996), or Jaeger (2007a) for earlier work on deep RNNs.</span></p>
<p><span class="font64">Graves </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013) were the first to show a significant benefit of decomposing the state of an RNN into multiple layers as in Fig. 10.13 (left). We can think&#160;of the lower layers in the hierarchy depicted in Fig. 10.13a as playing a role in&#160;transforming the raw input into a representation that is more appropriate, at&#160;the higher levels of the hidden state. Pascanu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014a) go a step further&#160;and propose to have a separate MLP (possibly deep) for each of the three blocks&#160;enumerated above, as illustrated in Fig. 10.13b. Considerations of representational&#160;capacity suggest to allocate enough capacity in each of these three steps, but doing&#160;so by adding depth may hurt learning by making optimization difficult. In general,&#160;it is easier to optimize shallower architectures, and adding the extra depth of&#160;Fig. 10.13b makes the shortest path from a variable in time step t to a variable&#160;in time step t + 1 become longer. For example, if an MLP with a single hidden&#160;layer is used for the state-to-state transition, we have doubled the length of the&#160;shortest path between variables in any two different time steps, compared with the&#160;ordinary RNN of Fig. 10.3. However, as argued by Pascanu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014a), this&#160;can be mitigated by introducing skip connections in the hidden-to-hidden path, as&#160;illustrated in Fig. 10.13c.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">10.6 Recursive Neural Networks</span></h4>
<p><span class="font64">Recursive neural networks<a id="footnote1"></a><sup><a href="#bookmark2">1</a></sup><sup></sup> represent yet another generalization of recurrent networks, with a different kind of computational graph, which is structured as a deep tree, rather than the chain-like structure of RNNs. The typical computational&#160;graph for a recursive network is illustrated in Fig. 10.14. Recursive neural networks&#160;were introduced by Pollack (1990) and their potential use for learning to reason&#160;was described by by Bottou (2011). Recursive networks have been successfully&#160;applied to processing </span><span class="font64" style="font-weight:bold;">data structures </span><span class="font64">as input to neural nets (Frasconi </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">1997, 1998), in natural language processing (Socher </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011a,c, 2013a) as well&#160;as in computer vision (Socher </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011b).</span></p>
<p><span class="font64">One clear advantage of recursive nets over recurrent nets is that for a sequence of the same length </span><span class="font63" style="font-variant:small-caps;">t</span><span class="font64">, the depth (measured as the number of compositions of&#160;nonlinear operations) can be drastically reduced from </span><span class="font63" style="font-variant:small-caps;">t </span><span class="font64">to O(log</span><span class="font63" style="font-variant:small-caps;">t</span><span class="font64">), which might&#160;help deal with long-term dependencies. An open question is how to best structure&#160;the tree. One option is to have a tree structure which does not depend on the data,</span></p><div><img src="main-130.jpg" alt=""/>
<p><span class="font64">Figure 10.14: A recursive network has a computational graph that generalizes that of the recurrent network from a chain to a tree. A variable-size sequencex<sup>(1)</sup>, x<sup>(2)</sup>,. .., x<sup>(t)</sup> can&#160;be mapped to a fixed-size representation (the output o), with a fixed set of parameters&#160;(the weight matrices U, V, W). The figure illustrates a supervised learning case in which&#160;some target y is provided which is associated with the whole sequence.</span></p></div>
<p><span class="font64">such as a balanced binary tree. In some application domains, external methods can suggest the appropriate tree structure. For example, when processing natural&#160;language sentences, the tree structure for the recursive network can be fixed to&#160;the structure of the parse tree of the sentence provided by a natural language&#160;parser (Socher </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2011a, 2013a). Ideally, one would like the learner itself to&#160;discover and infer the tree structure that is appropriate for any given input, as&#160;suggested by Bottou (2011).</span></p>
<p><span class="font64">Many variants of the recursive net idea are possible. For example, Frasconi </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (1997) and Frasconi </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (1998) associate the data with a tree structure,&#160;and associate the inputs and targets with individual nodes of the tree. The&#160;computation performed by each node does not have to be the traditional artificial&#160;neuron computation (affine transformation of all inputs followed by a monotone&#160;nonlinearity). For example, Socher </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2013a) propose using tensor operations&#160;and bilinear forms, which have previously been found useful to model relationships&#160;between concepts (Weston </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2010; Bordes </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2012) when the concepts are&#160;represented by continuous vectors (embeddings).</span></p><h4><a id="bookmark3"></a><span class="font65" style="font-weight:bold;">10.7 The Challenge of Long-Term Dependencies</span></h4>
<p><span class="font64">The mathematical challenge of learning long-term dependencies in recurrent networks was introduced in Sec. 8.2.5. The basic problem is that gradients propagated over many stages tend to either vanish (most of the time) or explode (rarely, but&#160;with much damage to the optimization). Even if we assume that the parameters are&#160;such that the recurrent network is stable (can store memories, with gradients not&#160;exploding), the difficulty with long-term dependencies arises from the exponentially&#160;smaller weights given to long-term interactions (involving the multiplication of&#160;many Jacobians) compared to short-term ones. Many other sources provide a&#160;deeper treatment (Hochreiter, 1991; Doya, 1993; Bengio </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 1994; Pascanu </span><span class="font64" style="font-style:italic;">et al.,&#160;</span><span class="font64">2013a) . In this section, we describe the problem in more detail. The remaining&#160;sections describe approaches to overcoming the problem.</span></p>
<p><span class="font64">Recurrent networks involve the composition of the same function multiple times, once per time step. These compositions can result in extremely nonlinear&#160;behavior, as illustrated in Fig. 10.15.</span></p>
<p><span class="font64">In particular, the function composition employed by recurrent neural networks somewhat resembles matrix multiplication. We can think of the recurrence relation</span></p>
<p><span class="font64">h<sup>(t)</sup> </span><span class="font64" style="font-style:italic;">= <sub>w</sub> T<sub>h</sub>(t<sup>-</sup>1)</span><span class="font64"> &#160;&#160;&#160;(10.36)</span></p>
<p><span class="font64">as a very simple recurrent neural network lacking a nonlinear activation function,</span></p><div><div><img src="main-131.jpg" alt=""/></div></div><div>
<p><span class="font63">4 3&#160;2&#160;1&#160;0&#160;1&#160;2</span></p>
<p><span class="font63">3</span></p>
<p><span class="font63">4</span></p></div><div><div>
<p><span class="font64">Repeated function composition</span></p><img src="main-132.jpg" alt=""/>
<p><span class="font63">60 &#160;&#160;&#160;40&#160;&#160;&#160;&#160;20&#160;&#160;&#160;&#160;0&#160;&#160;&#160;&#160;20&#160;&#160;&#160;&#160;40&#160;&#160;&#160;&#160;60</span></p>
<p><span class="font63">Input coordinate</span></p>
<p><span class="font64">Figure 10.15: When composing many nonlinear functions (like the lineartanh layer shown here), the result is highly nonlinear, typically with most of the values associated with a tiny&#160;derivative, some values with a large derivative, and many alternations between increasing&#160;and decreasing. In this plot, we plot a linear projection of a 100-dimensional hidden state&#160;down to a single dimension, plotted on the y-axis. The x-axis is the coordinate of the&#160;initial state along a random direction in the 100-dimensional space. We can thus view this&#160;plot as a linear cross-section of a high-dimensional function. The plots show the function&#160;after each time step, or equivalently, after each number of times the transition function&#160;has been composed.</span></p></div></div>
<p><span class="font64">and lacking inputs x. As described in Sec. 8.2.5, this recurrence relation essentially describes the power method. It may be simplified to</span></p>
<p><span class="font64">h<sup>(t)</sup> </span><span class="font64" style="font-style:italic;">=</span><span class="font64"> (W)<sup>T</sup> h<sup>(0)</sup>, &#160;&#160;&#160;(10.37)</span></p>
<p><span class="font64">and if W admits an eigendecomposition of the form</span></p>
<p><span class="font64" style="font-style:italic;">W =</span><span class="font64" style="font-variant:small-caps;"> QAQ<sup>t</sup> , &#160;&#160;&#160;(10.38)</span></p>
<p><span class="font64">with orthogonal Q, the recurrence may be simplified further to</span></p>
<p><span class="font64">h<sup>(t)</sup> = Q</span><span class="font64" style="font-variant:small-caps;"><sup>t</sup>A</span><span class="font64"><sup>t</sup>Qh<sup>(0)</sup>. &#160;&#160;&#160;(10.39)</span></p>
<p><span class="font64">The eigenvalues are raised to the power of t causing eigenvalues with magnitude less than one to decay to zero and eigenvalues with magnitude greater than one to&#160;explode. Any component of h<sup>(0)</sup> that is not aligned with the largest eigenvector&#160;will eventually be discarded.</span></p>
<p><span class="font64">This problem is particular to recurrent networks. In the scalar case, imagine multiplying a weight w by itself many times. The product w<sup>t</sup> will either vanish or&#160;explode depending on the magnitude of w• However, if we make a non-recurrent&#160;network that has a different weight w<sup>(t)</sup> at each time step, the situation is different.&#160;If the initial state is given by 1, then the state at time t is given by n<sub>t</sub> w<sup>(t)</sup>. Suppose&#160;that the w<sup>(t)</sup> values are generated randomly, independently from one another, with&#160;zero mean and variance v. The variance of the product is </span><span class="font64" style="font-style:italic;">O</span><span class="font64">(v<sup>n</sup>). To obtain some&#160;desired variance v* we may choose the individual weights with variance v = </span><span class="font64" style="font-style:italic;"><sup>n</sup>v</span><span class="font64">*.&#160;Very deep feedforward networks with carefully chosen scaling can thus avoid the&#160;vanishing and exploding gradient problem, as argued by Sussillo (2014).</span></p>
<p><span class="font64">The vanishing and exploding gradient problem for RNNs was independently discovered by separate researchers (Hochreiter, 1991; Bengio </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 1993, 1994).&#160;One may hope that the problem can be avoided simply by staying in a region of&#160;parameter space where the gradients do not vanish or explode. Unfortunately, in&#160;order to store memories in a way that is robust to small perturbations, the RNN&#160;must enter a region of parameter space where gradients vanish (Bengio </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 1993,&#160;1994). Specifically, whenever the model is able to represent long term dependencies,&#160;the gradient of a long term interaction has exponentially smaller magnitude than&#160;the gradient of a short term interaction. It does not mean that it is impossible&#160;to learn, but that it might take a very long time to learn long-term dependencies,&#160;because the signal about these dependencies will tend to be hidden by the smallest&#160;fluctuations arising from short-term dependencies. In practice, the experiments&#160;in Bengio </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (1994) show that as we increase the span of the dependencies that&#160;need to be captured, gradient-based optimization becomes increasingly difficult,&#160;with the probability of successful training of a traditional RNN via SGD rapidly&#160;reaching 0 for sequences of only length 10 or 20.</span></p>
<p><span class="font64">For a deeper treatment of recurrent networks as dynamical systems, see Doya (1993), Bengio </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (1994) and Siegelmann and Sontag (1995), with a review&#160;in Pascanu </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2013a). The remaining sections of this chapter discuss various&#160;approaches that have been proposed to reduce the difficulty of learning longterm dependencies (in some cases allowing an RNN to learn dependencies across&#160;hundreds of steps), but the problem of learning long-term dependencies remains&#160;one of the main challenges in deep learning.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">10.8 Echo State Networks</span></h4>
<p><span class="font64">The recurrent weights mapping from h<sup>(t-1)</sup> to h<sup>(t)</sup> and the input weights mapping from to h<sup>(t)</sup> are some of the most difficult parameters to learn in a recurrent&#160;network. One proposed (Jaeger, 2003; Maass </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2002; Jaeger and Haas, 2004;&#160;Jaeger, 2007b) approach to avoiding this difficulty is to set the recurrent weights&#160;such that the recurrent hidden units do a good job of capturing the history of&#160;past inputs, and </span><span class="font64" style="font-weight:bold;">only learn the output weights</span><span class="font64">. This is the idea that was&#160;independently proposed for </span><span class="font64" style="font-style:italic;">echo state networks</span><span class="font64"> or ESNs (Jaeger and Haas, 2004;&#160;Jaeger, 2007b) and </span><span class="font64" style="font-style:italic;">liquid state machines</span><span class="font64"> (Maass </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2002). The latter is similar,&#160;except that it uses spiking neurons (with binary outputs) instead of the continuousvalued hidden units used for ESNs. Both ESNs and liquid state machines are&#160;termed </span><span class="font64" style="font-style:italic;">reservoir computing</span><span class="font64"> (Lukosevicius and Jaeger, 2009) to denote the fact&#160;that the hidden units form of reservoir of temporal features which may capture&#160;different aspects of the history of inputs.</span></p>
<p><span class="font64">One way to think about these reservoir computing recurrent networks is that they are similar to kernel machines: they map an arbitrary length sequence (the&#160;history of inputs up to time </span><span class="font64" style="font-style:italic;">t)</span><span class="font64"> into a fixed-length vector (the recurrent state h<sup>(t)</sup>),&#160;on which a linear predictor (typically a linear regression) can be applied to solve&#160;the problem of interest. The training criterion may then be easily designed to be&#160;convex as a function of the output weights. For example, if the output consists&#160;of linear regression from the hidden units to the output targets, and the training&#160;criterion is mean squared error, then it is convex and may be solved reliably with&#160;simple learning algorithms (Jaeger, 2003).</span></p>
<p><span class="font64">The important question is therefore: how do we set the input and recurrent weights so that a rich set of histories can be represented in the recurrent neural&#160;network state? The answer proposed in the reservoir computing literature is to&#160;view the recurrent net as a dynamical system, and set the input and recurrent&#160;weights such that the dynamical system is near the edge of stability.</span></p>
<p><span class="font64">The original idea was to make the eigenvalues of the Jacobian of the state-to-state transition function be close to 1. As explained in Sec. 8.2.5, an important characteristic of a recurrent network is the eigenvalue spectrum of the Jacobians&#160;J= </span><span class="font64" style="font-weight:bold;text-decoration:line-through;">dSs</span><span class="font64" style="text-decoration:line-through;">-</span><span class="font18" style="text-decoration:line-through;">1</span><span class="font64" style="text-decoration:line-through;">)</span><span class="font64">. Of particular importance is the </span><span class="font64" style="font-weight:bold;font-style:italic;">spectral radius</span><span class="font64"> of J, defined to be&#160;the maximum of the absolute values of its eigenvalues.</span></p>
<p><span class="font64">To understand the effect of the spectral radius, consider the simple case of back-propagation with a Jacobian matrix J that does not change with t. This&#160;case happens, for example, when the network is purely linear. Suppose that J has&#160;an eigenvector v with corresponding eigenvalue A. Consider what happens as we&#160;propagate a gradient vector backwards through time. If we begin with a gradient&#160;vector g, then after one step of back-propagation, we will have </span><span class="font64" style="font-style:italic;">Jg</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> and after n&#160;steps we will have J</span><span class="font64" style="font-style:italic;"><sup>n</sup>g</span><span class="font64" style="font-weight:bold;font-style:italic;">.</span><span class="font64"> Now consider what happens if we instead back-propagate&#160;a perturbed version of g. If we begin with g + </span><span class="font64" style="font-weight:bold;font-style:italic;">5</span><span class="font64" style="font-style:italic;">v</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> then after one step, we will&#160;have J (g + </span><span class="font64" style="font-weight:bold;font-style:italic;">S</span><span class="font64" style="font-style:italic;">v</span><span class="font64" style="font-weight:bold;font-style:italic;">).</span><span class="font64"> After n steps, we will have J</span><span class="font64" style="font-weight:bold;"><sup>n</sup></span><span class="font64">(g + </span><span class="font64" style="font-weight:bold;font-style:italic;">S</span><span class="font64" style="font-style:italic;">v</span><span class="font64">). From this we can see&#160;that back-propagation starting from g and back-propagation starting from g + Sv&#160;diverge by SJ</span><span class="font64" style="font-weight:bold;"><sup>n</sup></span><span class="font64">v after n steps of back-propagation. If v is chosen to be a unit&#160;eigenvector of J with eigenvalue A, then multiplication by the Jacobian simply&#160;scales the difference at each step. The two executions of back-propagation are&#160;separated by a distance of S|A|</span><span class="font64" style="font-weight:bold;"><sup>n</sup></span><span class="font64">. When v corresponds to the largest value of | A|,&#160;this perturbation achieves the widest possible separation of an initial perturbation&#160;of size S.</span></p>
<p><span class="font64">When |A| &gt; 1, the deviation size S|A|</span><span class="font64" style="font-weight:bold;"><sup>n</sup> </span><span class="font64">grows exponentially large. When |A| &lt; 1, the deviation size becomes exponentially small.</span></p>
<p><span class="font64">Of course, this example assumed that the Jacobian was the same at every time step, corresponding to a recurrent network with no nonlinearity. When a&#160;nonlinearity is present, the derivative of the nonlinearity will approach zero on&#160;many time steps, and help to prevent the explosion resulting from a large spectral&#160;radius. Indeed, the most recent work on echo state networks advocates using a&#160;spectral radius much larger than unity (Yildiz </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012; Jaeger, 2012).</span></p>
<p><span class="font64">Everything we have said about back-propagation via repeated matrix multiplication applies equally to forward propagation in a network with no nonlinearity, where the state h<sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>t</sup></span><span class="font64">+<sup>1)</sup> = h<sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>t</sup></span><span class="font64"><sup>)T</sup>W.</span></p>
<p><span class="font64">When a linear map </span><span class="font64" style="font-style:italic;">W</span><span class="font64"><sup>T</sup> always shrinks h as measured by the L</span><span class="font18"><sup>2</sup></span><span class="font64"> norm, then we say that the map is </span><span class="font64" style="font-weight:bold;font-style:italic;">contractive.</span><span class="font64"> When the spectral radius is less than one, the&#160;mapping from h<sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>t</sup></span><span class="font64"><sup>)</sup> to h<sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>t</sup></span><span class="font64">+<sup>1)</sup> is contractive, so a small change becomes smaller after&#160;each time step. This necessarily makes the network forget information about the&#160;past when we use a finite level of precision (such as 32 bit integers) to store the&#160;state vector.</span></p>
<p><span class="font64">The Jacobian matrix tells us how a small change of h<sup>(t)</sup> propagates one step forward, or equivalently, how the gradient on </span><span class="font64" style="font-weight:bold;font-style:italic;">h<sup>(t</sup>+<sup>1</sup>1</span><span class="font64"> propagates one step backward,&#160;during back-propagation. Note that neither W nor J need to be symmetric (although they are square and real), so they can have complex-valued eigenvalues and&#160;eigenvectors, with imaginary components corresponding to potentially oscillatory&#160;behavior (if the same Jacobian was applied iteratively). Even though h<sup>(t)</sup> or a&#160;small variation of hof interest in back-propagation are real-valued, they can&#160;be expressed in such a complex-valued basis. What matters is what happens to&#160;the magnitude (complex absolute value) of these possibly complex-valued basis&#160;coefficients, when we multiply the matrix by the vector. An eigenvalue with&#160;magnitude greater than one corresponds to magnification (exponential growth, if&#160;applied iteratively) or shrinking (exponential decay, if applied iteratively).</span></p>
<p><span class="font64">With a nonlinear map, the Jacobian is free to change at each step. The dynamics therefore become more complicated. However, it remains true that a&#160;small initial variation can turn into a large variation after several steps. One&#160;difference between the purely linear case and the nonlinear case is that the use of&#160;a squashing nonlinearity such as tanh can cause the recurrent dynamics to become&#160;bounded. Note that it is possible for back-propagation to retain unbounded&#160;dynamics even when forward propagation has bounded dynamics, for example,&#160;when a sequence of tanh units are all in the middle of their linear regime and are&#160;connected by weight matrices with spectral radius greater than 1. However, it is&#160;rare for all of the tanh units to simultaneously lie at their linear activation point.</span></p>
<p><span class="font64">The strategy of echo state networks is simply to fix the weights to have some spectral radius such as 3, where information is carried forward through time but&#160;does not explode due to the stabilizing effect of saturating nonlinearities like tanh.</span></p>
<p><span class="font64">More recently, it has been shown that the techniques used to set the weights in ESNs could be used to initialize the weights in a fully trainable recurrent network (with the hidden-to-hidden recurrent weights trained using back-propagation&#160;through time), helping to learn long-term dependencies (Sutskever, 2012; Sutskever&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013). In this setting, an initial spectral radius of 1.2 performs well, combined&#160;with the sparse initialization scheme described in Sec. 8.4.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">10.9 Leaky Units and Other Strategies for Multiple&#160;Time Scales</span></h4>
<p><span class="font64">One way to deal with long-term dependencies is to design a model that operates at multiple time scales, so that some parts of the model operate at fine-grained&#160;time scales and can handle small details, while other parts operate at coarse time&#160;scales and transfer information from the distant past to the present more efficiently.&#160;Various strategies for building both fine and coarse time scales are possible. These&#160;include the addition of skip connections across time, “leaky units” that integrate&#160;signals with different time constants, and the removal of some of the connections&#160;used to model fine-grained time scales.</span></p><h5><a id="bookmark6"></a><span class="font64" style="font-weight:bold;">10.9.1 &#160;&#160;&#160;Adding Skip Connections through Time</span></h5>
<p><span class="font64">One way to obtain coarse time scales is to add direct connections from variables in the distant past to variables in the present. The idea of using such skip connections&#160;dates back to Lin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (1996) and follows from the idea of incorporating delays in&#160;feedforward neural networks (Lang and Hinton, 1988). In an ordinary recurrent&#160;network, a recurrent connection goes from a unit at time t to a unit at time t + </span><span class="font18">1</span><span class="font64">.&#160;It is possible to construct recurrent networks with longer delays (Bengio, 1991).</span></p>
<p><span class="font64">As we have seen in Sec. 8.2.5, gradients may vanish or explode exponentially </span><span class="font64" style="font-weight:bold;">with respect to the number of time steps</span><span class="font64">. Lin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (1996) introduced&#160;recurrent connections with a time-delay of d to mitigate this problem. Gradients&#160;now diminish exponentially as a function of d rather than t. Since there are both&#160;delayed and single step connections, gradients may still explode exponentially in t.&#160;This allows the learning algorithm to capture longer dependencies although not all&#160;long-term dependencies may be represented well in this way.</span></p><h5><a id="bookmark7"></a><span class="font64" style="font-weight:bold;">10.9.2 &#160;&#160;&#160;Leaky Units and a Spectrum of Different Time Scales</span></h5>
<p><span class="font64">Another way to obtain paths on which the product of derivatives is close to one is to have units with </span><span class="font64" style="font-weight:bold;">linear </span><span class="font64">self-connections and a weight near one on these connections.</span></p>
<p><span class="font64">When we accumulate a running average of some value vby applying the update p<sup>(t)</sup> ^ ap<sup>(t-1)</sup> + </span><span class="font18">(1</span><span class="font64"> — a) v<sup>(t)</sup> the a parameter is an example of a linear selfconnection from p<sup>(t-1)</sup> to&#160;&#160;&#160;&#160;. When a is near one, the running average remembers</span></p>
<p><span class="font64">information about the past for a long time, and when a is near zero, information about the past is rapidly discarded. Hidden units with linear self-connections can&#160;behave similarly to such running averages. Such hidden units are called </span><span class="font64" style="font-weight:bold;font-style:italic;">leaky units.</span></p>
<p><span class="font64">Skip connections through d time steps are a way of ensuring that a unit can always learn to be influenced by a value from d time steps earlier. The use of a&#160;linear self-connection with a weight near one is a different way of ensuring that the&#160;unit can access values from the past. The linear self-connection approach allows&#160;this effect to be adapted more smoothly and flexibly by adjusting the real-valued&#160;a rather than by adjusting the integer-valued skip length.</span></p>
<p><span class="font64">These ideas were proposed by Mozer (1992) and by El Hihi and Bengio (1996). Leaky units were also found to be useful in the context of echo state networks&#160;(Jaeger </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2007).</span></p>
<p><span class="font64">There are two basic strategies for setting the time constants used by leaky units. One strategy is to manually fix them to values that remain constant, for&#160;example by sampling their values from some distribution once at initialization time.&#160;Another strategy is to make the time constants free parameters and learn them.&#160;Having such leaky units at different time scales appears to help with long-term&#160;dependencies (Mozer, 1992; Pascanu </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2013a).</span></p><h5><a id="bookmark8"></a><span class="font64" style="font-weight:bold;">10.9.3 Removing Connections</span></h5>
<p><span class="font64">Another approach to handle long-term dependencies is the idea of organizing the state of the RNN at multiple time-scales (El Hihi and Bengio, 1996), with&#160;information flowing more easily through long distances at the slower time scales.</span></p>
<p><span class="font64">This idea differs from the skip connections through time discussed earlier because it involves actively </span><span class="font64" style="font-weight:bold;">removing </span><span class="font64">length-one connections and replacing them&#160;with longer connections. Units modified in such a way are forced to operate on a&#160;long time scale. Skip connections through time </span><span class="font64" style="font-weight:bold;">add </span><span class="font64">edges. Units receiving such&#160;new connections may learn to operate on a long time scale but may also choose to&#160;focus on their other short-term connections.</span></p>
<p><span class="font64">There are different ways in which a group of recurrent units can be forced to operate at different time scales. One option is to make the recurrent units leaky,&#160;but to have different groups of units associated with different fixed time scales.&#160;This was the proposal in Mozer (1992) and has been successfully used in Pascanu&#160;</span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2013a). Another option is to have explicit and discrete updates taking place&#160;at different times, with a different frequency for different groups of units. This is&#160;the approach of El Hihi and Bengio (1996) and Koutnik </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2014). It worked&#160;well on a number of benchmark datasets.</span></p><h4><a id="bookmark9"></a><span class="font65" style="font-weight:bold;">10.10 The Long Short-Term Memory and Other Gated&#160;RNNs</span></h4>
<p><span class="font64">As of this writing, the most effective sequence models used in practical applications are called </span><span class="font64" style="font-style:italic;">gated RNNs.</span><span class="font64"> These include the </span><span class="font64" style="font-style:italic;">long short-term memory</span><span class="font64"> and networks&#160;based on the </span><span class="font64" style="font-style:italic;">gated recurrent unit.</span></p>
<p><span class="font64">Like leaky units, gated RNNs are based on the idea of creating paths through time that have derivatives that neither vanish nor explode. Leaky units did&#160;this with connection weights that were either manually chosen constants or were&#160;parameters. Gated RNNs generalize this to connection weights that may change&#160;at each time step.</span></p>
<p><span class="font64">Leaky units allow the network to </span><span class="font64" style="font-weight:bold;">accumulate </span><span class="font64">information (such as evidence for a particular feature or category) over a long duration. However, once that&#160;information has been used, it might be useful for the neural network to </span><span class="font64" style="font-weight:bold;">forget </span><span class="font64">the&#160;old state. For example, if a sequence is made of sub-sequences and we want a leaky&#160;unit to accumulate evidence inside each sub-subsequence, we need a mechanism to&#160;forget the old state by setting it to zero. Instead of manually deciding when to&#160;clear the state, we want the neural network to learn to decide when to do it. This&#160;is what gated RNNs do.</span></p><h5><a id="bookmark10"></a><span class="font64" style="font-weight:bold;">10.10.1 LSTM</span></h5>
<p><span class="font64">The clever idea of introducing self-loops to produce paths where the gradient can flow for long durations is a core contribution of the initial </span><span class="font64" style="font-style:italic;">long short-term memory&#160;(LSTM)</span><span class="font64"> model (Hochreiter and Schmidhuber, 1997). A crucial addition has been&#160;to make the weight on this self-loop conditioned on the context, rather than fixed&#160;(Gers </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2000). By making the weight of this self-loop gated (controlled by&#160;another hidden unit), the time scale of integration can be changed dynamically. In&#160;this case, we mean that even for an LSTM with fixed parameters, the time scale of&#160;integration can change based on the input sequence, because the time constants&#160;are output by the model itself. The LSTM has been found extremely successful&#160;in many applications, such as unconstrained handwriting recognition (Graves&#160;</span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2009), speech recognition (Graves </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2013; Graves and Jaitly, 2014),&#160;handwriting generation (Graves, 2013), machine translation (Sutskever </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2014),&#160;image captioning (Kiros </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2014b; Vinyals </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2014b; Xu </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2015) and&#160;parsing (Vinyals </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2014a).</span></p>
<p><span class="font64">The LSTM block diagram is illustrated in Fig. 10.16. The corresponding forward propagation equations are given below, in the case of a shallow recurrent</span></p><div><div><img src="main-133.jpg" alt=""/>
<p><span class="font64">Figure 10.16: Block diagram of the LSTM recurrent network “cell.” Cells are connected recurrently to each other, replacing the usual hidden units of ordinary recurrent networks.&#160;An input feature is computed with a regular artificial neuron unit. Its value can be&#160;accumulated into the state if the sigmoidal input gate allows it. The state unit has a&#160;linear self-loop whose weight is controlled by the forget gate. The output of the cell can&#160;be shut off by the output gate. All the gating units have a sigmoid nonlinearity, while the&#160;input unit can have any squashing nonlinearity. The state unit can also be used as an&#160;extra input to the gating units. The black square indicates a delay of a single time step.</span></p></div></div>
<p><span class="font64">network architecture. Deeper architectures have also been successfully used (Graves </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2013; Pascanu </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2014a). Instead of a unit that simply applies an elementwise nonlinearity to the affine transformation of inputs and recurrent units, LSTM&#160;recurrent networks have “LSTM cells” that have an internal recurrence (a self-loop),&#160;in addition to the outer recurrence of the RNN. Each cell has the same inputs&#160;and outputs as an ordinary recurrent network, but has more parameters and a&#160;system of gating units that controls the flow of information. The most important&#160;component is the state unit s<sup>(t)</sup> that has a linear self-loop similar to the leaky&#160;units described in the previous section. However, here, the self-loop weight (or the&#160;associated time constant) is controlled by a </span><span class="font64" style="font-style:italic;">forget gate</span><span class="font64"> unit f<sup>(t)</sup> (for time step t&#160;and cell i), that sets this weight to a value between 0 and 1 via a sigmoid unit:</span></p><div>
<p><span class="font64">(10.40)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">a</span></p></div>
<p><span class="font66">I</span><span class="font64" style="font-weight:bold;">f </span><span class="font64" style="font-weight:bold;font-style:italic;">+ </span><span class="font65" style="font-style:italic;">y</span><span class="font66"> u</span><span class="font64" style="font-weight:bold;"><sup>f</sup> 3 </span><span class="font64" style="font-weight:bold;font-style:italic;">+ </span><span class="font65" style="font-style:italic;">y </span><span class="font64" style="font-weight:bold;font-style:italic;">wf.</span><span class="font64"> h</span><span class="font64" style="font-weight:bold;"><sup>(t-I</sup>&gt;</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>1</sup> &#160;&#160;&#160;i3 3</span><span class="font64">&#160;&#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;"><sup>1,</sup>3 3</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">3 &#160;&#160;&#160;<sup>3</sup></span></p>
<p><span class="font64">where x<sup>(t)</sup> is the current input vector and h<sup>(t)</sup> is the current hidden layer vector, containing the outputs of all the LSTM cells, and </span><span class="font64" style="font-style:italic;">b</span><span class="font64"> , U<sup>f</sup>, </span><span class="font64" style="font-style:italic;">W<sup>f</sup></span><span class="font64"> are respectively&#160;biases, input weights and recurrent weights for the forget gates. The LSTM cell&#160;internal state is thus updated as follows, but with a conditional self-loop weight</span></p>
<p><span class="font64" style="font-style:italic;"><sub>f</sub> (t)<sub>: </sub><sup>f</sup> i</span><span class="font64"> <sup>:</sup></span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">St =</span><span class="font64"> ff <sub>s</sub>f<sup>-1)</sup> + g (<sup>t)</sup>a</span></p></div><div>
<p><span class="font64" style="font-style:italic;">i</span></p></div><div>
<p><span class="font66">i</span></p></div><div>
<p><span class="font64">+ sfa I b</span><span class="font64" style="font-weight:bold;">i </span><span class="font64">+ E U</span><span class="font64" style="font-weight:bold;">i,3</span><span class="font64">x</span><span class="font64" style="font-weight:bold;">&lt;<sup>&lt;</sup></span><span class="font64"><sup>)</sup> + E</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>w</sup>‘j<sup>h&lt;</sup>3</span><span class="font64" style="font-style:italic;">‘<sup>-I)</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">33</span></p></div><div>
<p><span class="font67" style="font-weight:bold;">A </span><span class="font66">J ־</span></p></div><div>
<p><span class="font64">(10.41)</span></p></div>
<p><span class="font64">where b, U and W respectively denote the biases, input weights and recurrent weights into the LSTM cell. The </span><span class="font64" style="font-style:italic;">external input gate</span><span class="font64"> unit g<sup>(t)</sup> is computed similarly&#160;to the forget gate (with a sigmoid unit to obtain a gating value between 0 and 1),&#160;but with its own parameters:</span></p><div>
<p><span class="font64">g</span></p></div><div>
<p><span class="font64" style="font-style:italic;">(t)</span><span class="font64"> _</span></p></div><div>
<p><span class="font64">= a I b<sup>9</sup> + V </span><span class="font64" style="font-style:italic;">U<sup>9</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">.</span><span class="font64" style="font-style:italic;">x</span><span class="font64"><sup>(t)</sup> + V</span><span class="font64" style="font-style:italic;">W<sup>a</sup> h<sup>(t-I)</sup></span><span class="font64"> I .</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>1</sup> &#160;&#160;&#160;1,3 3&#160;&#160;&#160;&#160;1,3 3</span></p></div><div>
<p><span class="font64">(10.42)</span></p></div>
<p><span class="font64">The output h<sup>(t)</sup> of the LSTM cell can also be shut off, via the </span><span class="font64" style="font-style:italic;">output gate q</span><span class="font64"><sup>(t)</sup>, which also uses a sigmoid unit for gating:</span></p><div>
<p><span class="font64">(10.43)</span></p>
<p><span class="font64">(10.44)</span></p></div>
<p><span class="font64">h<sup>(t)</sup> = tanh ^s<sup>(t)</sup>^ q<sup>(t)</sup></span></p>
<p><span class="font64"><sup>q</sup></span><span class="font64" style="font-weight:bold;"><sup>(t)</sup>=</span><span class="font64">״ I<sup>6</sup></span><span class="font64" style="font-weight:bold;">? + </span><span class="font66">E <sup>U</sup> </span><span class="font64" style="font-weight:bold;">°3■ </span><span class="font64">x</span><span class="font64" style="font-weight:bold;">3<sup>t)</sup>+</span><span class="font66">E W</span><span class="font64" style="font-weight:bold;">3 </span><span class="font64"><sup>h</sup></span><span class="font64" style="font-weight:bold;">3<sup>t-I)</sup></span></p>
<p><span class="font64">which has parameters </span><span class="font64" style="font-weight:bold;">b</span><span class="font64"><sup>o</sup>, </span><span class="font64" style="font-weight:bold;font-style:italic;">U<sup>o</sup>, W<sup>o</sup></span><span class="font64"> for its biases, input weights and recurrent weights, respectively. Among the variants, one can choose to use the cell state s<sup>(t)&#160;</sup>as an extra input (with its weight) into the three gates of the i-th unit, as shown&#160;in Fig. 10.16. This would require three additional parameters.</span></p>
<p><span class="font64">LSTM networks have been shown to learn long-term dependencies more easily than the simple recurrent architectures, first on artificial data sets designed for&#160;testing the ability to learn long-term dependencies (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1994; Hochreiter&#160;and Schmidhuber, 1997; Hochreiter </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2001), then on challenging sequence&#160;processing tasks where state-of-the-art performance was obtained (Graves, 2012;&#160;Graves </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013; Sutskever </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014). Variants and alternatives to the LSTM&#160;have been studied and used and are discussed next.</span></p><h5><a id="bookmark11"></a><span class="font64" style="font-weight:bold;">10.10.2 Other Gated RNNs</span></h5>
<p><span class="font64">Which pieces of the LSTM architecture are actually necessary? What other successful architectures could be designed that allow the network to dynamically&#160;control the time scale and forgetting behavior of different units?</span></p>
<p><span class="font64">Some answers to these questions are given with the recent work on gated RNNs, whose units are also known as gated recurrent units or GRUs (Cho </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014b;&#160;Chung </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014, 2015a; Jozefowicz </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015; Chrupala </span><span class="font64" style="font-weight:bold;font-style:italic;">et</span><span class="font64"> al., 2015). The main&#160;difference with the LSTM is that a single gating unit simultaneously controls the&#160;forgetting factor and the decision to update the state unit. The update equations&#160;are the following:</span></p><div>
<p><span class="font64" style="font-style:italic;">(</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span></p></div><div>
<p dir="rtl"><span class="font66">ץ</span></p>
<p dir="rtl"><span class="font66">־ ץ</span></p>
<p dir="rtl"><span class="font64">(10.45)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">(t-</span><span class="font18">1</span><span class="font64">) </span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>h</sub>(t-</span><span class="font18">1</span><span class="font64">)</span></p>
<p><span class="font64"><sup>i-</sup>j ׳ j &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>h</sup>j</span></p></div><div>
<p><span class="font64">h<sup>(t)</sup> = u<sup>(t </sup></span><span class="font18"><sup>1</sup></span><span class="font64"><sup>)</sup>h<sup>(t 1)</sup> + </span><span class="font18">(1</span><span class="font64"> - u<sup>(t 1</sup>V ( </span><span class="font64" style="font-weight:bold;font-style:italic;">bi</span><span class="font64"> + £ </span><span class="font64" style="font-weight:bold;font-style:italic;">Uj<sup>x</sup>jj</span><span class="font64"> <sup>1)</sup> + £ </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>Wi</sup>-j &#160;&#160;&#160;<sup>h</sup>j</span></p></div><div>
<p><span class="font67" style="font-weight:bold;font-style:italic;">X</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>j</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>j</sup></span></p></div><div>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;">u </span><span class="font64">stands for “update” gate and </span><span class="font64" style="font-weight:bold;">r </span><span class="font64">for “reset” gate. Their value is defined as usual:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(t)</sup></span><span class="font64"> - - (<sub>b</sub>u + &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">Tiu</span><span class="font64"> <sub>x</sub> </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(t)</sup></span><span class="font64"> +&#160;&#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>W</sub>u <sub>h</sub><sup>(t)</sup></span><span class="font64"> ץ</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">u</span></p></div><div>
<p><span class="font64">- <sup>(b</sup>u + £ Uj </span><span class="font64" style="font-weight:bold;font-style:italic;">j</span><span class="font64">+£ Wj <sup>h</sup>j</span></p></div><div>
<p><span class="font64">(10.46)</span></p></div>
<p><span class="font64">and</span></p><div>
<p><span class="font64">(10.47)</span></p></div>
<p><span class="font64">ri׳ - </span><span class="font64" style="font-weight:bold;font-style:italic;">-lb</span><span class="font64"> r +£ </span><span class="font65" style="font-style:italic;">U</span><span class="font64">[j </span><span class="font65" style="font-style:italic;">j +Y,</span><span class="font66"> Wj h j<sup>t)</sup>| .</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>j</sup></span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>j</sup></span></p>
<p><span class="font64">The reset and updates gates can individually “ignore” parts of the state vector. The update gates act like conditional leaky integrators that can linearly gate any&#160;dimension, thus choosing to copy it (at one extreme of the sigmoid) or completely&#160;ignore it (at the other extreme) by replacing it by the new “target state” value&#160;(towards which the leaky integrator wants to converge). The reset gates control&#160;which parts of the state get used to compute the next target state, introducing an&#160;additional nonlinear effect in the relationship between past state and future state.</span></p>
<p><span class="font64">Many more variants around this theme can be designed. For example the reset gate (or forget gate) output could be shared across multiple hidden units.&#160;Alternately, the product of a global gate (covering a whole group of units, such as&#160;an entire layer) and a local gate (per unit) could be used to combine global control&#160;and local control. However, several investigations over architectural variations&#160;of the LSTM and GRU found no variant that would clearly beat both of these&#160;across a wide range of tasks (Greff </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2015; Jozefowicz </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2015). Greff&#160;</span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2015) found that a crucial ingredient is the forget gate, while Jozefowicz&#160;</span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2015) found that adding a bias of 1 to the LSTM forget gate, a practice&#160;advocated by Gers </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2000), makes the LSTM as strong as the best of the&#160;explored architectural variants.</span></p><h4><a id="bookmark12"></a><span class="font65" style="font-weight:bold;">10.11 Optimization for Long-Term Dependencies</span></h4>
<p><span class="font64">Sec. 8.2.5 and Sec. 10.7 have described the vanishing and exploding gradient problems that occur when optimizing RNNs over many time steps.</span></p>
<p><span class="font64">An interesting idea proposed by Martens and Sutskever (2011) is that second derivatives may vanish at the same time that first derivatives vanish. Second-order&#160;optimization algorithms may roughly be understood as dividing the first derivative&#160;by the second derivative (in higher dimension, multiplying the gradient by the&#160;inverse Hessian). If the second derivative shrinks at a similar rate to the first&#160;derivative, then the ratio of first and second derivatives may remain relatively&#160;constant. Unfortunately, second-order methods have many drawbacks, including&#160;high computational cost, the need for a large minibatch, and a tendency to be&#160;attracted to saddle points. Martens and Sutskever (2011) found promising results&#160;using second-order methods. Later, Sutskever </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2013) found that simpler&#160;methods such as Nesterov momentum with careful initialization could achieve&#160;similar results. See Sutskever (2012) for more detail. Both of these approaches&#160;have largely been replaced by simply using SGD (even without momentum) applied&#160;to LSTMs. This is part of a continuing theme in machine learning that it is often&#160;much easier to design a model that is easy to optimize than it is to design a more&#160;powerful optimization algorithm.</span></p><h5><a id="bookmark13"></a><span class="font64" style="font-weight:bold;">10.11.1 Clipping Gradients</span></h5>
<p><span class="font64">As discussed in Sec. 8.2.4, strongly nonlinear functions such as those computed by a recurrent net over many time steps tend to have derivatives that can be either&#160;very large or very small in magnitude. This is illustrated in Fig. 8.3 and Fig. 10.17,&#160;in which we see that the objective function (as a function of the parameters) has a&#160;“landscape” in which one finds “cliffs”: wide and rather flat regions separated by&#160;tiny regions where the objective function changes quickly, forming a kind of cliff.</span></p>
<p><span class="font64">The difficulty that arises is that when the parameter gradient is very large, a gradient descent parameter update could throw the parameters very far, into a&#160;region where the objective function is larger, undoing much of the work that had&#160;been done to reach the current solution. The gradient tells us the direction that&#160;corresponds to the steepest descent within an infinitesimal region surrounding the&#160;current parameters. Outside of this infinitesimal region, the cost function may&#160;begin to curve back upwards. The update must be chosen to be small enough to&#160;avoid traversing too much upward curvature. We typically use learning rates that&#160;decay slowly enough that consecutive steps have approximately the same learning&#160;rate. A step size that is appropriate for a relatively linear part of the landscape is&#160;often inappropriate and causes uphill motion if we enter a more curved part of the&#160;landscape on the next step.</span></p>
<p><span class="font64">Without clipping</span></p><div>
<p><span class="font64">With clipping</span></p></div>
<p><span class="font64">Figure 10.17: Example of the effect of gradient clipping in a recurrent network with two parameters w and b. Gradient clipping can make gradient descent perform more&#160;reasonably in the vicinity of extremely steep cliffs. These steep cliffs commonly occur&#160;in recurrent networks near where a recurrent network behaves approximately linearly.&#160;The cliff is exponentially steep in the number of time steps because the weight matrix&#160;is multiplied by itself once for each time step. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> Gradient descent without gradient&#160;clipping overshoots the bottom of this small ravine, then receives a very large gradient&#160;from the cliff face. The large gradient catastrophically propels the parameters outside the&#160;axes of the plot. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> Gradient descent with gradient clipping has a more moderate&#160;reaction to the cliff. While it does ascend the cliff face, the step size is restricted so that&#160;it cannot be propelled away from steep region near the solution. Figure adapted with&#160;permission from Pascanu </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2013a).</span></p>
<p><span class="font64">A simple type of solution has been in use by practitioners for many years: </span><span class="font64" style="font-style:italic;">clipping the gradient</span><span class="font64" style="font-weight:bold;font-style:italic;">.</span><span class="font64"> There are different instances of this idea (Mikolov, 2012;&#160;Pascanu </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> 2013a). One option is to clip the parameter gradient from a&#160;minibatch </span><span class="font64" style="font-style:italic;">element-wise</span><span class="font64"> (Mikolov, 2012) just before the parameter update. Another&#160;is to </span><span class="font64" style="font-style:italic;">clip the norm</span><span class="font64"> ||g|| </span><span class="font64" style="font-style:italic;">of the gradient </span><span class="font64" style="font-weight:bold;font-style:italic;">g</span><span class="font64"> (Pascanu </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64" style="font-weight:bold;font-style:italic;">,</span><span class="font64"> 2013a) just before the&#160;parameter update:</span></p><div>
<p><span class="font64">if ||g|| &gt; </span><span class="font64" style="font-style:italic;">v</span></p></div><div>
<p><span class="font64"><sup>g</sup></span></p></div><div>
<p><span class="font64">gv</span></p>
<p><span class="font64">| g|</span></p></div><div>
<p><span class="font64">(10.48)</span></p>
<p><span class="font64">(10.49)</span></p></div>
<p><span class="font64">where </span><span class="font64" style="font-style:italic;">v</span><span class="font64"> is the norm threshold and g is used to update parameters. Because the gradient of all the parameters (including different groups of parameters, such as&#160;weights and biases) is renormalized jointly with a single scaling factor, the latter&#160;method has the advantage that it guarantees that each step is still in the gradient&#160;direction, but experiments suggest that both forms work similarly. Although</span></p>
<p><span class="font64">the parameter update has the same direction as the true gradient, with gradient norm clipping, the parameter update vector norm is now bounded. This bounded&#160;gradient avoids performing a detrimental step when the gradient explodes. In&#160;fact, even simply taking a </span><span class="font64" style="font-weight:bold;font-style:italic;">random step</span><span class="font64"> when the gradient magnitude is above&#160;a threshold tends to work almost as well. If the explosion is so severe that the&#160;gradient is numerically Inf or Nan (considered infinite or not-a-number), then&#160;a random step of size v can be taken and will typically move away from the&#160;numerically unstable configuration. Clipping the gradient norm per-minibatch will&#160;not change the direction of the gradient for an individual minibatch. However,&#160;taking the average of the norm-clipped gradient from many minibatches is not&#160;equivalent to clipping the norm of the true gradient (the gradient formed from&#160;using all examples). Examples that have large gradient norm, as well as examples&#160;that appear in the same minibatch as such examples, will have their contribution&#160;to the final direction diminished. This stands in contrast to traditional minibatch&#160;gradient descent, where the true gradient direction is equal to the average over all&#160;minibatch gradients. Put another way, traditional stochastic gradient descent uses&#160;an unbiased estimate of the gradient, while gradient descent with norm clipping&#160;introduces a heuristic bias that we know empirically to be useful. With elementwise clipping, the direction of the update is not aligned with the true gradient&#160;or the minibatch gradient, but it is still a descent direction. It has also been&#160;proposed (Graves, 2013) to clip the back-propagated gradient (with respect to&#160;hidden units) but no comparison has been published between these variants; we&#160;conjecture that all these methods behave similarly.</span></p><h5><a id="bookmark14"></a><span class="font64" style="font-weight:bold;">10.11.2 Regularizing to Encourage Information Flow</span></h5>
<p><span class="font64">Gradient clipping helps to deal with exploding gradients, but it does not help with vanishing gradients. To address vanishing gradients and better capture long-term&#160;dependencies, we discussed the idea of creating paths in the computational graph of&#160;the unfolded recurrent architecture along which the product of gradients associated&#160;with arcs is near 1. One approach to achieve this is with LSTMs and other self-loops&#160;and gating mechanisms, described above in Sec. 10.10. Another idea is to regularize&#160;or constrain the parameters so as to encourage “information flow.” In particular,&#160;we would like the gradient vector V </span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>h</sub>(t) L</span><span class="font64"> being back-propagated to maintain its&#160;magnitude, even if the loss function only penalizes the output at the end of the&#160;sequence. Formally, we want</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span><span class="font64"> h<sup>(t-1)</sup></span></p></div>
<p><span class="font64">(10.50)</span></p>
<p><span class="font64">(10.51)</span></p><div>
<p><span class="font64">Vh(t)<sup>L</sup>•</span></p></div><div>
<p><span class="font64">to be as large as</span></p></div>
<p><span class="font64">With this objective, Pascanu </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2013a) propose the following regularizes</span></p><div>
<p><span class="font64">I (V h(t) L)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">_d</span><span class="font64">h</span><span class="font64" style="font-weight:bold;"><sup>(t)</sup>_ 1 d</span><span class="font64">h</span><span class="font64" style="font-weight:bold;"><sup>(t-1)&#160;&#160;&#160;&#160;1</sup></span></p></div><div>
<p><span class="font64">2</span></p></div><div><h4><a id="bookmark15"></a><span class="font65" style="font-weight:bold;">I|Vh(t) L||</span></h4></div><div>
<p><span class="font18">1</span></p></div><div>
<p><span class="font64">(10.52)</span></p></div>
<p><span class="font64">Computing the gradient of this regularizer may appear difficult, but Pascanu </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2013a) propose an approximation in which we consider the back-propagated&#160;vectors Vh </span><span class="font64" style="font-style:italic;">(<sub>t</sub>)L</span><span class="font64"> as if they were constants (for the purpose of this regularizer, so&#160;that there is no need to back-propagate through them). The experiments with&#160;this regularizer suggest that, if combined with the norm clipping heuristic (which&#160;handles gradient explosion), the regularizer can considerably increase the span of&#160;the dependencies that an RNN can learn. Because it keeps the RNN dynamics&#160;on the edge of explosive gradients, the gradient clipping is particularly important.&#160;Without gradient clipping, gradient explosion prevents learning from succeeding.</span></p>
<p><span class="font64">A key weakness of this approach is that it is not as effective as the LSTM for tasks where data is abundant, such as language modeling.</span></p><h4><a id="bookmark16"></a><span class="font65" style="font-weight:bold;">10.12 Explicit Memory</span></h4>
<p><span class="font64">Intelligence requires knowledge and acquiring knowledge can be done via learning, which has motivated the development of large-scale deep architectures. However,&#160;there are different kinds of knowledge. Some knowledge can be implicit, subconscious, and difficult to verbalize—such as how to walk, or how a dog looks&#160;different from a cat. Other knowledge can be explicit, declarative, and relatively&#160;straightforward to put into words—every day commonsense knowledge, like “a cat&#160;is a kind of animal,” or very specific facts that you need to know to accomplish&#160;your current goals, like “the meeting with the sales team is at 3:00 PM in room&#160;141.”</span></p>
<p><span class="font64">Neural networks excel at storing implicit knowledge. However, they struggle to memorize facts. Stochastic gradient descent requires many presentations of&#160;the same input before it can be stored in a neural network parameters, and even&#160;then, that input will not be stored especially precisely. Graves </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2014b)&#160;hypothesized that this is because neural networks lack the equivalent of the </span><span class="font64" style="font-style:italic;">working&#160;memory</span><span class="font64"> system that allows human beings to explicitly hold and manipulate pieces&#160;of information that are relevant to achieving some goal. Such explicit memory</span></p><div>
<p><span class="font63">Memory cells</span></p><img src="main-134.jpg" alt=""/>
<p><span class="font64">Figure 10.18: A schematic of an example of a network with an explicit memory, capturing some of the key design elements of the neural Turing machine. In this diagram we&#160;distinguish the “representation” part of the model (the “task network,” here a recurrent&#160;net in the bottom) from the “memory” part of the model (the set of cells), which can&#160;store facts. The task network learns to “control” the memory, deciding where to read from&#160;and where to write to within the memory (through the reading and writing mechanisms,&#160;indicated by bold arrows pointing at the reading and writing addresses).</span></p></div>
<p><span class="font64">components would allow our systems not only to rapidly and “intentionally” store and retrieve specific facts but also to sequentially reason with them. The need&#160;for neural networks that can process information in a sequence of steps, changing&#160;the way the input is fed into the network at each step, has long been recognized&#160;as important for the ability to reason rather than to make automatic, intuitive&#160;responses to the input (Hinton, 1990).</span></p>
<p><span class="font64">To resolve this difficulty, Weston </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2014) introduced </span><span class="font64" style="font-style:italic;">memory networks</span><span class="font64"> that include a set of memory cells that can be accessed via an addressing mechanism.&#160;Memory networks originally required a supervision signal instructing them how&#160;to use their memory cells. Graves </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2014b) introduced the </span><span class="font64" style="font-style:italic;">neural Turing&#160;machine,</span><span class="font64"> which is able to learn to read from and write arbitrary content to memory&#160;cells without explicit supervision about which actions to undertake, and allowed&#160;end-to-end training without this supervision signal, via the use of a content-based&#160;soft attention mechanism (see Bahdanau </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2015) and Sec. 12.4.5.1). This&#160;soft addressing mechanism has become standard with other related architectures&#160;emulating algorithmic mechanisms in a way that still allows gradient-based optimization (Sukhbaatar </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2015; Joulin and Mikolov, 2015; Kumar </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2015;&#160;Vinyals </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2015a; Grefenstette </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2015).</span></p>
<p><span class="font64">Each memory cell can be thought of as an extension of the memory cells in LSTMs and GRUs. The difference is that the network outputs an internal state&#160;that chooses which cell to read from or write to, just as memory accesses in a&#160;digital computer read from or write to a specific address.</span></p>
<p><span class="font64">It is difficult to optimize functions that produce exact, integer addresses. To alleviate this problem, NTMs actually read to or write from many memory cells&#160;simultaneously. To read, they take a weighted average of many cells. To write, they&#160;modify multiple cells by different amounts. The coefficients for these operations&#160;are chosen to be focused on a small number of cells, for example, by producing&#160;them via a softmax function. Using these weights with non-zero derivatives allows&#160;the functions controlling access to the memory to be optimized using gradient&#160;descent. The gradient on these coefficients indicates whether each of them should&#160;be increased or decreased, but the gradient will typically be large only for those&#160;memory addresses receiving a large coefficient.</span></p>
<p><span class="font64">These memory cells are typically augmented to contain a vector, rather than the single scalar stored by an LSTM or GRU memory cell. There are two reasons&#160;to increase the size of the memory cell. One reason is that we have increased the&#160;cost of accessing a memory cell. We pay the computational cost of producing a&#160;coefficient for many cells, but we expect these coefficients to cluster around a small&#160;number of cells. By reading a vector value, rather than a scalar value, we can&#160;offset some of this cost. Another reason to use vector-valued memory cells is that&#160;they allow for </span><span class="font64" style="font-weight:bold;font-style:italic;">content-based addressing</span><span class="font64">, where the weight used to read to or write&#160;from a cell is a function of that cell. Vector-valued cells allow us to retrieve a&#160;complete vector-valued memory if we are able to produce a pattern that matches&#160;some but not all of its elements. This is analogous to the way that people can&#160;recall the lyrics of a song based on a few words. We can think of a content-based&#160;read instruction as saying, “Retrieve the lyrics of the song that has the chorus ‘We&#160;all live in a yellow submarine.’” Content-based addressing is more useful when we&#160;make the objects to be retrieved large—if every letter of the song was stored in a&#160;separate memory cell, we would not be able to find them this way. By comparison,&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">location-based addressing</span><span class="font64"> is not allowed to refer to the content of the memory. We&#160;can think of a location-based read instruction as saying “Retrieve the lyrics of&#160;the song in slot 347.” Location-based addressing can often be a perfectly sensible&#160;mechanism even when the memory cells are small.</span></p>
<p><span class="font64">If the content of a memory cell is copied (not forgotten) at most time steps, then the information it contains can be propagated forward in time and the gradients&#160;propagated backward in time without either vanishing or exploding.</span></p>
<p><span class="font64">The explicit memory approach is illustrated in Fig. 10.18, where we see that a “task neural network” is coupled with a memory. Although that task neural&#160;network could be feedforward or recurrent, the overall system is a recurrent network.&#160;The task network can choose to read from or write to specific memory addresses.&#160;Explicit memory seems to allow models to learn tasks that ordinary RNNs or LSTM&#160;RNNs cannot learn. One reason for this advantage may be because information and&#160;gradients can be propagated (forward in time or backwards in time, respectively)&#160;for very long durations.</span></p>
<p><span class="font64">As an alternative to back-propagation through weighted averages of memory cells, we can interpret the memory addressing coefficients as probabilities and&#160;stochastically read just one cell (Zaremba and Sutskever, 2015). Optimizing models&#160;that make discrete decisions requires specialized optimization algorithms, described&#160;in Sec. 20.9.1. So far, training these stochastic architectures that make discrete&#160;decisions remains harder than training deterministic algorithms that make soft&#160;decisions.</span></p>
<p><span class="font64">Whether it is soft (allowing back-propagation) or stochastic and hard, the mechanism for choosing an address is in its form identical to the </span><span class="font64" style="font-weight:bold;font-style:italic;">attention mechanism </span><span class="font64">which had been previously introduced in the context of machine translation (Bah-danau </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64" style="font-style:italic;">,</span><span class="font64"> 2015) and discussed in Sec. 12.4.5.1. The idea of attention mechanisms&#160;for neural networks was introduced even earlier, in the context of handwriting&#160;generation (Graves, 2013), with an attention mechanism that was constrained to&#160;move only forward in time through the sequence. In the case of machine translation&#160;and memory networks, at each step, the focus of attention can move to a completely&#160;different place, compared to the previous step.</span></p>
<p><span class="font64">Recurrent neural networks provide a way to extend deep learning to sequential data. They are the last major tool in our deep learning toolbox. Our discussion now&#160;moves to how to choose and use these tools and how to apply them to real-world&#160;tasks.</span></p>
<p><a id="bookmark2"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font64"><sup></sup> We suggest to not abbreviate “recursive neural network” as “RNN” to avoid confusion with&#160;“recurrent neural network.”</span></p>
</body>
</html>