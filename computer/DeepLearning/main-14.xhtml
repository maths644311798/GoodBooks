<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 7</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Regularization for Deep Learning</span></h2>
<p><span class="font64">A central problem in machine learning is how to make an algorithm that will perform well not just on the training data, but also on new inputs. Many strategies&#160;used in machine learning are explicitly designed to reduce the test error, possibly&#160;at the expense of increased training error. These strategies are known collectively&#160;as regularization. As we will see there are a great many forms of regularization&#160;available to the deep learning practitioner. In fact, developing more effective&#160;regularization strategies has been one of the major research efforts in the field.</span></p>
<p><span class="font64">Chapter 5 introduced the basic concepts of generalization, underfitting, overfitting, bias, variance and regularization. If you are not already familiar with these notions, please refer to that chapter before continuing with this one.</span></p>
<p><span class="font64">In this chapter, we describe regularization in more detail, focusing on regularization strategies for deep models or models that may be used as building blocks to form deep models.</span></p>
<p><span class="font64">Some sections of this chapter deal with standard concepts in machine learning. If you are already familiar with these concepts, feel free to skip the relevant&#160;sections. However, most of this chapter is concerned with the extension of these&#160;basic concepts to the particular case of neural networks.</span></p>
<p><span class="font64">In Sec. 5.2.2, we defined regularization as “any modification we make to a learning algorithm that is intended to reduce its generalization error but not&#160;its training error.” There are many regularization strategies. Some put extra&#160;constraints on a machine learning model, such as adding restrictions on the&#160;parameter values. Some add extra terms in the objective function that can be&#160;thought of as corresponding to a soft constraint on the parameter values. If chosen&#160;carefully, these extra constraints and penalties can lead to improved performance&#160;on the test set. Sometimes these constraints and penalties are designed to encode&#160;specific kinds of prior knowledge. Other times, these constraints and penalties&#160;are designed to express a generic preference for a simpler model class in order to&#160;promote generalization. Sometimes penalties and constraints are necessary to make&#160;an underdetermined problem determined. Other forms of regularization, known as&#160;ensemble methods, combine multiple hypotheses that explain the training data.</span></p>
<p><span class="font64">In the context of deep learning, most regularization strategies are based on regularizing estimators. Regularization of an estimator works by trading increased&#160;bias for reduced variance. An effective regularizer is one that makes a profitable&#160;trade, reducing variance significantly while not overly increasing the bias. When&#160;we discussed generalization and overfitting in Chapter 5, we focused on three&#160;situations, where the model family being trained either (1) excluded the true&#160;data generating process—corresponding to underfitting and inducing bias, or (2)&#160;matched the true data generating process, or (3) included the generating process&#160;but also many other possible generating processes—the overfitting regime where&#160;variance rather than bias dominates the estimation error. The goal of regularization&#160;is to take a model from the third regime into the second regime.</span></p>
<p><span class="font64">In practice, an overly complex model family does not necessarily include the target function or the true data generating process, or even a close approximation&#160;of either. We almost never have access to the true data generating process so&#160;we can never know for sure if the model family being estimated includes the&#160;generating process or not. However, most applications of deep learning algorithms&#160;are to domains where the true data generating process is almost certainly outside&#160;the model family. Deep learning algorithms are typically applied to extremely&#160;complicated domains such as images, audio sequences and text, for which the true&#160;generation process essentially involves simulating the entire universe. To some&#160;extent, we are always trying to fit a square peg (the data generating process) into&#160;a round hole (our model family).</span></p>
<p><span class="font64">What this means is that controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of&#160;parameters. Instead, we might find—and indeed in practical deep learning scenarios,&#160;we almost always do find—that the best fitting model (in the sense of minimizing&#160;generalization error) is a large model that has been regularized appropriately.</span></p>
<p><span class="font64">We now review several strategies for how to create such a large, deep, regularized model.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">7.1 Parameter Norm Penalties</span></h4>
<p><span class="font64">Regularization has been used for decades prior to the advent of deep learning. Linear models such as linear regression and logistic regression allow simple, straightforward,&#160;and effective regularization strategies.</span></p>
<p><span class="font64">Many regularization approaches are based on limiting the capacity of models, such as neural networks, linear regression, or logistic regression, by adding a parameter norm penalty </span><span class="font64" style="font-weight:bold;font-style:italic;">0,(0)</span><span class="font64"> to the objective function </span><span class="font64" style="font-weight:bold;font-style:italic;">J.</span><span class="font64"> We denote the regularized&#160;objective function by </span><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64">:</span></p>
<p><span class="font64">J(</span><span class="font64" style="font-weight:bold;">0</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">X</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">) </span><span class="font64" style="font-weight:bold;font-style:italic;">= J</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">0</span><span class="font64">; </span><span class="font64" style="font-weight:bold;font-style:italic;">X</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">) + afi(</span><span class="font64" style="font-weight:bold;">0</span><span class="font64">) &#160;&#160;&#160;(7.1)</span></p>
<p><span class="font64">where a E [0, to) is a hyperparameter that weights the relative contribution of the norm penalty term, 0, relative to the standard objective function J(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">; </span><span class="font64" style="font-weight:bold;font-style:italic;">0).&#160;</span><span class="font64">Setting a to 0 results in no regularization. Larger values of a correspond to more&#160;regularization.</span></p>
<p><span class="font64">When our training algorithm minimizes the regularized objective function </span><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64"> it will decrease both the original objective J on the training data and some measure&#160;of the size of the parameters </span><span class="font64" style="font-weight:bold;">0 </span><span class="font64">(or some subset of the parameters). Different&#160;choices for the parameter norm 0 can result in different solutions being preferred.&#160;In this section, we discuss the effects of the various norms when used as penalties&#160;on the model parameters.</span></p>
<p><span class="font64">Before delving into the regularization behavior of different norms, we note that for neural networks, we typically choose to use a parameter norm penalty 0 that&#160;penalizes </span><span class="font64" style="font-weight:bold;">only the weights </span><span class="font64">of the affine transformation at each layer and leaves&#160;the biases unregularized. The biases typically require less data to fit accurately&#160;than the weights. Each weight specifies how two variables interact. Fitting the&#160;weight well requires observing both variables in a variety of conditions. Each&#160;bias controls only a single variable. This means that we do not induce too much&#160;variance by leaving the biases unregularized. Also, regularizing the bias parameters&#160;can introduce a significant amount of underfitting. We therefore use the vector </span><span class="font64" style="font-weight:bold;">w&#160;</span><span class="font64">to indicate all of the weights that should be affected by a norm penalty, while the&#160;vector </span><span class="font64" style="font-weight:bold;">0 </span><span class="font64">denotes all of the parameters, including both </span><span class="font64" style="font-weight:bold;">w </span><span class="font64">and the unregularized&#160;parameters.</span></p>
<p><span class="font64">In the context of neural networks, it is sometimes desirable to use a separate penalty with a different a coefficient for each layer of the network. Because it can&#160;be expensive to search for the correct value of multiple hyperparameters, it is still&#160;reasonable to use the same weight decay at all layers just to reduce the size of&#160;search space.</span></p><h5><a id="bookmark2"></a><span class="font64" style="font-weight:bold;">7.1.1 L</span><span class="font64"><sup>2</sup> </span><span class="font64" style="font-weight:bold;">Parameter Regularization</span></h5>
<p><span class="font64">We have already seen, in Sec. 5.2.2, one of the simplest and most common kinds of parameter norm penalty: the L<sup>2</sup> parameter norm penalty commonly known as&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">weight decay.</span><span class="font64"> This regularization strategy drives the weights closer to the origin<a id="footnote1"></a><sup><a href="#bookmark3">1</a></sup><sup>&#160;</sup>by adding a regularization term Q(</span><span class="font64" style="font-weight:bold;">6</span><span class="font64">) = 2</span><span class="font64" style="font-weight:bold;">||w||</span><span class="font64">2 to the objective function. In&#160;other academic communities, L<sup>2</sup> regularization is also known as </span><span class="font64" style="font-weight:bold;font-style:italic;">ridge regression</span><span class="font64"> or&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">Tikhonov regularization.</span></p>
<p><span class="font64">We can gain some insight into the behavior of weight decay regularization by studying the gradient of the regularized objective function. To simplify the&#160;presentation, we assume no bias parameter, so </span><span class="font64" style="font-weight:bold;">6 </span><span class="font64">is just </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">. Such a model has the&#160;following total objective function:</span></p>
<p><span class="font64">a</span></p>
<p><span class="font64">We can see that the addition of the weight decay term has modified the learning rule to multiplicatively shrink the weight vector by a constant factor on each step,&#160;just before performing the usual gradient update. This describes what happens in&#160;a single step. But what happens over the entire course of training?</span></p>
<p><span class="font64">We will further simplify the analysis by making a quadratic approximation to the objective function in the neighborhood of the value of the weights that&#160;obtains minimal unregularized training cost, </span><span class="font64" style="font-weight:bold;">w* </span><span class="font64">= argmin<sub>w</sub> J(</span><span class="font64" style="font-weight:bold;">w</span><span class="font64">). If the objective&#160;function is truly quadratic, as in the case of fitting a linear regression model with</span></p><div>
<p><span class="font64">mean squared error, then the approximation is perfect. The approximation </span><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64"> is given by</span></p></div><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">J(e) = J</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">w*</span><span class="font64">) + <sub>2</sub>(</span><span class="font64" style="font-weight:bold;">w - w*</span><span class="font64" style="font-variant:small-caps;">) <sup>t</sup></span><span class="font64" style="font-weight:bold;">H</span><span class="font64" style="font-weight:bold;font-style:italic;">(w —</span><span class="font64" style="font-weight:bold;"> w*</span><span class="font64">)</span><span class="font64" style="font-weight:bold;">,</span></p>
<p><span class="font64">2</span></p></div><div>
<p><span class="font64">(7.6)</span></p></div><div>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">is the Hessian matrix of J with respect to </span><span class="font64" style="font-weight:bold;">w </span><span class="font64">evaluated at </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">*. There is no first-order term in this quadratic approximation, because </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">* is defined to be a&#160;minimum, where the gradient vanishes. Likewise, because </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">* is the location of a&#160;minimum of J, we can conclude that </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">is positive semidefinite.</span></p>
<p><span class="font64">The minimum of </span><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64"> occurs where its gradient</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">V </span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>w</sub> J(w)</span><span class="font64"> = </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">(</span><span class="font64" style="font-weight:bold;">w — w *</span><span class="font64">)</span></p></div><div>
<p><span class="font64">(7.7)</span></p></div><div>
<p><span class="font64">is equal to 0.</span></p>
<p><span class="font64">To study the effect of weight decay, we modify Eq. 7.7 by adding the weight decay gradient. We can now solve for the minimum of the regularized version of </span><span class="font64" style="font-weight:bold;font-style:italic;">J.&#160;</span><span class="font64">We use the variable w to represent the location of the minimum.</span></p></div><div>
<p><span class="font64">a</span><span class="font64" style="font-weight:bold;">w </span><span class="font64" style="font-weight:bold;font-style:italic;">+ H (</span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;">!) —</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">w*</span><span class="font64">) =0 (</span><span class="font64" style="font-weight:bold;">H </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;font-style:italic;">al )w = Hw*</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">w</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">(H + al</span><span class="font64"> )<sup>-1</sup> </span><span class="font64" style="font-weight:bold;">Hw </span><span class="font64">*</span><span class="font64" style="font-weight:bold;">.</span></p></div><div>
<p><span class="font64">(7.8)</span></p>
<p><span class="font64">(7.9) (7.10)</span></p></div><div>
<p><span class="font64">As a approaches 0, the regularized solution </span><span class="font64" style="font-weight:bold;font-style:italic;">w</span><span class="font64"> approaches </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">*. But what happens as a grows? Because </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">is real and symmetric, we can decompose it&#160;into a diagonal matrix A and an orthonormal basis of eigenvectors, </span><span class="font64" style="font-weight:bold;">Q</span><span class="font64">, such that&#160;</span><span class="font64" style="font-weight:bold;">H </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">QAQ</span><span class="font64" style="font-variant:small-caps;"><sup>t</sup>. Applying the decomposition to Eq.7.10, we obtain:</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">w =</span></p></div><div>
<p><span class="font13">....</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">(QAQ</span><span class="font64" style="font-variant:small-caps;"><sup>t</sup> + </span><span class="font64" style="font-weight:bold;font-style:italic;">al)<sup>-1</sup>QAQ</span><span class="font64"> '</span><span class="font64" style="font-weight:bold;">w</span></p>
<p dir="rtl"><span class="font64">1 ־ ר</span></p></div><div>
<p><span class="font61" style="font-weight:bold;">■vr</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">Q</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">A </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;font-style:italic;">al )Q</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">= Q(A + al</span><span class="font64"> )<sup>-1</sup></span><span class="font64" style="font-weight:bold;">A Q</span><span class="font64">'</span><span class="font64" style="font-weight:bold;">w</span><span class="font64">*</span><span class="font64" style="font-weight:bold;">.</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-variant:small-caps;">QAQ<sup>t</sup></span><span class="font64" style="font-weight:bold;"> w*</span></p></div><div>
<p><span class="font64">(7.11)</span></p>
<p><span class="font64">(7.12)</span></p>
<p><span class="font64">(7.13)</span></p></div><div>
<p><span class="font64">We see that the effect of weight decay is to rescale </span><span class="font64" style="font-weight:bold;">w </span><span class="font64">* along the axes defined by the eigenvectors of </span><span class="font64" style="font-weight:bold;">H</span><span class="font64">. Specifically, the component of </span><span class="font64" style="font-weight:bold;">w </span><span class="font64">* that is aligned with the</span></p></div><div>
<p><span class="font64">i-th eigenvector of </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">is rescaled by a factor of</span></p></div><div>
<p><span class="font64">A.</span></p></div><div>
<p><span class="font62">A</span><span class="font64" style="font-style:italic;">i</span><span class="font62"> *</span></p></div><div>
<p><span class="font64">(You may wish to review</span></p></div><div>
<p><span class="font64">how this kind of scaling works, first explained in Fig. 2.3).</span></p>
<p><span class="font64">Along the directions where the eigenvalues of </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">are relatively large, for example, where A<sub>i</sub> ^ a, the effect of regularization is relatively small. However, components&#160;with A<sub>i</sub> ^ a will be shrunk to have nearly zero magnitude. This effect is illustrated&#160;in Fig. 7.1.</span></p></div><div><img src="main-65.jpg" alt=""/></div>
<p><span class="font64">Figure 7.1: An illustration of the effect of L<sup>2</sup> (or weight decay) regularization on the value of the optimal w. The solid ellipses represent contours of equal value of the unregularized&#160;objective. The dotted circles represent contours of equal value of theL<sup>2</sup> regularizer. At&#160;the point </span><span class="font64" style="font-style:italic;">W</span><span class="font64">, these competing objectives reach an equilibrium. In the first dimension, the&#160;eigenvalue of the Hessian of J is small. The objective function does not increase much&#160;when moving horizontally away from w*. Because the objective function does not express&#160;a strong preference along this direction, the regularizer has a strong effect on this axis.&#160;The regularizer pulls w </span><span class="font19">1</span><span class="font64"> close to zero. In the second dimension, the objective function&#160;is very sensitive to movements away from w*. The corresponding eigenvalue is large,&#160;indicating high curvature. As a result, weight decay affects the position ofw<sub>2</sub> relatively&#160;little.</span></p>
<p><span class="font64">Only directions along which the parameters contribute significantly to reducing the objective function are preserved relatively intact. In directions that do not&#160;contribute to reducing the objective function, a small eigenvalue of the Hessian&#160;tells us that movement in this direction will not significantly increase the gradient.&#160;Components of the weight vector corresponding to such unimportant directions&#160;are decayed away through the use of the regularization throughout training.</span></p>
<p><span class="font64">So far we have discussed weight decay in terms of its effect on the optimization of an abstract, general, quadratic cost function. How do these effects relate to&#160;machine learning in particular? We can find out by studying linear regression, a&#160;model for which the true cost function is quadratic and therefore amenable to the&#160;same kind of analysis we have used so far. Applying the analysis again, we will&#160;be able to obtain a special case of the same results, but with the solution now&#160;phrased in terms of the training data. For linear regression, the cost function is</span></p>
<p><span class="font64">the sum of squared errors:</span></p><div>
<p><span class="font64">(7.14)</span></p></div>
<p><span class="font64">(Xw </span><span class="font64" style="font-weight:bold;font-style:italic;">— y)<sup>T</sup>(Xw — y</span><span class="font64">).</span></p>
<p><span class="font64">When we add L<a id="footnote2"></a><sup><a href="#bookmark4">2</a></sup><sup></sup> regularization, the objective function changes to</span></p><div>
<p><span class="font64">\T,</span></p></div><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64">(Xw — y)<sup>1</sup> (Xw — y) + aw<sup>T</sup> w.</span></p>
<p><span class="font64">2</span></p>
<p><span class="font64">This changes the normal equations for the solution from</span></p>
<p><span class="font64">w = (X <sup>T</sup>X)<sup>-1</sup>X y</span></p></div><div>
<p><span class="font64">(7.15)</span></p></div><div>
<p><span class="font64">(7.16)</span></p></div><div>
<p><span class="font64">to</span></p></div><div>
<p><span class="font64">-T</span></p></div><div>
<p><span class="font64">w = (X <sup>1</sup> X + </span><span class="font64" style="font-weight:bold;font-style:italic;">al )<sup>-1</sup>X</span><span class="font64"> <sup>T</sup>y.</span></p></div><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64">(7.17)</span></p>
<p><span class="font64">The matrix X <sup>T</sup>X in Eq. 7.16 is proportional to the covariance matrix <sup>1</sup> X<sup>T</sup> X.</span></p></div>
<p><span class="font64">Using L<sup>2</sup> regularization replaces this matrix with (X<sup>T</sup>X + a/) <sup>1</sup> in Eq. 7.17. The new matrix is the same as the original one, but with the addition of a to the&#160;diagonal. The diagonal entries of this matrix correspond to the variance of each&#160;input feature. We can see that L<sup>2</sup> regularization causes the learning algorithm&#160;to “perceive” the input X as having higher variance, which makes it shrink the&#160;weights on features whose covariance with the output target is low compared to&#160;this added variance.</span></p>
<p><span class="font64">7.1.2 </span><span class="font64" style="font-weight:bold;font-style:italic;">L<sup>1</sup></span><span class="font64"> Regularization</span></p>
<p><span class="font17" style="font-weight:bold;font-style:italic;">C\</span></p>
<p><span class="font64">While L<a id="footnote2"></a><sup><a href="#bookmark4">2</a></sup><sup></sup> weight decay is the most common form of weight decay, there are other ways to penalize the size of the model parameters. Another option is to use L<sup>1&#160;</sup>regularization.</span></p>
<p><span class="font64">Formally, L<sup>1</sup> regularization on the model parameter w is defined as:</span></p><div>
<p><span class="font64">(7.18)</span></p></div>
<p><span class="font64">fi(0) = ||w</span><span class="font18">||1</span><span class="font64"> = </span><span class="font64" style="font-weight:bold;font-style:italic;">^2</span><span class="font64"> |w |,</span></p>
<p><span class="font64">that is, as the sum of absolute values of the individual parameters.<a id="footnote2"></a><sup><a href="#bookmark4">2</a></sup><sup></sup> We will now discuss the effect of L<sup>1</sup> regularization on the simple linear regression model,&#160;with no bias parameter, that we studied in our analysis of L<a id="footnote2"></a><sup><a href="#bookmark4">2</a></sup><sup></sup> regularization. In&#160;particular, we are interested in delineating the differences between L<sup>1</sup> and L<a id="footnote2"></a><sup><a href="#bookmark4">2</a></sup><sup></sup> forms&#160;of regularization. As with L<sup>2</sup> weight decay, L<sup>1</sup> weight decay controls the strength&#160;of the regularization by scaling the penalty Q using a positive hyperparameter a.&#160;Thus, the regularized objective function </span><span class="font64" style="font-weight:bold;font-style:italic;">J(w; X</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">) is given by</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">J(w; X,</span><span class="font64" style="font-weight:bold;"> y</span><span class="font64">) = a|| </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">| </span><span class="font18">|1</span><span class="font64"> + J(</span><span class="font64" style="font-weight:bold;">w</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">X</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">), &#160;&#160;&#160;(7-19)</span></p>
<p><span class="font64">with the corresponding gradient (actually, sub-gradient):</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">V<sub>w</sub>J(w; X,</span><span class="font64" style="font-weight:bold;"> y</span><span class="font64">) = asign(</span><span class="font64" style="font-weight:bold;">w</span><span class="font64">) + </span><span class="font64" style="font-weight:bold;font-style:italic;">V<sub>W</sub>J(X, y;</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">) &#160;&#160;&#160;(7.20)</span></p>
<p><span class="font64">where sign(</span><span class="font64" style="font-weight:bold;">w</span><span class="font64">) is simply the sign of </span><span class="font64" style="font-weight:bold;">w </span><span class="font64">applied element-wise.</span></p>
<p><span class="font64">By inspecting Eq. 7.20, we can see immediately that the effect of L<sup>1</sup> regu-</span></p>
<p><span class="font17" style="font-weight:bold;font-style:italic;">C\</span></p>
<p><span class="font64">larization is quite different from that of </span><span class="font64" style="font-weight:bold;font-style:italic;">L</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">regularization. Specifically, we can see that the regularization contribution to the gradient no longer scales linearly&#160;with each </span><span class="font64" style="font-weight:bold;font-style:italic;">w<sub>i</sub>;</span><span class="font64"> instead it is a constant factor with a sign equal to sign(w<sub>i</sub>). One&#160;consequence of this form of the gradient is that we will not necessarily see clean&#160;algebraic solutions to quadratic approximations of J(</span><span class="font64" style="font-weight:bold;">X</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">) as we did for L<sup>2&#160;</sup>regularization.</span></p>
<p><span class="font64">Our simple linear model has a quadratic cost function that we can represent via its Taylor series. Alternately, we could imagine that this is a truncated Taylor&#160;series approximating the cost function of a more sophisticated model. The gradient&#160;in this setting is given by</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">V<sub>w</sub></span><span class="font64" style="font-weight:bold;"> </span><span class="font64">J(</span><span class="font64" style="font-weight:bold;">w</span><span class="font64">) = </span><span class="font64" style="font-weight:bold;font-style:italic;">H (w —</span><span class="font64" style="font-weight:bold;"> w</span><span class="font64">*), &#160;&#160;&#160;(7.21)</span></p>
<p><span class="font64">where, again, </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">is the Hessian matrix of J with respect to </span><span class="font64" style="font-weight:bold;">w </span><span class="font64">evaluated at </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">*.</span></p>
<p><span class="font64">Because the L<sup>1</sup> penalty does not admit clean algebraic expressions in the case of a fully general Hessian, we will also make the further simplifying assumption&#160;that the Hessian is diagonal, </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">= diag([H<sub>1;1</sub>,... , H<sub>n</sub>,<sub>n</sub>]), where each H </span><span class="font64" style="font-weight:bold;font-style:italic;">,i &gt;</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">0.&#160;This assumption holds if the data for the linear regression problem has been&#160;preprocessed to remove all correlation between the input features, which may be&#160;accomplished using PCA.</span></p>
<p><span class="font64">Our quadratic approximation of the L<sup>1</sup> regularized objective function decomposes into a sum over the parameters:</span></p><div>
<p><span class="font64">j(</span><span class="font64" style="font-weight:bold;">w</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">X</span><span class="font64">, </span><span class="font64" style="font-weight:bold;font-style:italic;">y) = J</span><span class="font64"> (</span><span class="font64" style="font-weight:bold;">w*</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">X</span><span class="font64">, </span><span class="font64" style="font-weight:bold;font-style:italic;">y) + J2</span></p></div><div>
<p><span class="font64">1</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Hi,i(wi — w*</span><span class="font64"> )<sup>2</sup> + a|wi|</span></p></div><div>
<p><span class="font64">(7.22)</span></p></div>
<p><span class="font64">The problem of minimizing this approximate cost function has an analytical solution (for each dimension i), with the following form:</span></p><div>
<p><span class="font64">(7.23)</span></p></div>
<p><span class="font64">wi = sign(w*) max j<sup>|w</sup> * <sup>—</sup> -HL, 0 j .</span></p>
<p><span class="font64">Consider the situation where w* &gt; 0 for all </span><span class="font64" style="font-weight:bold;font-style:italic;">i.</span><span class="font64"> There are two possible outcomes:</span></p>
<p><span class="font64">1. The case where w* &lt; H. • Here the optimal value of w<sub>i</sub> under the regularized objective is simply w<sub>i</sub> = 0• This occurs because the contribution of J(w</span><span class="font64" style="font-weight:bold;font-style:italic;">;</span><span class="font64"> X, y)&#160;to the regularized objective </span><span class="font64" style="font-weight:bold;font-style:italic;">J(w; X</span><span class="font64">, y) is overwhelmed—in direction i—by&#160;the L<sup>1</sup> regularization which pushes the value of w<sub>i</sub> to zero.</span></p><div>
<p><span class="font64">2. The case where w* &gt;</span></p></div><div>
<p><span class="font64">a</span></p></div><div>
<p><span class="font64" style="font-style:italic;">H i</span></p></div><div>
<p><span class="font64">In this case, the regularization does not move the</span></p></div>
<p><span class="font64">optimal value of w<sub>i</sub> to zero but instead it just shifts it in that direction by a distance equal to <sub>H</sub><sup>a</sup> .</span></p>
<p><span class="font64">A similar process happens when w * &lt; 0, but with the L<sup>1</sup> penalty making w<sub>i</sub> less negative by <sub>H</sub><sup>a</sup> , or 0.</span></p>
<p><span class="font64">In comparison to L<sup>2</sup> regularization, L<sup>1</sup> regularization results in a solution that is more </span><span class="font64" style="font-weight:bold;font-style:italic;">sparse</span><span class="font64">. Sparsity in this context refers to the fact that some parameters&#160;have an optimal value of zero. The sparsity of L<sup>1</sup> regularization is a qualitatively&#160;different behavior than arises with L<sup>2</sup> regularization. Eq. 7.13 gave the solution&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">w</span><span class="font64"> for L<sup>2</sup> regularization. If we revisit that equation using the assumption of a&#160;diagonal and positive definite Hessian H that we introduced for our analysis of L<sup>1</sup></span></p>
<p><span class="font64">regularization, we find that w = </span><span class="font64" style="text-decoration:line-through;"><sub>H</sub>.&quot;+</span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:line-through;"><sub>a</sub></span><span class="font64"> w*. If w* was nonzero, then w<sub>i</sub> remains</span></p>
<p><span class="font64">« </span><span class="font64" style="font-weight:bold;font-style:italic;">0 ’</span></p>
<p><span class="font64">nonzero. This demonstrates that </span><span class="font64" style="font-weight:bold;font-style:italic;">L</span><span class="font64"> regularization does not cause the parameters to become sparse, while L<sup>1</sup> regularization may do so for large enough a.</span></p>
<p><span class="font64">The sparsity property induced by L<sup>1</sup> regularization has been used extensively as a </span><span class="font64" style="font-weight:bold;font-style:italic;">feature selection</span><span class="font64"> mechanism. Feature selection simplifies a machine learning&#160;problem by choosing which subset of the available features should be used. In&#160;particular, the well known LASSO (Tibshirani, 1995) (least absolute shrinkage and&#160;selection operator) model integrates an L<sup>1</sup> penalty with a linear model and a least&#160;squares cost function. The L<sup>1</sup> penalty causes a subset of the weights to become&#160;zero, suggesting that the corresponding features may safely be discarded.</span></p>
<p><span class="font64">In Sec. 5.6.1, we saw that many regularization strategies can be interpreted as MAP Bayesian inference, and that in particular, L<sup>2</sup> regularization is equivalent&#160;to MAP Bayesian inference with a Gaussian prior on the weights. For L<sup>1</sup> regularization, the penalty a</span><span class="font64" style="font-weight:bold;font-style:italic;">Q(w) =</span><span class="font64"> a^<sub>i</sub> |w,j | used to regularize a cost function is&#160;equivalent to the log-prior term that is maximized by MAP Bayesian inference&#160;when the prior is an isotropic Laplace distribution (Eq. 3.26) over w E R<sup>n</sup>:</span></p>
<p><span class="font64">_ 1</span></p>
<p><span class="font64">log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(w)</span><span class="font64"> = &#160;&#160;&#160;logLaplace(w<sub>i</sub>; 0, —) = — aNwJL + n log a — n log 2.&#160;&#160;&#160;&#160;(7.24)</span></p>
<p><span class="font64">a</span></p>
<p><span class="font64">From the point of view of learning via maximization with respect to </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">, we can ignore the log a </span><span class="font64" style="font-weight:bold;font-style:italic;">—</span><span class="font64"> log 2 terms because they do not depend on </span><span class="font64" style="font-weight:bold;">w</span><span class="font64">.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">7.2 Norm Penalties as Constrained Optimization</span></h4>
<p><span class="font64">Consider the cost function regularized by a parameter norm penalty:</span></p>
<p><span class="font64">J&gt;; X, y) = J(6; </span><span class="font64" style="font-weight:bold;font-style:italic;">X</span><span class="font64">, y) + a0(6). &#160;&#160;&#160;(7.25)</span></p>
<p><span class="font64">Recall from Sec. 4.4 that we can minimize a function subject to constraints by constructing a generalized Lagrange function, consisting of the original objective&#160;function plus a set of penalties. Each penalty is a product between a coefficient,&#160;called a Karush-Kuhn-Tucker (KKT) multiplier, and a function representing&#160;whether the constraint is satisfied. If we wanted to constrain </span><span class="font64" style="font-weight:bold;font-style:italic;">0,(6</span><span class="font64">) to be less than&#160;some constant k, we could construct a generalized Lagrange function</span></p>
<p><span class="font64">L(6, a; X, </span><span class="font64" style="font-weight:bold;font-style:italic;">y) = J</span><span class="font64">(</span><span class="font64" style="font-weight:bold;font-style:italic;">6; X</span><span class="font64">, </span><span class="font64" style="font-weight:bold;font-style:italic;">y) +</span><span class="font64"> a(0(6) </span><span class="font64" style="font-weight:bold;font-style:italic;">— k).</span><span class="font64"> &#160;&#160;&#160;(7.26)</span></p>
<p><span class="font64">The solution to the constrained problem is given by</span></p>
<p><span class="font64">6* = argmin max L(6, a). &#160;&#160;&#160;(7.27)</span></p>
<p><span class="font64" style="font-variant:small-caps;">q &#160;&#160;&#160;</span><span class="font64">a,a&gt; 0</span></p>
<p><span class="font64">As described in Sec. 4.4, solving this problem requires modifying both 6 and a. Sec. 4.5 provides a worked example of linear regression with an L<sup>2</sup> constraint.&#160;Many different procedures are possible—some may use gradient descent, while&#160;others may use analytical solutions for where the gradient is zero—but in all&#160;procedures a must increase whenever </span><span class="font64" style="font-weight:bold;font-style:italic;">0(6) &gt; k</span><span class="font64"> and decrease whenever 0( 6) &lt; k.&#160;All positive a encourage </span><span class="font64" style="font-weight:bold;font-style:italic;">0(6)</span><span class="font64"> to shrink. The optimal value a* will encourage 0(6)&#160;to shrink, but not so strongly to make 0(6) become less than k.</span></p>
<p><span class="font64">To gain some insight into the effect of the constraint, we can fix a* and view the problem as just a function of 6:</span></p>
<p><span class="font64">6 * = argmin L(6, a*) = argmin J (6; X, y) + a *0(6). &#160;&#160;&#160;(7.28)</span></p>
<p><span class="font64" style="font-variant:small-caps;">q &#160;&#160;&#160;q</span></p>
<p><span class="font64">This is exactly the same as the regularized training problem of minimizing </span><span class="font64" style="font-weight:bold;font-style:italic;">J. </span><span class="font64">We can thus think of a parameter norm penalty as imposing a constraint on the&#160;weights. If 0 is the </span><span class="font64" style="font-weight:bold;font-style:italic;">L</span><span class="font64"> norm, then the weights are constrained to lie in an L<sup>2&#160;</sup>ball. If 0 is the L<sup>1</sup> norm, then the weights are constrained to lie in a region of&#160;limited </span><span class="font64" style="font-weight:bold;font-style:italic;">L<sup>1</sup></span><span class="font64"> norm. Usually we do not know the size of the constraint region that we&#160;impose by using weight decay with coefficient a* because the value of a* does not&#160;directly tell us the value of k. In principle, one can solve for k, but the relationship&#160;between k and a* depends on the form of J. While we do not know the exact size&#160;of the constraint region, we can control it roughly by increasing or decreasing a&#160;in order to grow or shrink the constraint region. Larger a will result in a smaller&#160;constraint region. Smaller a will result in a larger constraint region.</span></p>
<p><span class="font64">Sometimes we may wish to use explicit constraints rather than penalties. As described in Sec. 4.4, we can modify algorithms such as stochastic gradient descent&#160;to take a step downhill on J(</span><span class="font64" style="font-weight:bold;">6</span><span class="font64">) and then project </span><span class="font64" style="font-weight:bold;">6 </span><span class="font64">back to the nearest point&#160;that satisfies </span><span class="font64" style="font-weight:bold;font-style:italic;">0,(6</span><span class="font64">) &lt; k. This can be useful if we have an idea of what value of k&#160;is appropriate and do not want to spend time searching for the value of a that&#160;corresponds to this k.</span></p>
<p><span class="font64">Another reason to use explicit constraints and reprojection rather than enforcing constraints with penalties is that penalties can cause non-convex optimization&#160;procedures to get stuck in local minima corresponding to small </span><span class="font64" style="font-weight:bold;">6</span><span class="font64">. When training&#160;neural networks, this usually manifests as neural networks that train with several&#160;“dead units.” These are units that do not contribute much to the behavior of the&#160;function learned by the network because the weights going into or out of them are&#160;all very small. When training with a penalty on the norm of the weights, these&#160;configurations can be locally optimal, even if it is possible to significantly reduce&#160;J by making the weights larger. Explicit constraints implemented by re-projection&#160;can work much better in these cases because they do not encourage the weights&#160;to approach the origin. Explicit constraints implemented by re-projection only&#160;have an effect when the weights become large and attempt to leave the constraint&#160;region.</span></p>
<p><span class="font64">Finally, explicit constraints with reprojection can be useful because they impose some stability on the optimization procedure. When using high learning rates, it&#160;is possible to encounter a positive feedback loop in which large weights induce&#160;large gradients which then induce a large update to the weights. If these updates&#160;consistently increase the size of the weights, then </span><span class="font64" style="font-weight:bold;">6 </span><span class="font64">rapidly moves away from&#160;the origin until numerical overflow occurs. Explicit constraints with reprojection&#160;prevent this feedback loop from continuing to increase the magnitude of the weights&#160;without bound. Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2012c) recommend using constraints combined with&#160;a high learning rate to allow rapid exploration of parameter space while maintaining&#160;some stability.</span></p>
<p><span class="font64">In particular, Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2012c) recommend a strategy introduced by Srebro and Shraibman (2005): constraining the norm of each </span><span class="font64" style="font-weight:bold;">column </span><span class="font64">of the weight matrix&#160;of a neural net layer, rather than constraining the Frobenius norm of the entire&#160;weight matrix. Constraining the norm of each column separately prevents any one&#160;hidden unit from having very large weights. If we converted this constraint into a</span></p>
<p><span class="font61" style="font-style:italic;">C\</span></p>
<p><span class="font64">penalty in a Lagrange function, it would be similar to </span><span class="font64" style="font-weight:bold;font-style:italic;">L</span><span class="font64"> weight decay but with a separate KKT multiplier for the weights of each hidden unit. Each of these KKT&#160;multipliers would be dynamically updated separately to make each hidden unit&#160;obey the constraint. In practice, column norm limitation is always implemented as&#160;an explicit constraint with reprojection.</span></p><h4><a id="bookmark6"></a><span class="font65" style="font-weight:bold;">7.3 Regularization and Under-Constrained Problems</span></h4>
<p><span class="font64">In some cases, regularization is necessary for machine learning problems to be properly defined. Many linear models in machine learning, including linear regression and PCA, depend on inverting the matrix </span><span class="font64" style="font-weight:bold;">X<sup>T</sup>X</span><span class="font64">. This is not possible whenever&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">X<sup>T</sup> X</span><span class="font64"> is singular. This matrix can be singular whenever the data generating distribution truly has no variance in some direction, or when no variance in </span><span class="font64" style="font-weight:bold;">observed&#160;</span><span class="font64">in some direction because there are fewer examples (rows of </span><span class="font64" style="font-weight:bold;">X</span><span class="font64">) than input features&#160;(columns of </span><span class="font64" style="font-weight:bold;">X</span><span class="font64">). In this case, many forms of regularization correspond to inverting&#160;</span><span class="font64" style="font-weight:bold;">X<sup>T</sup> X </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;font-style:italic;">aI</span><span class="font64"> instead. This regularized matrix is guaranteed to be invertible.</span></p>
<p><span class="font64">These linear problems have closed form solutions when the relevant matrix is invertible. It is also possible for a problem with no closed form solution to be&#160;underdetermined. An example is logistic regression applied to a problem where&#160;the classes are linearly separable. If a weight vector </span><span class="font64" style="font-weight:bold;">w </span><span class="font64">is able to achieve perfect&#160;classification, then 2</span><span class="font64" style="font-weight:bold;">w </span><span class="font64">will also achieve perfect classification and higher likelihood.&#160;An iterative optimization procedure like stochastic gradient descent will continually&#160;increase the magnitude of </span><span class="font64" style="font-weight:bold;">w </span><span class="font64">and, in theory, will never halt. In practice, a numerical&#160;implementation of gradient descent will eventually reach sufficiently large weights&#160;to cause numerical overflow, at which point its behavior will depend on how the&#160;programmer has decided to handle values that are not real numbers.</span></p>
<p><span class="font64">Most forms of regularization are able to guarantee the convergence of iterative methods applied to underdetermined problems. For example, weight decay will&#160;cause gradient descent to quit increasing the magnitude of the weights when the&#160;slope of the likelihood is equal to the weight decay coefficient.</span></p>
<p><span class="font64">The idea of using regularization to solve underdetermined problems extends beyond machine learning. The same idea is useful for several basic linear algebra&#160;problems.</span></p>
<p><span class="font64">As we saw in Sec. 2.9, we can solve underdetermined linear equations using the Moore-Penrose pseudoinverse. Recall that one definition of the pseudoinverse&#160;X+ of a matrix </span><span class="font64" style="font-weight:bold;font-style:italic;">X</span><span class="font64"> is</span></p>
<p><span class="font64">X+ </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> lim (X<sup>T</sup> X + </span><span class="font64" style="font-weight:bold;font-style:italic;">aI)<sup>-l</sup>X</span><span class="font64"><sup>T</sup>. &#160;&#160;&#160;(7.29)</span></p>
<p><span class="font64">a\0</span></p>
<p><span class="font64">We can now recognize Eq. 7.29 as performing linear regression with weight decay. Specifically, Eq. 7.29 is the limit of Eq. 7.17 as the regularization coefficient shrinks&#160;to zero. We can thus interpret the pseudoinverse as stabilizing underdetermined&#160;problems using regularization.</span></p><h4><a id="bookmark7"></a><span class="font65" style="font-weight:bold;">7.4 Dataset Augmentation</span></h4>
<p><span class="font64">The best way to make a machine learning model generalize better is to train it on more data. Of course, in practice, the amount of data we have is limited. One way&#160;to get around this problem is to create fake data and add it to the training set.&#160;For some machine learning tasks, it is reasonably straightforward to create new&#160;fake data.</span></p>
<p><span class="font64">This approach is easiest for classification. A classifier needs to take a complicated, high dimensional input x and summarize it with a single category identity y. This means that the main task facing a classifier is to be invariant to a wide variety&#160;of transformations. We can generate new (x, y) pairs easily just by transforming&#160;the x inputs in our training set.</span></p>
<p><span class="font64">This approach is not as readily applicable to many other tasks. For example, it is difficult to generate new fake data for a density estimation task unless we have&#160;already solved the density estimation problem.</span></p>
<p><span class="font64">Dataset augmentation has been a particularly effective technique for a specific classification problem: object recognition. Images are high dimensional and include&#160;an enormous variety of factors of variation, many of which can be easily simulated.&#160;Operations like translating the training images a few pixels in each direction can&#160;often greatly improve generalization, even if the model has already been designed to&#160;be partially translation invariant by using the convolution and pooling techniques&#160;described in Chapter 9. Many other operations such as rotating the image or&#160;scaling the image have also proven quite effective.</span></p>
<p><span class="font64">One must be careful not to apply transformations that would change the correct class. For example, optical character recognition tasks require recognizing the&#160;difference between ‘b’ and ‘d’ and the difference between ‘6’ and ‘9’, so horizontal&#160;flips and 180° rotations are not appropriate ways of augmenting datasets for these&#160;tasks.</span></p>
<p><span class="font64">There are also transformations that we would like our classifiers to be invariant to, but which are not easy to perform. For example, out-of-plane rotation can not&#160;be implemented as a simple geometric operation on the input pixels.</span></p>
<p><span class="font64">Dataset augmentation is effective for speech recognition tasks as well (Jaitly and Hinton, 2013).</span></p>
<p><span class="font64">Injecting noise in the input to a neural network (Sietsma and Dow, 1991) can also be seen as a form of data augmentation. For many classification and&#160;even some regression tasks, the task should still be possible to solve even if small&#160;random noise is added to the input. Neural networks prove not to be very robust&#160;to noise, however (Tang and Eliasmith, 2010). One way to improve the robustness&#160;of neural networks is simply to train them with random noise applied to their&#160;inputs. Input noise injection is part of some unsupervised learning algorithms such&#160;as the denoising autoencoder (Vincent </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2008). Noise injection also works&#160;when the noise is applied to the hidden units, which can be seen as doing dataset&#160;augmentation at multiple levels of abstraction. Poole </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) recently showed&#160;that this approach can be highly effective provided that the magnitude of the&#160;noise is carefully tuned. Dropout, a powerful regularization strategy that will be&#160;described in Sec. 7.12, can be seen as a process of constructing new inputs by&#160;</span><span class="font64" style="font-weight:bold;">multiplying </span><span class="font64">by noise.</span></p>
<p><span class="font64">When comparing machine learning benchmark results, it is important to take the effect of dataset augmentation into account. Often, hand-designed dataset&#160;augmentation schemes can dramatically reduce the generalization error of a machine&#160;learning technique. To compare the performance of one machine learning algorithm&#160;to another, it is necessary to perform controlled experiments. When comparing&#160;machine learning algorithm A and machine learning algorithm B, it is necessary&#160;to make sure that both algorithms were evaluated using the same hand-designed&#160;dataset augmentation schemes. Suppose that algorithm A performs poorly with&#160;no dataset augmentation and algorithm B performs well when combined with&#160;numerous synthetic transformations of the input. In such a case it is likely the&#160;synthetic transformations caused the improved performance, rather than the use&#160;of machine learning algorithm B. Sometimes deciding whether an experiment&#160;has been properly controlled requires subjective judgment. For example, machine&#160;learning algorithms that inject noise into the input are performing a form of dataset&#160;augmentation. Usually, operations that are generally applicable (such as adding&#160;Gaussian noise to the input) are considered part of the machine learning algorithm,&#160;while operations that are specific to one application domain (such as randomly&#160;cropping an image) are considered to be separate pre-processing steps.</span></p><h4><a id="bookmark8"></a><span class="font65" style="font-weight:bold;">7.5 Noise Robustness</span></h4>
<p><span class="font64">Sec. 7.4 has motivated the use of noise applied to the inputs as a dataset augmentation strategy. For some models, the addition of noise with infinitesimal variance at the input of the model is equivalent to imposing a penalty on the&#160;norm of the weights (Bishop, 1995a,b). In the general case, it is important to&#160;remember that noise injection can be much more powerful than simply shrinking&#160;the parameters, especially when the noise is added to the hidden units. Noise&#160;applied to the hidden units is such an important topic as to merit its own separate&#160;discussion; the dropout algorithm described in Sec. 7.12 is the main development&#160;of that approach.</span></p>
<p><span class="font64">Another way that noise has been used in the service of regularizing models is by adding it to the weights. This technique has been used primarily in the&#160;context of recurrent neural networks (Jim </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1996; Graves, 2011). This can&#160;be interpreted as a stochastic implementation of a Bayesian inference over the&#160;weights. The Bayesian treatment of learning would consider the model weights&#160;to be uncertain and representable via a probability distribution that reflects this&#160;uncertainty. Adding noise to the weights is a practical, stochastic way to reflect&#160;this uncertainty (Graves, 2011).</span></p>
<p><span class="font64">This can also be interpreted as equivalent (under some assumptions) to a more traditional form of regularization. Adding noise to the weights has been&#160;shown to be an effective regularization strategy in the context of recurrent neural&#160;networks (Jim </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1996; Graves, 2011). In the following, we will present an&#160;analysis of the effect of weight noise on a standard feedforward neural network (as&#160;introduced in Chapter 6).</span></p>
<p><span class="font64">We study the regression setting, where we wish to train a function y(x) that maps a set of features x to a scalar using the least-squares cost function between&#160;the model predictions y(x) and the true values y:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>J</sup> </span><span class="font64" style="font-style:italic;">=</span><span class="font62"> <sup>E</sup>p(x,y) [<sup>(y(x) -</sup> y<sup>)2</sup>] • &#160;&#160;&#160;<sup>(7</sup>.<sup>30)</sup></span></p>
<p><span class="font64">The training set consists of m labeled examples {(x<sup>(1)</sup>, y<sup>(1)</sup>),..., (x<sup>(m)</sup>, y<sup>(m)</sup>)}.</span></p>
<p><span class="font64">We now assume that with each input presentation we also include a random perturbation ew ~ N( e; 0</span><span class="font64" style="font-weight:bold;font-style:italic;">,yl)</span><span class="font64"> of the network weights. Let us imagine that we&#160;have a standard l-layer MLP. We denote the perturbed model as y<sub>ew</sub> (x). Despite&#160;the injection of noise, we are still interested in minimizing the squared error of the&#160;output of the network. The objective function thus becomes:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>J</sup>W</span></p></div><div>
<p><span class="font62" style="font-variant:small-caps;">e</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p<sup>(</sup>x,y,e</span><span class="font64"><sub>w</sub><sup>)</sup></span></p></div><div>
<p><span class="font64">(j׳«w (<sup>x</sup>)<sup>-</sup> y)<sup>2</sup></span></p></div><div>
<p><span class="font64">(7.31)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> <sup>E</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x,y,e<sub>w</sub>) [few</span><span class="font64"> (<sup>X</sup>) <sup>- 2</sup>V^w (<sup>x</sup>) + V <sup>2</sup> • &#160;&#160;&#160;(<sup>732</sup>־)</span></p>
<p><span class="font64">For small </span><span class="font64" style="font-weight:bold;font-style:italic;">n</span><span class="font64">, the minimization of J with added weight noise (with covariance nI) is equivalent to minimization of J with an additional regularization term:&#160;nEp(<sub>x</sub>,<sub>y</sub>) [||V</span><span class="font64" style="font-variant:small-caps;">wV(x)||</span><span class="font64"><sup>2</sup>]. This form of regularization encourages the parameters to&#160;go to regions of parameter space where small perturbations of the weights have&#160;a relatively small influence on the output. In other words, it pushes the model&#160;into regions where the model is relatively insensitive to small variations in the&#160;weights, finding points that are not merely minima, but minima surrounded by&#160;flat regions (Hochreiter and Schmidhuber, 1995). In the simplified case of linear&#160;regression (where, for instance, f(x) = w<sup>T</sup>x + b), this regularization term collapses&#160;into nE</span><span class="font64" style="font-weight:bold;font-style:italic;">p(<sub>x</sub>)</span><span class="font64"> [||x|| <sup>2</sup>], which is not a function of parameters and therefore does not&#160;contribute to the gradient of </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">Jw</span><span class="font64"> with respect to the model parameters.</span></p>
<p><span class="font64">7.5.1 Injecting Noise at the Output Targets</span></p>
<p><span class="font64">Most datasets have some amount of mistakes in the y labels. It can be harmful to maximize logp(y | x) when y is a mistake. One way to prevent this is to&#160;explicitly model the noise on the labels. For example, we can assume that for some&#160;small constant e, the training set label y is correct with probability 1 — e, and&#160;otherwise any of the other possible labels might be correct. This assumption is&#160;easy to incorporate into the cost function analytically, rather than by explicitly&#160;drawing noise samples. For example, </span><span class="font64" style="font-weight:bold;font-style:italic;">label smoothing</span><span class="font64"> regularizes a model based&#160;on a softmax with k output values by replacing the hard 0 and 1 classification&#160;targets with targets of </span><span class="font64" style="font-weight:bold;font-style:italic;">ך</span><span class="font64"> and 1 — e, respectively. The standard cross-entropy&#160;loss may then be used with these soft targets. Maximum likelihood learning with a&#160;softmax classifier and hard targets may actually never converge—the softmax can&#160;never predict a probability of exactly 0 or exactly 1, so it will continue to learn&#160;larger and larger weights, making more extreme predictions forever. It is possible&#160;to prevent this scenario using other regularization strategies like weight decay.&#160;Label smoothing has the advantage of preventing the pursuit of hard probabilities&#160;without discouraging correct classification. This strategy has been used since&#160;the 1980s and continues to be featured prominently in modern neural networks&#160;(Szegedy </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015).</span></p><h4><a id="bookmark9"></a><span class="font65" style="font-weight:bold;">7.6 Semi-Supervised Learning</span></h4>
<p><span class="font64">In the paradigm of semi-supervised learning, both unlabeled examples from </span><span class="font64" style="font-weight:bold;font-style:italic;">P(x) </span><span class="font64">and labeled examples from P (x, y) are used to estimate P (y | x) or predict y from&#160;x.</span></p>
<p><span class="font64">In the context of deep learning, semi-supervised learning usually refers to learning a representation </span><span class="font64" style="font-weight:bold;font-style:italic;">h = f</span><span class="font64"> (x). The goal is to learn a representation so&#160;that examples from the same class have similar representations. Unsupervised&#160;learning can provide useful cues for how to group examples in representation&#160;space. Examples that cluster tightly in the input space should be mapped to&#160;similar representations. A linear classifier in the new space may achieve better&#160;generalization in many cases (Belkin and Niyogi, 2002; Chapelle </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2003). A&#160;long-standing variant of this approach is the application of principal components&#160;analysis as a pre-processing step before applying a classifier (on the projected&#160;data).</span></p>
<p><span class="font64">Instead of having separate unsupervised and supervised components in the model, one can construct models in which a generative model of either P (x) or&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(x, y) shares parameters with a discriminative model of P(y | x). One can&#160;then trade-off the supervised criterion — log P(y | x) with the unsupervised or&#160;generative one (such as — log P(x) or — logP(x, y)). The generative criterion then&#160;expresses a particular form of prior belief about the solution to the supervised&#160;learning problem (Lasserre </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006), namely that the structure of P(x) is&#160;connected to the structure of P(y | x) in a way that is captured by the shared&#160;parametrization. By controlling how much of the generative criterion is included&#160;in the total criterion, one can find a better trade-off than with a purely generative&#160;or a purely discriminative training criterion (Lasserre </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006; Larochelle and&#160;Bengio, 2008).</span></p>
<p><span class="font64">Salakhutdinov and Hinton (2008) describe a method for learning the kernel function of a kernel machine used for regression, in which the usage of unlabeled&#160;examples for modeling P (x) improves P (y | x) quite significantly.</span></p>
<p><span class="font64">See Chapelle </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2006) for more information about semi-supervised learning.</span></p><h4><a id="bookmark10"></a><span class="font65" style="font-weight:bold;">7.7 Multi-Task Learning</span></h4>
<p><span class="font64">Multi-task learning (Caruana, 1993) is a way to improve generalization by pooling the examples (which can be seen as soft constraints imposed on the parameters)&#160;arising out of several tasks. In the same way that additional training examples&#160;put more pressure on the parameters of the model towards values that generalize&#160;well, when part of a model is shared across tasks, that part of the model is more&#160;constrained towards good values (assuming the sharing is justified), often yielding&#160;better generalization.</span></p>
<p><span class="font64">Fig. 7.2 illustrates a very common form of multi-task learning, in which different supervised tasks (predicting y<sup>(i)</sup> given x) share the same input x, as well as some&#160;intermediate-level representation h<sup>(shared)</sup> capturing a common pool of factors. The&#160;model can generally be divided into two kinds of parts and associated parameters:</span></p>
<p><span class="font64">1. &#160;&#160;&#160;Task-specific parameters (which only benefit from the examples of their task&#160;to achieve good generalization). These are the upper layers of the neural&#160;network in Fig. 7.2.</span></p>
<p><span class="font64">2. &#160;&#160;&#160;Generic parameters, shared across all the tasks (which benefit from the&#160;pooled data of all the tasks). These are the lower layers of the neural network&#160;in Fig. 7.2.</span></p><div><img src="main-66.jpg" alt=""/></div>
<p><span class="font64">Figure 7.2: Multi-task learning can be cast in several ways in deep learning frameworks and this figure illustrates the common situation where the tasks share a common input but&#160;involve different target random variables. The lower layers of a deep network (whether it&#160;is supervised and feedforward or includes a generative component with downward arrows)&#160;can be shared across such tasks, while task-specific parameters (associated respectively&#160;with the weights into and from h<sup>(1)</sup> and h<sup>(2)</sup>) can be learned on top of those yielding a&#160;shared representation h<sup>(shared)</sup>. The underlying assumption is that there exists a common&#160;pool of factors that explain the variations in the input x, while each task is associated&#160;with a subset of these factors. In this example, it is additionally assumed that top-level&#160;hidden units h<sup>(1)</sup> and h<sup>(2)</sup> are specialized to each task (respectively predicting y<sup>(1)</sup> and&#160;y<sup>(2)</sup>) while some intermediate-level representationh<sup>(shared)</sup> is shared across all tasks. In&#160;the unsupervised learning context, it makes sense for some of the top-level factors to be&#160;associated with none of the output tasks (h<sup>(3)</sup>): these are the factors that explain some of&#160;the input variations but are not relevant for predicting y<sup>(1)</sup> or y<sup>(2)</sup>.</span></p><div><img src="main-67.jpg" alt=""/>
<p><span class="font64">Figure 7.3: Learning curves showing how the negative log-likelihood loss changes over time (indicated as number of training iterations over the dataset, or </span><span class="font64" style="font-style:italic;">epochs).</span><span class="font64"> In this&#160;example, we train a maxout network on MNIST. Observe that the training objective&#160;decreases consistently over time, but the validation set average loss eventually begins to&#160;increase again, forming an asymmetric U-shaped curve.</span></p></div>
<p><span class="font64">Improved generalization and generalization error bounds (Baxter, 1995) can be achieved because of the shared parameters, for which statistical strength can be&#160;greatly improved (in proportion with the increased number of examples for the&#160;shared parameters, compared to the scenario of single-task models). Of course this&#160;will happen only if some assumptions about the statistical relationship between&#160;the different tasks are valid, meaning that there is something shared across some&#160;of the tasks.</span></p>
<p><span class="font64">From the point of view of deep learning, the underlying prior belief is the following: </span><span class="font64" style="font-weight:bold;">among the factors that explain the variations observed in the&#160;data associated with the different tasks, some are shared across two or&#160;more tasks.</span></p><h4><a id="bookmark11"></a><span class="font65" style="font-weight:bold;">7.8 Early Stopping</span></h4>
<p><span class="font64">When training large models with sufficient representational capacity to overfit the task, we often observe that training error decreases steadily over time, but&#160;validation set error begins to rise again. See Fig. 7.3 for an example of this behavior.&#160;This behavior occurs very reliably.</span></p>
<p><span class="font64">This means we can obtain a model with better validation set error (and thus, hopefully better test set error) by returning to the parameter setting at the point&#160;in time with the lowest validation set error. Instead of running our optimization&#160;algorithm until we reach a (local) minimum of validation error, we run it until the&#160;error on the validation set has not improved for some amount of time. Every time&#160;the error on the validation set improves, we store a copy of the model parameters.&#160;When the training algorithm terminates, we return these parameters, rather than&#160;the latest parameters. This procedure is specified more formally in Algorithm 7.1.</span></p>
<p><span class="font64" style="font-weight:bold;">Algorithm 7.1 </span><span class="font64">The early stopping meta-algorithm for determining the best amount of time to train. This meta-algorithm is a general strategy that works&#160;well with a variety of training algorithms and ways of quantifying error on the&#160;validation set.</span></p>
<p><span class="font64">Let n be the number of steps between evaluations.</span></p>
<p><span class="font64">Let </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> be the “patience,” the number of times to observe worsening validation set error before giving up.</span></p>
<p><span class="font64">Let </span><span class="font64" style="font-weight:bold;">0</span><span class="font64"><sub>o</sub> be the initial parameters.</span></p>
<p><span class="font64" style="font-style:italic;">0 &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">— </span><span class="font64" style="font-style:italic;">Qo</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">1 &#160;&#160;&#160;—</span><span class="font64"> 0&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">j —</span><span class="font64"> 0</span></p>
<p><span class="font64">V </span><span class="font64" style="font-weight:bold;font-style:italic;">—</span><span class="font64">— X </span><span class="font64" style="font-weight:bold;">0</span><span class="font64">* — </span><span class="font64" style="font-weight:bold;">0</span></p>
<p><span class="font64">i* — i</span></p>
<p><span class="font64" style="font-weight:bold;">while </span><span class="font64">j &lt; p </span><span class="font64" style="font-weight:bold;">do</span></p>
<p><span class="font64">Update </span><span class="font64" style="font-weight:bold;">0 </span><span class="font64">by running the training algorithm for n steps. i — i + n</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">V —</span><span class="font64"> ValidationSetError(</span><span class="font64" style="font-weight:bold;">Q</span><span class="font64">) </span><span class="font64" style="font-weight:bold;">if </span><span class="font64" style="font-weight:bold;font-style:italic;">V &lt; v</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">then</span></p>
<p><span class="font64">j — 0 </span><span class="font64" style="font-weight:bold;">0</span><span class="font64"><sup>*</sup> — </span><span class="font64" style="font-weight:bold;">0</span></p>
<p><span class="font64">i<sup>*</sup> — i v — v</span><span class="font64" style="font-weight:bold;font-style:italic;">'&#160;</span><span class="font64" style="font-weight:bold;">else</span></p>
<p><span class="font64"><sup>j — j</sup> + <sup>1 </sup></span><span class="font64" style="font-weight:bold;">end if&#160;end while</span></p>
<p><span class="font64">Best parameters are </span><span class="font64" style="font-weight:bold;">0</span><span class="font64">*, best number of training steps is i*</span></p>
<p><span class="font64">This strategy is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">early stopping.</span><span class="font64"> It is probably the most commonly used form of regularization in deep learning. Its popularity is due both to its&#160;effectiveness and its simplicity.</span></p>
<p><span class="font64">One way to think of early stopping is as a very efficient hyperparameter selection algorithm. In this view, the number of training steps is just another hyperparameter.&#160;We can see in Fig. 7.3 that this hyperparameter has a U-shaped validation set&#160;performance curve. Most hyperparameters that control model capacity have such a&#160;U-shaped validation set performance curve, as illustrated in Fig. 5.3. In the case of&#160;early stopping, we are controlling the effective capacity of the model by determining&#160;how many steps it can take to fit the training set. Most hyperparameters must be&#160;chosen using an expensive guess and check process, where we set a hyperparameter&#160;at the start of training, then run training for several steps to see its effect. The&#160;“training time” hyperparameter is unique in that by definition a single run of&#160;training tries out many values of the hyperparameter. The only significant cost&#160;to choosing this hyperparameter automatically via early stopping is running the&#160;validation set evaluation periodically during training. Ideally, this is done in&#160;parallel to the training process on a separate machine, separate CPU, or separate&#160;GPU from the main training process. If such resources are not available, then the&#160;cost of these periodic evaluations may be reduced by using a validation set that is&#160;small compared to the training set or by evaluating the validation set error less&#160;frequently and obtaining a lower resolution estimate of the optimal training time.</span></p>
<p><span class="font64">An additional cost to early stopping is the need to maintain a copy of the best parameters. This cost is generally negligible, because it is acceptable to store&#160;these parameters in a slower and larger form of memory (for example, training in&#160;GPU memory, but storing the optimal parameters in host memory or on a disk&#160;drive). Since the best parameters are written to infrequently and never read during&#160;training, these occasional slow writes have little effect on the total training time.</span></p>
<p><span class="font64">Early stopping is a very unobtrusive form of regularization, in that it requires almost no change in the underlying training procedure, the objective function,&#160;or the set of allowable parameter values. This means that it is easy to use early&#160;stopping without damaging the learning dynamics. This is in contrast to weight&#160;decay, where one must be careful not to use too much weight decay and trap the&#160;network in a bad local minimum corresponding to a solution with pathologically&#160;small weights.</span></p>
<p><span class="font64">Early stopping may be used either alone or in conjunction with other regularization strategies. Even when using regularization strategies that modify the objective function to encourage better generalization, it is rare for the best generalization to&#160;occur at a local minimum of the training objective.</span></p>
<p><span class="font64">Early stopping requires a validation set, which means some training data is not fed to the model. To best exploit this extra data, one can perform extra training&#160;after the initial training with early stopping has completed. In the second, extra&#160;training step, all of the training data is included. There are two basic strategies&#160;one can use for this second training procedure.</span></p>
<p><span class="font64">One strategy (Algorithm 7.2) is to initialize the model again and retrain on all of the data. In this second training pass, we train for the same number of steps as&#160;the early stopping procedure determined was optimal in the first pass. There are&#160;some subtleties associated with this procedure. For example, there is not a good&#160;way of knowing whether to retrain for the same number of parameter updates or&#160;the same number of passes through the dataset. On the second round of training,&#160;each pass through the dataset will require more parameter updates because the&#160;training set is bigger.</span></p>
<p><span class="font64" style="font-weight:bold;">Algorithm 7.2 </span><span class="font64">A meta-algorithm for using early stopping to determine how long to train, then retraining on all the data.</span></p>
<p><span class="font64">Let </span><span class="font64" style="font-weight:bold;">X</span><span class="font64"><sup>(tra1n)</sup> and </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(tra1n)</sup> be the training set.</span></p>
<p><span class="font64">Split </span><span class="font64" style="font-weight:bold;">X</span><span class="font64"><sup>(tra1n)</sup> and </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(tra1n)</sup> into </span><span class="font64" style="font-weight:bold;font-style:italic;">(X</span><span class="font64"><sup>(subtra1n)</sup>, </span><span class="font64" style="font-weight:bold;font-style:italic;">X</span><span class="font64"><sup>(val1d)</sup>) and (</span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(subtra1n)</sup>, </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(val1d)</sup>) respectively.</span></p>
<p><span class="font64">Run early stopping (Algorithm 7.1) starting from random </span><span class="font64" style="font-weight:bold;">6 </span><span class="font64">using </span><span class="font64" style="font-weight:bold;">X</span><span class="font64"><sup>(subtra1n) </sup>and </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(subtra1n)</sup> for training data and </span><span class="font64" style="font-weight:bold;">X</span><span class="font64"><sup>(val1d)</sup> and </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(val1d)</sup> for validation data. This&#160;returns i*, the optimal number of steps.</span></p>
<p><span class="font64">Set </span><span class="font64" style="font-weight:bold;">6 </span><span class="font64">to random values again.</span></p>
<p><span class="font64">Train on </span><span class="font64" style="font-weight:bold;">X</span><span class="font64"><sup>(tra1n)</sup> and </span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(tra1n)</sup> for i* steps.</span></p>
<p><span class="font64">Another strategy for using all of the data is to keep the parameters obtained from the first round of training and then </span><span class="font64" style="font-weight:bold;">continue </span><span class="font64">training but now using all of&#160;the data. At this stage, we now no longer have a guide for when to stop in terms&#160;of a number of steps. Instead, we can monitor the average loss function on the&#160;validation set, and continue training until it falls below the value of the training&#160;set objective at which the early stopping procedure halted. This strategy avoids&#160;the high cost of retraining the model from scratch, but is not as well-behaved. For&#160;example, there is not any guarantee that the objective on the validation set will&#160;ever reach the target value, so this strategy is not even guaranteed to terminate.&#160;This procedure is presented more formally in Algorithm 7.3.</span></p>
<p><span class="font64">Early stopping is also useful because it reduces the computational cost of the training procedure. Besides the obvious reduction in cost due to limiting the number&#160;of training iterations, it also has the benefit of providing regularization without&#160;requiring the addition of penalty terms to the cost function or the computation of&#160;the gradients of such additional terms.</span></p>
<p><span class="font64">Algorithm 7.3 Meta-algorithm using early stopping to determine at what objective value we start to overfit, then continue training until that value is reached.</span></p>
<p><span class="font64">Let X<sup>(tra1n)</sup> and y<sup>(tra1n)</sup> be the training set.</span></p>
<p><span class="font64">Split X<sup>(tra1n)</sup> and y<sup>(tra1n)</sup> into </span><span class="font64" style="font-weight:bold;font-style:italic;">(X</span><span class="font64"><sup>(subtra1n)</sup>, X<sup>(val1d)</sup>) and (y<sup>(subtra1n)</sup>, y<sup>(val1d)</sup>) respectively.</span></p>
<p><span class="font64">Run early stopping (Algorithm 7.1) starting from random 6 using X<sup>(subtra1n) </sup>and y<sup>(subtra1n)</sup> for training data and X<sup>(val1d)</sup> and y<sup>(val1d)</sup> for validation data. This&#160;updates 6.</span></p>
<p><span class="font64">^ __ </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">j(6</span><span class="font64"> X(<sup>sub</sup>tra1n) y(subtra1n))</span></p>
<p><span class="font64">while J(6, X<sup>(val1d)</sup>, y<sup>(val1d)</sup>) &gt; <sub>e</sub> do</span></p>
<p><span class="font64">Train on X<sup>(tra1n)</sup> and y<sup>(tra1n)</sup> for n steps. end while</span></p>
<p><span class="font64">How early stopping acts as a regularizer: So far we have stated that early stopping </span><span class="font64" style="font-weight:bold;font-style:italic;">is</span><span class="font64"> a regularization strategy, but we have supported this claim only by&#160;showing learning curves where the validation set error has a U-shaped curve. What&#160;is the actual mechanism by which early stopping regularizes the model? Bishop&#160;(1995a) and Sjoberg and Ljung (1995) argued that early stopping has the effect of&#160;restricting the optimization procedure to a relatively small volume of parameter&#160;space in the neighborhood of the initial parameter value 6<sub>o</sub>. More specifically,&#160;imagine taking </span><span class="font63" style="font-variant:small-caps;">t </span><span class="font64">optimization steps (corresponding to </span><span class="font63" style="font-variant:small-caps;">t </span><span class="font64">training iterations) and&#160;with learning rate e. We can view the product </span><span class="font63" style="font-variant:small-caps;">et </span><span class="font64">as a measure of effective capacity.&#160;Assuming the gradient is bounded, restricting both the number of iterations and&#160;the learning rate limits the volume of parameter space reachable from 6<sub>o</sub>. In this&#160;sense, </span><span class="font63" style="font-variant:small-caps;">et </span><span class="font64">behaves as if it were the reciprocal of the coefficient used for weight decay.</span></p>
<p><span class="font64">Indeed, we can show how—in the case of a simple linear model with a quadratic error function and simple gradient descent—early stopping is equivalent to L<sup>2&#160;</sup>regularization.</span></p>
<p><span class="font32">. &#160;&#160;&#160;♦O&#160;&#160;&#160;&#160;♦&#160;&#160;&#160;&#160;♦&#160;&#160;&#160;&#160;♦&#160;&#160;&#160;&#160;♦</span></p>
<p><span class="font64">In order to compare with classical </span><span class="font64" style="font-weight:bold;font-style:italic;">L<sup>2</sup></span><span class="font64"> regularization, we examine a simple setting where the only parameters are linear weights (6 = w). We can model&#160;the cost function J with a quadratic approximation in the neighborhood of the&#160;empirically optimal value of the weights w*:</span></p><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">J(6) = J</span><span class="font64">(w*) + 2(w - w*)<sup>T</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">H(w -</span><span class="font64"> w*), 2</span></p></div><div>
<p><span class="font64">(7.33)</span></p></div>
<p><span class="font64">where H is the Hessian matrix of J with respect to w evaluated at w*. Given the assumption that w* is a minimum of J(w), we know that H is positive semidefinite.</span></p>
<p><span class="font64">Under a local Taylor series approximation, the gradient is given by:</span></p>
<p><span class="font64">Vw J(w) = H(w - w*). &#160;&#160;&#160;(7.34)</span></p>
<p><span class="font64">We are going to study the trajectory followed by the parameter vector during training. For simplicity, let us set the initial parameter vector to the origin,<a id="footnote3"></a><sup><a href="#bookmark12">3</a></sup><sup></sup> that&#160;is w<sup>(0)</sup> = 0. Let us suppose that we update the parameters via gradient descent:</span></p><div>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font64">;<sup>(r)</sup> = w<sup>(t-1)</sup> - eV<sub>w</sub> J(w<sup>(T-1)</sup>)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(7.35)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">= w </span><span class="font64" style="font-variant:small-caps;"><sup>(t-1)</sup></span><span class="font64"> - eH (</span><span class="font30" style="font-variant:small-caps;">w</span><span class="font64" style="font-variant:small-caps;"><sup>(t-1)</sup></span><span class="font64"> - w *)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(7.36)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">w* = (I </span><span class="font64" style="font-weight:bold;font-style:italic;">- eH)(w</span><span class="font64" style="font-variant:small-caps;"> <sup>(t-1)</sup></span><span class="font64"> - w*)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(7.37)</span></p></td></tr>
</table></div><div><div><img src="main-68.jpg" alt=""/>
<p><span class="font64">Figure 7.4: An illustration of the effect of early stopping. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> The solid contour lines indicate the contours of the negative log-likelihood. The dashed line indicates the&#160;trajectory taken by SGD beginning from the origin. Rather than stopping at the point&#160;w* that minimizes the cost, early stopping results in the trajectory stopping at an earlier&#160;point W. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> An illustration of the effect of L<sup>2</sup> regularization for comparison. The&#160;dashed circles indicate the contours of the L<sup>2</sup> penalty, which causes the minimum of the&#160;total cost to lie nearer the origin than the minimum of the unregularized cost.</span></p></div></div><div><div><img src="main-69.jpg" alt=""/></div></div>
<p><span class="font64">w</span></p>
<p><span class="font64">Let us now rewrite this expression in the space of the eigenvectors of H, exploiting the eigendecomposition of </span><span class="font64" style="font-weight:bold;font-style:italic;">H: H =</span><span class="font64"> QAQ<sup>T</sup>, where A is a diagonal matrix and Q&#160;is an orthonormal basis of eigenvectors.</span></p><div>
<p><span class="font64">w</span></p></div><div>
<p><span class="font64"><sup>(t)</sup> - w* = (I - eQAQ<sup>T</sup>)(w<sup>(T-1)</sup> - w*</span></p></div><div>
<p><span class="font64">w*)</span></p></div><div>
<p><span class="font64">(7.38)</span></p></div>
<p><span class="font30" style="font-variant:small-caps;">Q<sup>t</sup>(w</span><span class="font64" style="font-variant:small-caps;"><sup>(t)</sup></span><span class="font64"> - w*) = </span><span class="font64" style="font-weight:bold;font-style:italic;">(I - eA)Q<sup>T</sup></span><span class="font64"> (w<sup>(T-1)</sup> - w*) &#160;&#160;&#160;(7.39)</span></p>
<p><span class="font64">Assuming that w<sup>(0)</sup> = 0 and that e is chosen to be small enough to guarantee |1 — </span><span class="font64" style="font-weight:bold;font-style:italic;">e\i</span><span class="font64"> | &lt; 1, the parameter trajectory during training after </span><span class="font63" style="font-variant:small-caps;">t </span><span class="font64">parameter updates&#160;is as follows:</span></p><div>
<p><span class="font64">Q<sup>T</sup>w<sup>(t)</sup> = [I - (I - eA)<sup>T</sup>]Q<sup>T</sup>w*</span></p></div><div>
<p><span class="font64"><sub>w</sub>*</span></p></div><div>
<p><span class="font64">(7.40)</span></p></div><div>
<p><span class="font22">1 &#160;&#160;&#160;~&#160;&#160;&#160;&#160;o</span></p>
<p><span class="font64">Now, the expression for Q <sup>1</sup> w in Eq. 7.13 for </span><span class="font64" style="font-weight:bold;font-style:italic;">L</span><span class="font64"> regularization can be rearranged as:</span></p>
<p><span class="font64" style="font-variant:small-caps;">Q<sup>t</sup></span><span class="font64"> w = (A + </span><span class="font64" style="font-weight:bold;font-style:italic;">aI</span><span class="font64"> )<sup>-1</sup>A Q <sup>T</sup>w *</span></p></div><div>
<p><span class="font64">»T,</span></p></div><div>
<p><span class="font64">»T</span></p></div><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64">Q <sup>1</sup> w = [I - (A + al) <sup>1</sup>a]Q<sup>T</sup>w*</span></p></div><div>
<p><span class="font64">(7.41)</span></p>
<p><span class="font64">(7.42)</span></p></div>
<p><span class="font64">Comparing Eq. 7.40 and Eq. 7.42, we see that if the hyperparameters e, a, and </span><span class="font63" style="font-variant:small-caps;">t </span><span class="font64">are chosen such that</span></p>
<p><span class="font64">(I - eA)<sup>T</sup> = (A + aI)<sup>-1</sup> a, &#160;&#160;&#160;(7.43)</span></p>
<p><span class="font64">then L<sup>2</sup> regularization and early stopping can be seen to be equivalent (at least under the quadratic approximation of the objective function). Going even further,&#160;by taking logarithms and using the series expansion for log(1 + x), we can conclude&#160;that if all A<sub>i</sub> are small (that is, eA ^ 1 and A<sub>i</sub>/a ^ 1) then</span></p>
<p><span class="font64">1</span></p><div>
<p><span class="font63" style="font-variant:small-caps;">t</span></p></div><div>
<p><span class="font64">ea</span></p>
<p><span class="font64">1</span></p>
<p><span class="font64">Te</span></p></div>
<p><span class="font64">(7.44)</span></p><div>
<p><span class="font64">a</span></p></div>
<p><span class="font64">(7.45)</span></p>
<p><span class="font64">That is, under these assumptions, the number of training iterations </span><span class="font63" style="font-variant:small-caps;">t </span><span class="font64">plays a role</span></p>
<p><span class="font22" style="font-style:italic;">C\</span></p>
<p><span class="font64">inversely proportional to the </span><span class="font64" style="font-weight:bold;font-style:italic;">L</span><span class="font64"> regularization parameter, and the inverse of Te plays the role of the weight decay coefficient.</span></p>
<p><span class="font64">Parameter values corresponding to directions of significant curvature (of the objective function) are regularized less than directions of less curvature. Of course,&#160;in the context of early stopping, this really means that parameters that correspond&#160;to directions of significant curvature tend to learn early relative to parameters&#160;corresponding to directions of less curvature.</span></p>
<p><span class="font64">The derivations in this section have shown that a trajectory of length </span><span class="font63" style="font-variant:small-caps;">t </span><span class="font64">ends at a point that corresponds to a minimum of the L<sup>2</sup>-regularized objective. Early&#160;stopping is of course more than the mere restriction of the trajectory length;&#160;instead, early stopping typically involves monitoring the validation set error in&#160;order to stop the trajectory at a particularly good point in space. Early stopping&#160;therefore has the advantage over weight decay that early stopping automatically&#160;determines the correct amount of regularization while weight decay requires many&#160;training experiments with different values of its hyperparameter.</span></p>
<p><a id="bookmark3"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font64"><sup></sup> More generally, we could regularize the parameters to be near any specific point in space and, surprisingly, still get a regularization effect, but better results will be obtained for a value&#160;closer to the true one, with zero being a default value that makes sense when we do not know if&#160;the correct value should be positive or negative. Since it is far more common to regularize the&#160;model parameters towards zero, we will focus on this special case in our exposition.</span></p>
<p><a id="bookmark4"><sup><a href="#footnote2">2</a></sup></a></p>
<p><span class="font64"><sup></sup> As with L<sup>2</sup> regularization, we could regularize the parameters towards a value that is not zero, but instead towards some parameter value </span><span class="font64" style="font-style:italic;">w</span><span class="font64"><sup>(o)</sup>. In that case the L<sup>1</sup> regularization would&#160;introduce the term Q(9) </span><span class="font64" style="font-style:italic;">=</span><span class="font64"> || w — w<sup>(o)</sup> || <sub>1</sub> =&#160;&#160;&#160;&#160;|wj — w<sup>(o)</sup> |.</span></p>
<p><a id="bookmark12"><sup><a href="#footnote3">3</a></sup></a></p>
<p><span class="font64"><sup></sup> For neural networks, to obtain symmetry breaking between hidden units, we cannot initialize all the parameters to 0, as discussed in Sec. 6.2. However, the argument holds for any other&#160;initial value </span><span class="font64" style="font-style:italic;">w<sup>(0)</sup>.</span></p>
</body>
</html>