<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h4><a id="bookmark0"></a><span class="font65" style="font-weight:bold;">6.3 Hidden Units</span></h4>
<p><span class="font64">So far we have focused our discussion on design choices for neural networks that are common to most parametric machine learning models trained with gradient-based optimization. Now we turn to an issue that is unique to feedforward neural&#160;networks: how to choose the type of hidden unit to use in the hidden layers of the&#160;model.</span></p>
<p><span class="font64">The design of hidden units is an extremely active area of research and does not yet have many definitive guiding theoretical principles.</span></p>
<p><span class="font64">Rectified linear units are an excellent default choice of hidden unit. Many other types of hidden units are available. It can be difficult to determine when to use&#160;which kind (though rectified linear units are usually an acceptable choice). We&#160;describe here some of the basic intuitions motivating each type of hidden units.&#160;These intuitions can be used to suggest when to try out each of these units. It is&#160;usually impossible to predict in advance which will work best. The design process&#160;consists of trial and error, intuiting that a kind of hidden unit may work well,&#160;and then training a network with that kind of hidden unit and evaluating its&#160;performance on a validation set.</span></p>
<p><span class="font64">Some of the hidden units included in this list are not actually differentiable at all input points. For example, the rectified linear function </span><span class="font64" style="font-weight:bold;font-style:italic;">g(z</span><span class="font64">) = max{0 </span><span class="font64" style="font-weight:bold;font-style:italic;">,z</span><span class="font64">} is not&#160;differentiable at z = 0. This may seem like it invalidates g for use with a gradient-based learning algorithm. In practice, gradient descent still performs well enough&#160;for these models to be used for machine learning tasks. This is in part because&#160;neural network training algorithms do not usually arrive at a local minimum of&#160;the cost function, but instead merely reduce its value significantly, as shown in&#160;Fig. 4.3. These ideas will be described further in Chapter 8. Because we do not&#160;expect training to actually reach a point where the gradient is 0, it is acceptable&#160;for the minima of the cost function to correspond to points with undefined gradient.&#160;Hidden units that are not differentiable are usually non-differentiable at only a&#160;small number of points. In general, a function g(z) has a left derivative defined&#160;by the slope of the function immediately to the left of z and a right derivative&#160;defined by the slope of the function immediately to the right of z. A function&#160;is differentiable at z only if both the left derivative and the right derivative are&#160;defined and equal to each other. The functions used in the context of neural&#160;networks usually have defined left derivatives and defined right derivatives. In the&#160;case of g(z) = max{0, z}, the left derivative at z = 0 is 0 and the right derivative&#160;is 1. Software implementations of neural network training usually return one of&#160;the one-sided derivatives rather than reporting that the derivative is undefined or&#160;raising an error. This may be heuristically justified by observing that gradient-based optimization on a digital computer is subject to numerical error anyway.&#160;When a function is asked to evaluate g(0), it is very unlikely that the underlying&#160;value truly was 0. Instead, it was likely to be some small value e that was rounded&#160;to 0. In some contexts, more theoretically pleasing justifications are available, but&#160;these usually do not apply to neural network training. The important point is that&#160;in practice one can safely disregard the non-differentiability of the hidden unit&#160;activation functions described below.</span></p>
<p><span class="font64">Unless indicated otherwise, most hidden units can be described as accepting a vector of inputs x, computing an affine transformation </span><span class="font64" style="font-weight:bold;font-style:italic;">z = W</span><span class="font64"><sup>T</sup>x + b, and&#160;then applying an element-wise nonlinear function g( z). Most hidden units are&#160;distinguished from each other only by the choice of the form of the activation&#160;function </span><span class="font64" style="font-weight:bold;font-style:italic;">g(z</span><span class="font64">).</span></p><h5><a id="bookmark1"></a><span class="font64" style="font-weight:bold;">6.3.1 Rectified Linear Units and Their Generalizations</span></h5>
<p><span class="font64">Rectified linear units use the activation function </span><span class="font64" style="font-weight:bold;font-style:italic;">g(z</span><span class="font64">) = max{0, z}.</span></p>
<p><span class="font64">Rectified linear units are easy to optimize because they are so similar to linear units. The only difference between a linear unit and a rectified linear unit is&#160;that a rectified linear unit outputs zero across half its domain. This makes the&#160;derivatives through a rectified linear unit remain large whenever the unit is active.&#160;The gradients are not only large but also consistent. The second derivative of the&#160;rectifying operation is 0 almost everywhere, and the derivative of the rectifying&#160;operation is 1 everywhere that the unit is active. This means that the gradient&#160;direction is far more useful for learning than it would be with activation functions&#160;that introduce second-order effects.</span></p>
<p><span class="font64">Rectified linear units are typically used on top of an affine transformation:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font64"> = </span><span class="font64" style="font-weight:bold;font-style:italic;">g(W<sup>T</sup>x</span><span class="font64"> + b). &#160;&#160;&#160;(6.36)</span></p>
<p><span class="font64">When initializing the parameters of the affine transformation, it can be a good practice to set all elements of b to a small, positive value, such as 0.1. This makes&#160;it very likely that the rectified linear units will be initially active for most inputs&#160;in the training set and allow the derivatives to pass through.</span></p>
<p><span class="font64">Several generalizations of rectified linear units exist. Most of these generalizations perform comparably to rectified linear units and occasionally perform better.</span></p>
<p><span class="font64">One drawback to rectified linear units is that they cannot learn via gradient-based methods on examples for which their activation is zero. A variety of generalizations of rectified linear units guarantee that they receive gradient everywhere.</span></p>
<p><span class="font64">Three generalizations of rectified linear units are based on using a non-zero slope a when </span><span class="font64" style="font-weight:bold;font-style:italic;">Zi &lt;</span><span class="font64"> 0: h = g(z, </span><span class="font64" style="font-weight:bold;font-style:italic;">a) i =</span><span class="font64"> max(0, </span><span class="font64" style="font-weight:bold;font-style:italic;">Zi</span><span class="font64">) + </span><span class="font64" style="font-weight:bold;font-style:italic;">ai</span><span class="font64">min(0,zi). </span><span class="font64" style="font-weight:bold;font-style:italic;">Absolute value&#160;rectification</span><span class="font64"> fixes a<sub>i</sub> = — 1 to obtain g(z) = |z|. It is used for object recognition&#160;from images (Jarrett </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009), where it makes sense to seek features that are&#160;invariant under a polarity reversal of the input illumination. Other generalizations&#160;of rectified linear units are more broadly applicable. A </span><span class="font64" style="font-weight:bold;font-style:italic;">leaky ReLU</span><span class="font64"> (Maas </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2013) fixes a<sub>i</sub> to a small value like 0.01 while a </span><span class="font64" style="font-weight:bold;font-style:italic;">parametric ReLU</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">PReLU</span><span class="font64"> treats&#160;a<sub>i</sub> as a learnable parameter (He </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015).</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Maxout units</span><span class="font64"> (Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013a) generalize rectified linear units further. Instead of applying an element-wise function g (z), maxout units divide z into&#160;groups of </span><span class="font64" style="font-weight:bold;font-style:italic;">k</span><span class="font64"> values. Each maxout unit then outputs the maximum element of one</span></p><div>
<p><span class="font64">(6.37)</span></p></div><div>
<p><span class="font64">of these groups:</span></p></div>
<p><span class="font64">g(z</span><span class="font64" style="font-weight:bold;font-style:italic;">)i =</span><span class="font64"> max </span><span class="font64" style="font-weight:bold;font-style:italic;">zj</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">j&amp;G</span></p>
<p><span class="font64">where G<sup>(i)</sup> is the indices of the inputs for group i, { </span><span class="font64" style="font-weight:bold;font-style:italic;">(i —</span><span class="font64"> 1) k + 1,..., </span><span class="font64" style="font-weight:bold;font-style:italic;">ik}.</span><span class="font64"> This provides a way of learning a piecewise linear function that responds to multiple&#160;directions in the input x space.</span></p>
<p><span class="font64">A maxout unit can learn a piecewise linear, convex function with up to k pieces. Maxout units can thus be seen as </span><span class="font64" style="font-weight:bold;">learning the activation function </span><span class="font64">itself rather&#160;than just the relationship between units. With large enough k, a maxout unit can&#160;learn to approximate any convex function with arbitrary fidelity. In particular,&#160;a maxout layer with two pieces can learn to implement the same function of the&#160;input x as a traditional layer using the rectified linear activation function, absolute&#160;value rectification function, or the leaky or parametric ReLU, or can learn to&#160;implement a totally different function altogether. The maxout layer will of course&#160;be parametrized differently from any of these other layer types, so the learning&#160;dynamics will be different even in the cases where maxout learns to implement the&#160;same function of x as one of the other layer types.</span></p>
<p><span class="font64">Each maxout unit is now parametrized by k weight vectors instead of just one, so maxout units typically need more regularization than rectified linear units. They&#160;can work well without regularization if the training set is large and the number of&#160;pieces per unit is kept low (Cai </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013).</span></p>
<p><span class="font64">Maxout units have a few other benefits. In some cases, one can gain some statistical and computational advantages by requiring fewer parameters. Specifically, if the features captured by n different linear filters can be summarized without&#160;losing information by taking the max over each group of k features, then the next&#160;layer can get by with k times fewer weights.</span></p>
<p><span class="font64">Because each unit is driven by multiple filters, maxout units have some redundancy that helps them to resist a phenomenon called </span><span class="font64" style="font-weight:bold;font-style:italic;">catastrophic forgetting</span><span class="font64"> in which neural networks forget how to perform tasks that they were trained on in&#160;the past (Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014a).</span></p>
<p><span class="font64">Rectified linear units and all of these generalizations of them are based on the principle that models are easier to optimize if their behavior is closer to linear.&#160;This same general principle of using linear behavior to obtain easier optimization&#160;also applies in other contexts besides deep linear networks. Recurrent networks can&#160;learn from sequences and produce a sequence of states and outputs. When training&#160;them, one needs to propagate information through several time steps, which is much&#160;easier when some linear computations (with some directional derivatives being of&#160;magnitude near 1) are involved. One of the best-performing recurrent network&#160;architectures, the LSTM, propagates information through time via summation—a&#160;particular straightforward kind of such linear activation. This is discussed further&#160;in Sec. 10.10.</span></p><h5><a id="bookmark2"></a><span class="font64" style="font-weight:bold;">6.3.2 Logistic Sigmoid and Hyperbolic Tangent</span></h5>
<p><span class="font64">Prior to the introduction of rectified linear units, most neural networks used the logistic sigmoid activation function</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">g(z</span><span class="font64">) = a(z) &#160;&#160;&#160;(6.38)</span></p>
<p><span class="font64">or the hyperbolic tangent activation function</span></p>
<p><span class="font64">g(z) = tanh(z). &#160;&#160;&#160;(6.39)</span></p>
<p><span class="font64">These activation functions are closely related because tanh(z) = 2a(2z) </span><span class="font64" style="font-weight:bold;font-style:italic;">—</span><span class="font64"> 1.</span></p>
<p><span class="font64">We have already seen sigmoid units as output units, used to predict the probability that a binary variable is 1. Unlike piecewise linear units, sigmoidal&#160;units saturate across most of their domain—they saturate to a high value when&#160;z is very positive, saturate to a low value when z is very negative, and are only&#160;strongly sensitive to their input when z is near 0. The widespread saturation of&#160;sigmoidal units can make gradient-based learning very difficult. For this reason,&#160;their use as hidden units in feedforward networks is now discouraged. Their use&#160;as output units is compatible with the use of gradient-based learning when an&#160;appropriate cost function can undo the saturation of the sigmoid in the output&#160;layer.</span></p>
<p><span class="font64">When a sigmoidal activation function must be used, the hyperbolic tangent activation function typically performs better than the logistic sigmoid. It resembles&#160;the identity function more closely, in the sense that tanh (0) = 0 while </span><span class="font64" style="font-weight:bold;font-style:italic;">a</span><span class="font64"> (0) = 2.&#160;Because tanh is similar to identity near 0, training a deep neural network </span><span class="font64" style="font-weight:bold;font-style:italic;">y =&#160;</span><span class="font64">w<sup>T</sup> tanh(U<sup>T</sup>tanh(V<sup>T</sup>x)) resembles training a linear model y = w<sup>T</sup>U<sup>T</sup>V<sup>T</sup>x so&#160;long as the activations of the network can be kept small. This makes training the&#160;tanh network easier.</span></p>
<p><span class="font64">Sigmoidal activation functions are more common in settings other than feedforward networks. Recurrent networks, many probabilistic models, and some autoencoders have additional requirements that rule out the use of piecewise&#160;linear activation functions and make sigmoidal units more appealing despite the&#160;drawbacks of saturation.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">6.3.3 Other Hidden Units</span></h5>
<p><span class="font64">Many other types of hidden units are possible, but are used less frequently.</span></p>
<p><span class="font64">In general, a wide variety of differentiable functions perform perfectly well. Many unpublished activation functions perform just as well as the popular ones.&#160;To provide a concrete example, the authors tested a feedforward network using&#160;h = </span><span class="font64" style="font-weight:bold;font-style:italic;">cos(Wx</span><span class="font64"> + b) on the MNIST dataset and obtained an error rate of less than&#160;1%, which is competitive with results obtained using more conventional activation&#160;functions. During research and development of new techniques, it is common&#160;to test many different activation functions and find that several variations on&#160;standard practice perform comparably. This means that usually new hidden unit&#160;types are published only if they are clearly demonstrated to provide a significant&#160;improvement. New hidden unit types that perform roughly comparably to known&#160;types are so common as to be uninteresting.</span></p>
<p><span class="font64">It would be impractical to list all of the hidden unit types that have appeared in the literature. We highlight a few especially useful and distinctive ones.</span></p>
<p><span class="font64">One possibility is to not have an activation g(z) at all. One can also think of this as using the identity function as the activation function. We have already&#160;seen that a linear unit can be useful as the output of a neural network. It may&#160;also be used as a hidden unit. If every layer of the neural network consists of only&#160;linear transformations, then the network as a whole will be linear. However, it&#160;is acceptable for some layers of the neural network to be purely linear. Consider&#160;a neural network layer with n inputs and p outputs, h = g( W<sup>T</sup>x + b). We may&#160;replace this with two layers, with one layer using weight matrix U and the other&#160;using weight matrix V. If the first layer has no activation function, then we have&#160;essentially factored the weight matrix of the original layer based on W. The&#160;factored approach is to compute h = g(V<sup>T</sup>U<sup>T</sup>x + b). If U produces q outputs,&#160;then U and V together contain only (n + p)q parameters, while W contains np&#160;parameters. For small q, this can be a considerable saving in parameters. It&#160;comes at the cost of constraining the linear transformation to be low-rank, but&#160;these low-rank relationships are often sufficient. Linear hidden units thus offer an&#160;effective way of reducing the number of parameters in a network.</span></p>
<p><span class="font64">Softmax units are another kind of unit that is usually used as an output (as described in Sec. 6.2.2.3) but may sometimes be used as a hidden unit. Softmax&#160;units naturally represent a probability distribution over a discrete variable with k&#160;possible values, so they may be used as a kind of switch. These kinds of hidden&#160;units are usually only used in more advanced architectures that explicitly learn to&#160;manipulate memory, described in Sec. 10.12.</span></p>
<p><span class="font64">A few other reasonably common hidden unit types include:</span></p>
<p><span class="font64">• &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">Radial basis function</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">RBF</span><span class="font64"> unit: </span><span class="font64" style="font-weight:bold;font-style:italic;">hi =</span><span class="font64"> exp ^—^2 ||W&quot;,i — x||<sup>2</sup>^. This&#160;function becomes more active as x approaches a template </span><span class="font64" style="font-weight:bold;font-style:italic;">W</span><span class="font64">-<sub>7i</sub>. Because it&#160;saturates to 0 for most x, it can be difficult to optimize.</span></p>
<p><span class="font64">• </span><span class="font64" style="font-weight:bold;font-style:italic;">Softplus:</span><span class="font64"> g(a) </span><span class="font64" style="font-weight:bold;font-style:italic;">= Z</span><span class="font64">(a) </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> log(1 + e<sup>a</sup>). This is a smooth version of the rectifier, introduced by Dugas </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2001) for function approximation and by Nair&#160;and Hinton (2010) for the conditional distributions of undirected probabilistic&#160;models. Glorot </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011a) compared the softplus and rectifier and found&#160;better results with the latter. The use of the softplus is generally discouraged.&#160;The softplus demonstrates that the performance of hidden unit types can&#160;be very counterintuitive—one might expect it to have an advantage over&#160;the rectifier due to being differentiable everywhere or due to saturating less&#160;completely, but empirically it does not.</span></p>
<p><span class="font64">• &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">Hard</span><span class="font64"> tanh: this is shaped similarly to the tanh and the rectifier but unlike&#160;the latter, it is bounded, g(a) = max(— 1,min(1 ,a)). It was introduced&#160;by Collobert (2004).</span></p>
<p><span class="font64">Hidden unit design remains an active area of research and many useful hidden unit types remain to be discovered.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">6.4 Architecture Design</span></h4>
<p><span class="font64">Another key design consideration for neural networks is determining the architecture. The word </span><span class="font64" style="font-weight:bold;font-style:italic;">architecture</span><span class="font64"> refers to the overall structure of the network: how many&#160;units it should have and how these units should be connected to each other.</span></p>
<p><span class="font64">Most neural networks are organized into groups of units called layers. Most neural network architectures arrange these layers in a chain structure, with each&#160;layer being a function of the layer that preceded it. In this structure, the first layer&#160;is given by</span></p>
<p><span class="font64">h<sup>(1)</sup> = g<sup>(1)</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">(w</span><span class="font64"><sup>(1)T</sup>x + b<sup>(1)</sup>) , &#160;&#160;&#160;(6.40)</span></p>
<p><span class="font64">the second layer is given by</span></p>
<p><span class="font64">h<sup>(2)</sup> = g<sup>(2)</sup> (w<sup>(2)T</sup>h<sup>(1)</sup> + b<sup>(2)</sup>) , &#160;&#160;&#160;(6.41)</span></p>
<p><span class="font64">and so on.</span></p>
<p><span class="font64">In these chain-based architectures, the main architectural considerations are to choose the depth of the network and the width of each layer. As we will see,&#160;a network with even one hidden layer is sufficient to fit the training set. Deeper&#160;networks often are able to use far fewer units per layer and far fewer parameters&#160;and often generalize to the test set, but are also often harder to optimize. The&#160;ideal network architecture for a task must be found via experimentation guided by&#160;monitoring the validation set error.</span></p><h5><a id="bookmark5"></a><span class="font64" style="font-weight:bold;">6.4.1 Universal Approximation Properties and Depth</span></h5>
<p><span class="font64">A linear model, mapping from features to outputs via matrix multiplication, can by definition represent only linear functions. It has the advantage of being easy to&#160;train because many loss functions result in convex optimization problems when&#160;applied to linear models. Unfortunately, we often want to learn nonlinear functions.</span></p>
<p><span class="font64">At first glance, we might presume that learning a nonlinear function requires designing a specialized model family for the kind of nonlinearity we want to learn.&#160;Fortunately, feedforward networks with hidden layers provide a universal approximation framework. Specifically, the </span><span class="font64" style="font-weight:bold;font-style:italic;">universal approximation theorem</span><span class="font64"> (Hornik </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">1989; Cybenko, 1989) states that a feedforward network with a linear output layer&#160;and at least one hidden layer with any “squashing” activation function (such as&#160;the logistic sigmoid activation function) can approximate any Borel measurable&#160;function from one finite-dimensional space to another with any desired non-zero&#160;amount of error, provided that the network is given enough hidden units. The&#160;derivatives of the feedforward network can also approximate the derivatives of the&#160;function arbitrarily well (Hornik </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1990). The concept of Borel measurability&#160;is beyond the scope of this book; for our purposes it suffices to say that any&#160;continuous function on a closed and bounded subset of R<sup>n</sup> is Borel measurable&#160;and therefore may be approximated by a neural network. A neural network may&#160;also approximate any function mapping from any finite dimensional discrete space&#160;to another. While the original theorems were first stated in terms of units with&#160;activation functions that saturate both for very negative and for very positive&#160;arguments, universal approximation theorems have also been proven for a wider&#160;class of activation functions, which includes the now commonly used rectified linear&#160;unit (Leshno </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1993).</span></p>
<p><span class="font64">The universal approximation theorem means that regardless of what function we are trying to learn, we know that a large MLP will be able to </span><span class="font64" style="font-weight:bold;">represent </span><span class="font64">this&#160;function. However, we are not guaranteed that the training algorithm will be able&#160;to </span><span class="font64" style="font-weight:bold;">learn </span><span class="font64">that function. Even if the MLP is able to represent the function, learning&#160;can fail for two different reasons. First, the optimization algorithm used for training&#160;may not be able to find the value of the parameters that corresponds to the desired&#160;function. Second, the training algorithm might choose the wrong function due to&#160;overfitting. Recall from Sec. 5.2.1 that the “no free lunch” theorem shows that&#160;there is no universally superior machine learning algorithm. Feedforward networks&#160;provide a universal system for representing functions, in the sense that, given a&#160;function, there exists a feedforward network that approximates the function. There&#160;is no universal procedure for examining a training set of specific examples and&#160;choosing a function that will generalize to points not in the training set.</span></p>
<p><span class="font64">The universal approximation theorem says that there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not&#160;say how large this network will be. Barron (1993) provides some bounds on the&#160;size of a single-layer network needed to approximate a broad class of functions.&#160;Unfortunately, in the worse case, an exponential number of hidden units (possibly&#160;with one hidden unit corresponding to each input configuration that needs to be&#160;distinguished) may be required. This is easiest to see in the binary case: the&#160;number of possible binary functions on vectors v </span><span class="font64" style="font-weight:bold;font-style:italic;">&lt;E</span><span class="font64"> {0, 1}<sup>n</sup> is 2<sup>2 </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>n</sup></span><span class="font64"> and selecting&#160;one such function requires 2™ bits, which will in general require </span><span class="font64" style="font-weight:bold;font-style:italic;">O(2</span><span class="font64"><sup>n</sup>) degrees of&#160;freedom.</span></p>
<p><span class="font64">In summary, a feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and&#160;generalize correctly. In many circumstances, using deeper models can reduce the&#160;number of units required to represent the desired function and can reduce the&#160;amount of generalization error.</span></p>
<p><span class="font64">There exist families of functions which can be approximated efficiently by an architecture with depth greater than some value d, but which require a much larger&#160;model if depth is restricted to be less than or equal to d. In many cases, the number&#160;of hidden units required by the shallow model is exponential in n. Such results&#160;were first proven for models that do not resemble the continuous, differentiable&#160;neural networks used for machine learning, but have since been extended to these&#160;models. The first results were for circuits of logic gates (Hastad, 1986). Later&#160;work extended these results to linear threshold units with non-negative weights&#160;(Hastad and Goldmann, 1991; Hajnal </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1993), and then to networks with&#160;continuous-valued activations (Maass, 1992; Maass </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1994). Many modern&#160;neural networks use rectified linear units. Leshno </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (1993) demonstrated&#160;that shallow networks with a broad family of non-polynomial activation functions,&#160;including rectified linear units, have universal approximation properties, but these&#160;results do not address the questions of depth or efficiency—they specify only that&#160;a sufficiently wide rectifier network could represent any function. Pascanu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span></p>
<p><span class="font64">(2013b) and Montufar </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) showed that functions representable with a deep rectifier net can require an exponential number of hidden units with a shallow&#160;(one hidden layer) network. More precisely, they showed that piecewise linear&#160;networks (which can be obtained from rectifier nonlinearities or maxout units) can&#160;represent functions with a number of regions that is exponential in the depth of the&#160;network. Fig. 6.5 illustrates how a network with absolute value rectification creates&#160;mirror images of the function computed on top of some hidden unit, with respect&#160;to the input of that hidden unit. Each hidden unit specifies where to fold the&#160;input space in order to create mirror responses (on both sides of the absolute value&#160;nonlinearity). By composing these folding operations, we obtain an exponentially&#160;large number of piecewise linear regions which can capture all kinds of regular&#160;(e.g., repeating) patterns.</span></p><div><img src="main-55.jpg" alt=""/>
<p><span class="font64">Figure 6.5: An intuitive, geometric explanation of the exponential advantage of deeper rectifier networks formally shown by Pascanu </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2014a) and by Montufar </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2014).&#160;</span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> An absolute value rectification unit has the same output for every pair of mirror&#160;points in its input. The mirror axis of symmetry is given by the hyperplane defined by the&#160;weights and bias of the unit. A function computed on top of that unit (the green decision&#160;surface) will be a mirror image of a simpler pattern across that axis of symmetry. </span><span class="font64" style="font-style:italic;">(Center)&#160;</span><span class="font64">The function can be obtained by folding the space around the axis of symmetry. </span><span class="font64" style="font-style:italic;">(Right)&#160;</span><span class="font64">Another repeating pattern can be folded on top of the first (by another downstream unit)&#160;to obtain another symmetry (which is now repeated four times, with two hidden layers).</span></p></div>
<p><span class="font64">More precisely, the main theorem in Montufar </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) states that the number of linear regions carved out by a deep rectifier network with d inputs,&#160;depth l, and n units per hidden layer, is</span></p>
<p><span class="font64">O ((n)“<sup>-</sup>״n־4.־) . (־,</span></p>
<p><span class="font64">i.e., exponential in the depth l. In the case of maxout networks with k filters per unit, the number of linear regions is</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">0[k</span><span class="font64"><sup>(l-1)</sup>+<sup>d</sup>) . &#160;&#160;&#160;(6.43)</span></p>
<p><span class="font64">Of course, there is no guarantee that the kinds of functions we want to learn in applications of machine learning (and in particular for AI) share such a property.</span></p>
<p><span class="font64">We may also want to choose a deep model for statistical reasons. Any time we choose a specific machine learning algorithm, we are implicitly stating some&#160;set of prior beliefs we have about what kind of function the algorithm should&#160;learn. Choosing a deep model encodes a very general belief that the function we&#160;want to learn should involve composition of several simpler functions. This can be&#160;interpreted from a representation learning point of view as saying that we believe&#160;the learning problem consists of discovering a set of underlying factors of variation&#160;that can in turn be described in terms of other, simpler underlying factors of&#160;variation. Alternately, we can interpret the use of a deep architecture as expressing&#160;a belief that the function we want to learn is a computer program consisting of&#160;multiple steps, where each step makes use of the previous step’s output. These&#160;intermediate outputs are not necessarily factors of variation, but can instead be&#160;analogous to counters or pointers that the network uses to organize its internal&#160;processing. Empirically, greater depth does seem to result in better generalization&#160;for a wide variety of tasks (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2007; Erhan </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009; Bengio, 2009;&#160;Mesnil </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011; Ciresan </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012; Krizhevsky </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012; Sermanet </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2013; Farabet </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013; Couprie </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013; Kahou </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013; Goodfellow&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014d; Szegedy </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014a). See Fig. 6.6 and Fig. 6.7 for examples of some&#160;of these empirical results. This suggests that using deep architectures does indeed&#160;express a useful prior over the space of functions the model learns.</span></p><h5><a id="bookmark6"></a><span class="font64" style="font-weight:bold;">6.4.2 Other Architectural Considerations</span></h5>
<p><span class="font64">So far we have described neural networks as being simple chains of layers, with the main considerations being the depth of the network and the width of each layer.&#160;In practice, neural networks show considerably more diversity.</span></p>
<p><span class="font64">Many neural network architectures have been developed for specific tasks. Specialized architectures for computer vision called convolutional networks are&#160;described in Chapter 9. Feedforward networks may also be generalized to the&#160;recurrent neural networks for sequence processing, described in Chapter 10, which&#160;have their own architectural considerations.</span></p>
<p><span class="font64">In general, the layers need not be connected in a chain, even though this is the most common practice. Many architectures build a main chain but then add extra&#160;architectural features to it, such as skip connections going from layer </span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> to layer&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> + 2 or higher. These skip connections make it easier for the gradient to flow from&#160;output layers to layers nearer the input.</span></p><div><div><img src="main-56.jpg" alt=""/>
<p><span class="font63">Number of hidden layers</span></p>
<p><span class="font64">Figure 6.6: Empirical results showing that deeper networks generalize better when used to transcribe multi-digit numbers from photographs of addresses. Data from Goodfellow&#160;</span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2014d). The test set accuracy consistently increases with increasing depth. See&#160;Fig. 6.7 for a control experiment demonstrating that other increases to the model size do&#160;not yield the same effect.</span></p></div></div>
<p><span class="font64">Effect of Number of Parameters</span></p><div><div>
<p><span class="font61" style="font-variant:small-caps;">kn</span></p>
<p><span class="font64">O</span></p>
<p><span class="font64">c3</span></p>
<p><span class="font62">S-l</span></p>
<p dir="rtl"><span class="font64">ק</span></p>
<p><span class="font64">O</span></p>
<p><span class="font64">o</span></p>
<p><span class="font64">c3</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">V)</span></p>
<p><span class="font64">£</span></p><img src="main-57.jpg" alt=""/>
<p><span class="font64">Figure 6.7: Deeper models tend to perform better. This is not merely because the model is larger. This experiment from Goodfellow </span><span class="font64" style="font-style:italic;">et al.</span><span class="font64"> (2014d) shows that increasing the number&#160;of parameters in layers of convolutional networks without increasing their depth is not&#160;nearly as effective at increasing test set performance. The legend indicates the depth of&#160;network used to make each curve and whether the curve represents variation in the size of&#160;the convolutional or the fully connected layers. We observe that shallow models in this&#160;context overfit at around 20 million parameters while deep ones can benefit from having&#160;over 60 million. This suggests that using a deep model expresses a useful preference over&#160;the space of functions the model can learn. Specifically, it expresses a belief that the&#160;function should consist of many simpler functions composed together. This could result&#160;either in learning a representation that is composed in turn of simpler representations (e.g.,&#160;corners defined in terms of edges) or in learning a program with sequentially dependent&#160;steps (e.g., first locate a set of objects, then segment them from each other, then recognize&#160;them).</span></p></div></div>
<p><span class="font64">Another key consideration of architecture design is exactly how to connect a pair of layers to each other. In the default neural network layer described by a linear&#160;transformation via a matrix W, every input unit is connected to every output&#160;unit. Many specialized networks in the chapters ahead have fewer connections, so&#160;that each unit in the input layer is connected to only a small subset of units in&#160;the output layer. These strategies for reducing the number of connections reduce&#160;the number of parameters and the amount of computation required to evaluate&#160;the network, but are often highly problem-dependent. For example, convolutional&#160;networks, described in Chapter 9, use specialized patterns of sparse connections&#160;that are very effective for computer vision problems. In this chapter, it is difficult&#160;to give much more specific advice concerning the architecture of a generic neural&#160;network. Subsequent chapters develop the particular architectural strategies that&#160;have been found to work well for different application domains.</span></p><h4><a id="bookmark7"></a><span class="font65" style="font-weight:bold;">6.5 Back-Propagation and Other Differentiation Algorithms</span></h4>
<p><span class="font64">When we use a feedforward neural network to accept an input x and produce an output </span><span class="font64" style="font-weight:bold;font-style:italic;">y,</span><span class="font64"> information flows forward through the network. The inputs x provide&#160;the initial information that then propagates up to the hidden units at each layer&#160;and finally produces y. This is called </span><span class="font64" style="font-weight:bold;font-style:italic;">forward propagation.</span><span class="font64"> During training,&#160;forward propagation can continue onward until it produces a scalar cost J(6). The&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">back-propagation</span><span class="font64"> algorithm (Rumelhart </span><span class="font64" style="font-weight:bold;font-style:italic;">et al</span><span class="font64">., 1986a), often simply called </span><span class="font64" style="font-weight:bold;font-style:italic;">backprop,&#160;</span><span class="font64">allows the information from the cost to then flow backwards through the network,&#160;in order to compute the gradient.</span></p>
<p><span class="font64">Computing an analytical expression for the gradient is straightforward, but numerically evaluating such an expression can be computationally expensive. The&#160;back-propagation algorithm does so using a simple and inexpensive procedure.</span></p>
<p><span class="font64">The term back-propagation is often misunderstood as meaning the whole learning algorithm for multi-layer neural networks. Actually, back-propagation&#160;refers only to the method for computing the gradient, while another algorithm,&#160;such as stochastic gradient descent, is used to perform learning using this gradient.&#160;Furthermore, back-propagation is often misunderstood as being specific to multilayer neural networks, but in principle it can compute derivatives of any function&#160;(for some functions, the correct response is to report that the derivative of the&#160;function is undefined). Specifically, we will describe how to compute the gradient&#160;V<sub>x</sub> f (x, y) for an arbitrary function f, where x is a set of variables whose derivatives&#160;are desired, and y is an additional set of variables that are inputs to the function&#160;but whose derivatives are not required. In learning algorithms, the gradient we most&#160;often require is the gradient of the cost function with respect to the parameters,&#160;Vg </span><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">0</span><span class="font64">). Many machine learning tasks involve computing other derivatives, either&#160;as part of the learning process, or to analyze the learned model. The back-propagation algorithm can be applied to these tasks as well, and is not restricted&#160;to computing the gradient of the cost function with respect to the parameters. The&#160;idea of computing derivatives by propagating information through a network is&#160;very general, and can be used to compute values such as the Jacobian of a function&#160;f with multiple outputs. We restrict our description here to the most commonly&#160;used case where f has a single output.</span></p><h5><a id="bookmark8"></a><span class="font64" style="font-weight:bold;">6.5.1 Computational Graphs</span></h5>
<p><span class="font64">So far we have discussed neural networks with a relatively informal graph language. To describe the back-propagation algorithm more precisely, it is helpful to have a&#160;more precise </span><span class="font64" style="font-weight:bold;font-style:italic;">computational graph</span><span class="font64"> language.</span></p>
<p><span class="font64">Many ways of formalizing computation as graphs are possible.</span></p>
<p><span class="font64">Here, we use each node in the graph to indicate a variable. The variable may be a scalar, vector, matrix, tensor, or even a variable of another type.</span></p>
<p><span class="font64">To formalize our graphs, we also need to introduce the idea of an </span><span class="font64" style="font-weight:bold;font-style:italic;">operation. </span><span class="font64">An operation is a simple function of one or more variables. Our graph language&#160;is accompanied by a set of allowable operations. Functions more complicated&#160;than the operations in this set may be described by composing many operations&#160;together.</span></p>
<p><span class="font64">Without loss of generality, we define an operation to return only a single output variable. This does not lose generality because the output variable can have&#160;multiple entries, such as a vector. Software implementations of back-propagation&#160;usually support operations with multiple outputs, but we avoid this case in our&#160;description because it introduces many extra details that are not important to&#160;conceptual understanding.</span></p>
<p><span class="font64">If a variable y is computed by applying an operation to a variable x, then we draw a directed edge from x to y. We sometimes annotate the output node&#160;with the name of the operation applied, and other times omit this label when the&#160;operation is clear from context.</span></p>
<p><span class="font64">Examples of computational graphs are shown in Fig. 6.8.</span></p><div><div><img src="main-58.jpg" alt=""/></div></div><div><div><img src="main-59.jpg" alt=""/></div></div>
<p><span class="font63"><sup>(a) (b)</sup></span></p><div><div><img src="main-60.jpg" alt=""/></div></div><div><div><img src="main-61.jpg" alt=""/></div></div>
<p><span class="font63">(c) &#160;&#160;&#160;(d)</span></p>
<p><span class="font64">Figure 6.8: Examples of computational graphs. </span><span class="font64" style="font-style:italic;">(a)</span><span class="font64"> The graph using thex operation to compute z = xy. </span><span class="font64" style="font-style:italic;">(b)</span><span class="font64"> The graph for the logistic regression prediction </span><span class="font64" style="font-style:italic;">y = a</span><span class="font64"> (x<sup>T</sup> </span><span class="font65" style="font-style:italic;">w +</span><span class="font64"> 6).&#160;Some of the intermediate expressions do not have names in the algebraic expression&#160;but need names in the graph. We simply name the i-th such variable . </span><span class="font64" style="font-style:italic;">(c)</span><span class="font64"> The&#160;computational graph for the expression </span><span class="font65" style="font-style:italic;">H =</span><span class="font64"> max{0, XW + b}, which computes a design&#160;matrix of rectified linear unit activations H given a design matrix containing a minibatch&#160;of inputs X. </span><span class="font64" style="font-style:italic;">(d)</span><span class="font64"> Examples a-c applied at most one operation to each variable, but it&#160;is possible to apply more than one operation. Here we show a computation graph that&#160;applies more than one operation to the weights w of a linear regression model. The&#160;weights are used to make the both the prediction y and the weight decay penalty A ^ </span><span class="font64" style="font-style:italic;"><sub>i</sub> w</span><span class="font64"><sup>2</sup>.</span></p><h5><a id="bookmark9"></a><span class="font64" style="font-weight:bold;">6.5.2 Chain Rule of Calculus</span></h5>
<p><span class="font64">The chain rule of calculus (not to be confused with the chain rule of probability) is used to compute the derivatives of functions formed by composing other functions&#160;whose derivatives are known. Back-propagation is an algorithm that computes the&#160;chain rule, with a specific order of operations that is highly efficient.</span></p>
<p><span class="font64">Let x be a real number, and let f and g both be functions mapping from a real number to a real number. Suppose that y = g(x) and z = f (g(x)) = f (y). Then&#160;the chain rule states that</span></p><div>
<p><span class="font64">(6.44)</span></p></div>
<p><span class="font64">dz &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">dz dy</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dx &#160;&#160;&#160;dy dx</span></p>
<p><span class="font64">We can generalize this beyond the scalar case. Suppose that </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">G R<sup>m</sup>, </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">G R<sup>n</sup>, g maps from R<sup>m</sup> to R™, and f maps from R<sup>n</sup> to R. If </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">= g (</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) and z = f( </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">), then</span></p><div>
<p><span class="font64">dz</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dx.</span></p></div><div>
<p><span class="font62">. = £</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dz dyj dyj dx i</span></p></div><div>
<p><span class="font64">(6.45)</span></p></div>
<p><span class="font64">In vector notation, this may be equivalently written as</span></p><div>
<p dir="rtl"><span class="font64">־<sup>2</sup>״<sup>v</sup> ®) =*־v</span></p></div><div>
<p><span class="font64">T</span></p></div><div>
<p><span class="font64">(6.46)</span></p></div>
<p><span class="font64">where is the n </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> m Jacobian matrix of g.</span></p>
<p><span class="font64">From this we see that the gradient of a variable x can be obtained by multiplying a Jacobian matrix -gX by a gradient V<sub>y</sub> z. The back-propagation algorithm consists&#160;of performing such a Jacobian-gradient product for each operation in the graph.</span></p>
<p><span class="font64">Usually we do not apply the back-propagation algorithm merely to vectors, but rather to tensors of arbitrary dimensionality. Conceptually, this is exactly the&#160;same as back-propagation with vectors. The only difference is how the numbers&#160;are arranged in a grid to form a tensor. We could imagine flattening each tensor&#160;into a vector before we run back-propagation, computing a vector-valued gradient,&#160;and then reshaping the gradient back into a tensor. In this rearranged view,&#160;back-propagation is still just multiplying Jacobians by gradients.</span></p>
<p><span class="font64">To denote the gradient of a value z with respect to a tensor X, we write Vxz, just as if X were a vector. The indices into X now have multiple coordinates—for&#160;example, a 3-D tensor is indexed by three coordinates. We can abstract this away&#160;by using a single variable i to represent the complete tuple of indices. For all&#160;possible index tuples i, (Vxz)i gives 4jXz. This is exactly the same as how for all</span></p>
<p><span class="font64">possible integer indicesi into a vector, </span><span class="font64" style="font-weight:bold;font-style:italic;">(V<sub>x</sub>z</span><span class="font64">) gives </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">q*. ■</span><span class="font64"> Using this notation, we can write the chain rule as it applies to tensors. If Y = g(X) and z = f (Y), then</span></p><div>
<p><span class="font64" style="font-variant:small-caps;">Vx z = £(Vx </span><span class="font64" style="font-weight:bold;font-style:italic;">Yj</span><span class="font64">)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dz</span></p></div><div>
<p><span class="font64">(6.47)</span></p></div><div>
<p><span class="font64"><sup>j</sup></span></p></div><h5><a id="bookmark10"></a><span class="font64" style="font-weight:bold;">6.5.3 Recursively Applying the Chain Rule to Obtain Backprop</span></h5>
<p><span class="font64">Using the chain rule, it is straightforward to write down an algebraic expression for the gradient of a scalar with respect to any node in the computational graph that&#160;produced that scalar. However, actually evaluating that expression in a computer&#160;introduces some extra considerations.</span></p>
<p><span class="font64">Specifically, many subexpressions may be repeated several times within the overall expression for the gradient. Any procedure that computes the gradient&#160;will need to choose whether to store these subexpressions or to recompute them&#160;several times. An example of how these repeated subexpressions arise is given in&#160;Fig. 6.9. In some cases, computing the same subexpression twice would simply&#160;be wasteful. For complicated graphs, there can be exponentially many of these&#160;wasted computations, making a naive implementation of the chain rule infeasible.&#160;In other cases, computing the same subexpression twice could be a valid way to&#160;reduce memory consumption at the cost of higher runtime.</span></p>
<p><span class="font64">We first begin by a version of the back-propagation algorithm that specifies the actual gradient computation directly (Algorithm 6.2 along with Algorithm 6.1&#160;for the associated forward computation), in the order it will actually be done and&#160;according to the recursive application of chain rule. One could either directly&#160;perform these computations or view the description of the algorithm as a symbolic&#160;specification of the computational graph for computing the back-propagation. However, this formulation does not make explicit the manipulation and the construction&#160;of the symbolic graph that performs the gradient computation. Such a formulation&#160;is presented below in Sec. 6.5.6, with Algorithm 6.5, where we also generalize to&#160;nodes that contain arbitrary tensors.</span></p>
<p><span class="font64">First consider a computational graph describing how to compute a single scalar u<sup>(n)</sup> (say the loss on a training example). This scalar is the quantity whose&#160;gradient we want to obtain, with respect to the </span><span class="font64" style="font-weight:bold;font-style:italic;">n%</span><span class="font64"> input nodes u<sup>(1)</sup> to </span><span class="font64" style="font-weight:bold;font-style:italic;">u</span><span class="font64"><sup>(ni)</sup>. In&#160;other words we wish to compute dii) for all i G {1, 2,..., </span><span class="font64" style="font-weight:bold;font-style:italic;">n</span><span class="font64">}. In the application&#160;of back-propagation to computing gradients for gradient descent over parameters,&#160;u<sup>(n)</sup> will be the cost associated with an example or a minibatch, while u<sup>(1)</sup> to u<sup>(ni)&#160;</sup>correspond to the parameters of the model.</span></p>
<p><span class="font64">We will assume that the nodes of the graph have been ordered in such a way that we can compute their output one after the other, starting at u<sup>(ni+1)</sup> and&#160;going up to u<sup>(n)</sup>. As defined in Algorithm 6.1, each node u<sup>(i)</sup> is associated with an&#160;operation f<sup>(i)</sup> and is computed by evaluating the function</span></p>
<p><span class="font64">u<sup>(i)</sup> = f (A<sup>(i)</sup>) &#160;&#160;&#160;(6.48)</span></p>
<p><span class="font64">where A<sup>(i)</sup> is the set of all nodes that are parents of u<sup>(i)</sup>.</span></p>
<p><span class="font64" style="font-weight:bold;">Algorithm 6.1 </span><span class="font64">A procedure that performs the computations mapping </span><span class="font64" style="font-weight:bold;font-style:italic;">n%</span><span class="font64"> inputs u<sup>(1)</sup> to u<sup>(n</sup>^ to an output u<sup>(n)</sup>. This defines a computational graph where each node&#160;computes numerical value u<sup>(i)</sup> by applying a function f<sup>(i)</sup> to the set of arguments&#160;A<sup>(i)</sup> that comprises the values of previous nodes u<sup>(j)</sup>, j &lt; i, with j G Pa(u<sup>(i)</sup>). The&#160;input to the computational graph is the vector x, and is set into the first n% nodes&#160;u<sup>(1)</sup> to u<sup>(n</sup>). The output of the computational graph is read off the last (output)</span></p>
<p><span class="font64" style="text-decoration:underline;">node u<sup>(n)</sup>.</span><span class="font64">_</span></p>
<p><span class="font64" style="font-weight:bold;">for </span><span class="font64" style="font-weight:bold;font-style:italic;">i =</span><span class="font64"> 1,...,n% </span><span class="font64" style="font-weight:bold;">do </span><span class="font64">u<sup>(i)</sup> ^ x%&#160;</span><span class="font64" style="font-weight:bold;">end for</span></p>
<p><span class="font64" style="font-weight:bold;">for </span><span class="font64">i = n% + 1,..., n </span><span class="font64" style="font-weight:bold;">do </span><span class="font64">A<sup>(i)</sup> ^ {u<sup>(j)</sup> | j G Pa(u<sup>(i)</sup>)}&#160;u<sup>(i)</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">^ f</span><span class="font64"> <sup>(i)</sup>(A<sup>(i)</sup>)&#160;</span><span class="font64" style="font-weight:bold;">end for&#160;return </span><span class="font64">u<sup>(n)</sup></span></p>
<p><span class="font64">That algorithm specifies the forward propagation computation, which we could put in a graph G. In order to perform back-propagation, we can construct a&#160;computational graph that depends on G and adds to it an extra set of nodes. These&#160;form a subgraph </span><span class="font64" style="font-weight:bold;font-style:italic;">B</span><span class="font64"> with one node per node of G. Computation in </span><span class="font64" style="font-weight:bold;font-style:italic;">B</span><span class="font64"> proceeds in&#160;exactly the reverse of the order of computation in G, and each node of B computes&#160;the derivative </span><span class="font64" style="font-weight:bold;font-style:italic;">dujij</span><span class="font64"> associated with the forward graph node u<sup>(i)</sup>. This is done&#160;using the chain rule with respect to scalar output </span><span class="font18">1</span><span class="font64">׳i<sup>n)</sup>:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">du<sup>(n)</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">du<sup>(j</sup></span></p></div><div>
<p><span class="font64">= £</span></p></div><div>
<p><span class="font64">i:jgPa(u<sup>(l</sup>))</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">du</span><span class="font64"><sup>(n)</sup> du<sup>(i) </sup></span><span class="font64" style="font-weight:bold;font-style:italic;">du</span><span class="font64"><sup>(i)</sup> du<sup>(j)</sup></span></p></div><div>
<p><span class="font64">(6.49)</span></p></div>
<p><span class="font64">as specified by Algorithm 6.2. The subgraph B contains exactly one edge for each edge from node u<sup>(j)</sup> to node u<sup>(i)</sup> of G. The edge from u<sup>(j)</sup> to u<sup>(i)</sup> is associated with&#160;the computation of </span><span class="font64" style="font-weight:bold;font-style:italic;">dj</span><span class="font64">. In addition, a dot product is performed for each node,&#160;between the gradient already computed with respect to nodes u<sup>(i)</sup> that are children&#160;of </span><span class="font64" style="font-weight:bold;font-style:italic;">u</span><span class="font64"><sup>(j)</sup> and the vector containing the partial derivatives d<sup>u</sup>(j) for the same children&#160;nodes u<sup>(i)</sup>. To summarize, the amount of computation required for performing&#160;the back-propagation scales linearly with the number of edges in G, where the&#160;computation for each edge corresponds to computing a partial derivative (of one&#160;node with respect to one of its parents) as well as performing one multiplication&#160;and one addition. Below, we generalize this analysis to tensor-valued nodes, which&#160;is just a way to group multiple scalar values in the same node and enable more&#160;efficient implementations.</span></p>
<p><span class="font64" style="font-weight:bold;">Algorithm 6.2 </span><span class="font64">Simplified version of the back-propagation algorithm for computing the derivatives of u<sup>(n)</sup> with respect to the variables in the graph. This example is&#160;intended to further understanding by showing a simplified case where all variables&#160;are scalars, and we wish to compute the derivatives with respect to u<sup>(1)</sup>,..., u<sup>(ni)</sup>.&#160;This simplified version computes the derivatives of all nodes in the graph. The&#160;computational cost of this algorithm is proportional to the number of edges in&#160;the graph, assuming that the partial derivative associated with each edge requires&#160;a constant time. This is of the same order as the number of computations for&#160;the forward propagation. Each dj is a function of the parents u<sup>(j)</sup> of u<sup>(i)</sup>, thus&#160;linking the nodes of the forward graph to those added for the back-propagation&#160;graph.</span></p>
<p><span class="font64">Run forward propagation (Algorithm 6.1 for this example) to obtain the activations of the network</span></p>
<p><span class="font64">Initialize </span><span class="font64" style="font-weight:bold;">grad_table</span><span class="font64">, a data structure that will store the derivatives that have</span></p>
<p><span class="font64">been computed. The entry </span><span class="font64" style="font-weight:bold;">grad</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">table</span><span class="font64">[u<sup>(i)</sup>] will store the computed value of</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">du<sup>(n) </sup></span><span class="font64">du<sup>(i)</sup> .</span></p>
<p><span class="font64" style="font-weight:bold;">grad</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">table</span><span class="font64">[du<sup>(n)</sup> ] ^ 1 </span><span class="font64" style="font-weight:bold;">for </span><span class="font64" style="font-weight:bold;font-style:italic;">j = n — 1</span><span class="font64"> down to 1 </span><span class="font64" style="font-weight:bold;">do</span></p><div>
<p><span class="font64">Ei:jePa(u(0) fuS iuU) <sup>usin</sup>g <sup>stored values:</sup></span></p></div>
<p><span class="font64">The next line computes </span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:line-through;">dUu)</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">3u<sup>(i) </sup>du<sup>(j</sup></span><span class="font64"><sup>)</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;">grad_ table </span><span class="font64">[u<sup>(i)</sup>]</span></p></div><div>
<p><span class="font61" style="font-style:italic;">J </span><span class="font64" style="font-weight:bold;font-style:italic;">i: jePa(u</span><span class="font64"><sup>(i)</sup>)</span></p></div>
<p><span class="font64" style="font-weight:bold;">grad_table</span><span class="font64">[u<sup>(j)</sup> ] ^ E<sub>i</sub> -</span></p>
<p><span class="font64" style="font-weight:bold;">end for return </span><span class="font64">{</span><span class="font64" style="font-weight:bold;">grad_table </span><span class="font64">[u<sup>(i)</sup>] | </span><span class="font64" style="font-weight:bold;font-style:italic;">i = 1,..., n</span><span class="font64"><sub>i</sub>}</span></p>
<p><span class="font64">The back-propagation algorithm is designed to reduce the number of common subexpressions without regard to memory. Specifically, it performs on the order&#160;of one Jacobian product per node in the graph. This can be seen from the fact&#160;in Algorithm 6.2 that backprop visits each edge from node u<sup>(j)</sup> to node u<sup>(i)</sup> of&#160;the graph exactly once in order to obtain the associated partial derivative ^Up־).&#160;Back-propagation thus avoids the exponential explosion in repeated subexpressions.</span></p>
<p><span class="font64">However, other algorithms may be able to avoid more subexpressions by performing simplifications on the computational graph, or may be able to conserve memory by&#160;recomputing rather than storing some subexpressions. We will revisit these ideas&#160;after describing the back-propagation algorithm itself.</span></p><h5><a id="bookmark11"></a><span class="font64" style="font-weight:bold;">6.5.4 &#160;&#160;&#160;Back-Propagation Computation in Fully-Connected MLP</span></h5>
<p><span class="font64">To clarify the above definition of the back-propagation computation, let us consider the specific graph associated with a fully-connected multi-layer MLP.</span></p>
<p><span class="font64">Algorithm 6.3 first shows the forward propagation, which maps parameters to the supervised loss </span><span class="font64" style="font-weight:bold;font-style:italic;">L(y, y</span><span class="font64">) associated with a single (input,target) training example&#160;(x, </span><span class="font64" style="font-weight:bold;font-style:italic;">y),</span><span class="font64"> with </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"> the output of the neural network when </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> is provided in input.</span></p>
<p><span class="font64">Algorithm 6.4 then shows the corresponding computation to be done for applying the back-propagation algorithm to this graph.</span></p>
<p><span class="font64">Algorithm 6.3 and Algorithm 6.4 are demonstrations that are chosen to be simple and straightforward to understand. However, they are specialized to one&#160;specific problem.</span></p>
<p><span class="font64">Modern software implementations are based on the generalized form of back-propagation described in Sec. 6.5.6 below, which can accommodate any computational graph by explicitly manipulating a data structure for representing symbolic computation.</span></p><h5><a id="bookmark12"></a><span class="font64" style="font-weight:bold;">6.5.5 &#160;&#160;&#160;Symbol-to-Symbol Derivatives</span></h5>
<p><span class="font64">Algebraic expressions and computational graphs both operate on </span><span class="font64" style="font-weight:bold;font-style:italic;">symbols,</span><span class="font64"> or variables that do not have specific values. These algebraic and graph-based&#160;representations are called </span><span class="font64" style="font-weight:bold;font-style:italic;">symbolic</span><span class="font64"> representations. When we actually use or&#160;train a neural network, we must assign specific values to these symbols. We&#160;replace a symbolic input to the network x with a specific </span><span class="font64" style="font-weight:bold;font-style:italic;">numeric</span><span class="font64"> value, such as&#160;[1.2,3.765, —1.8]</span><span class="font64" style="font-variant:small-caps;"><sup>t</sup> .</span></p>
<p><span class="font64">Some approaches to back-propagation take a computational graph and a set of numerical values for the inputs to the graph, then return a set of numerical&#160;values describing the gradient at those input values. We call this approach “symbol-to-number” differentiation. This is the approach used by libraries such as Torch&#160;(Collobert </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011b) and Caffe (Jia, 2013).</span></p>
<p><span class="font64">Another approach is to take a computational graph and add additional nodes to the graph that provide a symbolic description of the desired derivatives. This</span></p>
<p><span class="font64" style="font-style:italic;">dz dw</span></p><div><div><img src="main-62.jpg" alt=""/>
<p><span class="font64">(6.50)</span></p>
<p><span class="font64">(6.51)</span></p></div></div>
<p><span class="font64" style="font-style:italic;">dz dy dx dy dx dw</span></p><div>
<p><span class="font64">(6.52)</span></p>
<p><span class="font64">(6.53)</span></p></div>
<p><span class="font64" style="font-style:italic;"><sup>(x)</sup>f</span><span class="font64">(w)</span></p>
<p><span class="font64" style="font-style:italic;"><sup>f</sup></span><span class="font64"><sup> /(f (f (w)))f/(f </sup></span><span class="font64" style="font-style:italic;"><sup>(w))f</sup>'<sup>(w)</sup></span></p>
<p><span class="font64">Eq. 6.52 suggests an implementation in which we compute the value of f (w) only once and store it in the variable x. This is the approach taken by the back-propagation&#160;algorithm. An alternative approach is suggested by Eq. 6.53, where the subexpression&#160;f(w) appears more than once. In the alternative approach, f (w) is recomputed each time&#160;it is needed. When the memory required to store the value of these expressions is low,&#160;the back-propagation approach of Eq. 6.52 is clearly preferable because of its reduced&#160;runtime. However, Eq. 6.53 is also a valid implementation of the chain rule, and is useful&#160;when memory is limited.</span></p>
<p><span class="font64" style="font-weight:bold;">Algorithm 6.3 </span><span class="font64">Forward propagation through a typical deep neural network and the computation of the cost function. The loss L(</span><span class="font64" style="font-weight:bold;">y</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">) depends on the output </span><span class="font64" style="font-weight:bold;">y&#160;</span><span class="font64">and on the target </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">(see Sec. 6.2.1.1 for examples of loss functions). To obtain the&#160;total cost J, the loss may be added to a regularizer 0(9), where </span><span class="font64" style="font-weight:bold;font-style:italic;">9</span><span class="font64"> contains all the&#160;parameters (weights and biases). Algorithm 6.4 shows how to compute gradients&#160;of J with respect to parameters </span><span class="font64" style="font-weight:bold;">W </span><span class="font64">and </span><span class="font64" style="font-weight:bold;font-style:italic;">b</span><span class="font64">. For simplicity, this demonstration uses&#160;only a single input example </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">. Practical applications should use a minibatch. See&#160;Sec. </span><span class="font64" style="text-decoration:underline;">6.5.7</span><span class="font64"> for a more realistic demonstration.</span></p>
<p><span class="font64" style="font-weight:bold;">Require: </span><span class="font64">Network depth, l</span></p>
<p><span class="font64" style="font-weight:bold;">Require: W</span><span class="font64"><sup>(i)</sup>, i £ {1,..., l}, the weight matrices of the model </span><span class="font64" style="font-weight:bold;">Require: b</span><span class="font64">W, i £ {1,..., l}, the bias parameters of the model&#160;</span><span class="font64" style="font-weight:bold;">Require: x</span><span class="font64">, the input to process&#160;</span><span class="font64" style="font-weight:bold;">Require: y</span><span class="font64">, the target output&#160;</span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(0)</sup> = </span><span class="font64" style="font-weight:bold;">x</span></p>
<p><span class="font64" style="font-weight:bold;">for </span><span class="font64" style="font-weight:bold;font-style:italic;">k =</span><span class="font64"> 1,..., l </span><span class="font64" style="font-weight:bold;">do</span></p>
<p><span class="font64" style="font-weight:bold;"><sub>a</sub> </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">b</span><span class="font64"><sup>(k)</sup> + </span><span class="font64" style="font-weight:bold;">W</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(k)</sup> h<sup>(k-1) </sup></span><span class="font64" style="font-weight:bold;">h</span><span class="font64"><sup>(k)</sup> = f (</span><span class="font64" style="font-weight:bold;">a</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(k)</sup>)&#160;</span><span class="font64" style="font-weight:bold;">end for</span></p>
<p><span class="font64" style="font-weight:bold;">y </span><span class="font64">=</span></p>
<p><span class="font64">J = L(</span><span class="font64" style="font-weight:bold;">y</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">) + A </span><span class="font64" style="font-weight:bold;font-style:italic;">0(9</span><span class="font64">)</span></p>
<p><span class="font64">is the approach taken by Theano (Bergstra </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010; Bastien </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012) and TensorFlow (Abadi </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015). An example of how this approach works&#160;is illustrated in Fig. 6.10. The primary advantage of this approach is that&#160;the derivatives are described in the same language as the original expression.&#160;Because the derivatives are just another computational graph, it is possible to run&#160;back-propagation again, differentiating the derivatives in order to obtain higher&#160;derivatives. Computation of higher-order derivatives is described in Sec. 6.5.10.</span></p>
<p><span class="font64">We will use the latter approach and describe the back-propagation algorithm in terms of constructing a computational graph for the derivatives. Any subset of the&#160;graph may then be evaluated using specific numerical values at a later time. This&#160;allows us to avoid specifying exactly when each operation should be computed.&#160;Instead, a generic graph evaluation engine can evaluate every node as soon as its&#160;parents’ values are available.</span></p>
<p><span class="font64">The description of the symbol-to-symbol based approach subsumes the symbol-to-number approach. The symbol-to-number approach can be understood as performing exactly the same computations as are done in the graph built by the&#160;symbol-to-symbol approach. The key difference is that the symbol-to-number</span></p>
<p><span class="font64" style="font-weight:bold;">Algorithm 6.4 </span><span class="font64" style="font-weight:bold;font-style:italic;">Backward</span><span class="font64"> computation for the deep neural network of Algorithm 6.3, which uses in addition to the input x a target y. This computation yields the gradients on the activations a<sup>(k)</sup> for each layer k, starting from the&#160;output layer and going backwards to the first hidden layer. From these gradients,&#160;which can be interpreted as an indication of how each layer’s output should change&#160;to reduce error, one can obtain the gradient on the parameters of each layer. The&#160;gradients on weights and biases can be immediately used as part of a stochastic gradient update (performing the update right after the gradients have been&#160;computed) or used with other gradient-based optimization methods.</span></p><div>
<p><span class="font31" style="font-style:italic;">y</span></p></div><div>
<p><span class="font63" style="font-style:italic;">X</span></p></div><div>
<p><span class="font55" style="font-style:italic;">w</span></p></div><div><div><img src="main-63.jpg" alt=""/>
<p><span class="font64">Figure 6.10: An example of the symbol-to-symbol approach to computing derivatives. In this approach, the back-propagation algorithm does not need to ever access any actual&#160;specific numeric values. Instead, it adds nodes to a computational graph describing how&#160;to compute these derivatives. A generic graph evaluation engine can later compute the&#160;derivatives for any specific numeric values. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> In this example, we begin with a graph&#160;representing z = f(f (f(w))). </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> We run the back-propagation algorithm, instructing&#160;it to construct the graph for the expression corresponding to JW. In this example, we do&#160;not explain how the back-propagation algorithm works. The purpose is only to illustrate&#160;what the desired result is: a computational graph with a symbolic description of the&#160;derivative.</span></p></div></div>
<p><span class="font64">approach does not expose the graph.</span></p>
</body>
</html>