<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 12</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Applications</span></h2>
<p><span class="font64">In this chapter, we describe how to use deep learning to solve applications in computer vision, speech recognition, natural language processing, and other application areas of commercial interest. We begin by discussing the large scale neural network&#160;implementations required for most serious AI applications. Next, we review several&#160;specific application areas that deep learning has been used to solve. While one&#160;goal of deep learning is to design algorithms that are capable of solving a broad&#160;variety of tasks, so far some degree of specialization is needed. For example, vision&#160;tasks require processing a large number of input features (pixels) per example.&#160;Language tasks require modeling a large number of possible values (words in the&#160;vocabulary) per input feature.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">12.1 Large Scale Deep Learning</span></h4>
<p><span class="font64">Deep learning is based on the philosophy of connectionism: while an individual biological neuron or an individual feature in a machine learning model is not&#160;intelligent, a large population of these neurons or features acting together can&#160;exhibit intelligent behavior. It truly is important to emphasize the fact that the&#160;number of neurons must be </span><span class="font64" style="font-weight:bold;font-style:italic;">large.</span><span class="font64"> One of the key factors responsible for the&#160;improvement in neural network’s accuracy and the improvement of the complexity&#160;of tasks they can solve between the 1980s and today is the dramatic increase in&#160;the size of the networks we use. As we saw in Sec. 1.2.3, network sizes have grown&#160;exponentially for the past three decades, yet artificial neural networks are only as&#160;large as the nervous systems of insects.</span></p>
<p><span class="font64">Because the size of neural networks is of paramount importance, deep learning</span></p>
<p><span class="font64">requires high performance hardware and software infrastructure.</span></p><h5><a id="bookmark2"></a><span class="font64" style="font-weight:bold;">12.1.1 &#160;&#160;&#160;Fast CPU Implementations</span></h5>
<p><span class="font64">Traditionally, neural networks were trained using the CPU of a single machine. Today, this approach is generally considered insufficient. We now mostly use GPU&#160;computing or the CPUs of many machines networked together. Before moving to&#160;these expensive setups, researchers worked hard to demonstrate that CPUs could&#160;not manage the high computational workload required by neural networks.</span></p>
<p><span class="font64">A description of how to implement efficient numerical CPU code is beyond the scope of this book, but we emphasize here that careful implementation for&#160;specific CPU families can yield large improvements. For example, in 2011, the best&#160;CPUs available could run neural network workloads faster when using fixed-point&#160;arithmetic rather than floating-point arithmetic. By creating a carefully tuned&#160;fixed-point implementation, Vanhoucke </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011) obtained a 3x speedup over&#160;a strong floating-point system. Each new model of CPU has different performance&#160;characteristics, so sometimes floating-point implementations can be faster too.&#160;The important principle is that careful specialization of numerical computation&#160;routines can yield a large payoff. Other strategies, besides choosing whether to use&#160;fixed or floating point, include optimizing data structures to avoid cache misses&#160;and using vector instructions. Many machine learning researchers neglect these&#160;implementation details, but when the performance of an implementation restricts&#160;the size of the model, the accuracy of the model suffers.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">12.1.2 &#160;&#160;&#160;GPU Implementations</span></h5>
<p><span class="font64">Most modern neural network implementations are based on graphics processing units. Graphics processing units (GPUs) are specialized hardware components&#160;that were originally developed for graphics applications. The consumer market for&#160;video gaming systems spurred development of graphics processing hardware. The&#160;performance characteristics needed for good video gaming systems turn out to be&#160;beneficial for neural networks as well.</span></p>
<p><span class="font64">Video game rendering requires performing many operations in parallel quickly. Models of characters and environments are specified in terms of lists of 3-D&#160;coordinates of vertices. Graphics cards must perform matrix multiplication and&#160;division on many vertices in parallel to convert these 3-D coordinates into 2-D&#160;on-screen coordinates. The graphics card must then perform many computations&#160;at each pixel in parallel to determine the color of each pixel. In both cases, the&#160;computations are fairly simple and do not involve much branching compared to&#160;the computational workload that a CPU usually encounters. For example, each&#160;vertex in the same rigid object will be multiplied by the same matrix; there is no&#160;need to evaluate an if statement per-vertex to determine which matrix to multiply&#160;by. The computations are also entirely independent of each other, and thus may&#160;be parallelized easily. The computations also involve processing massive buffers of&#160;memory, containing bitmaps describing the texture (color pattern) of each object&#160;to be rendered. Together, this results in graphics cards having been designed to&#160;have a high degree of parallelism and high memory bandwidth, at the cost of&#160;having a lower clock speed and less branching capability relative to traditional&#160;CPUs.</span></p>
<p><span class="font64">Neural network algorithms require the same performance characteristics as the real-time graphics algorithms described above. Neural networks usually involve&#160;large and numerous buffers of parameters, activation values, and gradient values,&#160;each of which must be completely updated during every step of training. These&#160;buffers are large enough to fall outside the cache of a traditional desktop computer&#160;so the memory bandwidth of the system often becomes the rate limiting factor.&#160;GPUs offer a compelling advantage over CPUs due to their high memory bandwidth.&#160;Neural network training algorithms typically do not involve much branching or&#160;sophisticated control, so they are appropriate for GPU hardware. Since neural&#160;networks can be divided into multiple individual “neurons” that can be processed&#160;independently from the other neurons in the same layer, neural networks easily&#160;benefit from the parallelism of GPU computing.</span></p>
<p><span class="font64">GPU hardware was originally so specialized that it could only be used for graphics tasks. Over time, GPU hardware became more flexible, allowing custom&#160;subroutines to be used to transform the coordinates of vertices or assign colors&#160;to pixels. In principle, there was no requirement that these pixel values actually&#160;be based on a rendering task. These GPUs could be used for scientific computing&#160;by writing the output of a computation to a buffer of pixel values. Steinkrau&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2005) implemented a two-layer fully connected neural network on a GPU&#160;and reported a 3X speedup over their CPU-based baseline. Shortly thereafter,&#160;Chellapilla </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2006) demonstrated that the same technique could be used to&#160;accelerate supervised convolutional networks.</span></p>
<p><span class="font64">The popularity of graphics cards for neural network training exploded after the advent of </span><span class="font64" style="font-weight:bold;font-style:italic;">general purpose GPUs.</span><span class="font64"> These GP-GPUs could execute arbitrary code,&#160;not just rendering subroutines. NVIDIA’s CUDA programming language provided&#160;a way to write this arbitrary code in a C-like language. With their relatively&#160;convenient programming model, massive parallelism, and high memory bandwidth,</span></p>
<p><span class="font64">GP-GPUs now offer an ideal platform for neural network programming. This platform was rapidly adopted by deep learning researchers soon after it became&#160;available (Raina </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009; Ciresan </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010).</span></p>
<p><span class="font64">Writing efficient code for GP-GPUs remains a difficult task best left to specialists. The techniques required to obtain good performance on GPU are very different from those used on CPU. For example, good CPU-based code is usually&#160;designed to read information from the cache as much as possible. On GPU, most&#160;writable memory locations are not cached, so it can actually be faster to compute&#160;the same value twice, rather than compute it once and read it back from memory.&#160;GPU code is also inherently multi-threaded and the different threads must be&#160;coordinated with each other carefully. For example, memory operations are faster&#160;if they can be </span><span class="font64" style="font-weight:bold;font-style:italic;">coalesced.</span><span class="font64"> Coalesced reads or writes occur when several threads can&#160;each read or write a value that they need simultaneously, as part of a single memory&#160;transaction. Different models of GPUs are able to coalesce different kinds of read&#160;or write patterns. Typically, memory operations are easier to coalesce if among n&#160;threads, thread i accesses byte i + j of memory, and j is a multiple of some power&#160;of 2. The exact specifications differ between models of GPU. Another common&#160;consideration for GPUs is making sure that each thread in a group executes the&#160;same instruction simultaneously. This means that branching can be difficult on&#160;GPU. Threads are divided into small groups called </span><span class="font64" style="font-weight:bold;font-style:italic;">warps.</span><span class="font64"> Each thread in a warp&#160;executes the same instruction during each cycle, so if different threads within the&#160;same warp need to execute different code paths, these different code paths must&#160;be traversed sequentially rather than in parallel.</span></p>
<p><span class="font64">Due to the difficulty of writing high performance GPU code, researchers should structure their workflow to avoid needing to write new GPU code in order to test&#160;new models or algorithms. Typically, one can do this by building a software library&#160;of high performance operations like convolution and matrix multiplication, then&#160;specifying models in terms of calls to this library of operations. For example, the&#160;machine learning library Pylearn2 (Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013c) specifies all of its&#160;machine learning algorithms in terms of calls to Theano (Bergstra </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010;&#160;Bastien </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012) and cuda-convnet (Krizhevsky, 2010), which provide these&#160;high-performance operations. This factored approach can also ease support for&#160;multiple kinds of hardware. For example, the same Theano program can run on&#160;either CPU or GPU, without needing to change any of the calls to Theano itself.&#160;Other libraries like TensorFlow (Abadi </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015) and Torch (Collobert </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font18">2011</span><span class="font64">b) provide similar features.</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">12.1.3 &#160;&#160;&#160;Large Scale Distributed Implementations</span></h5>
<p><span class="font64">In many cases, the computational resources available on a single machine are insufficient. We therefore want to distribute the workload of training and inference&#160;across many machines.</span></p>
<p><span class="font64">Distributing inference is simple, because each input example we want to process can be run by a separate machine. This is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">data parallelism.</span></p>
<p><span class="font64">It is also possible to get </span><span class="font64" style="font-weight:bold;font-style:italic;">model parallelism</span><span class="font64">, where multiple machines work together on a single datapoint, with each machine running a different part of the&#160;model. This is feasible for both inference and training.</span></p>
<p><span class="font64">Data parallelism during training is somewhat harder. We can increase the size of the minibatch used for a single SGD step, but usually we get less than linear&#160;returns in terms of optimization performance. It would be better to allow multiple&#160;machines to compute multiple gradient descent steps in parallel. Unfortunately,&#160;the standard definition of gradient descent is as a completely sequential algorithm:&#160;the gradient at step </span><span class="font64" style="font-weight:bold;font-style:italic;">t</span><span class="font64"> is a function of the parameters produced by step </span><span class="font64" style="font-weight:bold;font-style:italic;">t —</span><span class="font64"> </span><span class="font18">1</span><span class="font64">.</span></p>
<p><span class="font64">This can be solved using </span><span class="font64" style="font-weight:bold;font-style:italic;">a.synchronous stochastic gradient descent</span><span class="font64"> (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2001; Recht </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011). In this approach, several processor cores share&#160;the memory representing the parameters. Each core reads parameters without a&#160;lock, then computes a gradient, then increments the parameters without a lock.&#160;This reduces the average amount of improvement that each gradient descent step&#160;yields, because some of the cores overwrite each other’s progress, but the increased&#160;rate of production of steps causes the learning process to be faster overall. Dean&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (</span><span class="font18">2012</span><span class="font64">) pioneered the multi-machine implementation of this lock-free approach&#160;to gradient descent, where the parameters are managed by a </span><span class="font64" style="font-weight:bold;font-style:italic;">parameter server&#160;</span><span class="font64">rather than stored in shared memory. Distributed asynchronous gradient descent&#160;remains the primary strategy for training large deep networks and is used by&#160;most major deep learning groups in industry (Chilimbi </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014; Wu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2015). Academic deep learning researchers typically cannot afford the same scale&#160;of distributed learning systems but some research has focused on how to build&#160;distributed networks with relatively low-cost hardware available in the university&#160;setting (Coates </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013).</span></p><h5><a id="bookmark5"></a><span class="font64" style="font-weight:bold;">12.1.4 &#160;&#160;&#160;Model Compression</span></h5>
<p><span class="font64">In many commercial applications, it is much more important that the time and memory cost of running inference in a machine learning model be low than that&#160;the time and memory cost of training be low. For applications that do not require&#160;personalization, it is possible to train a model once, then deploy it to be used by&#160;billions of users. In many cases, the end user is more resource-constrained than&#160;the developer. For example, one might train a speech recognition network with a&#160;powerful computer cluster, then deploy it on mobile phones.</span></p>
<p><span class="font64">A key strategy for reducing the cost of inference is </span><span class="font64" style="font-weight:bold;font-style:italic;">model compression</span><span class="font64"> (Bucilua </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2006). The basic idea of model compression is to replace the original,&#160;expensive model with a smaller model that requires less memory and runtime to&#160;store and evaluate.</span></p>
<p><span class="font64">Model compression is applicable when the size of the original model is driven primarily by a need to prevent overfitting. In most cases, the model with the&#160;lowest generalization error is an ensemble of several independently trained models.&#160;Evaluating all </span><span class="font64" style="font-weight:bold;font-style:italic;">n</span><span class="font64"> ensemble members is expensive. Sometimes, even a single model&#160;generalizes better if it is large (for example, if it is regularized with dropout).</span></p>
<p><span class="font64">These large models learn some function f (x), but do so using many more parameters than are necessary for the task. Their size is necessary only due to&#160;the limited number of training examples. As soon as we have fit this function&#160;f (x), we can generate a training set containing infinitely many examples, simply&#160;by applying f to randomly sampled points x. We then train the new, smaller,&#160;model to match f (x) on these points. In order to most efficiently use the capacity&#160;of the new, small model, it is best to sample the new x points from a distribution&#160;resembling the actual test inputs that will be supplied to the model later. This can&#160;be done by corrupting training examples or by drawing points from a generative&#160;model trained on the original training set.</span></p>
<p><span class="font64">Alternatively, one can train the smaller model only on the original training points, but train it to copy other features of the model, such as its posterior&#160;distribution over the incorrect classes (Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014, 2015).</span></p><h5><a id="bookmark6"></a><span class="font64" style="font-weight:bold;">12.1.5 Dynamic Structure</span></h5>
<p><span class="font64">One strategy for accelerating data processing systems in general is to build systems that have </span><span class="font64" style="font-weight:bold;font-style:italic;">dynamic structure</span><span class="font64"> in the graph describing the computation needed to&#160;process an input. Data processing systems can dynamically determine which&#160;subset of many neural networks should be run on a given input. Individual neural&#160;networks can also exhibit dynamic structure internally by determining which subset&#160;of features (hidden units) to compute given information from the input. This&#160;form of dynamic structure inside neural networks is sometimes called </span><span class="font64" style="font-weight:bold;font-style:italic;">conditional&#160;computation</span><span class="font64"> (Bengio, 2013; Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013b). Since many components of the&#160;architecture may be relevant only for a small amount of possible inputs, the system</span></p>
<p><span class="font64">can run faster by computing these features only when they are needed.</span></p>
<p><span class="font64">Dynamic structure of computations is a basic computer science principle applied generally throughout the software engineering discipline. The simplest versions&#160;of dynamic structure applied to neural networks are based on determining which&#160;subset of some group of neural networks (or other machine learning models) should&#160;be applied to a particular input.</span></p>
<p><span class="font64">A venerable strategy for accelerating inference in a classifier is to use a </span><span class="font64" style="font-weight:bold;font-style:italic;">cascade </span><span class="font64">of classifiers. The cascade strategy may be applied when the goal is to detect the&#160;presence of a rare object (or event). To know for sure that the object is present,&#160;we must use a sophisticated classifier with high capacity, that is expensive to run.&#160;However, because the object is rare, we can usually use much less computation&#160;to reject inputs as not containing the object. In these situations, we can train&#160;a sequence of classifiers. The first classifiers in the sequence have low capacity,&#160;and are trained to have high recall. In other words, they are trained to make sure&#160;we do not wrongly reject an input when the object is present. The final classifier&#160;is trained to have high precision. At test time, we run inference by running the&#160;classifiers in a sequence, abandoning any example as soon as any one element in&#160;the cascade rejects it. Overall, this allows us to verify the presence of objects with&#160;high confidence, using a high capacity model, but does not force us to pay the cost&#160;of full inference for every example. There are two different ways that the cascade&#160;can achieve high capacity. One way is to make the later members of the cascade&#160;individually have high capacity. In this case, the system as a whole obviously has&#160;high capacity, because some of its individual members do. It is also possible to&#160;make a cascade in which every individual model has low capacity but the system&#160;as a whole has high capacity due to the combination of many small models. Viola&#160;and Jones (2001) used a cascade of boosted decision trees to implement a fast and&#160;robust face detector suitable for use in handheld digital cameras. Their classifier&#160;localizes a face using essentially a sliding window approach in which many windows&#160;are examined and rejected if they do not contain faces. Another version of cascades&#160;uses the earlier models to implement a sort of hard attention mechanism: the&#160;early members of the cascade localize an object and later members of the cascade&#160;perform further processing given the location of the object. For example, Google&#160;transcribes address numbers from Street View imagery using a two-step cascade&#160;that first locates the address number with one machine learning model and then&#160;transcribes it with another (Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al,</span><span class="font64"> 2014d).</span></p>
<p><span class="font64">Decision trees themselves are an example of dynamic structure, because each node in the tree determines which of its subtrees should be evaluated for each input.&#160;A simple way to accomplish the union of deep learning and dynamic structure&#160;is to train a decision tree in which each node uses a neural network to make the&#160;splitting decision (Guo and Gelfand, 1992), though this has typically not been&#160;done with the primary goal of accelerating inference computations.</span></p>
<p><span class="font64">In the same spirit, one can use a neural network, called the </span><span class="font64" style="font-weight:bold;font-style:italic;">gater</span><span class="font64"> to select which one out of several </span><span class="font64" style="font-weight:bold;font-style:italic;">expert networks</span><span class="font64"> will be used to compute the output, given the&#160;current input. The first version of this idea is called the </span><span class="font64" style="font-weight:bold;font-style:italic;">mixture of experts</span><span class="font64"> (Nowlan,&#160;1990; Jacobs </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1991), in which the gater outputs a set of probabilities or&#160;weights (obtained via a softmax nonlinearity), one per expert, and the final output&#160;is obtained by the weighted combination of the output of the experts. In that&#160;case, the use of the gater does not offer a reduction in computational cost, but if a&#160;single expert is chosen by the gater for each example, we obtain the </span><span class="font64" style="font-weight:bold;font-style:italic;">hard mixture&#160;of experts</span><span class="font64"> (Collobert </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2001, 2002), which can considerably accelerate training&#160;and inference time. This strategy works well when the number of gating decisions is&#160;small because it is not combinatorial. But when we want to select different subsets&#160;of units or parameters, it is not possible to use a “soft switch” because it requires&#160;enumerating (and computing outputs for) all the gater configurations. To deal&#160;with this problem, several approaches have been explored to train combinatorial&#160;gaters. Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013b) experiment with several estimators of the gradient&#160;on the gating probabilities, while Bacon </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) and Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015a) use&#160;reinforcement learning techniques (policy gradient) to learn a form of conditional&#160;dropout on blocks of hidden units and get an actual reduction in computational&#160;cost without impacting negatively on the quality of the approximation.</span></p>
<p><span class="font64">Another kind of dynamic structure is a switch, where a hidden unit can receive input from different units depending on the context. This dynamic routing&#160;approach can be interpreted as an attention mechanism (Olshausen </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1993).&#160;So far, the use of a hard switch has not proven effective on large-scale applications.&#160;Contemporary approaches instead use a weighted average over many possible inputs,&#160;and thus do not achieve all of the possible computational benefits of dynamic&#160;structure. Contemporary attention mechanisms are described in Sec. 12.4.5.1.</span></p>
<p><span class="font64">One major obstacle to using dynamically structured systems is the decreased degree of parallelism that results from the system following different code branches&#160;for different inputs. This means that few operations in the network can be described&#160;as matrix multiplication or batch convolution on a minibatch of examples. We&#160;can write more specialized sub-routines that convolve each example with different&#160;kernels or multiply each row of a design matrix by a different set of columns&#160;of weights. Unfortunately, these more specialized subroutines are difficult to&#160;implement efficiently. CPU implementations will be slow due to the lack of cache&#160;coherence and GPU implementations will be slow due to the lack of coalesced&#160;memory transactions and the need to serialize warps when members of a warp take&#160;different branches. In some cases, these issues can be mitigated by partitioning the&#160;examples into groups that all take the same branch, and processing these groups&#160;of examples simultaneously. This can be an acceptable strategy for minimizing&#160;the time required to process a fixed amount of examples in an offline setting. In&#160;a real-time setting where examples must be processed continuously, partitioning&#160;the workload can result in load-balancing issues. For example, if we assign one&#160;machine to process the first step in a cascade and another machine to process&#160;the last step in a cascade, then the first will tend to be overloaded and the last&#160;will tend to be underloaded. Similar issues arise if each machine is assigned to&#160;implement different nodes of a neural decision tree.</span></p><h5><a id="bookmark7"></a><span class="font64" style="font-weight:bold;">12.1.6 Specialized Hardware Implementations of Deep Networks</span></h5>
<p><span class="font64">Since the early days of neural networks research, hardware designers have worked on specialized hardware implementations that could speed up training and/or&#160;inference of neural network algorithms. See early and more recent reviews of&#160;specialized hardware for deep networks (Lindsey and Lindblad, 1994; Beiu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2003; Misra and Saha, 2010).</span></p>
<p><span class="font64">Different forms of specialized hardware (Graf and Jackel, 1989; Mead and Ismail, 2012; Kim </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009; Pham </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012; Chen </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014a,b) have&#160;been developed over the last decades, either with ASICs (application-specific integrated circuit), either with digital (based on binary representations of numbers),&#160;analog (Graf and Jackel, 1989; Mead and Ismail, 2012) (based on physical implementations of continuous values as voltages or currents) or hybrid implementations&#160;(combining digital and analog components). In recent years more flexible FPGA&#160;(field programmable gated array) implementations (where the particulars of the&#160;circuit can be written on the chip after it has been built) have been developed.</span></p>
<p><span class="font64">Though software implementations on general-purpose processing units (CPUs and GPUs) typically use 32 or 64 bits of precision to represent floating point&#160;numbers, it has long been known that it was possible to use less precision, at&#160;least at inference time (Holt and Baker, 1991; Holi and Hwang, 1993; Presley&#160;and Haggard, 1994; Simard and Graf, 1994; Wawrzynek </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1996; Savich </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2007). This has become a more pressing issue in recent years as deep learning&#160;has gained in popularity in industrial products, and as the great impact of faster&#160;hardware was demonstrated with GPUs. Another factor that motivates current&#160;research on specialized hardware for deep networks is that the rate of progress of&#160;a single CPU or GPU core has slowed down, and most recent improvements in&#160;computing speed have come from parallelization across cores (either in CPUs or</span></p>
<p><span class="font64">GPUs). This is very different from the situation of the 1990s (the previous neural network era) where the hardware implementations of neural networks (which might&#160;take two years from inception to availability of a chip) could not keep up with&#160;the rapid progress and low prices of general-purpose CPUs. Building specialized&#160;hardware is thus a way to push the envelope further, at a time when new hardware&#160;designs are being developed for low-power devices such as phones, aiming for&#160;general-public applications of deep learning (e.g., with speech, computer vision or&#160;natural language).</span></p>
<p><span class="font64">Recent work on low-precision implementations of backprop-based neural nets (Vanhoucke </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011; Courbariaux </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015; Gupta </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015) suggests&#160;that between </span><span class="font18">8</span><span class="font64"> and 16 bits of precision can suffice for using or training deep&#160;neural networks with back-propagation. What is clear is that more precision is&#160;required during training than at inference time, and that some forms of dynamic&#160;fixed point representation of numbers can be used to reduce how many bits are&#160;required per number. Traditional fixed point numbers are restricted to a fixed&#160;range (which corresponds to a given exponent in a floating point representation).&#160;Dynamic fixed point representations share that range among a set of numbers&#160;(such as all the weights in one layer). Using fixed point rather than floating point&#160;representations and using less bits per number reduces the hardware surface area,&#160;power requirements and computing time needed for performing multiplications,&#160;and multiplications are the most demanding of the operations needed to use or&#160;train a modern deep network with backprop.</span></p><h4><a id="bookmark8"></a><span class="font65" style="font-weight:bold;">12.2 Computer Vision</span></h4>
<p><span class="font64">Computer vision has traditionally been one of the most active research areas for deep learning applications, because vision is a task that is effortless for humans&#160;and many animals but challenging for computers (Ballard </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1983). Many of&#160;the most popular standard benchmark tasks for deep learning algorithms are forms&#160;of object recognition or optical character recognition.</span></p>
<p><span class="font64">Computer vision is a very broad field encompassing a wide variety of ways of processing images, and an amazing diversity of applications. Applications of&#160;computer vision range from reproducing human visual abilities, such as recognizing&#160;faces, to creating entirely new categories of visual abilities. As an example of&#160;the latter category, one recent computer vision application is to recognize sound&#160;waves from the vibrations they induce in objects visible in a video (Davis </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span></p>
<p><span class="font64">2014). Most deep learning research on computer vision has not focused on such exotic applications that expand the realm of what is possible with imagery but&#160;rather a small core of AI goals aimed at replicating human abilities. Most deep&#160;learning for computer vision is used for object recognition or detection of some&#160;form, whether this means reporting which object is present in an image, annotating&#160;an image with bounding boxes around each object, transcribing a sequence of&#160;symbols from an image, or labeling each pixel in an image with the identity of the&#160;object it belongs to. Because generative modeling has been a guiding principle&#160;of deep learning research, there is also a large body of work on image synthesis&#160;using deep models. While image synthesis </span><span class="font64" style="font-weight:bold;font-style:italic;">ex nihilo</span><span class="font64"> is usually not considered a&#160;computer vision endeavor, models capable of image synthesis are usually useful for&#160;image restoration, a computer vision task involving repairing defects in images or&#160;removing objects from images.</span></p><h5><a id="bookmark9"></a><span class="font64" style="font-weight:bold;">12.2.1 Preprocessing</span></h5>
<p><span class="font64">Many application areas require sophisticated preprocessing because the original input comes in a form that is difficult for many deep learning architectures to&#160;represent. Computer vision usually requires relatively little of this kind of preprocessing. The images should be standardized so that their pixels all lie in the same,&#160;reasonable range, like [0,1] or [-1, 1]. Mixing images that lie in [0,1] with images&#160;that lie in [0, 255] will usually result in failure. Formatting images to have the same&#160;scale is the only kind of preprocessing that is strictly necessary. Many computer&#160;vision architectures require images of a standard size, so images must be cropped or&#160;scaled to fit that size. However, even this rescaling is not always strictly necessary.&#160;Some convolutional models accept variably-sized inputs and dynamically adjust&#160;the size of their pooling regions to keep the output size constant (Waibel </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">1989). Other convolutional models have variable-sized output that automatically&#160;scales in size with the input, such as models that denoise or label each pixel in an&#160;image (Hadsell </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2007).</span></p>
<p><span class="font64">Dataset augmentation may be seen as a way of preprocessing the training set only. Dataset augmentation is an excellent way to reduce the generalization error&#160;of most computer vision models. A related idea applicable at test time is to show&#160;the model many different versions of the same input (for example, the same image&#160;cropped at slightly different locations) and have the different instantiations of the&#160;model vote to determine the output. This latter idea can be interpreted as an&#160;ensemble approach, and helps to reduce generalization error.</span></p>
<p><span class="font64">Other kinds of preprocessing are applied to both the train and the test set with the goal of putting each example into a more canonical form in order to reduce the&#160;amount of variation that the model needs to account for. Reducing the amount of&#160;variation in the data can both reduce generalization error and reduce the size of&#160;the model needed to fit the training set. Simpler tasks may be solved by smaller&#160;models, and simpler solutions are more likely to generalize well. Preprocessing&#160;of this kind is usually designed to remove some kind of variability in the input&#160;data that is easy for a human designer to describe and that the human designer&#160;is confident has no relevance to the task. When training with large datasets and&#160;large models, this kind of preprocessing is often unnecessary, and it is best to just&#160;let the model learn which kinds of variability it should become invariant to. For&#160;example, the AlexNet system for classifying ImageNet only has one preprocessing&#160;step: subtracting the mean across training examples of each pixel (Krizhevsky&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> </span><span class="font18">2012</span><span class="font64">).</span></p>
<p><span class="font64" style="font-weight:bold;">12.2.1.1 Contrast Normalization</span></p>
<p><span class="font64">One of the most obvious sources of variation that can be safely removed for many tasks is the amount of contrast in the image. Contrast simply refers to the&#160;magnitude of the difference between the bright and the dark pixels in an image.&#160;There are many ways of quantifying the contrast of an image. In the context of&#160;deep learning, contrast usually refers to the standard deviation of the pixels in an&#160;image or region of an image. Suppose we have an image represented by a tensor&#160;X £ R<sup>rxcx</sup></span><span class="font18"><sup>3</sup></span><span class="font64">, with </span><span class="font64" style="font-weight:bold;font-style:italic;">X</span><span class="font64">being the red intensity at row i and column j, Xj</span><span class="font62" style="font-style:italic;">2</span><span class="font64"> giving&#160;the green intensity and Xj</span><span class="font18">3</span><span class="font64"> giving the blue intensity. Then the contrast of the&#160;entire image is given by</span></p><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div>
<p><span class="font63" style="font-style:italic;">r c</span><span class="font64"> 3</span></p></div><div>
<p><span class="font66" style="font-style:italic;"><sub>3rc</sub> 'Xl'Xl</span><span class="font26">5Z(<sup>X</sup></span><span class="font64"><sup>i</sup>’<sup>j</sup>’<sup>fc </sup></span><span class="font26"><sup>X</sup>)</span></p></div><div>
<p><span class="font18">2</span></p></div><div>
<p><span class="font64">(</span><span class="font18">12</span><span class="font64">.</span><span class="font18">1</span><span class="font64">)</span></p></div><div>
<p><span class="font64">i</span><span class="font18">=1</span><span class="font64"> j</span><span class="font18">=1</span><span class="font64"> k= </span><span class="font18">1</span></p>
<p><span class="font64">where X is the mean intensity of the entire image:</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div>
<p><span class="font64">r c 3</span></p></div><div>
<p><span class="font64"><sup>X</sup></span><span class="font18">3</span><span class="font64"> ־Z<sup>Xj</sup>-<sup>k</sup> •</span></p></div><div>
<p><span class="font64">(</span><span class="font18">12</span><span class="font64">.</span><span class="font18">2</span><span class="font64">)</span></p></div><div>
<p><span class="font64">i</span><span class="font18">=1</span><span class="font64">j</span><span class="font18">=1</span><span class="font64"> k</span><span class="font18">=1</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Global contrast normalization</span><span class="font64"> (GCN) aims to prevent images from having varying amounts of contrast by subtracting the mean from each image, then&#160;rescaling it so that the standard deviation across its pixels is equal to some&#160;constant s. This approach is complicated by the fact that no scaling factor can&#160;change the contrast of a zero-contrast image (one whose pixels all have equal&#160;intensity). Images with very low but non-zero contrast often have little information&#160;content. Dividing by the true standard deviation usually accomplishes nothing&#160;more than amplifying sensor noise or compression artifacts in such cases. This</span></p>
<p><span class="font64">motivates introducing a small, positive regularization parameter A to bias the estimate of the standard deviation. Alternately, one can constrain the denominator&#160;to be at least e. Given an input image X, GCN produces an output image X',&#160;defined such that</span></p><div>
<p><span class="font64"><sup>X</sup>i,j,k &#160;&#160;&#160;<sup>S</sup></span></p></div><div>
<p><span class="font64">max e</span></p></div><div>
<p><span class="font63" style="font-style:italic;">{</span><span class="font64" style="font-weight:bold;font-style:italic;">e</span><span class="font63" style="font-style:italic;">^</span><span class="font64" style="font-weight:bold;font-style:italic;">A</span><span class="font64"> + ^ K</span><span class="font18">1</span><span class="font64">־ j E</span><span class="font64" style="font-weight:bold;">L</span><span class="font64" style="font-variant:small-caps;">i </span><span class="font64">(-j - <sup>X</sup> )</span><span class="font18"><sup>2</sup></span><span class="font64"> }</span></p></div><div>
<p><span class="font64">(12.3)</span></p></div>
<p><span class="font64">Datasets consisting of large images cropped to interesting objects are unlikely to contain any images with nearly constant intensity. In these cases, it is safe&#160;to practically ignore the small denominator problem by setting A = 0 and avoid&#160;division by </span><span class="font18">0</span><span class="font64"> in extremely rare cases by setting e to an extremely low value like&#160;10<sup>-8</sup>. This is the approach used by Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013a) on the CIFAR-10&#160;dataset. Small images cropped randomly are more likely to have nearly constant&#160;intensity, making aggressive regularization more useful. Coates </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011) used&#160;e = 0 and A = 10 on small, randomly selected patches drawn from CIFAR-10.</span></p>
<p><span class="font64">The scale parameter s can usually be set to 1, as done by Coates </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011), or chosen to make each individual pixel have standard deviation across examples&#160;close to 1, as done by Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013a).</span></p>
<p><span class="font64">The standard deviation in Eq. 12.3 is just a rescaling of the L</span><span class="font18"><sup>2</sup></span><span class="font64"> norm of the image (assuming the mean of the image has already been removed). It is preferable&#160;to define GCN in terms of standard deviation rather than L</span><span class="font18"><sup>2</sup></span><span class="font64"> norm because the&#160;standard deviation includes division by the number of pixels, so GCN based on&#160;standard deviation allows the same s to be used regardless of image size. However,&#160;the observation that the L</span><span class="font18"><sup>2</sup></span><span class="font64"> norm is proportional to the standard deviation can&#160;help build a useful intuition. One can understand GCN as mapping examples to&#160;a spherical shell. See Fig. 12.1 for an illustration. This can be a useful property&#160;because neural networks are often better at responding to directions in space rather&#160;than exact locations. Responding to multiple distances in the same direction&#160;requires hidden units with collinear weight vectors but different biases. Such&#160;coordination can be difficult for the learning algorithm to discover. Additionally,&#160;many shallow graphical models have problems with representing multiple separated&#160;modes along the same line. GCN avoids these problems by reducing each example&#160;to a direction rather than a direction and a distance.</span></p>
<p><span class="font64">Counterintuitively, there is a preprocessing operation known as </span><span class="font64" style="font-weight:bold;font-style:italic;">sphering</span><span class="font64"> and it is not the same operation as GCN. Sphering does not refer to making the data lie&#160;on a spherical shell, but rather to rescaling the principal components to have equal&#160;variance, so that the multivariate normal distribution used by PCA has spherical&#160;contours. Sphering is more commonly known as </span><span class="font64" style="font-weight:bold;font-style:italic;">whitening.</span></p><div>
<p><span class="font69">1.5</span></p></div><div>
<p><span class="font64">0.0</span></p></div><div>
<p><span class="font69">1.5</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">_L</span></p></div><div>
<p><span class="font64">Raw input</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">_L</span></p></div><div>
<p><span class="font64">GCN, A = 0</span></p></div><div><div>
<p><span class="font64"><sup>,</sup>'ft</span></p>
<p><span class="font64" style="font-weight:bold;">_L</span></p>
<p><span class="font29">-</span><span class="font69">1.5</span><span class="font29"> &#160;&#160;&#160;</span><span class="font69">0.0</span><span class="font29">&#160;&#160;&#160;&#160;</span><span class="font69">1.5</span></p>
<p><span class="font62"><sup>x</sup></span><span class="font69">0</span></p><img src="main-138.jpg" alt=""/>
<p><span class="font64" style="font-weight:bold;">_L</span></p>
<p><span class="font29">-</span><span class="font69">1.5</span><span class="font29"> &#160;&#160;&#160;</span><span class="font69">0.0</span><span class="font29">&#160;&#160;&#160;&#160;</span><span class="font69">1.5</span></p>
<p><span class="font62"><sup>x</sup></span><span class="font69">0</span></p></div></div><div>
<p><span class="font64">GCN, A = 10<sup>-2</sup></span></p>
<p dir="rtl"><span class="font63">־־1-1-1־</span></p></div><div>
<p><span class="font27" style="font-variant:small-caps;">Vl </span><span class="font28" style="font-weight:bold;font-style:italic;">)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">_L</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">_L</span></p></div>
<p><span class="font64">Figure 12.1: GCN maps examples onto a sphere. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> Raw input data may have any norm. </span><span class="font64" style="font-style:italic;">(Center)</span><span class="font64"> GCN with A =0 maps all non-zero examples perfectly onto a sphere.&#160;Here we use s = 1 and e = 10<sup>-8</sup>. Because we use GCN based on normalizing the standard&#160;deviation rather than the L<sup>2</sup> norm, the resulting sphere is not the unit sphere. </span><span class="font64" style="font-style:italic;">(Right)&#160;</span><span class="font64">Regularized GCN, with A &gt; 0, draws examples toward the sphere but does not completely&#160;discard the variation in their norm. We leave s and e the same as before.</span></p>
<p><span class="font64">Global contrast normalization will often fail to highlight image features we would like to stand out, such as edges and corners. If we have a scene with a large&#160;dark area and a large bright area (such as a city square with half the image in&#160;the shadow of a building) then global contrast normalization will ensure there is a&#160;large difference between the brightness of the dark area and the brightness of the&#160;light area. It will not, however, ensure that edges within the dark region stand out.</span></p>
<p><span class="font64">This motivates </span><span class="font64" style="font-weight:bold;font-style:italic;">local contrast normalization.</span><span class="font64"> Local contrast normalization ensures that the contrast is normalized across each small window, rather than over&#160;the image as a whole. See Fig. 12.2 for a comparison of global and local contrast&#160;normalization.</span></p>
<p><span class="font64">Various definitions of local contrast normalization are possible. In all cases, one modifies each pixel by subtracting a mean of nearby pixels and dividing by&#160;a standard deviation of nearby pixels. In some cases, this is literally the mean&#160;and standard deviation of all pixels in a rectangular window centered on the&#160;pixel to be modified (Pinto </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2008). In other cases, this is a weighted mean&#160;and weighted standard deviation using Gaussian weights centered on the pixel to&#160;be modified. In the case of color images, some strategies process different color&#160;channels separately while others combine information from different channels to&#160;normalize each pixel (Sermanet </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012).</span></p><div><div><img src="main-139.jpg" alt=""/>
<p><span class="font64">Input image</span></p></div></div><div><div><img src="main-140.jpg" alt=""/>
<p><span class="font64">GCN</span></p></div></div><div><div><img src="main-141.jpg" alt=""/>
<p><span class="font64">LCN</span></p></div></div>
<p><span class="font64">Figure 12.2: A comparison of global and local contrast normalization. Visually, the effects of global contrast normalization are subtle. It places all images on roughly the same&#160;scale, which reduces the burden on the learning algorithm to handle multiple scales. Local&#160;contrast normalization modifies the image much more, discarding all regions of constant&#160;intensity. This allows the model to focus on just the edges. Regions of fine texture,&#160;such as the houses in the second row, may lose some detail due to the bandwidth of the&#160;normalization kernel being too high.</span></p>
<p><span class="font64">Local contrast normalization can usually be implemented efficiently by using separable convolution (see Sec. 9.8) to compute feature maps of local means and&#160;local standard deviations, then using element-wise subtraction and element-wise&#160;division on different feature maps.</span></p>
<p><span class="font64">Local contrast normalization is a differentiable operation and can also be used as a nonlinearity applied to the hidden layers of a network, as well as a preprocessing&#160;operation applied to the input.</span></p>
<p><span class="font64">As with global contrast normalization, we typically need to regularize local contrast normalization to avoid division by zero. In fact, because local contrast&#160;normalization typically acts on smaller windows, it is even more important to&#160;regularize. Smaller windows are more likely to contain values that are all nearly&#160;the same as each other, and thus more likely to have zero standard deviation.</span></p>
<p><span class="font64" style="font-weight:bold;">12.2.1.2 Dataset Augmentation</span></p>
<p><span class="font64">As described in Sec. 7.4, it is easy to improve the generalization of a classifier by increasing the size of the training set by adding extra copies of the training&#160;examples that have been modified with transformations that do not change the&#160;class. Object recognition is a classification task that is especially amenable to&#160;this form of dataset augmentation because the class is invariant to so many&#160;transformations and the input can be easily transformed with many geometric&#160;operations. As described before, classifiers can benefit from random translations,&#160;rotations, and in some cases, flips of the input to augment the dataset. In specialized&#160;computer vision applications, more advanced transformations are commonly used&#160;for dataset augmentation. These schemes include random perturbation of the&#160;colors in an image (Krizhevsky </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012) and nonlinear geometric distortions of&#160;the input (LeCun </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1998b).</span></p><h4><a id="bookmark10"></a><span class="font65" style="font-weight:bold;">12.3 Speech Recognition</span></h4>
<p><span class="font64">The task of speech recognition is to map an acoustic signal containing a spoken natural language utterance into the corresponding sequence of words intended by&#160;the speaker. Let X = (x<sup>(1)</sup>,&#160;&#160;&#160;&#160;,..., x<sup>(T)</sup>) denote the sequence of acoustic input</span></p>
<p><span class="font64">vectors (traditionally produced by splitting the audio into 20ms frames). Most speech recognition systems preprocess the input using specialized hand-designed&#160;features, but some (Jaitly and Hinton, 2011) deep learning systems learn features&#160;from raw input. Let </span><span class="font64" style="font-weight:bold;font-style:italic;">y = (y</span><span class="font62" style="font-style:italic;">1</span><span class="font64">, y</span><span class="font18">2</span><span class="font64">,..., yN) denote the target output sequence (usually&#160;a sequence of words or characters). The </span><span class="font64" style="font-weight:bold;font-style:italic;">automatic speech recognition</span><span class="font64"> (ASR) task&#160;consists of creating a function fAsr that computes the most probable linguistic&#160;sequence y given the acoustic sequence X:</span></p>
<p><span class="font64">fAsa(X) = argmax</span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">* (y | X = X) &#160;&#160;&#160;(12.4)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">y</span></p>
<p><span class="font64">where P* is the true conditional distribution relating the inputs X to the targets</span></p>
<p><span class="font64">y.</span></p>
<p><span class="font64">Since the 1980s and until about 2009-2012, state-of-the art speech recognition systems primarily combined hidden Markov models (HMMs) and Gaussian mixture&#160;models (GMMs). GMMs modeled the association between acoustic features and&#160;phonemes (Bahl </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1987), while HMMs modeled the sequence of phonemes.&#160;The GMM-HMM model family treats acoustic waveforms as being generated&#160;by the following process: first an HMM generates a sequence of phonemes and&#160;discrete sub-phonemic states (such as the beginning, middle, and end of each&#160;phoneme), then a GMM transforms each discrete symbol into a brief segment of&#160;audio waveform. Although GMM-HMM systems dominated ASR until recently,&#160;speech recognition was actually one of the first areas where neural networks were&#160;applied, and numerous ASR systems from the late 1980s and early 1990s used&#160;neural nets (Bourlard and Wellekens, 1989; Waibel </span><span class="font64" style="font-weight:bold;font-style:italic;">et al</span><span class="font64">1989; Robinson and&#160;Fallside, 1991; Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1991, 1992; Konig </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1996). At the time, the&#160;performance of ASR based on neural nets approximately matched the performance&#160;of GMM-HMM systems. For example, Robinson and Fallside (1991) achieved&#160;26% phoneme error rate on the TIMIT (Garofolo </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1993) corpus (with 39&#160;phonemes to discriminate between), which was better than or comparable to&#160;HMM-based systems. Since then, TIMIT has been a benchmark for phoneme&#160;recognition, playing a role similar to the role MNIST plays for object recognition.&#160;However, because of the complex engineering involved in software systems for&#160;speech recognition and the effort that had been invested in building these systems&#160;on the basis of GMM-HMMs, the industry did not see a compelling argument&#160;for switching to neural networks. As a consequence, until the late 2000s, both&#160;academic and industrial research in using neural nets for speech recognition mostly&#160;focused on using neural nets to learn extra features for GMM-HMM systems.</span></p>
<p><span class="font64">Later, with </span><span class="font64" style="font-weight:bold;">much larger and deeper models </span><span class="font64">and much larger datasets, recognition accuracy was dramatically improved by using neural networks to&#160;replace GMMs for the task of associating acoustic features to phonemes (or subphonemic states). Starting in 2009, speech researchers applied a form of deep&#160;learning based on unsupervised learning to speech recognition. This approach&#160;to deep learning was based on training undirected probabilistic models called&#160;restricted Boltzmann machines (RBMs) to model the input data. RBMs will be&#160;described in Part III. To solve speech recognition tasks, unsupervised pretraining&#160;was used to build deep feedforward networks whose layers were each initialized&#160;by training an RBM. These networks take spectral acoustic representations in&#160;a fixed-size input window (around a center frame) and predict the conditional&#160;probabilities of HMM states for that center frame. Training such deep networks&#160;helped to significantly improve the recognition rate on TIMIT (Mohamed </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2009, 2012a), bringing down the phoneme error rate from about 26% to 20.7%.&#160;See Mohamed </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2012b) for an analysis of reasons for the success of these&#160;models. Extensions to the basic phone recognition pipeline included the addition&#160;of speaker-adaptive features (Mohamed </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011) that further reduced the&#160;error rate. This was quickly followed up by work to expand the architecture from&#160;phoneme recognition (which is what TIMIT is focused on) to large-vocabulary&#160;speech recognition (Dahl </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012), which involves not just recognizing phonemes&#160;but also recognizing sequences of words from a large vocabulary. Deep networks&#160;for speech recognition eventually shifted from being based on pretraining and&#160;Boltzmann machines to being based on techniques such as rectified linear units and&#160;dropout (Zeiler </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013; Dahl </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013). By that time, several of the major&#160;speech groups in industry had started exploring deep learning in collaboration with&#160;academic researchers. Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2012a) describe the breakthroughs achieved&#160;by these collaborators, which are now deployed in products such as mobile phones.</span></p>
<p><span class="font64">Later, as these groups explored larger and larger labeled datasets and incorporated some of the methods for initializing, training, and setting up the architecture of deep nets, they realized that the unsupervised pretraining phase was either&#160;unnecessary or did not bring any significant improvement.</span></p>
<p><span class="font64">These breakthroughs in recognition performance for word error rate in speech recognition were unprecedented (around 30% improvement) and were following a&#160;long period of about ten years during which error rates did not improve much with&#160;the traditional GMM-HMM technology, in spite of the continuously growing size&#160;of training sets (see Fig. 2.4 of Deng and Yu (2014)). This created a rapid shift in&#160;the speech recognition community towards deep learning. In a matter of roughly&#160;two years, most of the industrial products for speech recognition incorporated deep&#160;neural networks and this success spurred a new wave of research into deep learning&#160;algorithms and architectures for ASR, which is still ongoing today.</span></p>
<p><span class="font64">One of these innovations was the use of convolutional networks (Sainath </span><span class="font64" style="font-weight:bold;font-style:italic;">et al., </span><span class="font64">2013) that replicate weights across time and frequency, improving over the earlier&#160;time-delay neural networks that replicated weights only across time. The new&#160;two-dimensional convolutional models regard the input spectrogram not as one&#160;long vector but as an image, with one axis corresponding to time and the other to&#160;frequency of spectral components.</span></p>
<p><span class="font64">Another important push, still ongoing, has been towards end-to-end deep learning speech recognition systems that completely remove the HMM. The first&#160;major breakthrough in this direction came from Graves </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013) who trained a&#160;deep LSTM RNN (see Sec. 10.10), using MAP inference over the frame-to-phoneme&#160;alignment, as in LeCun </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (1998b) and in the CTC framework (Graves </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2006; Graves, 2012). A deep RNN (Graves </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013) has state variables from&#160;several layers at each time step, giving the unfolded graph two kinds of depth:&#160;ordinary depth due to a stack of layers, and depth due to time unfolding. This&#160;work brought the phoneme error rate on TIMIT to a record low of 17.7%. See&#160;Pascanu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014a) and Chung </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) for other variants of deep RNNs,&#160;applied in other settings.</span></p>
<p><span class="font64">Another contemporary step toward end-to-end deep learning ASR is to let the system learn how to “align” the acoustic-level information with the phonetic-level&#160;information (Chorowski </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014; Lu </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015).</span></p><h4><a id="bookmark11"></a><span class="font65" style="font-weight:bold;">12.4 Natural Language Processing</span></h4>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Natural language processing</span><span class="font64"> (NLP) is the use of human languages, such as English or French, by a computer. Computer programs typically read and emit specialized&#160;languages designed to allow efficient and unambiguous parsing by simple programs.&#160;More naturally occurring languages are often ambiguous and defy formal description.&#160;Natural language processing includes applications such as machine translation,&#160;in which the learner must read a sentence in one human language and emit an&#160;equivalent sentence in another human language. Many NLP applications are based&#160;on language models that define a probability distribution over sequences of words,&#160;characters or bytes in a natural language.</span></p>
<p><span class="font64">As with the other applications discussed in this chapter, very generic neural network techniques can be successfully applied to natural language processing.&#160;However, to achieve excellent performance and to scale well to large applications,&#160;some domain-specific strategies become important. To build an efficient model of&#160;natural language, we must usually use techniques that are specialized for processing&#160;sequential data. In many cases, we choose to regard natural language as a sequence&#160;of words, rather than a sequence of individual characters or bytes. Because the total&#160;number of possible words is so large, word-based language models must operate on&#160;an extremely high-dimensional and sparse discrete space. Several strategies have&#160;been developed to make models of such a space efficient, both in a computational&#160;and in a statistical sense.</span></p><h5><a id="bookmark12"></a><span class="font64" style="font-weight:bold;">12.4.1 &#160;&#160;&#160;n-grams</span></h5>
<p><span class="font64">A </span><span class="font64" style="font-weight:bold;font-style:italic;">language model</span><span class="font64"> defines a probability distribution over sequences of tokens in a natural language. Depending on how the model is designed, a token may be&#160;a word, a character, or even a byte. Tokens are always discrete entities. The&#160;earliest successful language models were based on models of fixed-length sequences&#160;of tokens called n-grams. An n-gram is a sequence of </span><span class="font64" style="font-weight:bold;font-style:italic;">n</span><span class="font64"> tokens.</span></p>
<p><span class="font64">Models based on n-grams define the conditional probability of the n-th token given the preceding n — 1 tokens. The model uses products of these conditional&#160;distributions to define the probability distribution over longer sequences:</span></p>
<p><span class="font64">T</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64"> (x!, . . . ,X</span><span class="font61" style="font-variant:small-caps;">t </span><span class="font64">) = P (x! , . . . ,x</span><span class="font18">1</span><span class="font64">-״) J] P (x </span><span class="font64" style="font-weight:bold;font-style:italic;">t</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;font-style:italic;">xt-n+</span><span class="font18">1</span><span class="font64">, • • . ,Xt-</span><span class="font18">1</span><span class="font64">). &#160;&#160;&#160;(12.5)</span></p>
<p><span class="font64">t=n</span></p>
<p><span class="font64">This decomposition is justified by the chain rule of probability. The probability distribution over the initial sequence P (x!,..., x<sub>n-</sub>!) may be modeled by a different&#160;model with a smaller value of n.</span></p>
<p><span class="font64">Training n-gram models is straightforward because the maximum likelihood estimate can be computed simply by counting how many times each possible n&#160;gram occurs in the training set. Models based on n-grams have been the core&#160;building block of statistical language modeling for many decades (Jelinek and&#160;Mercer, 1980; Katz, 1987; Chen and Goodman, 1999).</span></p>
<p><span class="font64">For small values of n, models have particular names: </span><span class="font64" style="font-weight:bold;font-style:italic;">unigram</span><span class="font64"> for n=1, </span><span class="font64" style="font-weight:bold;font-style:italic;">bigram </span><span class="font64">for n=2, and </span><span class="font64" style="font-weight:bold;font-style:italic;">trigram</span><span class="font64"> for n=3. These names derive from the Latin prefixes for the&#160;corresponding numbers and the Greek suffix “-gram” denoting something that is&#160;written.</span></p>
<p><span class="font64">Usually we train both an n-gram model and an n — 1 gram model simultaneously. This makes it easy to compute</span></p><div>
<p><span class="font64">(</span><span class="font18">12</span><span class="font64">.</span><span class="font18">6</span><span class="font64">)</span></p></div><div>
<p><span class="font64">P</span><span class="font64" style="font-weight:bold;font-style:italic;">(xt</span><span class="font64"> I <sup>x</sup>t-n+</span><span class="font18">1</span><span class="font64">, • • • לX) =</span></p></div><div>
<p><span class="font64"><sup>P</sup>n <sup>(x</sup>t-n</span><span class="font18">+1</span><span class="font64"> ל • • • ל <sup>x</sup>t<sup>)</sup></span></p></div><div>
<p><span class="font64"><sup>P</sup>n— </span><span class="font18">1</span><span class="font64"><sup>(x</sup>t-n</span><span class="font18">+1</span><span class="font64"> ל • • • ל </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>x</sup>t-</span><span class="font18">1</span><span class="font64"><sup>)</sup></span></p></div>
<p><span class="font64">simply by looking up two stored probabilities. For this to exactly reproduce inference in P<sub>n</sub>, we must omit the final character from each sequence when we&#160;train P<sub>n-</sub></span><span class="font18">1</span><span class="font64">.</span></p>
<p><span class="font64">As an example, we demonstrate how a trigram model computes the probability of the sentence “THE DOG RAN AWAY.” The first words of the sentence cannot be&#160;handled by the default formula based on conditional probability because there is no&#160;context at the beginning of the sentence. Instead, we must use the marginal probability over words at the start of the sentence. We thus evaluate P</span><span class="font18">3</span><span class="font64"> (THE DOG RAN).&#160;Finally, the last word may be predicted using the typical case, of using the conditional distribution P(AWAY | DOG RAN). Putting this together with Eq. 12.6, we&#160;obtain:</span></p>
<p><span class="font64">P(THE DOG RAN AWAY) = P<sub>3</sub>(THE DOG RAN)P<sub>3</sub>(DOG RAN AWAY)/P<sub>2</sub>(DOG RAN)•</span></p>
<p><span class="font64">(12.7)</span></p>
<p><span class="font64">A fundamental limitation of maximum likelihood for n-gram models is that P<sub>n </sub>as estimated from training set counts is very likely to be zero in many cases, even&#160;though the tuple (x<sub>t-n</sub>+</span><span class="font18">1</span><span class="font64">, • • • ל x<sub>t</sub>) may appear in the test set. This can cause two&#160;different kinds of catastrophic outcomes. When P</span><span class="font18"><sub>n-</sub>1</span><span class="font64"> is zero, the ratio is undefined,&#160;so the model does not even produce a sensible output. When P</span><span class="font18"><sub>n-</sub>1</span><span class="font64"> is non-zero but&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">P<sub>n</sub></span><span class="font64"> is zero, the test log-likelihood is —to. To avoid such catastrophic outcomes,&#160;most n-gram models employ some form of </span><span class="font64" style="font-weight:bold;font-style:italic;">smoothing.</span><span class="font64"> Smoothing techniques shift&#160;probability mass from the observed tuples to unobserved ones that are similar.&#160;See Chen and Goodman (1999) for a review and empirical comparisons. One basic&#160;technique consists of adding non-zero probability mass to all of the possible next&#160;symbol values. This method can be justified as Bayesian inference with a uniform&#160;or Dirichlet prior over the count parameters. Another very popular idea is to form&#160;a mixture model containing higher-order and lower-order n-gram models, with the&#160;higher-order models providing more capacity and the lower-order models being&#160;more likely to avoid counts of zero. </span><span class="font64" style="font-weight:bold;font-style:italic;">Back-off methods</span><span class="font64"> look-up the lower-order&#160;n-grams if the frequency of the context x<sub>t-</sub></span><span class="font18">1</span><span class="font64">,..., </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font62" style="font-style:italic;"><sub>t-n</sub>+1</span><span class="font64"> is too small to use the&#160;higher-order model. More formally, they estimate the distribution over x<sub>t</sub> by using&#160;contexts x<sub>t-n</sub>+k,..., x<sub>t-</sub></span><span class="font18">1</span><span class="font64">, for increasing k, until a sufficiently reliable estimate is&#160;found.</span></p>
<p><span class="font64">Classical n-gram models are particularly vulnerable to the curse of dimensionality. There are |V|<sup>n</sup> possible n-grams and |V| is often very large. Even with a massive training set and modest n, most n-grams will not occur in the training set.&#160;One way to view a classical n-gram model is that it is performing nearest-neighbor&#160;lookup. In other words, it can be viewed as a local non-parametric predictor,&#160;similar to k-nearest neighbors. The statistical problems facing these extremely&#160;local predictors are described in Sec. 5.11.2. The problem for a language model is&#160;even more severe than usual, because any two different words have the same distance from each other in one-hot vector space. It is thus difficult to leverage much&#160;information from any “neighbors”—only training examples that repeat literally the&#160;same context are useful for local generalization. To overcome these problems, a&#160;language model must be able to share knowledge between one word and other&#160;semantically similar words.</span></p>
<p><span class="font64">To improve the statistical efficiency of n-gram models, </span><span class="font64" style="font-weight:bold;font-style:italic;">class-based language models</span><span class="font64"> (Brown </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1992; Ney and Kneser, 1993; Niesler </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1998) introduce&#160;the notion of word categories and then share statistical strength between words that&#160;are in the same category. The idea is to use a clustering algorithm to partition the&#160;set of words into clusters or classes, based on their co-occurrence frequencies with&#160;other words. The model can then use word class IDs rather than individual word&#160;IDs to represent the context on the right side of the conditioning bar. Composite&#160;models combining word-based and class-based models via mixing or back-off are&#160;also possible. Although word classes provide a way to generalize between sequences&#160;in which some word is replaced by another of the same class, much information is&#160;lost in this representation.</span></p><h5><a id="bookmark13"></a><span class="font64" style="font-weight:bold;">12.4.2 Neural Language Models</span></h5>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Neural language models</span><span class="font64"> or NLMs are a class of language model designed to overcome the curse of dimensionality problem for modeling natural language sequences by&#160;using a distributed representation of words (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2001). Unlike class-based n-gram models, neural language models are able to recognize that two words</span></p>
<p><span class="font64">are similar without losing the ability to encode each word as distinct from the other. Neural language models share statistical strength between one word (and&#160;its context) and other similar words and contexts. The distributed representation&#160;the model learns for each word enables this sharing by allowing the model to treat&#160;words that have features in common similarly. For example, if the word dog and&#160;the word cat map to representations that share many attributes, then sentences&#160;that contain the word cat can inform the predictions that will be made by the&#160;model for sentences that contain the word dog, and vice-versa. Because there are&#160;many such attributes, there are many ways in which generalization can happen,&#160;transferring information from each training sentence to an exponentially large&#160;number of semantically related sentences. The curse of dimensionality requires the&#160;model to generalize to a number of sentences that is exponential in the sentence&#160;length. The model counters this curse by relating each training sentence to an&#160;exponential number of similar sentences.</span></p>
<p><span class="font64">We sometimes call these word representations </span><span class="font64" style="font-weight:bold;font-style:italic;">word embeddings.</span><span class="font64"> In this interpretation, we view the raw symbols as points in a space of dimension equal to the vocabulary size. The word representations embed those points in a feature space&#160;of lower dimension. In the original space, every word is represented by a one-hot&#160;vector, so every pair of words is at Euclidean distance 2/ץ from each other. In the&#160;embedding space, words that frequently appear in similar contexts (or any pair&#160;of words sharing some “features” learned by the model) are close to each other.&#160;This often results in words with similar meanings being neighbors. Fig. 12.3 zooms&#160;in on specific areas of a learned word embedding space to show how semantically&#160;similar words map to representations that are close to each other.</span></p>
<p><span class="font64">Neural networks in other domains also define embeddings. For example, a hidden layer of a convolutional network provides an “image embedding.” Usually&#160;NLP practitioners are much more interested in this idea of embeddings because&#160;natural language does not originally lie in a real-valued vector space. The hidden&#160;layer has provided a more qualitatively dramatic change in the way the data is&#160;represented.</span></p>
<p><span class="font64">The basic idea of using distributed representations to improve models for natural language processing is not restricted to neural networks. It may also be&#160;used with graphical models that have distributed representations in the form of&#160;multiple latent variables (Mnih and Hinton, 2007).</span></p><div>
<p><span class="font63">-6 -7&#160;-8&#160;-9&#160;-10&#160;-11&#160;-12&#160;-13&#160;-14</span></p>
<p><span class="font63">-34 &#160;&#160;&#160;-32&#160;&#160;&#160;&#160;-30&#160;&#160;&#160;&#160;-28&#160;&#160;&#160;&#160;-26</span></p></div><div>
<p><span class="font57">ר-1-1-r</span></p>
<p><span class="font63">France China .</span></p></div><div>
<p><span class="font63">Russian</span></p></div><div>
<p><span class="font63">gBgldh</span></p></div><div>
<p><span class="font63">Germany Iraq Ontario</span></p></div><div>
<p><span class="font63">South_</span></p></div><div>
<p><span class="font63">22 21&#160;20&#160;19&#160;18&#160;17</span></p></div><div>
<p><span class="font63">AtEUASEyni &#160;&#160;&#160;Japan</span></p>
<p><span class="font63">European</span></p>
<p><span class="font63">^<sup>rltlsh</sup>North &#160;&#160;&#160;,</span></p>
<p><span class="font63">Cainaiiff 1</span></p>
<table border="1">
<tr><td>
<p><span class="font63">-1-</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">-1-1-1-</span></p>
<p><span class="font63">22000098</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font63">2004</span></p>
<p><span class="font63">2003</span></p></td><td style="vertical-align:middle;">
<p><span class="font63">2006 200i<sup>07</sup>״</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font63">-</span></p></td><td style="vertical-align:middle;">
<p><span class="font63"><sup>2005</sup> 1999°°</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font63">1995</span></p>
<p><span class="font63">_1_</span></p></td><td style="vertical-align:bottom;">
<p><span class="font63"><sup>2002</sup> 1998<sup>996 </sup>_1_1_1_</span></p></td></tr>
</table>
<p><span class="font63">35.0 35.5 36.0 36.5 37.0 37.5 38.0</span></p></div>
<p><span class="font64">Figure 12.3: Two-dimensional visualizations of word embeddings obtained from a neural machine translation model (Bahdanau </span><span class="font64" style="font-style:italic;">et al.,</span><span class="font64"> 2015), zooming in on specific areas where&#160;semantically related words have embedding vectors that are close to each other. Countries&#160;appear on the left and numbers on the right. Keep in mind that these embeddings are 2-D&#160;for the purpose of visualization. In real applications, embeddings typically have higher&#160;dimensionality and can simultaneously capture many kinds of similarity between words.</span></p>
</body>
</html>