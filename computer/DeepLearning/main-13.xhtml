<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h5><a id="bookmark0"></a><span class="font64" style="font-weight:bold;">6.5.6 General Back-Propagation</span></h5>
<p><span class="font64">The back-propagation algorithm is very simple. To compute the gradient of some scalar z with respect to one of its ancestors x in the graph, we begin by observing&#160;that the gradient with respect to z is given by = 1. We can then compute&#160;the gradient with respect to each parent of z in the graph by multiplying the&#160;current gradient by the Jacobian of the operation that produced z. We continue&#160;multiplying by Jacobians traveling backwards through the graph in this way until&#160;we reach x. For any node that may be reached by going backwards from z through&#160;two or more paths, we simply sum the gradients arriving from different paths at&#160;that node.</span></p>
<p><span class="font64">More formally, each node in the graph G corresponds to a variable. To achieve maximum generality, we describe this variable as being a tensor V. Tensor can</span></p>
<p><span class="font64">in general have any number of dimensions, and subsume scalars, vectors, and matrices.</span></p>
<p><span class="font64">We assume that each variable V is associated with the following subroutines:</span></p>
<p><span class="font64">• &#160;&#160;&#160;get_operation(V): This returns the operation that computes V, represented by the edges coming into V in the computational graph. For example,&#160;there may be a Python or C++ class representing the matrix multiplication&#160;operation, and the get_operation function. Suppose we have a variable that&#160;is created by matrix multiplication, C = </span><span class="font64" style="font-weight:bold;font-style:italic;">AB.</span><span class="font64"> Then get_operation(V)&#160;returns a pointer to an instance of the corresponding C+—+ class.</span></p>
<p><span class="font64">• &#160;&#160;&#160;get_consumers(V, G): This returns the list of variables that are children of&#160;V in the computational graph G.</span></p>
<p><span class="font64">• &#160;&#160;&#160;get_inputs(V, G): This returns the list of variables that are parents of V&#160;in the computational graph G.</span></p>
<p><span class="font64">Each operation op is also associated with a bprop operation. This bprop operation can compute a Jacobian-vector product as described by Eq. 6.47. This&#160;is how the back-propagation algorithm is able to achieve great generality. Each&#160;operation is responsible for knowing how to back-propagate through the edges in&#160;the graph that it participates in. For example, we might use a matrix multiplication&#160;operation to create a variable C = AB. Suppose that the gradient of a scalar z with&#160;respect to C is given by G. The matrix multiplication operation is responsible for&#160;defining two back-propagation rules, one for each of its input arguments. If we call&#160;the bprop method to request the gradient with respect to A given that the gradient&#160;on the output is G, then the bprop method of the matrix multiplication operation&#160;must state that the gradient with respect to A is given by GB<sup>T</sup>. Likewise, if we&#160;call the bprop method to request the gradient with respect to B, then the matrix&#160;operation is responsible for implementing the bprop method and specifying that&#160;the desired gradient is given by A<sup>T</sup>G. The back-propagation algorithm itself does&#160;not need to know any differentiation rules. It only needs to call each operation’s&#160;bprop rules with the right arguments. Formally, op.bprop (inputs, X, G) must&#160;return</span></p>
<p><span class="font64" style="font-variant:small-caps;">^(Vxop.f (inputs)) </span><span class="font64" style="font-weight:bold;font-style:italic;">Gi,</span><span class="font64"> &#160;&#160;&#160;(6.54)</span></p>
<p><span class="font64">i</span></p>
<p><span class="font64">which is just an implementation of the chain rule as expressed in Eq. 6.47. Here, inputs is a list of inputs that are supplied to the operation, op.f is the&#160;mathematical function that the operation implements, X is the input whose gradient&#160;we wish to compute, and G is the gradient on the output of the operation.</span></p>
<p><span class="font64">The op. bprop method should always pretend that all of its inputs are distinct from each other, even if they are not. For example, if the mul operator is passed</span></p>
<p><span class="font61" style="font-style:italic;">C\</span></p>
<p><span class="font64">two copies of x to compute x , the op.bprop method should still return </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> as the derivative with respect to both inputs. The back-propagation algorithm will later&#160;add both of these arguments together to obtain 2x, which is the correct total&#160;derivative on x.</span></p>
<p><span class="font64">Software implementations of back-propagation usually provide both the operations and their bprop methods, so that users of deep learning software libraries are able to back-propagate through graphs built using common operations like matrix&#160;multiplication, exponents, logarithms, and so on. Software engineers who build a&#160;new implementation of back-propagation or advanced users who need to add their&#160;own operation to an existing library must usually derive the op. bprop method for&#160;any new operations manually.</span></p>
<p><span class="font64">The back-propagation algorithm is formally described in Algorithm 6.5.</span></p>
<p><span class="font64">Algorithm 6.5 The outermost skeleton of the back-propagation algorithm. This portion does simple setup and cleanup work. Most of the important work happens&#160;in the build_grad subroutine of Algorithm 6.6</span></p>
<p><span class="font64">Require: T, the target set of variables whose gradients must be computed. Require: G, the computational graph&#160;Require: z, the variable to be differentiated</span></p>
<p><span class="font64">Let G</span><span class="font64" style="font-weight:bold;font-style:italic;">'</span><span class="font64"> be G pruned to contain only nodes that are ancestors of z and descendents of nodes in T.</span></p>
<p><span class="font64">Initialize grad_table, a data structure associating tensors to their gradients grad_table[z] ^ 1&#160;for V in T do</span></p>
<p><span class="font64">build_grad(V, G, G</span><span class="font64" style="font-weight:bold;font-style:italic;">'</span><span class="font64">, grad_table) end for</span></p>
<p><span class="font64">Return grad_table restricted to T</span></p>
<p><span class="font64">In Sec. 6.5.2, we motivated back-propagation as a strategy for avoiding computing the same subexpression in the chain rule multiple times. The naive algorithm could have exponential runtime due to these repeated subexpressions. Now that&#160;we have specified the back-propagation algorithm, we can understand its computational cost. If we assume that each operation evaluation has roughly the&#160;same cost, then we may analyze the computational cost in terms of the number&#160;of operations executed. Keep in mind here that we refer to an operation as the&#160;fundamental unit of our computational graph, which might actually consist of very</span></p>
<p><span class="font64" style="font-weight:bold;">Algorithm 6.6 </span><span class="font64">The inner loop subroutine </span><span class="font64" style="font-weight:bold;">build</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">grad(V</span><span class="font64">, G, G, </span><span class="font64" style="font-weight:bold;">grad</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">table) </span><span class="font64">of</span></p>
<p><span class="font64">the back-propagation algorithm, called by the back-propagation algorithm defined</span></p>
<p><span class="font64">in Algorithm 6.5.</span></p>
<p><span class="font64" style="font-weight:bold;">Require: V</span><span class="font64">, the variable whose gradient should be added to G and </span><span class="font64" style="font-weight:bold;">grad_table</span><span class="font64">.</span></p>
<p><span class="font64" style="font-weight:bold;">Require: </span><span class="font64">G, the graph to modify.</span></p>
<p><span class="font64" style="font-weight:bold;">Require: </span><span class="font64">G, the restriction of G to nodes that participate in the gradient.</span></p>
<p><span class="font64" style="font-weight:bold;">Require: grad_table</span><span class="font64">, a data structure mapping nodes to their gradients </span><span class="font64" style="font-weight:bold;">if V </span><span class="font64">is in </span><span class="font64" style="font-weight:bold;">grad_table then&#160;</span><span class="font64">Return </span><span class="font64" style="font-weight:bold;">grad</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">table </span><span class="font64">[</span><span class="font64" style="font-weight:bold;">V</span><span class="font64">]&#160;</span><span class="font64" style="font-weight:bold;">end if&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">i ^</span><span class="font64"> 1</span></p>
<p><span class="font64" style="font-weight:bold;">for C </span><span class="font64">in </span><span class="font64" style="font-weight:bold;">get</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">consumers</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">V</span><span class="font64">, G) </span><span class="font64" style="font-weight:bold;">do op </span><span class="font64">^ </span><span class="font64" style="font-weight:bold;">get</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">operation</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">C</span><span class="font64">)</span></p>
<p><span class="font64" style="font-weight:bold;">D </span><span class="font64">^ </span><span class="font64" style="font-weight:bold;">build</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">grad</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">C</span><span class="font64">, </span><span class="font64" style="font-weight:bold;font-style:italic;">G</span><span class="font64">, </span><span class="font64" style="font-weight:bold;font-style:italic;">G'</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">grad</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">table</span><span class="font64">)</span></p>
<p><span class="font64" style="font-weight:bold;">G<sup>(i)</sup> </span><span class="font64">^ </span><span class="font64" style="font-weight:bold;">op</span><span class="font64">.</span><span class="font64" style="font-weight:bold;">bprop</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">get</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">inputs</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">C</span><span class="font64">, G'), </span><span class="font64" style="font-weight:bold;">V</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">D</span><span class="font64">) i ^ i + 1&#160;</span><span class="font64" style="font-weight:bold;">end for</span></p>
<p><span class="font64" style="font-weight:bold;">G </span><span class="font64">^ E</span><span class="font64" style="font-weight:bold;">־ G<sup>(־)</sup></span></p>
<p><span class="font64" style="font-weight:bold;">grad</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">table</span><span class="font64">[</span><span class="font64" style="font-weight:bold;">V</span><span class="font64">] = </span><span class="font64" style="font-weight:bold;">G</span></p>
<p><span class="font64">Insert </span><span class="font64" style="font-weight:bold;">G </span><span class="font64">and the operations creating it into G Return </span><span class="font64" style="font-weight:bold;">G</span></p>
<p><span class="font64">many arithmetic operations (for example, we might have a graph that treats matrix multiplication as a single operation). Computing a gradient in a graph with n nodes&#160;will never execute more than O(n<sup>2</sup>) operations or store the output of more than&#160;O(n<sup>2</sup>) operations. Here we are counting operations in the computational graph, not&#160;individual operations executed by the underlying hardware, so it is important to&#160;remember that the runtime of each operation may be highly variable. For example,&#160;multiplying two matrices that each contain millions of entries might correspond to&#160;a single operation in the graph. We can see that computing the gradient requires as&#160;most </span><span class="font64" style="font-weight:bold;font-style:italic;">O(n</span><span class="font64"> <sup>2</sup>) operations because the forward propagation stage will at worst execute&#160;all n nodes in the original graph (depending on which values we want to compute,&#160;we may not need to execute the entire graph). The back-propagation algorithm&#160;adds one Jacobian-vector product, which should be expressed with </span><span class="font64" style="font-weight:bold;font-style:italic;">O(</span><span class="font64"> 1) nodes, per&#160;edge in the original graph. Because the computational graph is a directed acyclic&#160;graph it has at most O(n<sup>2</sup>) edges. For the kinds of graphs that are commonly used&#160;in practice, the situation is even better. Most neural network cost functions are&#160;roughly chain-structured, causing back-propagation to have O(n) cost. This is far</span></p>
<p><span class="font64">better than the naive approach, which might need to execute exponentially many nodes. This potentially exponential cost can be seen by expanding and rewriting&#160;the recursive chain rule (Eq. 6.49) non-recursively:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">du</span><span class="font64"><sup>(n)</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Qu^k</span><span class="font64"> <sup>)</sup></span></p></div><div>
<p><span class="font64">path </span><span class="font64" style="font-weight:bold;font-style:italic;">(u<sup>(n1</sup> \u</span><span class="font64"> <sup>(71</sup>2) &#160;&#160;&#160;)),</span></p>
<p><span class="font64">from </span><span class="font64" style="font-weight:bold;font-style:italic;">n</span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;">=j</span><span class="font64"> to n<sub>t</sub>=n</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">k=2</span></p></div><div>
<p><span class="font64">-1)'</span></p></div><div>
<p><span class="font64">(6.55)</span></p></div>
<p><span class="font64">Since the number of paths from node j to node n can grow up to exponentially in the length of these paths, the number of terms in the above sum, which is the number&#160;of such paths, can grow exponentially with the depth of the forward propagation&#160;graph. This large cost would be incurred because the same computation for&#160;</span><span class="font64" style="text-decoration:line-through;">dU(j)</span><span class="font64"> would be redone many times. To avoid such recomputation, we can think&#160;of back-propagation as a table-filling algorithm that takes advantage of storing&#160;intermediate results . Each node in the graph has a corresponding slot in a&#160;table to store the gradient for that node. By filling in these table entries in order,&#160;back-propagation avoids repeating many common subexpressions. This table-filling&#160;strategy is sometimes called </span><span class="font64" style="font-weight:bold;font-style:italic;">dynamic programming.</span></p><h5><a id="bookmark1"></a><span class="font64" style="font-weight:bold;">6.5.7 Example: Back-Propagation for MLP Training</span></h5>
<p><span class="font64">As an example, we walk through the back-propagation algorithm as it is used to train a multilayer perceptron.</span></p>
<p><span class="font64">Here we develop a very simple multilayer perception with a single hidden layer. To train this model, we will use minibatch stochastic gradient descent.&#160;The back-propagation algorithm is used to compute the gradient of the cost on a&#160;single minibatch. Specifically, we use a minibatch of examples from the training&#160;set formatted as a design matrix </span><span class="font64" style="font-weight:bold;">X </span><span class="font64">and a vector of associated class labels </span><span class="font64" style="font-weight:bold;">y</span><span class="font64">.&#160;The network computes a layer of hidden features </span><span class="font64" style="font-weight:bold;font-style:italic;">H =</span><span class="font64"> max{0, </span><span class="font64" style="font-weight:bold;">XW</span><span class="font64"><sup>(1)</sup>}. To&#160;simplify the presentation we do not use biases in this model. We assume that our&#160;graph language includes a </span><span class="font64" style="font-weight:bold;">relu </span><span class="font64">operation that can compute max{</span><span class="font64" style="font-weight:bold;">0</span><span class="font64">, </span><span class="font64" style="font-weight:bold;">Z</span><span class="font64">} elementwise. The predictions of the unnormalized log probabilities over classes are then&#160;given by </span><span class="font64" style="font-weight:bold;">HW</span><span class="font64"><sup>(2)</sup>. We assume that our graph language includes a </span><span class="font64" style="font-weight:bold;">cross_entropy&#160;</span><span class="font64">operation that computes the cross-entropy between the targets </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">and the probability&#160;distribution defined by these unnormalized log probabilities. The resulting crossentropy defines the cost J</span><span class="font64" style="font-variant:small-caps;">mle </span><span class="font64">. Minimizing this cross-entropy performs maximum&#160;likelihood estimation of the classifier. However, to make this example more realistic,</span></p><div><div><img src="main-64.jpg" alt=""/>
<p><span class="font64">Figure 6.11: The computational graph used to compute the cost used to train our example of a single-layer MLP using the cross-entropy loss and weight decay.</span></p></div></div>
<p><span class="font64">we also include a regularization term. The total cost</span></p><div>
<p><span class="font64" style="font-variant:small-caps;"><sup>J</sup> = Jmle + A fe «)</span><span class="font64"><sup>2</sup> + £ (wj)</span></p></div><div>
<p><span class="font64">2</span></p></div><div>
<p><span class="font64">(6.56)</span></p></div>
<p><span class="font64">consists of the cross-entropy and a weight decay term with coefficient A. The computational graph is illustrated in Fig. 6.11.</span></p>
<p><span class="font64">The computational graph for the gradient of this example is large enough that it would be tedious to draw or to read. This demonstrates one of the benefits&#160;of the back-propagation algorithm, which is that it can automatically generate&#160;gradients that would be straightforward but tedious for a software engineer to&#160;derive manually.</span></p>
<p><span class="font64">We can roughly trace out the behavior of the back-propagation algorithm by looking at the forward propagation graph in Fig. 6.11. To train, we wish&#160;to compute both V </span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>W</sub>(</span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;">) J</span><span class="font64"> and&#160;&#160;&#160;&#160;(</span><span class="font18">2</span><span class="font64">)J. There are two different paths leading</span></p>
<p><span class="font64">backward from J to the weights: one through the cross-entropy cost, and one through the weight decay cost. The weight decay cost is relatively simple; it will&#160;always contribute&#160;&#160;&#160;&#160;to the gradient on W<sup>(i)</sup>.</span></p>
<p><span class="font64">The other path through the cross-entropy cost is slightly more complicated. Let G be the gradient on the unnormalized log probabilities U<sup>(2)</sup> provided by&#160;the </span><span class="font64" style="font-weight:bold;">cross_entropy </span><span class="font64">operation. The back-propagation algorithm now needs to&#160;explore two different branches. On the shorter branch, it adds H<sup>T</sup>G to the&#160;gradient on W<sup>(2)</sup>, using the back-propagation rule for the second argument to&#160;the matrix multiplication operation. The other branch corresponds to the longer&#160;chain descending further along the network. First, the back-propagation algorithm&#160;computes </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">Vh J</span><span class="font64"> = GW<sup>(2)T</sup> using the back-propagation rule for the first argument&#160;to the matrix multiplication operation. Next, the </span><span class="font64" style="font-weight:bold;">relu </span><span class="font64">operation uses its back-propagation rule to zero out components of the gradient corresponding to entries&#160;of U<sup>(1)</sup> that were less than 0. Let the result be called </span><span class="font64" style="font-weight:bold;font-style:italic;">G'.</span><span class="font64"> The last step of the&#160;back-propagation algorithm is to use the back-propagation rule for the second&#160;argument of the </span><span class="font64" style="font-weight:bold;">matmul </span><span class="font64">operation to add X<sup>T</sup>G' to the gradient on W<sup>(1)</sup>.</span></p>
<p><span class="font64">After these gradients have been computed, it is the responsibility of the gradient descent algorithm, or another optimization algorithm, to use these gradients to&#160;update the parameters.</span></p>
<p><span class="font64">For the MLP, the computational cost is dominated by the cost of matrix multiplication. During the forward propagation stage, we multiply by each weight&#160;matrix, resulting in O(w) multiply-adds, where </span><span class="font64" style="font-weight:bold;font-style:italic;">w</span><span class="font64"> is the number of weights. During&#160;the backward propagation stage, we multiply by the transpose of each weight&#160;matrix, which has the same computational cost. The main memory cost of the&#160;algorithm is that we need to store the input to the nonlinearity of the hidden layer.&#160;This value is stored from the time it is computed until the backward pass has&#160;returned to the same point. The memory cost is thus </span><span class="font64" style="font-weight:bold;font-style:italic;">O(mn,</span><span class="font64">), where m is the&#160;number of examples in the minibatch and n, is the number of hidden units.</span></p><h5><a id="bookmark2"></a><span class="font64" style="font-weight:bold;">6.5.8 Complications</span></h5>
<p><span class="font64">Our description of the back-propagation algorithm here is simpler than the implementations actually used in practice.</span></p>
<p><span class="font64">As noted above, we have restricted the definition of an operation to be a function that returns a single tensor. Most software implementations need to&#160;support operations that can return more than one tensor. For example, if we wish&#160;to compute both the maximum value in a tensor and the index of that value, it is&#160;best to compute both in a single pass through memory, so it is most efficient to&#160;implement this procedure as a single operation with two outputs.</span></p>
<p><span class="font64">We have not described how to control the memory consumption of back-propagation. Back-propagation often involves summation of many tensors together.</span></p>
<p><span class="font64">In the naive approach, each of these tensors would be computed separately, then all of them would be added in a second step. The naive approach has an overly&#160;high memory bottleneck that can be avoided by maintaining a single buffer and&#160;adding each value to that buffer as it is computed.</span></p>
<p><span class="font64">Real-world implementations of back-propagation also need to handle various data types, such as 32-bit floating point, 64-bit floating point, and integer values.&#160;The policy for handling each of these types takes special care to design.</span></p>
<p><span class="font64">Some operations have undefined gradients, and it is important to track these cases and determine whether the gradient requested by the user is undefined.</span></p>
<p><span class="font64">Various other technicalities make real-world differentiation more complicated. These technicalities are not insurmountable, and this chapter has described the key&#160;intellectual tools needed to compute derivatives, but it is important to be aware&#160;that many more subtleties exist.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">6.5.9 Differentiation outside the Deep Learning Community</span></h5>
<p><span class="font64">The deep learning community has been somewhat isolated from the broader computer science community and has largely developed its own cultural attitudes&#160;concerning how to perform differentiation. More generally, the field of </span><span class="font64" style="font-weight:bold;font-style:italic;">automatic&#160;differentiation</span><span class="font64"> is concerned with how to compute derivatives algorithmically. The&#160;back-propagation algorithm described here is only one approach to automatic&#160;differentiation. It is a special case of a broader class of techniques called </span><span class="font64" style="font-weight:bold;font-style:italic;">reverse&#160;mode accumulation.</span><span class="font64"> Other approaches evaluate the subexpressions of the chain rule&#160;in different orders. In general, determining the order of evaluation that results in&#160;the lowest computational cost is a difficult problem. Finding the optimal sequence&#160;of operations to compute the gradient is NP-complete (Naumann, 2008), in the&#160;sense that it may require simplifying algebraic expressions into their least expensive&#160;form.</span></p>
<p><span class="font64">For example, suppose we have variables pi ,p</span><span class="font18">2</span><span class="font64">,... , p<sub>n</sub> representing probabilities and variables </span><span class="font64" style="font-weight:bold;font-style:italic;">zi</span><span class="font64">, </span><span class="font64" style="font-weight:bold;font-style:italic;">z</span><span class="font62" style="font-style:italic;">2</span><span class="font64"> ,..., zn representing unnormalized log probabilities. Suppose&#160;we define</span></p>
<p><span class="font64">_ </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>ex</sup>P<sup>(z</sup>i<sup>)</sup></span></p>
<p><span class="font64">* &#160;&#160;&#160;i exp(zi)<sup>,&#160;&#160;&#160;&#160;(</sup>' <sup>י</sup></span></p>
<p><span class="font64">where we build the softmax function out of exponentiation, summation and division operations, and construct a cross-entropy loss J = — ^</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>i</sub>pi</span><span class="font64"> log </span><span class="font64" style="font-weight:bold;font-style:italic;">q<sub>i</sub>.</span><span class="font64"> A human&#160;mathematician can observe that the derivative of J with respect to z <sub>i</sub> takes a very&#160;simple form: </span><span class="font64" style="font-weight:bold;font-style:italic;">q<sub>i</sub> — p<sub>i</sub>.</span><span class="font64"> The back-propagation algorithm is not capable of simplifying&#160;the gradient this way, and will instead explicitly propagate gradients through all of&#160;the logarithm and exponentiation operations in the original graph. Some software&#160;libraries such as Theano (Bergstra </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010; Bastien </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012) are able to&#160;perform some kinds of algebraic substitution to improve over the graph proposed&#160;by the pure back-propagation algorithm.</span></p>
<p><span class="font64">When the forward graph G has a single output node and each partial derivative dU(j) can be computed with a constant amount of computation, back-propagation&#160;guarantees that the number of computations for the gradient computation is of&#160;the same order as the number of computations for the forward computation: this&#160;can be seen in Algorithm 6.2 because each local partial derivative </span><span class="font64" style="font-weight:bold;font-style:italic;">dujj)</span><span class="font64"> needs&#160;to be computed only once along with an associated multiplication and addition&#160;for the recursive chain-rule formulation (Eq. 6.49). The overall computation is&#160;therefore O(# edges). However, it can potentially be reduced by simplifying the&#160;computational graph constructed by back-propagation, and this is an NP-complete&#160;task. Implementations such as Theano and TensorFlow use heuristics based on&#160;matching known simplification patterns in order to iteratively attempt to simplify&#160;the graph. We defined back-propagation only for the computation of a gradient of a&#160;scalar output but back-propagation can be extended to compute a Jacobian (either&#160;of k different scalar nodes in the graph, or of a tensor-valued node containing </span><span class="font64" style="font-weight:bold;font-style:italic;">k&#160;</span><span class="font64">values). A naive implementation may then need k times more computation: for&#160;each scalar internal node in the original forward graph, the naive implementation&#160;computes k gradients instead of a single gradient. When the number of outputs&#160;of the graph is larger than the number of inputs, it is sometimes preferable to&#160;use another form of automatic differentiation called </span><span class="font64" style="font-weight:bold;font-style:italic;">forward mode accumulation.&#160;</span><span class="font64">Forward mode computation has been proposed for obtaining real-time computation&#160;of gradients in recurrent networks, for example (Williams and Zipser, 1989). This&#160;also avoids the need to store the values and gradients for the whole graph, trading&#160;off computational efficiency for memory. The relationship between forward mode&#160;and backward mode is analogous to the relationship between left-multiplying versus&#160;right-multiplying a sequence of matrices, such as</span></p><div>
<p><span class="font64">(6.58)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">ABCD</span><span class="font64">,</span></p>
<p><span class="font64">where the matrices can be thought of as Jacobian matrices. For example, if D is a column vector while A has many rows, this corresponds to a graph with a&#160;single output and many inputs, and starting the multiplications from the end&#160;and going backwards only requires matrix-vector products. This corresponds to&#160;the backward mode. Instead, starting to multiply from the left would involve a&#160;series of matrix-matrix products, which makes the whole computation much more&#160;expensive. However, if A has fewer rows than D has columns, it is cheaper to run&#160;the multiplications left-to-right, corresponding to the forward mode.</span></p>
<p><span class="font64">In many communities outside of machine learning, it is more common to implement differentiation software that acts directly on traditional programming&#160;language code, such as Python or C code, and automatically generates programs&#160;that different functions written in these languages. In the deep learning community,&#160;computational graphs are usually represented by explicit data structures created by&#160;specialized libraries. The specialized approach has the drawback of requiring the&#160;library developer to define the </span><span class="font64" style="font-weight:bold;">bprop </span><span class="font64">methods for every operation and limiting the&#160;user of the library to only those operations that have been defined. However, the&#160;specialized approach also has the benefit of allowing customized back-propagation&#160;rules to be developed for each operation, allowing the developer to improve speed&#160;or stability in non-obvious ways that an automatic procedure would presumably&#160;be unable to replicate.</span></p>
<p><span class="font64">Back-propagation is therefore not the only way or the optimal way of computing the gradient, but it is a very practical method that continues to serve the deep&#160;learning community very well. In the future, differentiation technology for deep&#160;networks may improve as deep learning practitioners become more aware of advances&#160;in the broader field of automatic differentiation.</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">6.5.10 Higher-Order Derivatives</span></h5>
<p><span class="font64">Some software frameworks support the use of higher-order derivatives. Among the deep learning software frameworks, this includes at least Theano and TensorFlow.&#160;These libraries use the same kind of data structure to describe the expressions for&#160;derivatives as they use to describe the original function being differentiated. This&#160;means that the symbolic differentiation machinery can be applied to derivatives.</span></p>
<p><span class="font64">In the context of deep learning, it is rare to compute a single second derivative of a scalar function. Instead, we are usually interested in properties of the Hessian&#160;matrix. If we have a function f : R<sup>n</sup> ^ R, then the Hessian matrix is of size n x n.&#160;In typical deep learning applications, n will be the number of parameters in the&#160;model, which could easily number in the billions. The entire Hessian matrix is&#160;thus infeasible to even represent.</span></p>
<p><span class="font64">Instead of explicitly computing the Hessian, the typical deep learning approach is to use </span><span class="font64" style="font-weight:bold;font-style:italic;">Krylov methods.</span><span class="font64"> Krylov methods are a set of iterative techniques for&#160;performing various operations like approximately inverting a matrix or finding&#160;approximations to its eigenvectors or eigenvalues, without using any operation&#160;other than matrix-vector products.</span></p>
<p><span class="font64">In order to use Krylov methods on the Hessian, we only need to be able to compute the product between the Hessian matrix </span><span class="font64" style="font-weight:bold;">H </span><span class="font64">and an arbitrary vector </span><span class="font64" style="font-weight:bold;">v</span><span class="font64">. A</span></p>
<p><span class="font64">straightforward technique (Christianson, 1992) for doing so is to compute</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Hv = V</span><span class="font64"> x</span></p></div><div>
<p><span class="font64"><sup>(V</sup>*f (x))<sup>T</sup> v</span></p></div><div>
<p><span class="font64">(6.59)</span></p></div>
<p><span class="font64">Both of the gradient computations in this expression may be computed automatically by the appropriate software library. Note that the outer gradient expression takes the gradient of a function of the inner gradient expression.</span></p>
<p><span class="font64">If v is itself a vector produced by a computational graph, it is important to specify that the automatic differentiation software should not differentiate through&#160;the graph that produced v.</span></p>
<p><span class="font64">While computing the Hessian is usually not advisable, it is possible to do with Hessian vector products. One simply computes He<sup>(i)</sup> for all </span><span class="font64" style="font-weight:bold;font-style:italic;">i =</span><span class="font64"> 1,..., n, where</span></p><div>
<p><span class="font64">e<sup>(i)</sup> is the one-hot vector with e<sup>(i)</sup> = 1 and all other entries equal to 0.</span></p></div><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">6.6 Historical Notes</span></h4>
<p><span class="font64">Feedforward networks can be seen as efficient nonlinear function approximators based on using gradient descent to minimize the error in a function approximation.&#160;From this point of view, the modern feedforward network is the culmination of&#160;centuries of progress on the general function approximation task.</span></p>
<p><span class="font64">The chain rule that underlies the back-propagation algorithm was invented in the 17th century (Leibniz, 1676; L’Hopital, 1696). Calculus and algebra have&#160;long been used to solve optimization problems in closed form, but gradient descent&#160;was not introduced as a technique for iteratively approximating the solution to&#160;optimization problems until the 19th century (Cauchy, 1847).</span></p>
<p><span class="font64">Beginning in the 1940s, these function approximation techniques were used to motivate machine learning models such as the perceptron. However, the earliest&#160;models were based on linear models. Critics including Marvin Minsky pointed&#160;out several of the flaws of the linear model family, such as it inability to learn the&#160;XOR function, which led to a backlash against the entire neural network approach.</span></p>
<p><span class="font64">Learning nonlinear functions required the development of a multilayer per-ceptron and a means of computing the gradient through such a model. Efficient applications of the chain rule based on dynamic programming began to appear in the&#160;1960s and 1970s, mostly for control applications (Kelley, 1960; Bryson and Denham,&#160;1961; Dreyfus, 1962; Bryson and Ho, 1969; Dreyfus, 1973) but also for sensitivity&#160;analysis (Linnainmaa, 1976). Werbos (1981) proposed applying these techniques&#160;to training artificial neural networks. The idea was finally developed in practice&#160;after being independently rediscovered in different ways (LeCun, 1985; Parker,&#160;1985; Rumelhart </span><span class="font64" style="font-weight:bold;font-style:italic;">et al</span><span class="font64">1986a). The book </span><span class="font64" style="font-weight:bold;font-style:italic;">Parallel Distributed Processing</span><span class="font64"> presented&#160;the results of some of the first successful experiments with back-propagation in a&#160;chapter (Rumelhart </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1986b) that contributed greatly to the popularization&#160;of back-propagation and initiated a very active period of research in multi-layer&#160;neural networks. However, the ideas put forward by the authors of that book&#160;and in particular by Rumelhart and Hinton go much beyond back-propagation.&#160;They include crucial ideas about the possible computational implementation of&#160;several central aspects of cognition and learning, which came under the name of&#160;“connectionism” because of the importance given the connections between neurons&#160;as the locus of learning and memory. In particular, these ideas include the notion&#160;of distributed representation (Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1986).</span></p>
<p><span class="font64">Following the success of back-propagation, neural network research gained popularity and reached a peak in the early 1990s. Afterwards, other machine learning techniques became more popular until the modern deep learning renaissance that&#160;began in 2006.</span></p>
<p><span class="font64">The core ideas behind modern feedforward networks have not changed substantially since the 1980s. The same back-propagation algorithm and the same approaches to gradient descent are still in use. Most of the improvement in neural&#160;network performance from 1986 to 2015 can be attributed to two factors. First,&#160;larger datasets have reduced the degree to which statistical generalization is a&#160;challenge for neural networks. Second, neural networks have become much larger,&#160;due to more powerful computers, and better software infrastructure. However, a&#160;small number of algorithmic changes have improved the performance of neural&#160;networks noticeably.</span></p>
<p><span class="font64">One of these algorithmic changes was the replacement of mean squared error with the cross-entropy family of loss functions. Mean squared error was popular in&#160;the 1980s and 1990s, but was gradually replaced by cross-entropy losses and the&#160;principle of maximum likelihood as ideas spread between the statistics community&#160;and the machine learning community. The use of cross-entropy losses greatly&#160;improved the performance of models with sigmoid and softmax outputs, which&#160;had previously suffered from saturation and slow learning when using the mean&#160;squared error loss.</span></p>
<p><span class="font64">The other major algorithmic change that has greatly improved the performance of feedforward networks was the replacement of sigmoid hidden units with piecewise&#160;linear hidden units, such as rectified linear units. Rectification using the max{0, z}&#160;function was introduced in early neural network models and dates back at least&#160;as far as the Cognitron and Neocognitron (Fukushima, 1975, 1980). These early&#160;models did not use rectified linear units, but instead applied rectification to&#160;nonlinear functions. Despite the early popularity of rectification, rectification was&#160;largely replaced by sigmoids in the 1980s, perhaps because sigmoids perform better&#160;when neural networks are very small. As of the early 2000s, rectified linear units&#160;were avoided due to a somewhat superstitious belief that activation functions with&#160;non-differentiable points must be avoided. This began to change in about 2009.&#160;Jarrett </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2009) observed that “using a rectifying nonlinearity is the single most&#160;important factor in improving the performance of a recognition system” among&#160;several different factors of neural network architecture design.</span></p>
<p><span class="font64">For small datasets, Jarrett </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2009) observed that using rectifying nonlinearities is even more important than learning the weights of the hidden layers. Random weights are sufficient to propagate useful information through a rectified&#160;linear network, allowing the classifier layer at the top to learn how to map different&#160;feature vectors to class identities.</span></p>
<p><span class="font64">When more data is available, learning begins to extract enough useful knowledge to exceed the performance of randomly chosen parameters. Glorot </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011a)&#160;showed that learning is far easier in deep rectified linear networks than in deep&#160;networks that have curvature or two-sided saturation in their activation functions.</span></p>
<p><span class="font64">Rectified linear units are also of historical interest because they show that neuroscience has continued to have an influence on the development of deep&#160;learning algorithms. Glorot </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2011a) motivate rectified linear units from&#160;biological considerations. The half-rectifying nonlinearity was intended to capture&#160;these properties of biological neurons: 1) For some inputs, biological neurons are&#160;completely inactive. 2) For some inputs, a biological neuron’s output is proportional&#160;to its input. 3) Most of the time, biological neurons operate in the regime where&#160;they are inactive (i.e., they should have </span><span class="font64" style="font-weight:bold;font-style:italic;">sparse activations).</span></p>
<p><span class="font64">When the modern resurgence of deep learning began in 2006, feedforward networks continued to have a bad reputation. From about 2006-2012, it was widely&#160;believed that feedforward networks would not perform well unless they were assisted&#160;by other models, such as probabilistic models. Today, it is now known that with the&#160;right resources and engineering practices, feedforward networks perform very well.&#160;Today, gradient-based learning in feedforward networks is used as a tool to develop&#160;probabilistic models, such as the variational autoencoder and generative adversarial&#160;networks, described in Chapter 20. Rather than being viewed as an unreliable&#160;technology that must be supported by other techniques, gradient-based learning in&#160;feedforward networks has been viewed since 2012 as a powerful technology that&#160;may be applied to many other machine learning tasks. In 2006, the community&#160;used unsupervised learning to support supervised learning, and now, ironically, it&#160;is more common to use supervised learning to support unsupervised learning.</span></p>
<p><span class="font64">Feedforward networks continue to have unfulfilled potential. In the future, we expect they will be applied to many more tasks, and that advances in optimization&#160;algorithms and model design will improve their performance even further. This&#160;chapter has primarily described the neural network family of models. In the&#160;subsequent chapters, we turn to how to use these models—how to regularize and&#160;train them.</span></p>
</body>
</html>