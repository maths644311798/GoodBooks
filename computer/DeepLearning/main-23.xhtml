<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 11</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Practical Methodology</span></h2>
<p><span class="font64">Successfully applying deep learning techniques requires more than just a good knowledge of what algorithms exist and the principles that explain how they&#160;work. A good machine learning practitioner also needs to know how to choose an&#160;algorithm for a particular application and how to monitor and respond to feedback&#160;obtained from experiments in order to improve a machine learning system. During&#160;day to day development of machine learning systems, practitioners need to decide&#160;whether to gather more data, increase or decrease model capacity, add or remove&#160;regularizing features, improve the optimization of a model, improve approximate&#160;inference in a model, or debug the software implementation of the model. All of&#160;these operations are at the very least time-consuming to try out, so it is important&#160;to be able to determine the right course of action rather than blindly guessing.</span></p>
<p><span class="font64">Most of this book is about different machine learning models, training algorithms, and objective functions. This may give the impression that the most important ingredient to being a machine learning expert is knowing a wide variety&#160;of machine learning techniques and being good at different kinds of math. In practice, one can usually do much better with a correct application of a commonplace&#160;algorithm than by sloppily applying an obscure algorithm. Correct application of&#160;an algorithm depends on mastering some fairly simple methodology. Many of the&#160;recommendations in this chapter are adapted from Ng (2015).</span></p>
<p><span class="font64">We recommend the following practical design process:</span></p>
<p><span class="font64">• &#160;&#160;&#160;Determine your goals—what error metric to use, and your target value for&#160;this error metric. These goals and error metrics should be driven by the&#160;problem that the application is intended to solve.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Establish a working end-to-end pipeline as soon as possible, including the</span></p>
<p><span class="font64">estimation of the appropriate performance metrics.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Instrument the system well to determine bottlenecks in performance. Diagnose which components are performing worse than expected and whether it&#160;is due to overfitting, underfitting, or a defect in the data or software.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Repeatedly make incremental changes such as gathering new data, adjusting&#160;hyperparameters, or changing algorithms, based on specific findings from&#160;your instrumentation.</span></p>
<p><span class="font64">As a running example, we will use Street View address number transcription system (Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014d). The purpose of this application is to add&#160;buildings to Google Maps. Street View cars photograph the buildings and record&#160;the GPS coordinates associated with each photograph. A convolutional network&#160;recognizes the address number in each photograph, allowing the Google Maps&#160;database to add that address in the correct location. The story of how this&#160;commercial application was developed gives an example of how to follow the design&#160;methodology we advocate.</span></p>
<p><span class="font64">We now describe each of the steps in this process.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">11.1 Performance Metrics</span></h4>
<p><span class="font64">Determining your goals, in terms of which error metric to use, is a necessary first step because your error metric will guide all of your future actions. You should&#160;also have an idea of what level of performance you desire.</span></p>
<p><span class="font64">Keep in mind that for most applications, it is impossible to achieve absolute zero error. The Bayes error defines the minimum error rate that you can hope to&#160;achieve, even if you have infinite training data and can recover the true probability&#160;distribution. This is because your input features may not contain complete&#160;information about the output variable, or because the system might be intrinsically&#160;stochastic. You will also be limited by having a finite amount of training data.</span></p>
<p><span class="font64">The amount of training data can be limited for a variety of reasons. When your goal is to build the best possible real-world product or service, you can typically&#160;collect more data but must determine the value of reducing error further and weigh&#160;this against the cost of collecting more data. Data collection can require time,&#160;money, or human suffering (for example, if your data collection process involves&#160;performing invasive medical tests). When your goal is to answer a scientific question&#160;about which algorithm performs better on a fixed benchmark, the benchmark&#160;specification usually determines the training set and you are not allowed to collect&#160;more data.</span></p>
<p><span class="font64">How can one determine a reasonable level of performance to expect? Typically, in the academic setting, we have some estimate of the error rate that is attainable&#160;based on previously published benchmark results. In the real-word setting, we&#160;have some idea of the error rate that is necessary for an application to be safe,&#160;cost-effective, or appealing to consumers. Once you have determined your realistic&#160;desired error rate, your design decisions will be guided by reaching this error rate.</span></p>
<p><span class="font64">Another important consideration besides the target value of the performance metric is the choice of which metric to use. Several different performance metrics&#160;may be used to measure the effectiveness of a complete application that includes&#160;machine learning components. These performance metrics are usually different&#160;from the cost function used to train the model. As described in Sec. 5.1.2, it is&#160;common to measure the accuracy, or equivalently, the error rate, of a system.</span></p>
<p><span class="font64">However, many applications require more advanced metrics.</span></p>
<p><span class="font64">Sometimes it is much more costly to make one kind of a mistake than another. For example, an e-mail spam detection system can make two kinds of mistakes:&#160;incorrectly classifying a legitimate message as spam, and incorrectly allowing a&#160;spam message to appear in the inbox. It is much worse to block a legitimate&#160;message than to allow a questionable message to pass through. Rather than&#160;measuring the error rate of a spam classifier, we may wish to measure some form&#160;of total cost, where the cost of blocking legitimate messages is higher than the cost&#160;of allowing spam messages.</span></p>
<p><span class="font64">Sometimes we wish to train a binary classifier that is intended to detect some rare event. For example, we might design a medical test for a rare disease. Suppose&#160;that only one in every million people has this disease. We can easily achieve&#160;99.9999% accuracy on the detection task, by simply hard-coding the classifier&#160;to always report that the disease is absent. Clearly, accuracy is a poor way to&#160;characterize the performance of such a system. One way to solve this problem is to&#160;instead measure </span><span class="font64" style="font-weight:bold;font-style:italic;">precision</span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">recall.</span><span class="font64"> Precision is the fraction of detections reported&#160;by the model that were correct, while recall is the fraction of true events that&#160;were detected. A detector that says no one has the disease would achieve perfect&#160;precision, but zero recall. A detector that says everyone has the disease would&#160;achieve perfect recall, but precision equal to the percentage of people who have&#160;the disease (0.0001% in our example of a disease that only one people in a million&#160;have). When using precision and recall, it is common to plot a </span><span class="font64" style="font-weight:bold;font-style:italic;">PR curve,</span><span class="font64"> with&#160;precision on the y-axis and recall on the x-axis. The classifier generates a score&#160;that is higher if the event to be detected occurred. For example, a feedforward&#160;network designed to detect a disease outputs y = </span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64">(y =1 | x), estimating the&#160;probability that a person whose medical results are described by features x has&#160;the disease. We choose to report a detection whenever this score exceeds some&#160;threshold. By varying the threshold, we can trade precision for recall. In many&#160;cases, we wish to summarize the performance of the classifier with a single number&#160;rather than a curve. To do so, we can convert precision p and recall r into an&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">F-score</span><span class="font64"> given by</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">F =</span><span class="font64"> &#160;&#160;&#160;(11.1)</span></p>
<p><span class="font64">p + r</span></p>
<p><span class="font64">Another option is to report the total area lying beneath the PR curve.</span></p>
<p><span class="font64">In some applications, it is possible for the machine learning system to refuse to make a decision. This is useful when the machine learning algorithm can estimate&#160;how confident it should be about a decision, especially if a wrong decision can&#160;be harmful and if a human operator is able to occasionally take over. The Street&#160;View transcription system provides an example of this situation. The task is to&#160;transcribe the address number from a photograph in order to associate the location&#160;where the photo was taken with the correct address in a map. Because the value&#160;of the map degrades considerably if the map is inaccurate, it is important to add&#160;an address only if the transcription is correct. If the machine learning system&#160;thinks that it is less likely than a human being to obtain the correct transcription,&#160;then the best course of action is to allow a human to transcribe the photo instead.&#160;Of course, the machine learning system is only useful if it is able to dramatically&#160;reduce the amount of photos that the human operators must process. A natural&#160;performance metric to use in this situation is </span><span class="font64" style="font-weight:bold;font-style:italic;">coverage.</span><span class="font64"> Coverage is the fraction of&#160;examples for which the machine learning system is able to produce a response. It&#160;is possible to trade coverage for accuracy. One can always obtain 100% accuracy&#160;by refusing to process any example, but this reduces the coverage to 0%. For the&#160;Street View task, the goal for the project was to reach human-level transcription&#160;accuracy while maintaining 95% coverage. Human-level performance on this task&#160;is 98% accuracy.</span></p>
<p><span class="font64">Many other metrics are possible. We can for example, measure click-through rates, collect user satisfaction surveys, and so on. Many specialized application&#160;areas have application-specific criteria as well.</span></p>
<p><span class="font64">What is important is to determine which performance metric to improve ahead of time, then concentrate on improving this metric. Without clearly defined goals,&#160;it can be difficult to tell whether changes to a machine learning system make&#160;progress or not.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">11.2 Default Baseline Models</span></h4>
<p><span class="font64">After choosing performance metrics and goals, the next step in any practical application is to establish a reasonable end-to-end system as soon as possible. In&#160;this section, we provide recommendations for which algorithms to use as the first&#160;baseline approach in various situations. Keep in mind that deep learning research&#160;progresses quickly, so better default algorithms are likely to become available soon&#160;after this writing.</span></p>
<p><span class="font64">Depending on the complexity of your problem, you may even want to begin without using deep learning. If your problem has a chance of being solved by&#160;just choosing a few linear weights correctly, you may want to begin with a simple&#160;statistical model like logistic regression.</span></p>
<p><span class="font64">If you know that your problem falls into an “AI-complete” category like object recognition, speech recognition, machine translation, and so on, then you are likely&#160;to do well by beginning with an appropriate deep learning model.</span></p>
<p><span class="font64">First, choose the general category of model based on the structure of your data. If you want to perform supervised learning with fixed-size vectors as input,&#160;use a feedforward network with fully connected layers. If the input has known&#160;topological structure (for example, if the input is an image), use a convolutional&#160;network. In these cases, you should begin by using some kind of piecewise linear&#160;unit (ReLUs or their generalizations like Leaky ReLUs, PreLus and maxout). If&#160;your input or output is a sequence, use a gated recurrent net (LSTM or GRU).</span></p>
<p><span class="font64">A reasonable choice of optimization algorithm is SGD with momentum with a decaying learning rate (popular decay schemes that perform better or worse on&#160;different problems include decaying linearly until reaching a fixed minimum learning&#160;rate, decaying exponentially, or decreasing the learning rate by a factor of 2-10&#160;each time validation error plateaus). Another very reasonable alternative is Adam.&#160;Batch normalization can have a dramatic effect on optimization performance,&#160;especially for convolutional networks and networks with sigmoidal nonlinearities.&#160;While it is reasonable to omit batch normalization from the very first baseline, it&#160;should be introduced quickly if optimization appears to be problematic.</span></p>
<p><span class="font64">Unless your training set contains tens of millions of examples or more, you should include some mild forms of regularization from the start. Early stopping&#160;should be used almost universally. Dropout is an excellent regularizer that is easy&#160;to implement and compatible with many models and training algorithms. Batch&#160;normalization also sometimes reduces generalization error and allows dropout to&#160;be omitted, due to the noise in the estimate of the statistics used to normalize&#160;each variable.</span></p>
<p><span class="font64">If your task is similar to another task that has been studied extensively, you will probably do well by first copying the model and algorithm that is already&#160;known to perform best on the previously studied task. You may even want to copy&#160;a trained model from that task. For example, it is common to use the features&#160;from a convolutional network trained on ImageNet to solve other computer vision&#160;tasks (Girshick </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015).</span></p>
<p><span class="font64">A common question is whether to begin by using unsupervised learning, described further in Part III. This is somewhat domain specific. Some domains, such as natural language processing, are known to benefit tremendously from unsupervised learning techniques such as learning unsupervised word embeddings. In other&#160;domains, such as computer vision, current unsupervised learning techniques do&#160;not bring a benefit, except in the semi-supervised setting, when the number of&#160;labeled examples is very small (Kingma </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014; Rasmus </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2015). If your&#160;application is in a context where unsupervised learning is known to be important,&#160;then include it in your first end-to-end baseline. Otherwise, only use unsupervised&#160;learning in your first attempt if the task you want to solve is unsupervised. You&#160;can always try adding unsupervised learning later if you observe that your initial&#160;baseline overfits.</span></p><h4><a id="bookmark3"></a><span class="font65" style="font-weight:bold;">11.3 Determining Whether to Gather More Data</span></h4>
<p><span class="font64">After the first end-to-end system is established, it is time to measure the performance of the algorithm and determine how to improve it. Many machine learning novices are tempted to make improvements by trying out many different algorithms.&#160;However, it is often much better to gather more data than to improve the learning&#160;algorithm.</span></p>
<p><span class="font64">How does one decide whether to gather more data? First, determine whether the performance on the training set is acceptable. If performance on the training&#160;set is poor, the learning algorithm is not using the training data that is already&#160;available, so there is no reason to gather more data. Instead, try increasing the&#160;size of the model by adding more layers or adding more hidden units to each layer.&#160;Also, try improving the learning algorithm, for example by tuning the learning rate&#160;hyperparameter. If large models and carefully tuned optimization algorithms do&#160;not work well, then the problem might be the </span><span class="font64" style="font-weight:bold;">quality </span><span class="font64">of the training data. The&#160;data may be too noisy or may not include the right inputs needed to predict the&#160;desired outputs. This suggests starting over, collecting cleaner data or collecting a&#160;richer set of features.</span></p>
<p><span class="font64">If the performance on the training set is acceptable, then measure the performance on a test set. If the performance on the test set is also acceptable, then there is nothing left to be done. If test set performance is much worse than&#160;training set performance, then gathering more data is one of the most effective&#160;solutions. The key considerations are the cost and feasibility of gathering more&#160;data, the cost and feasibility of reducing the test error by other means, and the&#160;amount of data that is expected to be necessary to improve test set performance&#160;significantly. At large internet companies with millions or billions of users, it is&#160;feasible to gather large datasets, and the expense of doing so can be considerably&#160;less than the other alternatives, so the answer is almost always to gather more&#160;training data. For example, the development of large labeled datasets was one of&#160;the most important factors in solving object recognition. In other contexts, such as&#160;medical applications, it may be costly or infeasible to gather more data. A simple&#160;alternative to gathering more data is to reduce the size of the model or improve&#160;regularization, by adjusting hyperparameters such as weight decay coefficients,&#160;or by adding regularization strategies such as dropout. If you find that the gap&#160;between train and test performance is still unacceptable even after tuning the&#160;regularization hyperparameters, then gathering more data is advisable.</span></p>
<p><span class="font64">When deciding whether to gather more data, it is also necessary to decide how much to gather. It is helpful to plot curves showing the relationship between&#160;training set size and generalization error, like in Fig. 5.4. By extrapolating such&#160;curves, one can predict how much additional training data would be needed to&#160;achieve a certain level of performance. Usually, adding a small fraction of the total&#160;number of examples will not have a noticeable impact on generalization error. It is&#160;therefore recommended to experiment with training set sizes on a logarithmic scale,&#160;for example doubling the number of examples between consecutive experiments.</span></p>
<p><span class="font64">If gathering much more data is not feasible, the only other way to improve generalization error is to improve the learning algorithm itself. This becomes the&#160;domain of research and not the domain of advice for applied practitioners.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">11.4 Selecting Hyperparameters</span></h4>
<p><span class="font64">Most deep learning algorithms come with many hyperparameters that control many aspects of the algorithm’s behavior. Some of these hyperparameters affect the time&#160;and memory cost of running the algorithm. Some of these hyperparameters affect&#160;the quality of the model recovered by the training process and its ability to infer&#160;correct results when deployed on new inputs.</span></p>
<p><span class="font64">There are two basic approaches to choosing these hyperparameters: choosing them manually and choosing them automatically. Choosing the hyperparameters&#160;manually requires understanding what the hyperparameters do and how machine&#160;learning models achieve good generalization. Automatic hyperparameter selection&#160;algorithms greatly reduce the need to understand these ideas, but they are often&#160;much more computationally costly.</span></p><h5><a id="bookmark5"></a><span class="font64" style="font-weight:bold;">11.4.1 Manual Hyperparameter Tuning</span></h5>
<p><span class="font64">To set hyperparameters manually, one must understand the relationship between hyperparameters, training error, generalization error and computational resources&#160;(memory and runtime). This means establishing a solid foundation on the fundamental ideas concerning the effective capacity of a learning algorithm from&#160;Chapter 5.</span></p>
<p><span class="font64">The goal of manual hyperparameter search is usually to find the lowest generalization error subject to some runtime and memory budget. We do not discuss how to determine the runtime and memory impact of various hyperparameters here&#160;because this is highly platform-dependent.</span></p>
<p><span class="font64">The primary goal of manual hyperparameter search is to adjust the effective capacity of the model to match the complexity of the task. Effective capacity&#160;is constrained by three factors: the representational capacity of the model, the&#160;ability of the learning algorithm to successfully minimize the cost function used to&#160;train the model, and the degree to which the cost function and training procedure&#160;regularize the model. A model with more layers and more hidden units per layer has&#160;higher representational capacity—it is capable of representing more complicated&#160;functions. It can not necessarily actually learn all of these functions though, if&#160;the training algorithm cannot discover that certain functions do a good job of&#160;minimizing the training cost, or if regularization terms such as weight decay forbid&#160;some of these functions.</span></p>
<p><span class="font64">The generalization error typically follows a U-shaped curve when plotted as a function of one of the hyperparameters, as in Fig. 5.3. At one extreme, the&#160;hyperparameter value corresponds to low capacity, and generalization error is high&#160;because training error is high. This is the underfitting regime. At the other extreme,&#160;the hyperparameter value corresponds to high capacity, and the generalization&#160;error is high because the gap between training and test error is high. Somewhere&#160;in the middle lies the optimal model capacity, which achieves the lowest possible&#160;generalization error, by adding a medium generalization gap to a medium amount&#160;of training error.</span></p>
<p><span class="font64">For some hyperparameters, overfitting occurs when the value of the hyperparameter is large. The number of hidden units in a layer is one such example, because increasing the number of hidden units increases the capacity of the model.&#160;For some hyperparameters, overfitting occurs when the value of the hyperparameter is small. For example, the smallest allowable weight decay coefficient of zero&#160;corresponds to the greatest effective capacity of the learning algorithm.</span></p>
<p><span class="font64">Not every hyperparameter will be able to explore the entire U-shaped curve. Many hyperparameters are discrete, such as the number of units in a layer or the&#160;number of linear pieces in a maxout unit, so it is only possible to visit a few points&#160;along the curve. Some hyperparameters are binary. Usually these hyperparameters&#160;are switches that specify whether or not to use some optional component of&#160;the learning algorithm, such as a preprocessing step that normalizes the input&#160;features by subtracting their mean and dividing by their standard deviation. These&#160;hyperparameters can only explore two points on the curve. Other hyperparameters&#160;have some minimum or maximum value that prevents them from exploring some&#160;part of the curve. For example, the minimum weight decay coefficient is zero. This&#160;means that if the model is underfitting when weight decay is zero, we can not enter&#160;the overfitting region by modifying the weight decay coefficient. In other words,&#160;some hyperparameters can only subtract capacity.</span></p>
<p><span class="font64">The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate. It controls the effective capacity of the model in a more complicated way than other&#160;hyperparameters—the effective capacity of the model is highest when the learning&#160;rate is </span><span class="font64" style="font-weight:bold;">correct </span><span class="font64">for the optimization problem, not when the learning rate is especially large or especially small. The learning rate has a U-shaped curve for </span><span class="font64" style="font-weight:bold;font-style:italic;">training&#160;</span><span class="font64">error, illustrated in Fig. 11.1. When the learning rate is too large, gradient descent&#160;can inadvertently increase rather than decrease the training error. In the idealized&#160;quadratic case, this occurs if the learning rate is at least twice as large as its&#160;optimal value (LeCun </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1998a). When the learning rate is too small, training&#160;is not only slower, but may become permanently stuck with a high training error.&#160;This effect is poorly understood (it would not happen for a convex loss function).</span></p>
<p><span class="font64">Tuning the parameters other than the learning rate requires monitoring both training and test error to diagnose whether your model is overfitting or underfitting,&#160;then adjusting its capacity appropriately.</span></p>
<p><span class="font64">If your error on the training set is higher than your target error rate, you have no choice but to increase capacity. If you are not using regularization and you are&#160;confident that your optimization algorithm is performing correctly, then you must&#160;add more layers to your network or add more hidden units. Unfortunately, this&#160;increases the computational costs associated with the model.</span></p>
<p><span class="font64">If your error on the test set is higher than than your target error rate, you can</span></p><div><div>
<p><span class="font61">S־H</span></p>
<p><span class="font61" style="font-variant:small-caps;">S־h</span></p>
<p><span class="font62">נס</span></p>
<p><span class="font10">bO</span></p>
<p><span class="font64">.s</span></p>
<p><span class="font64">*3</span></p>
<p><span class="font64">*3</span></p><img src="main-135.png" alt=""/></div></div><div><div>
<p><span class="font64">o</span></p><img src="main-136.jpg" alt=""/>
<p><span class="font64">Figure 11.1: Typical relationship between the learning rate and the training error. Notice the sharp rise in error when the learning is above an optimal value. This is for a fixed&#160;training time, as a smaller learning rate may sometimes only slow down training by a&#160;factor proportional to the learning rate reduction. Generalization error can follow this&#160;curve or be complicated by regularization effects arising out of having a too large or&#160;too small learning rates, since poor optimization can, to some degree, reduce or prevent&#160;overfitting, and even points with equivalent training error can have different generalization&#160;error.</span></p></div></div>
<p><span class="font64">now take two kinds of actions. The test error is the sum of the training error and the gap between training and test error. The optimal test error is found by trading&#160;off these quantities. Neural networks typically perform best when the training&#160;error is very low (and thus, when capacity is high) and the test error is primarily&#160;driven by the gap between train and test error. Your goal is to reduce this gap&#160;without increasing training error faster than the gap decreases. To reduce the gap,&#160;change regularization hyperparameters to reduce effective model capacity, such as&#160;by adding dropout or weight decay. Usually the best performance comes from a&#160;large model that is regularized well, for example by using dropout.</span></p>
<p><span class="font64">Most hyperparameters can be set by reasoning about whether they increase or decrease model capacity. Some examples are included in Table 11.1.</span></p>
<p><span class="font64">While manually tuning hyperparameters, do not lose sight of your end goal: good performance on the test set. Adding regularization is only one way to achieve&#160;this goal. As long as you have low training error, you can always reduce generalization error by collecting more training data. The brute force way to practically&#160;guarantee success is to continually increase model capacity and training set size&#160;until the task is solved. This approach does of course increase the computational&#160;cost of training and inference, so it is only feasible given appropriate resources. In</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font64">Hyperparameter</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">Increases capacity&#160;when. . .</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">Reason</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">Caveats</span></p></td></tr>
<tr><td>
<p><span class="font64">Number of hidden units</span></p></td><td>
<p><span class="font64">increased</span></p></td><td>
<p><span class="font64">Increasing the number of hidden units increases the&#160;representational capacity&#160;of the model.</span></p></td><td>
<p><span class="font64">Increasing the number of hidden units increases&#160;both the time and memory&#160;cost of essentially every operation on the model.</span></p></td></tr>
<tr><td>
<p><span class="font64">Learning rate</span></p></td><td>
<p><span class="font64">tuned optimally</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">An improper learning rate, whether too high or too&#160;low, results in a model&#160;with low effective capacity&#160;due to optimization failure</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font64">Convolution kernel width</span></p></td><td>
<p><span class="font64">increased</span></p></td><td>
<p><span class="font64">Increasing the kernel width increases the number of parameters in the model</span></p></td><td>
<p><span class="font64">A wider kernel results in a narrower output dimension, reducing model capacity unless you use implicit zero padding to reduce this effect. Wider&#160;kernels require more memory for parameter storage&#160;and increase runtime, but&#160;a narrower output reduces&#160;memory cost.</span></p></td></tr>
<tr><td>
<p><span class="font64">Implicit zero padding</span></p></td><td>
<p><span class="font64">increased</span></p></td><td>
<p><span class="font64">Adding implicit zeros before convolution keeps the representation size large</span></p></td><td>
<p><span class="font64">Increased time and memory cost of most operations.</span></p></td></tr>
<tr><td>
<p><span class="font64">Weight decay coefficient</span></p></td><td>
<p><span class="font64">decreased</span></p></td><td>
<p><span class="font64">Decreasing the weight decay coefficient frees the model parameters to become larger</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font64">Dropout rate</span></p></td><td>
<p><span class="font64">decreased</span></p></td><td>
<p><span class="font64">Dropping units less often gives the units more opportunities to “conspire” with&#160;each other to fit the training set</span></p></td><td>
<p></p></td></tr>
</table>
<p><span class="font64">Table 11.1: The effect of various hyperparameters on model capacity.</span></p>
<p><span class="font64">principle, this approach could fail due to optimization difficulties, but for many problems optimization does not seem to be a significant barrier, provided that the&#160;model is chosen appropriately.</span></p><h5><a id="bookmark6"></a><span class="font64" style="font-weight:bold;">11.4.2 &#160;&#160;&#160;Automatic Hyperparameter Optimization Algorithms</span></h5>
<p><span class="font64">The ideal learning algorithm just takes a dataset and outputs a function, without requiring hand-tuning of hyperparameters. The popularity of several learning&#160;algorithms such as logistic regression and SVMs stems in part from their ability to&#160;perform well with only one or two tuned hyperparameters. Neural networks can&#160;sometimes perform well with only a small number of tuned hyperparameters, but&#160;often benefit significantly from tuning of forty or more hyperparameters. Manual&#160;hyperparameter tuning can work very well when the user has a good starting point,&#160;such as one determined by others having worked on the same type of application&#160;and architecture, or when the user has months or years of experience in exploring&#160;hyperparameter values for neural networks applied to similar tasks. However,&#160;for many applications, these starting points are not available. In these cases,&#160;automated algorithms can find useful values of the hyperparameters.</span></p>
<p><span class="font64">If we think about the way in which the user of a learning algorithm searches for good values of the hyperparameters, we realize that an optimization is taking&#160;place: we are trying to find a value of the hyperparameters that optimizes an&#160;objective function, such as validation error, sometimes under constraints (such as a&#160;budget for training time, memory or recognition time). It is therefore possible, in&#160;principle, to develop </span><span class="font64" style="font-weight:bold;font-style:italic;">hyperparameter optimization</span><span class="font64"> algorithms that wrap a learning&#160;algorithm and choose its hyperparameters, thus hiding the hyperparameters of the&#160;learning algorithm from the user. Unfortunately, hyperparameter optimization&#160;algorithms often have their own hyperparameters, such as the range of values that&#160;should be explored for each of the learning algorithm’s hyperparameters. However,&#160;these secondary hyperparameters are usually easier to choose, in the sense that&#160;acceptable performance may be achieved on a wide range of tasks using the same&#160;secondary hyperparameters for all tasks.</span></p><h5><a id="bookmark7"></a><span class="font64" style="font-weight:bold;">11.4.3 &#160;&#160;&#160;Grid Search</span></h5>
<p><span class="font64">When there are three or fewer hyperparameters, the common practice is to perform </span><span class="font64" style="font-weight:bold;font-style:italic;">grid search.</span><span class="font64"> For each hyperparameter, the user selects a small finite set of values to&#160;explore. The grid search algorithm then trains a model for every joint specification&#160;of hyperparameter values in the Cartesian product of the set of values for each&#160;individual hyperparameter. The experiment that yields the best validation set</span></p><div><div><img src="main-137.jpg" alt=""/>
<p><span class="font64">Random</span></p>
<p><span class="font64">Grid</span></p></div></div>
<p><span class="font64">Figure 11.2: Comparison of grid search and random search. For illustration purposes we display two hyperparameters but we are typically interested in having many more. </span><span class="font64" style="font-style:italic;">(Left)&#160;</span><span class="font64">To perform grid search, we provide a set of values for each hyperparameter. The search&#160;algorithm runs training for every joint hyperparameter setting in the cross product of these&#160;sets. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> To perform random search, we provide a probability distribution over joint&#160;hyperparameter configurations. Usually most of these hyperparameters are independent&#160;from each other. Common choices for the distribution over a single hyperparameter include&#160;uniform and log-uniform (to sample from a log-uniform distribution, take the exp of a&#160;sample from a uniform distribution). The search algorithm then randomly samples joint&#160;hyperparameter configurations and runs training with each of them. Both grid search&#160;and random search evaluate the validation set error and return the best configuration.&#160;The figure illustrates the typical case where only some hyperparameters have a significant&#160;influence on the result. In this illustration, only the hyperparameter on the horizontal axis&#160;has a significant effect. Grid search wastes an amount of computation that is exponential&#160;in the number of non-influential hyperparameters, while random search tests a unique&#160;value of every influential hyperparameter on nearly every trial.</span></p>
<p><span class="font64">error is then chosen as having found the best hyperparameters. See the left of Fig. 11.2 for an illustration of a grid of hyperparameter values.</span></p>
<p><span class="font64">How should the lists of values to search over be chosen? In the case of numerical (ordered) hyperparameters, the smallest and largest element of each list is chosen&#160;conservatively, based on prior experience with similar experiments, to make sure&#160;that the optimal value is very likely to be in the selected range. Typically, a&#160;grid search involves picking values approximately on a </span><span class="font64" style="font-weight:bold;">logarithmic scale</span><span class="font64">, e.g., a&#160;learning rate taken within the set {.</span><span class="font18">1</span><span class="font64">, .</span><span class="font18">01</span><span class="font64">,</span><span class="font18">10</span><span class="font64"><sup>_3</sup>, </span><span class="font18">10</span><span class="font64"><sup>_4</sup>, </span><span class="font18">10</span><span class="font64"><sup>-5</sup>}, or a number of hidden&#160;units taken with the set {50,100, 200,500,1000,2000}.</span></p>
<p><span class="font64">Grid search usually performs best when it is performed repeatedly. For example, suppose that we ran a grid search over a hyperparameter a using values of {-</span><span class="font18">1</span><span class="font64">, </span><span class="font18">0</span><span class="font64">, </span><span class="font18">1</span><span class="font64">}.&#160;If the best value found is 1, then we underestimated the range in which the best a&#160;lies and we should shift the grid and run another search with a in, for example,&#160;{1, 2, 3}. If we find that the best value of a is 0, then we may wish to refine our&#160;estimate by zooming in and running a grid search over {-.</span><span class="font18">1</span><span class="font64">, </span><span class="font18">0</span><span class="font64">, .</span><span class="font18">1</span><span class="font64">}.</span></p>
<p><span class="font64">The obvious problem with grid search is that its computational cost grows exponentially with the number of hyperparameters. If there are m hyperparameters,&#160;each taking at most n values, then the number of training and evaluation trials&#160;required grows as </span><span class="font64" style="font-weight:bold;font-style:italic;">O (n</span><span class="font64"><sup>m</sup>). The trials may be run in parallel and exploit loose&#160;parallelism (with almost no need for communication between different machines&#160;carrying out the search) Unfortunately, due to the exponential cost of grid search,&#160;even parallelization may not provide a satisfactory size of search.</span></p><h5><a id="bookmark8"></a><span class="font64" style="font-weight:bold;">11.4.4 Random Search</span></h5>
<p><span class="font64">Fortunately, there is an alternative to grid search that is as simple to program, more convenient to use, and converges much faster to good values of the hyperparameters:&#160;random search (Bergstra and Bengio, 2012).</span></p>
<p><span class="font64">A random search proceeds as follows. First we define a marginal distribution for each hyperparameter, e.g., a Bernoulli or multinoulli for binary or discrete&#160;hyperparameters, or a uniform distribution on a log-scale for positive real-valued&#160;hyperparameters. For example,</span></p>
<p><span class="font64" style="font-weight:bold;">log</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">learning</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">rate </span><span class="font64">~ u(—</span><span class="font18">1</span><span class="font64">, -5) &#160;&#160;&#160;(</span><span class="font18">11</span><span class="font64">.</span><span class="font18">2</span><span class="font64">)</span></p>
<p><span class="font64" style="font-weight:bold;">learning</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">rate </span><span class="font64">= </span><span class="font18">10</span><span class="font64"><sup>log</sup>-<sup>leaming</sup>_<sup>rate</sup> &#160;&#160;&#160;(11.3)</span></p>
<p><span class="font64">where u(a, </span><span class="font64" style="font-weight:bold;font-style:italic;">b</span><span class="font64">) indicates a sample of the uniform distribution in the interval </span><span class="font64" style="font-weight:bold;font-style:italic;">(a,b). </span><span class="font64">Similarly the </span><span class="font64" style="font-weight:bold;">log</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">number</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">of </span><span class="font64">_</span><span class="font64" style="font-weight:bold;">hidden</span><span class="font64">_</span><span class="font64" style="font-weight:bold;">units </span><span class="font64">may be sampled from u(log(50),&#160;log(</span><span class="font18">2000</span><span class="font64">)).</span></p>
<p><span class="font64">Unlike in the case of a grid search, one </span><span class="font64" style="font-weight:bold;">should not discretize </span><span class="font64">or bin the values of the hyperparameters. This allows one to explore a larger set of values, and&#160;does not incur additional computational cost. In fact, as illustrated in Fig. 11.2, a&#160;random search can be exponentially more efficient than a grid search, when there&#160;are several hyperparameters that do not strongly affect the performance measure.&#160;This is studied at length in Bergstra and Bengio (2012), who found that random&#160;search reduces the validation set error much faster than grid search, in terms of&#160;the number of trials run by each method.</span></p>
<p><span class="font64">As with grid search, one may often want to run repeated versions of random search, to refine the search based on the results of the first run.</span></p>
<p><span class="font64">The main reason why random search finds good solutions faster than grid search is that the there are no wasted experimental runs, unlike in the case of grid search,&#160;when two values of a hyperparameter (given values of the other hyperparameters)&#160;would give the same result. In the case of grid search, the other hyperparameters&#160;would have the same values for these two runs, whereas with random search, they&#160;would usually have different values. Hence if the change between these two values&#160;does not marginally make much difference in terms of validation set error, grid&#160;search will unnecessarily repeat two equivalent experiments while random search&#160;will still give two independent explorations of the other hyperparameters.</span></p><h5><a id="bookmark9"></a><span class="font64" style="font-weight:bold;">11.4.5 Model-Based Hyperparameter Optimization</span></h5>
<p><span class="font64">The search for good hyperparameters can be cast as an optimization problem. The decision variables are the hyperparameters. The cost to be optimized is the&#160;validation set error that results from training using these hyperparameters. In&#160;simplified settings where it is feasible to compute the gradient of some differentiable&#160;error measure on the validation set with respect to the hyperparameters, we can&#160;simply follow this gradient (Bengio </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1999; Bengio, 2000; Maclaurin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2015). Unfortunately, in most practical settings, this gradient is unavailable, either&#160;due to its high computation and memory cost, or due to hyperparameters having&#160;intrinsically non-differentiable interactions with the validation set error, as in the&#160;case of discrete-valued hyperparameters.</span></p>
<p><span class="font64">To compensate for this lack of a gradient, we can build a model of the validation set error, then propose new hyperparameter guesses by performing optimization&#160;within this model. Most model-based algorithms for hyperparameter search use a&#160;Bayesian regression model to estimate both the expected value of the validation set&#160;error for each hyperparameter and the uncertainty around this expectation. Optimization thus involves a tradeoff between exploration (proposing hyperparameters&#160;for which there is high uncertainty, which may lead to a large improvement but may&#160;also perform poorly) and exploitation (proposing hyperparameters which the model&#160;is confident will perform as well as any hyperparameters it has seen so far—usually&#160;hyperparameters that are very similar to ones it has seen before). Contemporary&#160;approaches to hyperparameter optimization include Spearmint (Snoek </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012),&#160;TPE (Bergstra </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011) and SMAC (Hutter </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011).</span></p>
<p><span class="font64">Currently, we cannot unambiguously recommend Bayesian hyperparameter optimization as an established tool for achieving better deep learning results or&#160;for obtaining those results with less effort. Bayesian hyperparameter optimization&#160;sometimes performs comparably to human experts, sometimes better, but fails&#160;catastrophically on other problems. It may be worth trying to see if it works on&#160;a particular problem but is not yet sufficiently mature or reliable. That being&#160;said, hyperparameter optimization is an important field of research that, while&#160;often driven primarily by the needs of deep learning, holds the potential to benefit&#160;not only the entire field of machine learning but the discipline of engineering in&#160;general.</span></p>
<p><span class="font64">One drawback common to most hyperparameter optimization algorithms with more sophistication than random search is that they require for a training experiment to run to completion before they are able to extract any information&#160;from the experiment. This is much less efficient, in the sense of how much information can be gleaned early in an experiment, than manual search by a human&#160;practitioner, since one can usually tell early on if some set of hyperparameters is&#160;completely pathological. Swersky </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) have introduced an early version&#160;of an algorithm that maintains a set of multiple experiments. At various time&#160;points, the hyperparameter optimization algorithm can choose to begin a new&#160;experiment, to “freeze” a running experiment that is not promising, or to “thaw”&#160;and resume an experiment that was earlier frozen but now appears promising given&#160;more information.</span></p><h4><a id="bookmark10"></a><span class="font65" style="font-weight:bold;">11.5 Debugging Strategies</span></h4>
<p><span class="font64">When a machine learning system performs poorly, it is usually difficult to tell whether the poor performance is intrinsic to the algorithm itself or whether there&#160;is a bug in the implementation of the algorithm. Machine learning systems are&#160;difficult to debug for a variety of reasons.</span></p>
<p><span class="font64">In most cases, we do not know a priori what the intended behavior of the algorithm is. In fact, the entire point of using machine learning is that it will&#160;discover useful behavior that we were not able to specify ourselves. If we train a&#160;neural network on a </span><span class="font64" style="font-weight:bold;font-style:italic;">new</span><span class="font64"> classification task and it achieves 5% test error, we have&#160;no straightforward way of knowing if this is the expected behavior or sub-optimal&#160;behavior.</span></p>
<p><span class="font64">A further difficulty is that most machine learning models have multiple parts that are each adaptive. If one part is broken, the other parts can adapt and still&#160;achieve roughly acceptable performance. For example, suppose that we are training&#160;a neural net with several layers parametrized by weights W and biases b. Suppose&#160;further that we have manually implemented the gradient descent rule for each&#160;parameter separately, and we made an error in the update for the biases:</span></p>
<p><span class="font64">b ^ b — a &#160;&#160;&#160;(11.4)</span></p>
<p><span class="font64">where a is the learning rate. This erroneous update does not use the gradient at all. It causes the biases to constantly become negative throughout learning, which&#160;is clearly not a correct implementation of any reasonable learning algorithm. The&#160;bug may not be apparent just from examining the output of the model though.&#160;Depending on the distribution of the input, the weights may be able to adapt to&#160;compensate for the negative biases.</span></p>
<p><span class="font64">Most debugging strategies for neural nets are designed to get around one or both of these two difficulties. Either we design a case that is so simple that the&#160;correct behavior actually can be predicted, or we design a test that exercises one&#160;part of the neural net implementation in isolation.</span></p>
<p><span class="font64">Some important debugging tests include:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Visualize the model in action</span><span class="font64"> : When training a model to detect objects in images, view some images with the detections proposed by the model displayed&#160;superimposed on the image. When training a generative model of speech, listen to&#160;some of the speech samples it produces. This may seem obvious, but it is easy to&#160;fall into the practice of only looking at quantitative performance measurements&#160;like accuracy or log-likelihood. Directly observing the machine learning model&#160;performing its task will help you to determine whether the quantitative performance&#160;numbers it achieves seem reasonable. Evaluation bugs can be some of the most&#160;devastating bugs because they can mislead you into believing your system is&#160;performing well when it is not.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Visualize the worst mistakes</span><span class="font64"> : Most models are able to output some sort of confidence measure for the task they perform. For example, classifiers based on a&#160;softmax output layer assign a probability to each class. The probability assigned&#160;to the most likely class thus gives an estimate of the confidence the model has in&#160;its classification decision. Typically, maximum likelihood training results in these&#160;values being overestimates rather than accurate probabilities of correct prediction,&#160;but they are somewhat useful in the sense that examples that are actually less&#160;likely to be correctly labeled receive smaller probabilities under the model. By&#160;viewing the training set examples that are the hardest to model correctly, one can&#160;often discover problems with the way the data has been preprocessed or labeled.&#160;For example, the Street View transcription system originally had a problem where&#160;the address number detection system would crop the image too tightly and omit&#160;some of the digits. The transcription network then assigned very low probability&#160;to the correct answer on these images. Sorting the images to identify the most&#160;confident mistakes showed that there was a systematic problem with the cropping.&#160;Modifying the detection system to crop much wider images resulted in much better&#160;performance of the overall system, even though the transcription network needed&#160;to be able to process greater variation in the position and scale of the address&#160;numbers.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Reasoning about software using train and test error.</span><span class="font64"> It is often difficult to determine whether the underlying software is correctly implemented. Some clues&#160;can be obtained from the train and test error. If training error is low but test error&#160;is high, then it is likely that that the training procedure works correctly, and the&#160;model is overfitting for fundamental algorithmic reasons. An alternative possibility&#160;is that the test error is measured incorrectly due to a problem with saving the&#160;model after training then reloading it for test set evaluation, or if the test data&#160;was prepared differently from the training data. If both train and test error are&#160;high, then it is difficult to determine whether there is a software defect or whether&#160;the model is underfitting due to fundamental algorithmic reasons. This scenario&#160;requires further tests, described next.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Fit a tiny dataset:</span><span class="font64"> If you have high error on the training set, determine whether it is due to genuine underfitting or due to a software defect. Usually even small&#160;models can be guaranteed to be able fit a sufficiently small dataset. For example,&#160;a classification dataset with only one example can be fit just by setting the biases&#160;of the output layer correctly. Usually if you cannot train a classifier to correctly&#160;label a single example, an autoencoder to successfully reproduce a single example&#160;with high fidelity, or a generative model to consistently emit samples resembling a&#160;single example, there is a software defect preventing successful optimization on the&#160;training set. This test can be extended to a small dataset with few examples.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Compare ba,ck-propagated derivatives to numerical derivatives:</span><span class="font64"> If you are using a software framework that requires you to implement your own gradient computations, or if you are adding a new operation to a differentiation library and&#160;must define its </span><span class="font64" style="font-weight:bold;">bprop </span><span class="font64">method, then a common source of error is implementing this&#160;gradient expression incorrectly. One way to verify that these derivatives are correct</span></p>
<p><span class="font64">is to compare the derivatives computed by your implementation of automatic differentiation to the derivatives computed by a </span><span class="font64" style="font-weight:bold;font-style:italic;">finite differences.</span><span class="font64"> Because</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">f</span><span class="font64"> (x) — lim</span></p>
<p><span class="font64">e-► 0</span></p></div><div>
<p><span class="font64"><sup>f</sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(x</sup></span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">e) <sup>-</sup></span><span class="font64"><sup> f(x)</sup></span></p></div><div>
<p><span class="font64">e</span></p></div><div>
<p><span class="font64">(11.5)</span></p></div>
<p><span class="font64">we can approximate the derivative by using a small, finite e:</span></p><div>
<p><span class="font64"><sup>f</sup>׳<sup>(x)</sup></span></p></div><div>
<p><span class="font64"><sup>f (x</sup> + <sup>e) - f (x)</sup></span></p></div><div>
<p><span class="font64">e</span></p></div><div>
<p><span class="font64">(</span><span class="font18">11</span><span class="font64">.</span><span class="font18">6</span><span class="font64">)</span></p></div>
<p><span class="font64">We can improve the accuracy of the approximation by using the </span><span class="font64" style="font-weight:bold;font-style:italic;">centered difference:</span></p><div>
<p><span class="font64">f ׳(x)</span></p></div><div>
<p><span class="font64">f (x + </span><span class="font18">2</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>e) - f (x -</sup> l</span><span class="font64"> e)</span></p></div><div>
<p><span class="font64">(11.7)</span></p></div>
<p><span class="font64">The perturbation size e must chosen to be large enough to ensure that the perturbation is not rounded down too much by finite-precision numerical computations.</span></p>
<p><span class="font64">Usually, we will want to test the gradient or Jacobian of a vector-valued function g : R<sup>m</sup> ^ R<sup>n</sup>. Unfortunately, finite differencing only allows us to take a single&#160;derivative at a time. We can either run finite differencing </span><span class="font64" style="font-weight:bold;font-style:italic;">mn</span><span class="font64"> times to evaluate all&#160;of the partial derivatives of g, or we can apply the test to a new function that uses&#160;random projections at both the input and output of g. For example, we can apply&#160;our test of the implementation of the derivatives to f (x) where f (x) — u<sup>T</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">g</span><span class="font64">(vx),&#160;where u and v are randomly chosen vectors. Computing f ׳(x) correctly requires&#160;being able to back-propagate through g correctly, yet is efficient to do with finite&#160;differences because f has only a single input and a single output. It is usually&#160;a good idea to repeat this test for more than one value of u and v to reduce&#160;the chance that the test overlooks mistakes that are orthogonal to the random&#160;projection.</span></p>
<p><span class="font64">If one has access to numerical computation on complex numbers, then there is a very efficient way to numerically estimate the gradient by using complex numbers&#160;as input to the function (Squire and Trapp, 1998). The method is based on the&#160;observation that</span></p><div>
<p><span class="font64">f (x + </span><span class="font64" style="font-weight:bold;font-style:italic;">ie)</span><span class="font64"> — </span><span class="font64" style="font-weight:bold;font-style:italic;">f (x)</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">ief(x)</span><span class="font64"> + O(e<sup>2</sup>)</span></p></div><div>
<p><span class="font64">real(f (x + ie)) — f (x) + O(e<sup>2</sup>), imag(</span></p></div><div>
<p><span class="font64">f (x + ie)</span></p></div><div>
<p><span class="font64">e</span></p></div><div>
<p><span class="font64"><sup>)</sup> — </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>f (x)</sup></span><span class="font64"> + <sup>O(e2</sup>),</span></p></div><div>
<p><span class="font64">(</span><span class="font18">11</span><span class="font64">.</span><span class="font18">8</span><span class="font64">)</span></p>
<p><span class="font64">(11.9)</span></p></div>
<p><span class="font64">where i — &#160;&#160;&#160;-1. Unlike in the real-valued case above, there is no cancellation effect</span></p>
<p><span class="font64">due to taking the difference between the value of f at different points. This allows the use of tiny values of e like e — </span><span class="font18">10</span><span class="font64"> <sup>-150</sup>, which make the O(e<sup>2</sup>) error insignificant&#160;for all practical purposes.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Monitor histograms of activations and gradient</span><span class="font64">: It is often useful to visualize statistics of neural network activations and gradients, collected over a large amount&#160;of training iterations (maybe one epoch). The pre-activation value of hidden units&#160;can tell us if the units saturate, or how often they do. For example, for rectifiers,&#160;how often are they off? Are there units that are always off? For tanh units,&#160;the average of the absolute value of the pre-activations tells us how saturated&#160;the unit is. In a deep network where the propagated gradients quickly grow or&#160;quickly vanish, optimization may be hampered. Finally, it is useful to compare the&#160;magnitude of parameter gradients to the magnitude of the parameters themselves.&#160;As suggested by Bottou (2015), we would like the magnitude of parameter updates&#160;over a minibatch to represent something like </span><span class="font18">1</span><span class="font64">% of the magnitude of the parameter,&#160;not 50% or 0.001% (which would make the parameters move too slowly). It may&#160;be that some groups of parameters are moving at a good pace while others are&#160;stalled. When the data is sparse (like in natural language), some parameters may&#160;be very rarely updated, and this should be kept in mind when monitoring their&#160;evolution.</span></p>
<p><span class="font64">Finally, many deep learning algorithms provide some sort of guarantee about the results produced at each step. For example, in Part III, we will see some&#160;approximate inference algorithms that work by using algebraic solutions to optimization problems. Typically these can be debugged by testing each of their&#160;guarantees. Some guarantees that some optimization algorithms offer include that&#160;the objective function will never increase after one step of the algorithm, that&#160;the gradient with respect to some subset of variables will be zero after each step&#160;of the algorithm, and that the gradient with respect to all variables will be zero&#160;at convergence. Usually due to rounding error, these conditions will not hold&#160;exactly in a digital computer, so the debugging test should include some tolerance&#160;parameter.</span></p><h4><a id="bookmark11"></a><span class="font65" style="font-weight:bold;">11.6 Example: Multi-Digit Number Recognition</span></h4>
<p><span class="font64">To provide an end-to-end description of how to apply our design methodology in practice, we present a brief account of the Street View transcription system,&#160;from the point of view of designing the deep learning components. Obviously,&#160;many other components of the complete system, such as the Street View cars, the&#160;database infrastructure, and so on, were of paramount importance.</span></p>
<p><span class="font64">From the point of view of the machine learning task, the process began with data collection. The cars collected the raw data and human operators provided&#160;labels. The transcription task was preceded by a significant amount of dataset&#160;curation, including using other machine learning techniques to detect the house&#160;numbers prior to transcribing them.</span></p>
<p><span class="font64">The transcription project began with a choice of performance metrics and desired values for these metrics. An important general principle is to tailor the&#160;choice of metric to the business goals for the project. Because maps are only useful&#160;if they have high accuracy, it was important to set a high accuracy requirement&#160;for this project. Specifically, the goal was to obtain human-level, 98% accuracy.&#160;This level of accuracy may not always be feasible to obtain. In order to reach&#160;this level of accuracy, the Street View transcription system sacrifices coverage.&#160;Coverage thus became the main performance metric optimized during the project,&#160;with accuracy held at 98%. As the convolutional network improved, it became&#160;possible to reduce the confidence threshold below which the network refuses to&#160;transcribe the input, eventually exceeding the goal of 95% coverage.</span></p>
<p><span class="font64">After choosing quantitative goals, the next step in our recommended methodology is to rapidly establish a sensible baseline system. For vision tasks, this means a convolutional network with rectified linear units. The transcription project began&#160;with such a model. At the time, it was not common for a convolutional network&#160;to output a sequence of predictions. In order to begin with the simplest possible&#160;baseline, the first implementation of the output layer of the model consisted of n&#160;different softmax units to predict a sequence of n characters. These softmax units&#160;were trained exactly the same as if the task were classification, with each softmax&#160;unit trained independently.</span></p>
<p><span class="font64">Our recommended methodology is to iteratively refine the baseline and test whether each change makes an improvement. The first change to the Street View&#160;transcription system was motivated by a theoretical understanding of the coverage&#160;metric and the structure of the data. Specifically, the network refuses to classify&#160;an input x whenever the probability of the output sequence p (y | x ) &lt; </span><span class="font64" style="font-weight:bold;font-style:italic;">t</span><span class="font64"> for&#160;some threshold t. Initially, the definition of p(y | x) was ad-hoc, based on simply&#160;multiplying all of the softmax outputs together. This motivated the development&#160;of a specialized output layer and cost function that actually computed a principled&#160;log-likelihood. This approach allowed the example rejection mechanism to function&#160;much more effectively.</span></p>
<p><span class="font64">At this point, coverage was still below 90%, yet there were no obvious theoretical problems with the approach. Our methodology therefore suggests to instrument&#160;the train and test set performance in order to determine whether the problem&#160;is underfitting or overfitting. In this case, train and test set error were nearly&#160;identical. Indeed, the main reason this project proceeded so smoothly was the&#160;availability of a dataset with tens of millions of labeled examples. Because train&#160;and test set error were so similar, this suggested that the problem was either due&#160;to underfitting or due to a problem with the training data. One of the debugging&#160;strategies we recommend is to visualize the model’s worst errors. In this case, that&#160;meant visualizing the incorrect training set transcriptions that the model gave the&#160;highest confidence. These proved to mostly consist of examples where the input&#160;image had been cropped too tightly, with some of the digits of the address being&#160;removed by the cropping operation. For example, a photo of an address “1849”&#160;might be cropped too tightly, with only the “849” remaining visible. This problem&#160;could have been resolved by spending weeks improving the accuracy of the address&#160;number detection system responsible for determining the cropping regions. Instead,&#160;the team took a much more practical decision, to simply expand the width of the&#160;crop region to be systematically wider than the address number detection system&#160;predicted. This single change added ten percentage points to the transcription&#160;system’s coverage.</span></p>
<p><span class="font64">Finally, the last few percentage points of performance came from adjusting hyperparameters. This mostly consisted of making the model larger while maintaining some restrictions on its computational cost. Because train and test error&#160;remained roughly equal, it was always clear that any performance deficits were due&#160;to underfitting, as well as due to a few remaining problems with the dataset itself.</span></p>
<p><span class="font64">Overall, the transcription project was a great success, and allowed hundreds of millions of addresses to be transcribed both faster and at lower cost than would&#160;have been possible via human effort.</span></p>
<p><span class="font64">We hope that the design principles described in this chapter will lead to many other similar successes.</span></p>
</body>
</html>