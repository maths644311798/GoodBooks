<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 2</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Linear Algebra</span></h2>
<p><span class="font64">Linear algebra is a branch of mathematics that is widely used throughout science and engineering. However, because linear algebra is a form of continuous rather&#160;than discrete mathematics, many computer scientists have little experience with it.&#160;A good understanding of linear algebra is essential for understanding and working&#160;with many machine learning algorithms, especially deep learning algorithms. We&#160;therefore precede our introduction to deep learning with a focused presentation of&#160;the key linear algebra prerequisites.</span></p>
<p><span class="font64">If you are already familiar with linear algebra, feel free to skip this chapter. If you have previous experience with these concepts but need a detailed reference&#160;sheet to review key formulas, we recommend </span><span class="font64" style="font-weight:bold;font-style:italic;">The Matrix Cookbook</span><span class="font64"> (Petersen and&#160;Pedersen, 2006). If you have no exposure at all to linear algebra, this chapter&#160;will teach you enough to read this book, but we highly recommend that you also&#160;consult another resource focused exclusively on teaching linear algebra, such as&#160;Shilov (1977). This chapter will completely omit many important linear algebra&#160;topics that are not essential for understanding deep learning.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">2.1 Scalars, Vectors, Matrices and Tensors</span></h4>
<p><span class="font64">The study of linear algebra involves several types of mathematical objects:</span></p>
<p><span class="font64">• </span><span class="font64" style="font-weight:bold;font-style:italic;">Scalars:</span><span class="font64"> A scalar is just a single number, in contrast to most of the other objects studied in linear algebra, which are usually arrays of multiple numbers.&#160;We write scalars in italics. We usually give scalars lower-case variable names.&#160;When we introduce them, we specify what kind of number they are. For&#160;example, we might say “Let s G R be the slope of the line,” while defining a&#160;real-valued scalar, or “Let n </span><span class="font64" style="font-weight:bold;font-style:italic;">G</span><span class="font64"> N be the number of units,” while defining a&#160;natural number scalar.</span></p>
<p><span class="font63">• </span><span class="font64" style="font-weight:bold;font-style:italic;">Vectors:</span><span class="font64"> A vector is an array of numbers. The numbers are arranged in order. We can identify each individual number by its index in that ordering.&#160;Typically we give vectors lower case names written in bold typeface, such&#160;as x. The elements of the vector are identified by writing its name in italic&#160;typeface, with a subscript. The first element of x is x!, the second element&#160;is </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font18">2</span><span class="font64"> and so on. We also need to say what kind of numbers are stored in&#160;the vector. If each element is in R, and the vector has n elements, then the&#160;vector lies in the set formed by taking the Cartesian product of R n times,&#160;denoted as R<sup>n</sup>. When we need to explicitly identify the elements of a vector,&#160;we write them as a column enclosed in square brackets:</span></p><div>
<p><span class="font64">x =</span></p></div><div>
<p><span class="font64"><sup>x</sup>1</span></p>
<p><span class="font64"><sup>x</sup>2</span></p></div><div>
<p><span class="font64">(2.1)</span></p></div>
<p><span class="font64">Xn</span></p>
<p><span class="font64">We can think of vectors as identifying points in space, with each element giving the coordinate along a different axis.</span></p>
<p><span class="font64">Sometimes we need to index a set of elements of a vector. In this case, we define a set containing the indices and write the set as a subscript. For&#160;example, to access x </span><span class="font18">1</span><span class="font64">, X</span><span class="font18">3</span><span class="font64"> and X</span><span class="font18">6</span><span class="font64">, we define the set </span><span class="font64" style="font-weight:bold;font-style:italic;">S = {</span><span class="font64">1, 3</span><span class="font18">,6</span><span class="font63">} and write&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">xs</span><span class="font64">. We use the — sign to index the complement of a set. For example x<sub>-</sub></span><span class="font18">1</span><span class="font64"> is&#160;the vector containing all elements of x except for x!, and x<sub>-</sub>s is the vector&#160;containing all of the elements of x except for x!, x</span><span class="font18">3</span><span class="font64"> and x</span><span class="font18">6</span><span class="font64">.</span></p>
<p><span class="font63">• </span><span class="font64" style="font-weight:bold;font-style:italic;">Matrices:</span><span class="font64"> A matrix is a 2-D array of numbers, so each element is identified by two indices instead of just one. We usually give matrices upper-case variable&#160;names with bold typeface, such as A. If a real-valued matrix </span><span class="font64" style="font-weight:bold;font-style:italic;">A</span><span class="font64"> has a height&#160;of m and a width of n, then we say that A G R<sup>mxn</sup>. We usually identify&#160;the elements of a matrix using its name in italic but not bold font, and the&#160;indices are listed with separating commas. For example, </span><span class="font64" style="font-weight:bold;font-style:italic;">A!,!</span><span class="font64"> is the upper&#160;left entry of A and </span><span class="font64" style="font-weight:bold;font-style:italic;">A<sub>m</sub>,<sub>n</sub></span><span class="font64"> is the bottom right entry. We can identify all of&#160;the numbers with vertical coordinate i by writing a “:” for the horizontal&#160;coordinate. For example, A<sub>i;:</sub> denotes the horizontal cross section of A with&#160;vertical coordinate i. This is known as the i-th </span><span class="font64" style="font-weight:bold;font-style:italic;">row</span><span class="font64"> of A. Likewise, A<sub>:;i</sub> is</span></p>
<p><span class="font64">the i-th </span><span class="font64" style="font-weight:bold;font-style:italic;">column</span><span class="font64"> of </span><span class="font64" style="font-weight:bold;font-style:italic;">A.</span><span class="font64"> When we need to explicitly identify the elements of a matrix, we write them as an array enclosed in square brackets:</span></p><div>
<p><span class="font64">(2.2)</span></p></div><div><div><img src="main-18.jpg" alt=""/>
<p><span class="font70"><sup>A</sup>1,1</span></p>
<p><span class="font70">A1,2</span></p>
<p><span class="font64">Figure 2.1: The transpose of the matrix can be thought of as a mirror image across the main diagonal.</span></p></div></div><div>
<p><span class="font70"><sup>A</sup>2,1</span></p>
<p><span class="font70">A2,2</span></p></div><div>
<p><span class="font70"><sup>A</sup>3,1</span></p>
<p><span class="font70">A3,2</span></p></div>
<p><span class="font64">A1,1 &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">A</span><span class="font64"> 1,2</span></p>
<p><span class="font64">A2,1 &#160;&#160;&#160;A 2,2</span></p>
<p><span class="font64">Sometimes we may need to index matrix-valued expressions that are not just a single letter. In this case, we use subscripts after the expression, but do&#160;not convert anything to lower case. For example, f (</span><span class="font64" style="font-weight:bold;">A</span><span class="font64">)i,j gives element (i, j)&#160;of the matrix computed by applying the function f to </span><span class="font64" style="font-weight:bold;">A</span><span class="font64">.</span></p>
<p><span class="font64">• </span><span class="font64" style="font-weight:bold;font-style:italic;">Tensors:</span><span class="font64"> In some cases we will need an array with more than two axes. In the general case, an array of numbers arranged on a regular grid with a&#160;variable number of axes is known as a </span><span class="font64" style="font-weight:bold;font-style:italic;">tensor.</span><span class="font64"> We denote a tensor named “A”&#160;with this typeface: A. We identify the element of A at coordinates (i, j, k)&#160;by writing </span><span class="font64" style="font-weight:bold;font-style:italic;">Aj■</span></p>
<p><span class="font64">One important operation on matrices is the </span><span class="font64" style="font-weight:bold;font-style:italic;">transpose.</span><span class="font64"> The transpose of a matrix is the mirror image of the matrix across a diagonal line, called the </span><span class="font64" style="font-weight:bold;font-style:italic;">main&#160;diagonal,</span><span class="font64"> running down and to the right, starting from its upper left corner. See&#160;Fig. 2.1 for a graphical depiction of this operation. We denote the transpose of a&#160;matrix </span><span class="font64" style="font-weight:bold;">A </span><span class="font64">as </span><span class="font64" style="font-weight:bold;">A</span><span class="font64"><sup>T</sup>, and it is defined such that</span></p>
<p><span class="font64"><sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>A </sup></span><span class="font64"><sup>)</sup>i,j = <sup>A</sup>j,i<sup>2</sup>-<sup>3)</sup> &#160;&#160;&#160;־<sup>)</sup></span></p>
<p><span class="font64">Vectors can be thought of as matrices that contain only one column. The transpose of a vector is therefore a matrix with only one row. Sometimes we&#160;define a vector by writing out its elements in the text inline as a row matrix,&#160;then using the transpose operator to turn it into a standard column vector, e.g.,</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">X</span><span class="font64"> = [X1,X2,X3] <sup>T</sup>.</span></p>
<p><span class="font64">A scalar can be thought of as a matrix with only a single entry. From this, we can see that a scalar is its own transpose: a </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> a<sup>T</sup>.</span></p>
<p><span class="font64">We can add matrices to each other, as long as they have the same shape, just by adding their corresponding elements: C = A + B where C<sub>i;</sub>j </span><span class="font64" style="font-weight:bold;font-style:italic;">= A<sub>i7</sub>j</span><span class="font64"> + B</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>i7</sub>j.</span></p>
<p><span class="font64">We can also add a scalar to a matrix or multiply a matrix by a scalar, just by performing that operation on each element of a matrix: </span><span class="font64" style="font-weight:bold;font-style:italic;">D = a • B</span><span class="font64"> + c where&#160;<sup>D</sup>i,j — <sup>a</sup> • <sup>B</sup>i,j + <sup>c</sup>•</span></p>
<p><span class="font64">In the context of deep learning, we also use some less conventional notation. We allow the addition of matrix and a vector, yielding another matrix: C — A + b,&#160;where C<sub>i;</sub>j — A<sub>i;</sub>j + </span><span class="font64" style="font-weight:bold;font-style:italic;">bj</span><span class="font65" style="font-style:italic;">.</span><span class="font63"> In other words, the vector b is added to each row of the&#160;matrix. This shorthand eliminates the need to define a matrix with b copied into&#160;each row before doing the addition. This implicit copying of b to many locations&#160;is called </span><span class="font64" style="font-weight:bold;font-style:italic;">broadcasting</span><span class="font64">.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">2.2 Multiplying Matrices and Vectors</span></h4>
<p><span class="font64">One of the most important operations involving matrices is multiplication of two matrices. The </span><span class="font64" style="font-weight:bold;font-style:italic;">matrix product</span><span class="font64"> of matrices A and B is a third matrix C. In order&#160;for this product to be defined, A must have the same number of columns as B has&#160;rows. If A is of shape </span><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font64"> x n and B is of shape n x p, then C is of shape </span><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font64"> x p.&#160;We can write the matrix product just by placing two or more matrices together,</span></p><div>
<p><span class="font64">e.g.</span></p></div><div>
<p><span class="font64">C — AB.</span></p></div><div>
<p><span class="font64">(2.4)</span></p></div>
<p><span class="font64">The product operation is defined by</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>C</sup>i,j &#160;&#160;&#160;<sup>A</sup>i,k<sup>B</sup>k,j.</span><span class="font64">&#160;&#160;&#160;&#160;<sup>(2</sup>-<sup>5)</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">k</span></p>
<p><span class="font64">Note that the standard product of two matrices is </span><span class="font64" style="font-weight:bold;font-style:italic;">not</span><span class="font64"> just a matrix containing the product of the individual elements. Such an operation exists and is called the&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">element-wise product</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">Hadamard product,</span><span class="font64"> and is denoted as A 0 B.</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">dot product</span><span class="font64"> between two vectors x and y of the same dimensionality is the matrix product x<sup>T</sup>y. We can think of the matrix product C — AB as computing&#160;C<sub>i;</sub>j as the dot product between row </span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> of A and column j of B.</span></p>
<p><span class="font64">Matrix product operations have many useful properties that make mathematical analysis of matrices more convenient. For example, matrix multiplication is&#160;distributive:</span></p>
<p><span class="font64">This allows us to demonstrate Eq. 2.8, by exploiting the fact that the value of such a product is a scalar and therefore equal to its own transpose:</span></p>
<p><span class="font64">xy = (x <sup>T</sup>y) = y<sup>T</sup> x. &#160;&#160;&#160;(2.10)</span></p>
<p><span class="font64">Since the focus of this textbook is not linear algebra, we do not attempt to develop a comprehensive list of useful properties of the matrix product here, but&#160;the reader should be aware that many more exist.</span></p>
<p><span class="font64">We now know enough linear algebra notation to write down a system of linear equations:</span></p><div>
<p><span class="font64">(2.11)</span></p></div>
<p><span class="font64">Ax = b</span></p>
<p><span class="font64">where A </span><span class="font64" style="font-weight:bold;">G </span><span class="font64">R<sup>mxn</sup> is a known matrix, b </span><span class="font64" style="font-weight:bold;">G </span><span class="font64">R<sup>m</sup> is a known vector, and x </span><span class="font64" style="font-weight:bold;">G </span><span class="font64">R<sup>n</sup> is a vector of unknown variables we would like to solve for. Each element x<sub>i</sub> of x is one&#160;of these unknown variables. Each row of A and each element of b provide another&#160;constraint. We can rewrite Eq. 2.11 as:</span></p><div>
<table border="1">
<tr><td>
<p><span class="font64" style="font-weight:bold;">A</span><span class="font64">1,:</span></p></td><td>
<p><span class="font64" style="font-weight:bold;">x </span><span class="font64">=</span></p></td><td>
<p><span class="font64">- </span><span class="font64" style="font-weight:bold;font-style:italic;">b1</span></p></td><td>
<p><span class="font64">(2.12)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64" style="font-weight:bold;"><sup>A</sup></span><span class="font64">2,:</span></p></td><td style="vertical-align:middle;">
<p><span class="font64" style="font-weight:bold;">x </span><span class="font64">=</span></p></td><td style="vertical-align:middle;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">= b2</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(2.13)</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(2.14)</span></p></td></tr>
<tr><td>
<p><span class="font64" style="font-weight:bold;font-style:italic;">A</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>A</sup>m,</span></p></td><td>
<p><span class="font64"><sub>:</sub> </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">=</span></p></td><td style="vertical-align:middle;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">- <sup>b</sup>m</span></p></td><td>
<p><span class="font64">(2.15)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">,2X2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">+ ■</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">' ' + </span><span class="font64" style="font-weight:bold;"><sup>A</sup> </span><span class="font64">1,n<sup>x</sup>n = </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>b</sup>1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(2.16)</span></p></td></tr>
</table></div>
<p><span class="font64">or, even more explicitly, as:</span></p>
<p><span class="font64">1 0 0 0 1 0&#160;0 0 1</span></p>
<p><span class="font64">Figure 2.2: </span><span class="font64" style="font-style:italic;">Example identity matrix:</span><span class="font64"> This is I.</span></p>
<p><span class="font64" style="font-weight:bold;">A</span><span class="font64">2,1 X</span><span class="font18">1</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;">A</span><span class="font64">2,2x2 +-----+ </span><span class="font64" style="font-weight:bold;font-style:italic;">A 2,nXn</span><span class="font64"> = &amp;2 &#160;&#160;&#160;(2-17)</span></p>
<p><span class="font64">... (2.18) </span><span class="font64" style="font-weight:bold;"><sup>A</sup></span><span class="font63">m,1<sup>x</sup>1 + </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>A</sup>m,</span><span class="font64">2<sup>x</sup>2 + ' ' ' + </span><span class="font64" style="font-weight:bold;"><sup>A</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">m,n<sup>x</sup>n = fym •</span><span class="font64">&#160;&#160;&#160;&#160;<sup>(2</sup>-<sup>19)</sup></span></p>
<p><span class="font64">Matrix-vector product notation provides a more compact representation for equations of this form.</span></p><h4><a id="bookmark3"></a><span class="font65" style="font-weight:bold;">2.3 Identity and Inverse Matrices</span></h4>
<p><span class="font64">Linear algebra offers a powerful tool called </span><span class="font64" style="font-weight:bold;font-style:italic;">matrix inversion</span><span class="font64"> that allows us to analytically solve Eq. 2.11 for many values of A.</span></p>
<p><span class="font64">To describe matrix inversion, we first need to define the concept of an </span><span class="font64" style="font-weight:bold;font-style:italic;">identity matrix.</span><span class="font64"> An identity matrix is a matrix that does not change any vector when we&#160;multiply that vector by that matrix. We denote the identity matrix that preserves&#160;n-dimensional vectors as </span><span class="font64" style="font-weight:bold;font-style:italic;">I<sub>n</sub>.</span><span class="font64"> Formally, </span><span class="font64" style="font-weight:bold;font-style:italic;">I<sub>n</sub></span><span class="font64"> G R<sup>nxn</sup>, and</span></p>
<p><span class="font64">Vx G R<sup>n</sup>, I</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>n</sub>x = x.</span><span class="font64"> &#160;&#160;&#160;(2.20)</span></p>
<p><span class="font64">The structure of the identity matrix is simple: all of the entries along the main diagonal are 1, while all of the other entries are zero. See Fig. 2.2 for an example.</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">matrix inverse</span><span class="font64"> of A is denoted as A<sup>-1</sup>, and it is defined as the matrix such that</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">A<sup>-1</sup> A</span><span class="font64"> = In.</span></p>
<p><span class="font64">We can now solve Eq. 2.11 by the following steps:</span></p></td><td>
<p><span class="font64">(2.21)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">Ax = b</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(2.22)</span></p></td></tr>
<tr><td>
<p><span class="font64" style="font-weight:bold;font-style:italic;">A<sup>-1</sup> Ax = A<sup>-1</sup>b</span></p></td><td>
<p><span class="font64">(2.23)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">I<sub>n</sub> x =</span><span class="font64"> A<sup>-1</sup> b</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(2.24)</span></p></td></tr>
</table>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x =</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">A</span><span class="font64"><sup>-1</sup></span><span class="font64" style="font-weight:bold;">b</span><span class="font64">. &#160;&#160;&#160;(2.25)</span></p>
<p><span class="font64">Of course, this depends on it being possible to find </span><span class="font64" style="font-weight:bold;">A</span><span class="font64"><sup>-1</sup>. We discuss the conditions for the existence of </span><span class="font64" style="font-weight:bold;">A</span><span class="font64"><sup>-1</sup> in the following section.</span></p>
<p><span class="font64">When </span><span class="font64" style="font-weight:bold;">A</span><span class="font64"><sup>-1</sup> exists, several different algorithms exist for finding it in closed form. In theory, the same inverse matrix can then be used to solve the equation many&#160;times for different values of </span><span class="font64" style="font-weight:bold;">b</span><span class="font64">. However, </span><span class="font64" style="font-weight:bold;">A </span><span class="font64"><sup>-1</sup> is primarily useful as a theoretical&#160;tool, and should not actually be used in practice for most software applications.&#160;Because </span><span class="font64" style="font-weight:bold;">A</span><span class="font64"><sup>-1</sup> can be represented with only limited precision on a digital computer,&#160;algorithms that make use of the value of </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">can usually obtain more accurate&#160;estimates of </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">2.4 Linear Dependence and Span</span></h4>
<p><span class="font64">In order for </span><span class="font64" style="font-weight:bold;">A</span><span class="font64"><sup>-1</sup> to exist, Eq. 2.11 must have exactly one solution for every value of </span><span class="font64" style="font-weight:bold;">b</span><span class="font64">. However, it is also possible for the system of equations to have no solutions&#160;or infinitely many solutions for some values of </span><span class="font64" style="font-weight:bold;">b</span><span class="font64">. It is not possible to have more&#160;than one but less than infinitely many solutions for a particular </span><span class="font64" style="font-weight:bold;">b</span><span class="font64">; if both </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">and </span><span class="font64" style="font-weight:bold;">y&#160;</span><span class="font64">are solutions then</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">z = ax</span><span class="font64"> + (1 — </span><span class="font64" style="font-weight:bold;font-style:italic;">a)y</span><span class="font64"> &#160;&#160;&#160;(2.26)</span></p>
<p><span class="font64">is also a solution for any real a.</span></p>
<p><span class="font64">To analyze how many solutions the equation has, we can think of the columns of </span><span class="font64" style="font-weight:bold;">A </span><span class="font64">as specifying different directions we can travel from the </span><span class="font64" style="font-weight:bold;font-style:italic;">origin</span><span class="font64"> (the point&#160;specified by the vector of all zeros), and determine how many ways there are of&#160;reaching </span><span class="font64" style="font-weight:bold;">b</span><span class="font64">. In this view, each element of </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">specifies how far we should travel in&#160;each of these directions, with x<sub>i</sub> specifying how far to move in the direction of&#160;column </span><span class="font64" style="font-weight:bold;font-style:italic;">i:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Ax</span><span class="font64"> = ^ x</span><span class="font64" style="font-weight:bold;font-style:italic;">iA</span><span class="font64">:,i. &#160;&#160;&#160;(2.27)</span></p>
<p><span class="font64">i</span></p>
<p><span class="font64">In general, this kind of operation is called a </span><span class="font64" style="font-weight:bold;font-style:italic;">linear combination.</span><span class="font64"> Formally, a linear combination of some set of vectors {</span><span class="font64" style="font-weight:bold;">v</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;">v</span><span class="font64"><sup>(n)</sup>} is given by multiplying each&#160;vector </span><span class="font64" style="font-weight:bold;">v</span><span class="font64"><sup>(i)</sup> by a corresponding scalar coefficient and adding the results:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">J^ci v</span><span class="font64"><sup>(i)</sup>. &#160;&#160;&#160;(2.28)</span></p>
<p><span class="font64">i</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">span</span><span class="font64"> of a set of vectors is the set of all points obtainable by linear combination of the original vectors.</span></p>
<p><span class="font64">Determining whether </span><span class="font64" style="font-weight:bold;font-style:italic;">Ax = b</span><span class="font64"> has a solution thus amounts to testing whether </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">is in the span of the columns of </span><span class="font64" style="font-weight:bold;">A</span><span class="font64">. This particular span is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">column&#160;space</span><span class="font64"> or the </span><span class="font64" style="font-weight:bold;font-style:italic;">range</span><span class="font64"> of </span><span class="font64" style="font-weight:bold;">A</span><span class="font64">.</span></p>
<p><span class="font64">In order for the system </span><span class="font64" style="font-weight:bold;">Ax </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">to have a solution for all values of </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">G R<sup>m</sup>, we therefore require that the column space of </span><span class="font64" style="font-weight:bold;">A </span><span class="font64">be all of R<sup>m</sup>. If any point inR </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>m&#160;</sup></span><span class="font64">is excluded from the column space, that point is a potential value of </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">that has&#160;no solution. The requirement that the column space of </span><span class="font64" style="font-weight:bold;">A </span><span class="font64">be all of R<sup>m</sup> implies&#160;immediately that </span><span class="font64" style="font-weight:bold;">A </span><span class="font64">must have at least m columns, i.e., </span><span class="font64" style="font-weight:bold;font-style:italic;">n &gt; m.</span><span class="font64"> Otherwise, the&#160;dimensionality of the column space would be less than m. For example, consider a&#160;3 x 2 matrix. The target </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">is 3-D, but </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">is only 2-D, so modifying the value of </span><span class="font64" style="font-weight:bold;">x&#160;</span><span class="font64">at best allows us to trace out a 2-D plane within R <sup>3</sup> The equation has a solution&#160;if and only if </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">lies on that plane.</span></p>
<p><span class="font64">Having n &gt; m is only a necessary condition for every point to have a solution. It is not a sufficient condition, because it is possible for some of the columns to&#160;be redundant. Consider a 2 x 2 matrix where both of the columns are identical.&#160;This has the same column space as a 2 x 1 matrix containing only one copy of the&#160;replicated column. In other words, the column space is still just a line, and fails to&#160;encompass all of R<sup>2</sup>, even though there are two columns.</span></p>
<p><span class="font64">Formally, this kind of redundancy is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">linear dependence.</span><span class="font64"> A set of vectors is </span><span class="font64" style="font-weight:bold;font-style:italic;">linearly independent</span><span class="font64"> if no vector in the set is a linear combination of the&#160;other vectors. If we add a vector to a set that is a linear combination of the other&#160;vectors in the set, the new vector does not add any points to the set’s span. This&#160;means that for the column space of the matrix to encompass all of R<sup>m</sup>, the matrix&#160;must contain at least one set of m linearly independent columns. This condition&#160;is both necessary and sufficient for Eq. 2.11 to have a solution for every value of&#160;</span><span class="font64" style="font-weight:bold;">b</span><span class="font64">. Note that the requirement is for a set to have exactly m linear independent&#160;columns, not at least m. No set of m-dimensional vectors can have more than m&#160;mutually linearly independent columns, but a matrix with more than m columns&#160;may have more than one such set.</span></p>
<p><span class="font64">In order for the matrix to have an inverse, we additionally need to ensure that Eq. 2.11 has </span><span class="font64" style="font-weight:bold;font-style:italic;">at most</span><span class="font64"> one solution for each value of </span><span class="font64" style="font-weight:bold;">b</span><span class="font64">. To do so, we need to ensure&#160;that the matrix has at most m columns. Otherwise there is more than one way of&#160;parametrizing each solution.</span></p>
<p><span class="font64">Together, this means that the matrix must be </span><span class="font64" style="font-weight:bold;font-style:italic;">square,</span><span class="font64"> that is, we require that m = n and that all of the columns must be linearly independent. A square matrix&#160;with linearly dependent columns is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">singular.</span></p>
<p><span class="font64">If </span><span class="font64" style="font-weight:bold;">A </span><span class="font64">is not square or is square but singular, it can still be possible to solve the equation. However, we can not use the method of matrix inversion to find the&#160;solution.</span></p>
<p><span class="font64">So far we have discussed matrix inverses as being multiplied on the left. It is also possible to define an inverse that is multiplied on the right:</span></p>
<p><span class="font64">AA<sup>-1</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">= I</span><span class="font64">. &#160;&#160;&#160;(2.29)</span></p>
<p><span class="font64">For square matrices, the left inverse and right inverse are equal.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">2.5 Norms</span></h4>
<p><span class="font64">Sometimes we need to measure the size of a vector. In machine learning, we usually measure the size of vectors using a function called a </span><span class="font64" style="font-weight:bold;font-style:italic;">norm.</span><span class="font64"> Formally, the L<sup>p</sup> norm&#160;is given by</span></p><div>
<p><span class="font64">x</span></p></div><div>
<p><span class="font63"><sup>p</sup></span></p></div><div>
<p dir="rtl"><span class="font67" style="font-weight:bold;">(״?)</span></p></div><div>
<p><span class="font64">(2.30)</span></p></div>
<p><span class="font64">for p e R,p &gt; 1.</span></p>
<p><span class="font64">Norms, including the L<sup>p</sup> norm, are functions mapping vectors to non-negative values. On an intuitive level, the norm of a vector x measures the distance from&#160;the origin to the point x. More rigorously, a norm is any function f that satisfies&#160;the following properties:</span></p>
<p><span class="font64">• &#160;&#160;&#160;f (x) = 0 ^ x = 0</span></p>
<p><span class="font64">• f (x + y) </span><span class="font64" style="font-weight:bold;font-style:italic;">&lt; f</span><span class="font64"> (x) + f (y) (the </span><span class="font64" style="font-weight:bold;font-style:italic;">triangle inequality)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">• Va</span><span class="font64"> e R, f (ax) = |a|f (x)</span></p>
<p><span class="font64">The L<sup>2</sup> norm, with p = 2, is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">Euclidean norm.</span><span class="font64"> It is simply the Euclidean distance from the origin to the point identified by x. The L<sup>2</sup> norm is&#160;used so frequently in machine learning that it is often denoted simply as ||x||, with&#160;the subscript 2 omitted. It is also common to measure the size of a vector using&#160;the squared L<sup>2</sup> norm, which can be calculated simply as x<sup>T</sup>x.</span></p>
<p><span class="font64">The squared L<sup>2</sup> norm is more convenient to work with mathematically and computationally than the L<sup>2</sup> norm itself. For example, the derivatives of the&#160;squared L<sup>2</sup> norm with respect to each element of x each depend only on the&#160;corresponding element of x, while all of the derivatives of the </span><span class="font64" style="font-weight:bold;font-style:italic;">L</span><span class="font64"> norm depend&#160;on the entire vector. In many contexts, the squared L<sup>2</sup> norm may be undesirable</span></p>
<p><span class="font64">because it increases very slowly near the origin. In several machine learning applications, it is important to discriminate between elements that are exactly&#160;zero and elements that are small but nonzero. In these cases, we turn to a function&#160;that grows at the same rate in all locations, but retains mathematical simplicity:&#160;the L<sup>1</sup> norm. The L<sup>1</sup> norm may be simplified to</span></p><div>
<p><span class="font64"><sup>x</sup>1</span></p></div><div>
<p><span class="font65" style="font-style:italic;">Y</span><span class="font63"> <sup>|x</sup>i 1•</span></p></div><div>
<p><span class="font64">(2.31)</span></p></div>
<p><span class="font64">The L<sup>1</sup> norm is commonly used in machine learning when the difference between zero and nonzero elements is very important. Every time an element of x moves&#160;away from 0 by e, the L<sup>1</sup> norm increases by e.</span></p>
<p><span class="font64">We sometimes measure the size of the vector by counting its number of nonzero elements. Some authors refer to this function as the “L<sup>0</sup> norm,” but this is incorrect&#160;terminology. The number of non-zero entries in a vector is not a norm, because&#160;scaling the vector by a does not change the number of nonzero entries. The L<sup>1&#160;</sup>norm is often used as a substitute for the number of nonzero entries.</span></p>
<p><span class="font64">One other norm that commonly arises in machine learning is the L<sup>M</sup> norm, also known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">max norm.</span><span class="font64"> This norm simplifies to the absolute value of the&#160;element with the largest magnitude in the vector,</span></p><div>
<p><span class="font64">x</span></p></div><div>
<p><span class="font64">max </span><span class="font64" style="font-weight:bold;font-style:italic;">\x<sub>i</sub></span><span class="font64"> |•</span></p></div><div>
<p><span class="font64">(2.32)</span></p></div><div>
<p><span class="font63">i</span></p></div>
<p><span class="font64">Sometimes we may also wish to measure the size of a matrix. In the context of deep learning, the most common way to do this is with the otherwise obscure&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">Frobenius norm</span></p>
<p><span class="font64">(2.33)</span></p>
<p><span class="font64">which is analogous to the L<sup>2</sup> norm of a vector.</span></p>
<p><span class="font64">The dot product of two vectors can be rewritten in terms of norms. Specifically,</span></p>
<p><span class="font64">x<sup>T</sup> y = ||x||</span><span class="font18">2</span><span class="font64">||y|| </span><span class="font18">2</span><span class="font64"> cos 6 &#160;&#160;&#160;(2.34)</span></p>
<p><span class="font64">where 6 is the angle between x and y.</span></p><h4><a id="bookmark6"></a><span class="font65" style="font-weight:bold;">2.6 Special Kinds of Matrices and Vectors</span></h4>
<p><span class="font64">Some special kinds of matrices and vectors are particularly useful.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Diagonal</span><span class="font64"> matrices consist mostly of zeros and have non-zero entries only along the main diagonal. Formally, a matrix D is diagonal if and only if </span><span class="font64" style="font-weight:bold;font-style:italic;">D<sub>i</sub>,j =</span><span class="font64"> 0 for&#160;all </span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> = j. We have already seen one example of a diagonal matrix: the identity&#160;matrix, where all of the diagonal entries are 1. We write diag(v) to denote a square&#160;diagonal matrix whose diagonal entries are given by the entries of the vector v.&#160;Diagonal matrices are of interest in part because multiplying by a diagonal matrix&#160;is very computationally efficient. To compute diag(v)x, we only need to scale each&#160;element </span><span class="font64" style="font-weight:bold;font-style:italic;">x<sub>i</sub></span><span class="font64"> by v<sub>i</sub>. In other words, diag( v) </span><span class="font64" style="font-weight:bold;font-style:italic;">x = v 0 x.</span><span class="font64"> Inverting a square diagonal&#160;matrix is also efficient. The inverse exists only if every diagonal entry is nonzero,&#160;and in that case, diag(v)<sup>-1</sup> = diag([1/v </span><span class="font18">1</span><span class="font64">,..., </span><span class="font64" style="font-weight:bold;font-style:italic;">1/v<sub>n</sub></span><span class="font64">]<sup>T</sup>). In many cases, we may&#160;derive some very general machine learning algorithm in terms of arbitrary matrices,&#160;but obtain a less expensive (and less descriptive) algorithm by restricting some&#160;matrices to be diagonal.</span></p>
<p><span class="font64">Not all diagonal matrices need be square. It is possible to construct a rectangular diagonal matrix. Non-square diagonal matrices do not have inverses but it is still&#160;possible to multiply by them cheaply. For a non-square diagonal matrix </span><span class="font64" style="font-weight:bold;font-style:italic;">D,</span><span class="font64"> the&#160;product Dx will involve scaling each element of x, and either concatenating some&#160;zeros to the result if D is taller than it is wide, or discarding some of the last&#160;elements of the vector if D is wider than it is tall.</span></p>
<p><span class="font64">A </span><span class="font64" style="font-weight:bold;font-style:italic;">symmetric</span><span class="font64"> matrix is any matrix that is equal to its own transpose:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">A = A</span><span class="font64"> <sup>T</sup>. &#160;&#160;&#160;(2.35)</span></p>
<p><span class="font64">Symmetric matrices often arise when the entries are generated by some function of two arguments that does not depend on the order of the arguments. For example,&#160;if A is a matrix of distance measurements, with A<sub>i;j</sub>■ giving the distance from point&#160;i to point j, then Aj j = A<sub>j;i</sub> because distance functions are symmetric.</span></p>
<p><span class="font64">A </span><span class="font64" style="font-weight:bold;font-style:italic;">unit vector</span><span class="font64"> is a vector with </span><span class="font64" style="font-weight:bold;font-style:italic;">unit norm:</span></p>
<p><span class="font64">||x</span><span class="font18">||2</span><span class="font64"> = 1. &#160;&#160;&#160;(2.36)</span></p>
<p><span class="font64">A vector x and a vector y are </span><span class="font64" style="font-weight:bold;font-style:italic;">orthogonal</span><span class="font64"> to each other if x<sup>T</sup>y = 0. If both vectors have nonzero norm, this means that they are at a 90 degree angle to each&#160;other. In R<sup>n</sup> ,at most </span><span class="font64" style="font-weight:bold;font-style:italic;">n</span><span class="font64"> vectors may be mutually orthogonal with nonzero norm.&#160;If the vectors are not only orthogonal but also have unit norm, we call them&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">orthonormal.</span></p>
<p><span class="font64">An </span><span class="font64" style="font-weight:bold;font-style:italic;">orthogonal matrix</span><span class="font64"> is a square matrix whose rows are mutually orthonormal and whose columns are mutually orthonormal:</span></p>
<p><span class="font64" style="font-variant:small-caps;">A<sup>t</sup> A = AA<sup>t</sup> = I. &#160;&#160;&#160;(2.37)</span></p>
<p><span class="font64">This implies that</span></p>
<p><span class="font64" style="font-variant:small-caps;">A<sup>-1</sup> = A<sup>t</sup> , &#160;&#160;&#160;(2.38)</span></p>
<p><span class="font64">so orthogonal matrices are of interest because their inverse is very cheap to compute. Pay careful attention to the definition of orthogonal matrices. Counterintuitively,&#160;their rows are not merely orthogonal but fully orthonormal. There is no special&#160;term for a matrix whose rows or columns are orthogonal but not orthonormal.</span></p><h4><a id="bookmark7"></a><span class="font65" style="font-weight:bold;">2.7 Eigendecomposition</span></h4>
<p><span class="font64">Many mathematical objects can be understood better by breaking them into constituent parts, or finding some properties of them that are universal, not caused&#160;by the way we choose to represent them.</span></p>
<p><span class="font64">For example, integers can be decomposed into prime factors. The way we represent the number 12 will change depending on whether we write it in base ten&#160;or in binary, but it will always be true that 12 = 2 x 2 x 3. From this representation&#160;we can conclude useful properties, such as that 12 is not divisible by 5, or that any&#160;integer multiple of 12 will be divisible by 3.</span></p>
<p><span class="font64">Much as we can discover something about the true nature of an integer by decomposing it into prime factors, we can also decompose matrices in ways that&#160;show us information about their functional properties that is not obvious from the&#160;representation of the matrix as an array of elements.</span></p>
<p><span class="font64">One of the most widely used kinds of matrix decomposition is called </span><span class="font64" style="font-weight:bold;font-style:italic;">eigen-decomposition,</span><span class="font64"> in which we decompose a matrix into a set of eigenvectors and eigenvalues.</span></p>
<p><span class="font64">An </span><span class="font64" style="font-weight:bold;font-style:italic;">eigenvector</span><span class="font64"> of a square matrix A is a non-zero vector v such that multiplication by A alters only the scale of v:</span></p>
<p><span class="font64">Av = Av. &#160;&#160;&#160;(2.39)</span></p>
<p><span class="font64">The scalar A is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">eigenvalue</span><span class="font64"> corresponding to this eigenvector. (One can also find a </span><span class="font64" style="font-weight:bold;font-style:italic;">left eigenvector</span><span class="font64"> such that v</span><span class="font64" style="font-variant:small-caps;"><sup>t</sup></span><span class="font64"> A = Av<sup>T</sup>, but we are usually concerned&#160;with right eigenvectors).</span></p>
<p><span class="font64">If v is an eigenvector of A, then so is any rescaled vector </span><span class="font64" style="font-weight:bold;font-style:italic;">sv</span><span class="font64"> for s </span><span class="font64" style="font-weight:bold;">G </span><span class="font64">R, s = 0. Moreover, sv still has the same eigenvalue. For this reason, we usually only look&#160;for unit eigenvectors.</span></p>
<p><span class="font64">Suppose that a matrix A has n linearly independent eigenvectors, </span><span class="font64" style="font-weight:bold;">{</span><span class="font64">v<sup>(1)</sup>,..., v<sup>(n)</sup></span><span class="font64" style="font-weight:bold;">}</span><span class="font64">, with corresponding eigenvalues </span><span class="font64" style="font-weight:bold;">{</span><span class="font64">A</span><span class="font18">1</span><span class="font64">,..., A<sub>n</sub></span><span class="font64" style="font-weight:bold;">}</span><span class="font64">. We may concatenate all of the</span></p>
<p><span class="font64">Effect of eigenvectors and eigenvalues</span></p><div><div>
<p><span class="font64"><sub>Q</sub> Before multiplication</span></p><img src="main-19.jpg" alt=""/>
<p><span class="font62" style="font-style:italic;"><sup>x</sup> 0</span></p></div></div><div><div>
<p><span class="font64">After multiplication</span></p><img src="main-20.jpg" alt=""/></div></div>
<p><span class="font64">Figure 2.3: An example of the effect of eigenvectors and eigenvalues. Here, we have a matrix A with two orthonormal eigenvectors, v<sup>(1)</sup> with eigenvalue A! and v<sup>(2)</sup> with&#160;eigenvalue A<sub>2</sub>. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> We plot the set of all unit vectors </span><span class="font64" style="font-style:italic;">u E</span><span class="font64"> R<sup>2</sup> as a unit circle. </span><span class="font64" style="font-style:italic;">(Right)&#160;</span><span class="font64">We plot the set of all points Au. By observing the way that A distorts the unit circle, we&#160;can see that it scales space in direction v<sup>(i)</sup> by A</span><span class="font64" style="font-style:italic;">i.</span></p>
<p><span class="font64">eigenvectors to form a matrix </span><span class="font64" style="font-weight:bold;">V </span><span class="font64">with one eigenvector per column: </span><span class="font64" style="font-weight:bold;">V </span><span class="font64" style="font-weight:bold;font-style:italic;">= </span><span class="font64" style="font-weight:bold;">[v</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;">v</span><span class="font64"><sup>(n)</sup>]. Likewise, we can concatenate the eigenvalues to form a vector </span><span class="font64" style="font-weight:bold;">A </span><span class="font64">= [A<sub>1</sub>,...,&#160;A<sub>n</sub>]<sup>T</sup>. The </span><span class="font64" style="font-weight:bold;font-style:italic;">eigendecomposition</span><span class="font64"> of </span><span class="font64" style="font-weight:bold;">A </span><span class="font64">is then given by</span></p>
<p><span class="font64" style="font-weight:bold;">A </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">V </span><span class="font64">diag(</span><span class="font64" style="font-weight:bold;">A</span><span class="font64">) </span><span class="font64" style="font-weight:bold;">V</span><span class="font64"><sup>-1</sup>. &#160;&#160;&#160;(2.40)</span></p>
<p><span class="font64">We have seen that </span><span class="font64" style="font-weight:bold;font-style:italic;">constructing</span><span class="font64"> matrices with specific eigenvalues and eigenvectors allows us to stretch space in desired directions. However, we often want to </span><span class="font64" style="font-weight:bold;font-style:italic;">decompose</span><span class="font64"> matrices into their eigenvalues and eigenvectors. Doing so can help us&#160;to analyze certain properties of the matrix, much as decomposing an integer into&#160;its prime factors can help us understand the behavior of that integer.</span></p>
<p><span class="font64">Not every matrix can be decomposed into eigenvalues and eigenvectors. In some cases, the decomposition exists, but may involve complex rather than real numbers.&#160;Fortunately, in this book, we usually need to decompose only a specific class of&#160;matrices that have a simple decomposition. Specifically, every real symmetric&#160;matrix can be decomposed into an expression using only real-valued eigenvectors&#160;and eigenvalues:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">A =</span><span class="font64"> QAQ<sup>t</sup>, &#160;&#160;&#160;(2.41)</span></p>
<p><span class="font64">where Q is an orthogonal matrix composed of eigenvectors of A, and A is a diagonal matrix. The eigenvalue A%,% is associated with the eigenvector in column i&#160;of Q, denoted as </span><span class="font64" style="font-weight:bold;font-style:italic;">Q,%.</span><span class="font64"> Because Q is an orthogonal matrix, we can think of A as&#160;scaling space by A </span><span class="font64" style="font-weight:bold;font-style:italic;">%</span><span class="font64"> in direction v<sup>(i)</sup>. See Fig. 2.3 for an example.</span></p>
<p><span class="font64">While any real symmetric matrix A is guaranteed to have an eigendecomposi-tion, the eigendecomposition may not be unique. If any two or more eigenvectors share the same eigenvalue, then any set of orthogonal vectors lying in their span&#160;are also eigenvectors with that eigenvalue, and we could equivalently choose a Q&#160;using those eigenvectors instead. By convention, we usually sort the entries of A&#160;in descending order. Under this convention, the eigendecomposition is unique only&#160;if all of the eigenvalues are unique.</span></p>
<p><span class="font64">The eigendecomposition of a matrix tells us many useful facts about the matrix. The matrix is singular if and only if any of the eigenvalues are zero.&#160;The eigendecomposition of a real symmetric matrix can also be used to optimize&#160;quadratic expressions of the form f (</span><span class="font64" style="font-weight:bold;font-style:italic;">x)</span><span class="font64"> = x<sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">Ax</span><span class="font64"> subject to ||x||<sub>2</sub> = 1. Whenever </span><span class="font64" style="font-weight:bold;font-style:italic;">x&#160;</span><span class="font64">is equal to an eigenvector of A, f takes on the value of the corresponding eigenvalue.&#160;The maximum value of f within the constraint region is the maximum eigenvalue&#160;and its minimum value within the constraint region is the minimum eigenvalue.</span></p>
<p><span class="font64">A matrix whose eigenvalues are all positive is called </span><span class="font64" style="font-weight:bold;font-style:italic;">positive definite.</span><span class="font64"> A matrix whose eigenvalues are all positive or zero-valued is called </span><span class="font64" style="font-weight:bold;font-style:italic;">positive semidefinite.&#160;</span><span class="font64">Likewise, if all eigenvalues are negative, the matrix is </span><span class="font64" style="font-weight:bold;font-style:italic;">negative definite,</span><span class="font64"> and if&#160;all eigenvalues are negative or zero-valued, it is </span><span class="font64" style="font-weight:bold;font-style:italic;">negative semidefinite.</span><span class="font64"> Positive&#160;semidefinite matrices are interesting because they guarantee that Vx, xAx &gt; 0.&#160;Positive definite matrices additionally guarantee that </span><span class="font64" style="font-weight:bold;font-style:italic;">x Ax =</span><span class="font64"> 0 ^ x = 0.</span></p><h4><a id="bookmark8"></a><span class="font65" style="font-weight:bold;">2.8 Singular Value Decomposition</span></h4>
<p><span class="font64">In Sec. 2.7, we saw how to decompose a matrix into eigenvectors and eigenvalues. The </span><span class="font64" style="font-weight:bold;font-style:italic;">singular value decomposition</span><span class="font64"> (SVD) provides another way to factorize a matrix,&#160;into </span><span class="font64" style="font-weight:bold;font-style:italic;">singular vectors</span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">singular values.</span><span class="font64"> The SVD allows us to discover some of&#160;the same kind of information as the eigendecomposition. However, the SVD is&#160;more generally applicable. Every real matrix has a singular value decomposition,&#160;but the same is not true of the eigenvalue decomposition. For example, if a matrix&#160;is not square, the eigendecomposition is not defined, and we must use a singular&#160;value decomposition instead.</span></p>
<p><span class="font64">Recall that the eigendecomposition involves analyzing a matrix A to discover a matrix V of eigenvectors and a vector of eigenvalues A such that we can rewrite&#160;A as</span></p>
<p><span class="font64">A = V diag(A) V<sup>-1</sup>. &#160;&#160;&#160;(2.42)</span></p>
<p><span class="font64">The singular value decomposition is similar, except this time we will write A as a product of three matrices:</span></p>
<p><span class="font64">A = </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">UDV<sup>t</sup></span><span class="font64"> . &#160;&#160;&#160;(2.43)</span></p>
<p><span class="font64">Suppose that A is an m x n matrix. Then U is defined to be an m x m matrix, D to be an m x n matrix, and V to be an n x n matrix.</span></p>
<p><span class="font64">Each of these matrices is defined to have a special structure. The matrices U and V are both defined to be orthogonal matrices. The matrix D is defined to be&#160;a diagonal matrix. Note that D is not necessarily square.</span></p>
<p><span class="font64">The elements along the diagonal of D are known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">singular values</span><span class="font64"> of the matrix A. The columns of U are known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">left-singular vectors.</span><span class="font64"> The columns&#160;of V are known as as the </span><span class="font64" style="font-weight:bold;font-style:italic;">right-singular vectors.</span></p>
<p><span class="font64">We can actually interpret the singular value decomposition of A in terms of the eigendecomposition of functions of A. The left-singular vectors of A are the&#160;eigenvectors of AA</span><span class="font64" style="font-variant:small-caps;"><sup>t</sup></span><span class="font64"> . The right-singular vectors of A are the eigenvectors of A</span><span class="font64" style="font-variant:small-caps;"><sup>t</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">A&#160;</span><span class="font64">The non-zero singular values of A are the square roots of the eigenvalues of A</span><span class="font64" style="font-variant:small-caps;"><sup>t</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">A&#160;</span><span class="font64">The same is true for AA </span><span class="font64" style="font-variant:small-caps;"><sup>t</sup> .</span></p>
<p><span class="font64">Perhaps the most useful feature of the SVD is that we can use it to partially generalize matrix inversion to non-square matrices, as we will see in the next&#160;section.</span></p><h4><a id="bookmark9"></a><span class="font65" style="font-weight:bold;">2.9 The Moore-Penrose Pseudoinverse</span></h4>
<p><span class="font64">Matrix inversion is not defined for matrices that are not square. Suppose we want to make a left-inverse B of a matrix A, so that we can solve a linear equation</span></p>
<p><span class="font64">Ax = y &#160;&#160;&#160;(2.44)</span></p>
<p><span class="font64">by left-multiplying each side to obtain</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x = By.</span><span class="font64"> &#160;&#160;&#160;(2.45)</span></p>
<p><span class="font64">Depending on the structure of the problem, it may not be possible to design a unique mapping from A to B.</span></p>
<p><span class="font64">If A is taller than it is wide, then it is possible for this equation to have no solution. If A is wider than it is tall, then there could be multiple possible&#160;solutions.</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">Moore-Penrose pseudoinverse</span><span class="font64"> allows us to make some headway in these cases. The pseudoinverse of A is defined as a matrix</span></p>
<p><span class="font64">A+ = lim(A <sup>t</sup>A + al )<sup>-1</sup>A<sup>T</sup>. &#160;&#160;&#160;(2.46)</span></p>
<p><span class="font64">a\0</span></p>
<p><span class="font64">Practical algorithms for computing the pseudoinverse are not based on this definition, but rather the formula</span></p>
<p><span class="font64">A + = VD+ U<sup>T</sup>, &#160;&#160;&#160;(2.47)</span></p>
<p><span class="font64">where U, D and V are the singular value decomposition of A, and the pseudoinverse D+ of a diagonal matrix D is obtained by taking the reciprocal of its non-zero&#160;elements then taking the transpose of the resulting matrix.</span></p>
<p><span class="font64">When A has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions. Specifically, it provides&#160;the solution x = A+ y with minimal Euclidean norm ||x</span><span class="font18">||2</span><span class="font64"> among all possible&#160;solutions.</span></p>
<p><span class="font64">When A has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives us the x for which Ax is as close as&#160;possible to y in terms of Euclidean norm ||Ax — y||</span><span class="font18">2</span><span class="font64">.</span></p><h4><a id="bookmark10"></a><span class="font65" style="font-weight:bold;">2.10 The Trace Operator</span></h4>
<p><span class="font64">The trace operator gives the sum of all of the diagonal entries of a matrix:</span></p>
<p><span class="font64">Tr(A) &#160;&#160;&#160;Am .&#160;&#160;&#160;&#160;(2.48)</span></p>
<p><span class="font63">i</span></p>
<p><span class="font64">The trace operator is useful for a variety of reasons. Some operations that are difficult to specify without resorting to summation notation can be specified using&#160;matrix products and the trace operator. For example, the trace operator provides&#160;an alternative way of writing the Frobenius norm of a matrix:</span></p>
<p><span class="font64">||A|| </span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>F</sub></span><span class="font64"> = ^Tr(AA<sup>T</sup>). &#160;&#160;&#160;(2.49)</span></p>
<p><span class="font64">Writing an expression in terms of the trace operator opens up opportunities to manipulate the expression using many useful identities. For example, the trace&#160;operator is invariant to the transpose operator:</span></p>
<p><span class="font64">Tr(A) = Tr(A<sup>T</sup>). &#160;&#160;&#160;(2.50)</span></p>
<p><span class="font64">The trace of a square matrix composed of many factors is also invariant to moving the last factor into the first position, if the shapes of the corresponding&#160;matrices allow the resulting product to be defined:</span></p>
<p><span class="font64">Tr( ABC) = Tr(CAB) = Tr(BCA) &#160;&#160;&#160;(2.51)</span></p>
<p><span class="font64">or more generally,</span></p>
<p><span class="font64">n &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">n-1</span></p><div>
<p><span class="font64">(2.52)</span></p></div>
<p><span class="font64">Tr(JJ F<sup>(i)</sup>) = Tr(F<sup>(n)</sup> &#160;&#160;&#160;F<sup>(i)</sup>).</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1 &#160;&#160;&#160;i=1</span></p>
<p><span class="font64">This invariance to cyclic permutation holds even if the resulting product has a different shape. For example, for A </span><span class="font64" style="font-weight:bold;">G </span><span class="font64">R<sup>mxn</sup> and B </span><span class="font64" style="font-weight:bold;">G </span><span class="font64">R<sup>nxm</sup>, we have</span></p><div>
<p><span class="font64">(2.53)</span></p></div><div>
<p><span class="font64">Tr(a).</span></p></div>
<p><span class="font64">Tr(AB) = Tr(BA)</span></p>
<p><span class="font64">even though AB </span><span class="font64" style="font-weight:bold;">G </span><span class="font64">R<sup>mxm</sup> and BA </span><span class="font64" style="font-weight:bold;">G </span><span class="font64">R<sup>nxn</sup>.</span></p>
<p><span class="font64">Another useful fact to keep in mind is that a scalar is its own trace: a</span></p><h4><a id="bookmark11"></a><span class="font65" style="font-weight:bold;">2.11 The Determinant</span></h4>
<p><span class="font64">The determinant of a square matrix, denoted det(A), is a function mapping matrices to real scalars. The determinant is equal to the product of all the&#160;eigenvalues of the matrix. The absolute value of the determinant can be thought&#160;of as a measure of how much multiplication by the matrix expands or contracts&#160;space. If the determinant is 0, then space is contracted completely along at least&#160;one dimension, causing it to lose all of its volume. If the determinant is 1, then&#160;the transformation is volume-preserving.</span></p><h4><a id="bookmark12"></a><span class="font65" style="font-weight:bold;">2.12 Example: Principal Components Analysis</span></h4>
<p><span class="font64">One simple machine learning algorithm, </span><span class="font64" style="font-weight:bold;font-style:italic;">principal components analysis</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">PCA</span><span class="font64"> can be derived using only knowledge of basic linear algebra.</span></p>
<p><span class="font64">Suppose we have a collection of m points {</span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(m)</sup>} in </span><span class="font64" style="font-weight:bold;">R</span><span class="font64"><sup>n</sup>. Suppose we would like to apply lossy compression to these points. Lossy compression means&#160;storing the points in a way that requires less memory but may lose some precision.&#160;We would like to lose as little precision as possible.</span></p>
<p><span class="font64">One way we can encode these points is to represent a lower-dimensional version of them. For each point </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(i)</sup> </span><span class="font64" style="font-weight:bold;">e R</span><span class="font64"><sup>n</sup> we will find a corresponding code vector </span><span class="font64" style="font-weight:bold;">c</span><span class="font64"><sup>(i)</sup> </span><span class="font64" style="font-weight:bold;">e R</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>l</sup>.&#160;</span><span class="font64">If l is smaller than n, it will take less memory to store the code points than the&#160;original data. We will want to find some encoding function that produces the code&#160;for an input, f </span><span class="font64" style="font-weight:bold;font-style:italic;">(x) =</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">c</span><span class="font64">, and a decoding function that produces the reconstructed&#160;input given its code, </span><span class="font64" style="font-weight:bold;">x&#160;&#160;&#160;&#160;</span><span class="font64">g<sup>(f (</sup></span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>))</sup>.</span></p>
<p><span class="font64">PCA is defined by our choice of the decoding function. Specifically, to make the decoder very simple, we choose to use matrix multiplication to map the code back&#160;into </span><span class="font64" style="font-weight:bold;">R<sup>n</sup></span><span class="font64">. Let g(</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">) = </span><span class="font64" style="font-weight:bold;font-style:italic;">Dc,</span><span class="font64"> where </span><span class="font64" style="font-weight:bold;">D e R<sup>nx1</sup> </span><span class="font64">is the matrix defining the decoding.</span></p>
<p><span class="font64">Computing the optimal code for this decoder could be a difficult problem. To keep the encoding problem easy, PCA constrains the columns of </span><span class="font64" style="font-weight:bold;">D </span><span class="font64">to be orthogonal&#160;to each other. (Note that </span><span class="font64" style="font-weight:bold;">D </span><span class="font64">is still not technically “an orthogonal matrix” unless&#160;l = n)</span></p>
<p><span class="font64">With the problem as described so far, many solutions are possible, because we can increase the scale of </span><span class="font64" style="font-weight:bold;font-style:italic;">D<sub>:i</sub></span><span class="font64"> if we decrease c<sub>i</sub> proportionally for all points. To give&#160;the problem a unique solution, we constrain all of the columns of </span><span class="font64" style="font-weight:bold;">D </span><span class="font64">to have unit&#160;norm.</span></p>
<p><span class="font64">In order to turn this basic idea into an algorithm we can implement, the first thing we need to do is figure out how to generate the optimal code point </span><span class="font64" style="font-weight:bold;">c</span><span class="font64">* for&#160;each input point </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">. One way to do this is to minimize the distance between the&#160;input point </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">and its reconstruction, g( </span><span class="font64" style="font-weight:bold;">c</span><span class="font64">*). We can measure this distance using a&#160;norm. In the principal components algorithm, we use the L<sup>2</sup> norm:</span></p>
<p><span class="font64" style="font-weight:bold;">c</span><span class="font64">* = argmin||</span><span class="font64" style="font-weight:bold;">x </span><span class="font64">— g(</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">)||<sub>2</sub>. &#160;&#160;&#160;(2.54)</span></p>
<p><span class="font64">c</span></p>
<p><span class="font64">We can switch to the squared </span><span class="font64" style="font-weight:bold;font-style:italic;">L<sup>2</sup></span><span class="font64"> norm instead of the L<sup>2</sup> norm itself, because both are minimized by the same value of </span><span class="font64" style="font-weight:bold;">c</span><span class="font64">. This is because the L<sup>2</sup> norm is nonnegative and the squaring operation is monotonically increasing for non-negative</span></p>
<p><span class="font64">arguments.</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;">c</span><span class="font64">* = argmin</span><span class="font64" style="font-weight:bold;">||x — </span><span class="font64">g(</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">) </span><span class="font64" style="font-weight:bold;">||</span><span class="font64">2.</span></p>
<p><span class="font64">c</span></p></td><td>
<p><span class="font64">(2.55)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">The function being minimized simplifies to</span></p></td><td>
<p></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64"><sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>x </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>—</sup></span><span class="font64"> g<sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>c</sup></span><span class="font64"><sup>))T </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(</sup>x <sup>—</sup></span><span class="font64"> g<sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>c</sup></span><span class="font64"><sup>))</sup></span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(2.56)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">(by the definition of the L<sup>2</sup> norm, Eq. 2.30)</span></p></td><td>
<p></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">= </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;">x — x</span><span class="font64"><sup>T</sup>g(</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">) </span><span class="font64" style="font-weight:bold;">— </span><span class="font64">g (</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">)<sup>T</sup> </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">+ g(</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">)<sup>T</sup>g<sup>(</sup></span><span class="font64" style="font-weight:bold;"><sup>c</sup></span><span class="font64"><sup>)</sup></span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(2.57)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">(by the distributive property)</span></p></td><td>
<p></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">= </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>T </sup></span><span class="font64" style="font-weight:bold;"><sup>x — </sup></span><span class="font64"><sup>2</sup></span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>T</sup> g(</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">) + g(</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">) <sup>T</sup>g(</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">)</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(2.58)</span></p></td></tr>
<tr><td colspan="2" style="vertical-align:middle;">
<p><span class="font64">(because the scalar g(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">)<sup>T</sup></span><span class="font64" style="font-weight:bold;">x </span><span class="font64">is equal to the transpose of itself).</span></p>
<p><span class="font64">We can now change the function being minimized again, to omit the first term, since this term does not depend on </span><span class="font64" style="font-weight:bold;">c</span><span class="font64">:</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">c* =</span><span class="font64"> argmin </span><span class="font64" style="font-weight:bold;">—</span><span class="font64">2</span><span class="font64" style="font-weight:bold;">x </span><span class="font64"><sup>T</sup>g(</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">) + g (</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">) <sup>T</sup>g(</span><span class="font64" style="font-weight:bold;">c</span><span class="font64">).</span></p>
<p><span class="font64">c</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(2.59)</span></p></td></tr>
<tr><td colspan="2" style="vertical-align:bottom;">
<p><span class="font64">To make further progress, we must substitute in the definition of </span><span class="font64" style="font-weight:bold;font-style:italic;">g(c):</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">c* =</span><span class="font64"> argmin </span><span class="font64" style="font-weight:bold;">—</span><span class="font64">2</span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">Dc +</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">c</span><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;">D</span><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">Dc</span></p>
<p><span class="font64">c</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(2.60)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> argmin </span><span class="font64" style="font-weight:bold;">— </span><span class="font64">2</span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">Dc +</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">c</span><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">Ii c</span></p>
<p><span class="font64">c</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(2.61)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">(by the orthogonality and unit norm constraints on </span><span class="font64" style="font-weight:bold;">D</span><span class="font64">)</span></p></td><td>
<p></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font64">= argmin </span><span class="font64" style="font-weight:bold;">— </span><span class="font64">2</span><span class="font64" style="font-weight:bold;">x </span><span class="font64" style="font-weight:bold;font-style:italic;">Dc</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;">c</span><span class="font64"><sup>T</sup></span><span class="font64" style="font-weight:bold;">c</span></p>
<p><span class="font64">c</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(2.62)</span></p></td></tr>
<tr><td colspan="2" style="vertical-align:middle;">
<p><span class="font64">We can solve this optimization problem using vector calculus (see Sec. 4.3 if you do not know how to do this):</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;">V</span><span class="font64"><sub>c</sub>(</span><span class="font64" style="font-weight:bold;">—</span><span class="font64" style="font-weight:bold;font-style:italic;">2xDc</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;">c </span><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;">c</span><span class="font64">) = </span><span class="font64" style="font-weight:bold;">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(2.63)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64" style="font-weight:bold;">— </span><span class="font64">2</span><span class="font64" style="font-weight:bold;">Dx </span><span class="font64">+ 2</span><span class="font64" style="font-weight:bold;">c </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(2.64)</span></p></td></tr>
<tr><td>
<p><span class="font64" style="font-weight:bold;">c </span><span class="font64">= </span><span class="font64" style="font-weight:bold;">D x</span><span class="font64">.</span></p></td><td>
<p><span class="font64">(2.65)</span></p></td></tr>
<tr><td colspan="2" style="vertical-align:middle;">
<p><span class="font64">This makes the algorithm efficient: we can optimally encode </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">just using a matrix-vector operation. To encode a vector, we apply the encoder function</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">f (</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) = </span><span class="font64" style="font-weight:bold;">D </span><span class="font64"><sup>T</sup></span><span class="font64" style="font-weight:bold;">x</span><span class="font64">.</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(2.66)</span></p></td></tr>
</table>
<p><span class="font64">Using a further matrix multiplication, we can also define the PCA reconstruction operation:</span></p>
<p><span class="font64">r(x) </span><span class="font64" style="font-weight:bold;font-style:italic;">= g (f</span><span class="font64"> (x)) = DD<sup>T</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">x.</span><span class="font64"> &#160;&#160;&#160;(2.67)</span></p>
<p><span class="font64">Next, we need to choose the encoding matrix D. To do so, we revisit the</span></p>
<p><span class="font12" style="font-style:italic;">C\</span></p>
<p><span class="font64">idea of minimizing the </span><span class="font64" style="font-weight:bold;font-style:italic;">L</span><span class="font64"> distance between inputs and reconstructions. However, since we will use the same matrix D to decode all of the points, we can no longer&#160;consider the points in isolation. Instead, we must minimize the Frobenius norm of&#160;the matrix of errors computed over all dimensions and all points:</span></p><div>
<p><span class="font64">D *</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> arg min D</span></p></div><div>
<p><span class="font64">in /^~^ </span><span class="font64" style="font-weight:bold;font-style:italic;">(yXj — r</span><span class="font64">(x</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(i)</sup>)^</span><span class="font64"> subject to</span></p></div><div>
<p><span class="font64" style="font-variant:small-caps;">D<sup>t</sup>D = I</span></p></div><div>
<p><span class="font64">(2.68)</span></p></div>
<p><span class="font64">To derive the algorithm for finding D*, we will start by considering the case where l = 1. In this case, D is just a single vector, d. Substituting Eq. 2.67 into&#160;Eq. 2.68 and simplifying D into d, the problem reduces to</span></p><div>
<p><span class="font12" style="font-weight:bold;">d</span></p>
<p><span class="font64">= arg min d</span></p></div><div><h3><a id="bookmark13"></a><span class="font66">E</span></h3></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">\x<sup>(i)</sup> - dd<sup>T</sup>x<sup>(i)</sup></span><span class="font64"> I</span></p></div><div>
<p><span class="font61">v &#160;&#160;&#160;1 1&#160;&#160;&#160;&#160;1 1</span></p>
<p><span class="font64">2 subject to ||d</span><span class="font18">||2</span><span class="font64"> = 1.</span></p></div><div>
<p><span class="font64">(2.69)</span></p></div>
<p><span class="font64">The above formulation is the most direct way of performing the substitution, but is not the most stylistically pleasing way to write the equation. It places the&#160;scalar value d<sup>T</sup>x<sup>(i)</sup> on the right of the vector d. It is more conventional to write&#160;scalar coefficients on the left of vector they operate on. We therefore usually write&#160;such a formula as</span></p><div>
<p><span class="font64">d * = arg min d</span></p></div><div><h3><a id="bookmark14"></a><span class="font66">E</span></h3></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x<sup>(i)</sup></span></p></div><div>
<p><span class="font64">dx <sup>(i)</sup>d||2 subject to ||d||<sub>2</sub> = 1,</span></p></div><div>
<p><span class="font64">(2.70)</span></p></div>
<p><span class="font64">or, exploiting the fact that a scalar is its own transpose, as</span></p><div>
<p><span class="font64">d * = arg min d</span></p></div><div><h3><a id="bookmark15"></a><span class="font66">E</span></h3></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x<sup>(i)</sup></span><span class="font64"> — x<sup>(i)T</sup>dd||<sup>2</sup> subject to ||d||<sub>2</sub> = 1.</span></p></div><div>
<p><span class="font64">(2.71)</span></p></div>
<p><span class="font64">The reader should aim to become familiar with such cosmetic rearrangements.</span></p>
<p><span class="font64">At this point, it can be helpful to rewrite the problem in terms of a single design matrix of examples, rather than as a sum over separate example vectors.&#160;This will allow us to use more compact notation. Let X G R<sup>mxn</sup> be the matrix&#160;defined by stacking all of the vectors describing the points, such that </span><span class="font64" style="font-weight:bold;font-style:italic;">X<sub>i:</sub></span><span class="font64"> = x</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(i)</sup> .&#160;</span><span class="font64">We can now rewrite the problem as</span></p>
<p><span class="font64">d* = argmin||X — Xdd<sup>T</sup>|F subject to d<sup>T</sup>d = 1. &#160;&#160;&#160;(2.72)</span></p>
<p><span class="font64">d</span></p><div>
<p><span class="font64">Disregarding the constraint for the moment, we can simplify the Frobenius norm portion as follows:</span></p>
<p><span class="font64">^ ~ &#160;&#160;&#160;(2.73)</span></p></div><div>
<p><span class="font19">11 &#160;&#160;&#160;<sup>-</sup>r 11o</span></p>
<p><span class="font64">argmin ||X — Xdd<sup>1</sup> ||<sub>F</sub></span></p></div><div>
<p><span class="font64">d</span></p></div><div>
<p><span class="font64">argminTr </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">^(x — Xdd^ (x —</span><span class="font64"> Xdd<sup>T</sup>)^</span></p></div><div>
<p><span class="font64">(2.74)</span></p></div><div>
<p><span class="font64">(by Eq. 2.49)</span></p></div><div>
<p><span class="font64">argminTr(X 'X — X <sup>1</sup> Xdd<sup>1</sup> — dd <sup>1</sup> X 'X + dd<sup>T</sup>X<sup>T</sup>Xdd<sup>T</sup>) &#160;&#160;&#160;(2.75)</span></p></div><div>
<p><span class="font64">min d</span></p></div><div>
<p><span class="font64">-T-</span></p></div><div>
<p><span class="font24" style="font-variant:small-caps;">-TvjjT</span></p></div>
<p><span class="font64">= argminTr(X<sup>T</sup> X) — Tr(X<sup>T</sup>Xdd<sup>T</sup>) — Tr(dd<sup>T</sup> X <sup>T</sup>X) + Tr(dd<sup>T</sup> X<sup>T</sup> Xdd<sup>T</sup>)</span></p>
<p><span class="font64">d</span></p>
<p><span class="font64">(2.76)</span></p>
<p><span class="font64">= argmin — Tr(X<sup>T</sup>Xdd<sup>T</sup>) — Tr(dd<sup>T</sup> X<sup>T</sup>X) + Tr(dd<sup>T</sup>X<sup>T</sup>Xdd<sup>T</sup>) &#160;&#160;&#160;(2.77)</span></p>
<p><span class="font64">d</span></p>
<p><span class="font64">(because terms not involving d do not affect the arg min)</span></p>
<p><span class="font64">= argmin —2Tr(X <sup>T</sup> Xdd<sup>T</sup>) + Tr(dd<sup>T</sup> X <sup>T</sup> Xdd<sup>T</sup>) &#160;&#160;&#160;(2.78)</span></p>
<p><span class="font64">d</span></p>
<p><span class="font64">(because we can cycle the order of the matrices inside a trace, Eq. 2.52)</span></p>
<p><span class="font64">= argmin —2Tr(X <sup>T</sup> Xdd<sup>T</sup>) + Tr(X <sup>T</sup>Xdd<sup>T</sup>dd<sup>T</sup>) &#160;&#160;&#160;(2.79)</span></p>
<p><span class="font64">d</span></p>
<p><span class="font64">(using the same property again)</span></p>
<p><span class="font64">At this point, we re-introduce the constraint:</span></p>
<p><span class="font64">argmin —2Tr(X<sup>T</sup>Xdd<sup>T</sup>) + Tr(X<sup>T</sup>Xdd<sup>T</sup>dd<sup>T</sup>) subject to </span><span class="font64" style="font-weight:bold;font-style:italic;">d d</span><span class="font64"> = 1 &#160;&#160;&#160;(2.80)</span></p>
<p><span class="font64">d</span></p>
<p><span class="font64">= argmin —2Tr(X<sup>T</sup>Xdd<sup>T</sup>) + Tr(X<sup>T</sup>Xdd<sup>T</sup>) subject to d<sup>T</sup>d = 1 &#160;&#160;&#160;(2.81)</span></p>
<p><span class="font64">d</span></p>
<p><span class="font64">(due to the constraint)</span></p>
<p><span class="font64">= argmin — Tr(X<sup>T</sup>Xdd<sup>T</sup>) subject to </span><span class="font64" style="font-weight:bold;font-style:italic;">d d</span><span class="font64"> = 1 &#160;&#160;&#160;(2.82)</span></p>
<p><span class="font64">d</span></p>
<p><span class="font64">= argmaxTr(X<sup>T</sup>Xdd<sup>T</sup>) subject to d<sup>T</sup>d = 1 &#160;&#160;&#160;(2.83)</span></p>
<p><span class="font64">d</span></p>
<p><span class="font64">= argmaxTr(d<sup>T</sup>X<sup>T</sup>Xd) subject to d<sup>T</sup>d = 1 &#160;&#160;&#160;(2.84)</span></p>
<p><span class="font64">d</span></p>
<p><span class="font64">This optimization problem may be solved using eigendecomposition. Specifically, the optimal d is given by the eigenvector of X<sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">X</span><span class="font64"> corresponding to the largest&#160;eigenvalue.</span></p>
<p><span class="font64">In the general case, where l &gt; 1, the matrix D is given by the l eigenvectors corresponding to the largest eigenvalues. This may be shown using proof by&#160;induction. We recommend writing this proof as an exercise.</span></p>
<p><span class="font64">Linear algebra is one of the fundamental mathematical disciplines that is necessary to understand deep learning. Another key area of mathematics that is&#160;ubiquitous in machine learning is probability theory, presented next.</span></p>
</body>
</html>