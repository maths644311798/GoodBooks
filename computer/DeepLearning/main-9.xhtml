<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h4><a id="bookmark0"></a><span class="font65" style="font-weight:bold;">5.3 Hyperparameters and Validation Sets</span></h4>
<p><span class="font64">Most machine learning algorithms have several settings that we can use to control the behavior of the learning algorithm. These settings are called </span><span class="font64" style="font-weight:bold;font-style:italic;">hyperparameters.&#160;</span><span class="font64">The values of hyperparameters are not adapted by the learning algorithm itself&#160;(though we can design a nested learning procedure where one learning algorithm&#160;learns the best hyperparameters for another learning algorithm).</span></p>
<p><span class="font64">In the polynomial regression example we saw in Fig. 5.2, there is a single hyperparameter: the degree of the polynomial, which acts as a </span><span class="font64" style="font-weight:bold;font-style:italic;">capacity</span><span class="font64"> hyperparameter. The A value used to control the strength of weight decay is another example of a&#160;hyp erparameter.</span></p>
<p><span class="font64">Sometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because it is difficult to optimize. More frequently, we do not learn the hyperparameter because it is not appropriate to learn that hyperparameter on the training set. This applies to all hyperparameters that control&#160;model capacity. If learned on the training set, such hyperparameters would always&#160;choose the maximum possible model capacity, resulting in overfitting (refer to&#160;Fig. 5.3). For example, we can always fit the training set better with a higher&#160;degree polynomial and a weight decay setting of A = 0 than we could with a lower&#160;degree polynomial and a positive weight decay setting.</span></p>
<p><span class="font64">To solve this problem, we need a </span><span class="font64" style="font-weight:bold;font-style:italic;">validation set</span><span class="font64"> of examples that the training algorithm does not observe.</span></p>
<p><span class="font64">Earlier we discussed how a held-out test set, composed of examples coming from the same distribution as the training set, can be used to estimate the generalization&#160;error of a learner, after the learning process has completed. It is important that the&#160;test examples are not used in any way to make choices about the model, including&#160;its hyperparameters. For this reason, no example from the test set can be used&#160;in the validation set. Therefore, we always construct the validation set from the&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">training</span><span class="font64"> data. Specifically, we split the training data into two disjoint subsets. One&#160;of these subsets is used to learn the parameters. The other subset is our validation&#160;set, used to estimate the generalization error during or after training, allowing&#160;for the hyperparameters to be updated accordingly. The subset of data used to&#160;learn the parameters is still typically called the training set, even though this&#160;may be confused with the larger pool of data used for the entire training process.&#160;The subset of data used to guide the selection of hyperparameters is called the&#160;validation set. Typically, one uses about 80% of the training data for training and&#160;20% for validation. Since the validation set is used to “train” the hyperparameters,&#160;the validation set error will underestimate the generalization error, though typically&#160;by a smaller amount than the training error. After all hyperparameter optimization&#160;is complete, the generalization error may be estimated using the test set.</span></p>
<p><span class="font64">In practice, when the same test set has been used repeatedly to evaluate performance of different algorithms over many years, and especially if we consider&#160;all the attempts from the scientific community at beating the reported state-of-the-art performance on that test set, we end up having optimistic evaluations with&#160;the test set as well. Benchmarks can thus become stale and then do not reflect the&#160;true field performance of a trained system. Thankfully, the community tends to&#160;move on to new (and usually more ambitious and larger) benchmark datasets.</span></p><h5><a id="bookmark1"></a><span class="font64" style="font-weight:bold;">5.3.1 Cross-Validation</span></h5>
<p><span class="font64">Dividing the dataset into a fixed training set and a fixed test set can be problematic if it results in the test set being small. A small test set implies statistical uncertainty&#160;around the estimated average test error, making it difficult to claim that algorithm&#160;A works better than algorithm B on the given task.</span></p>
<p><span class="font64">When the dataset has hundreds of thousands of examples or more, this is not a serious issue. When the dataset is too small, there are alternative procedures,&#160;which allow one to use all of the examples in the estimation of the mean test&#160;error, at the price of increased computational cost. These procedures are based on&#160;the idea of repeating the training and testing computation on different randomly&#160;chosen subsets or splits of the original dataset. The most common of these is the&#160;k-fold cross-validation procedure, shown in Algorithm 5.1, in which a partition&#160;of the dataset is formed by splitting it into k non-overlapping subsets. The test&#160;error may then be estimated by taking the average test error across k trials. On&#160;trial i, the i-th subset of the data is used as the test set and the rest of the data is&#160;used as the training set. One problem is that there exist no unbiased estimators of&#160;the variance of such average error estimators (Bengio and Grandvalet, 2004), but&#160;approximations are typically used.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">5.4 Estimators, Bias and Variance</span></h4>
<p><span class="font64">The field of statistics gives us many tools that can be used to achieve the machine learning goal of solving a task not only on the training set but also to generalize.&#160;Foundational concepts such as parameter estimation, bias and variance are useful&#160;to formally characterize notions of generalization, underfitting and overfitting.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">5.4.1 Point Estimation</span></h5>
<p><span class="font64">Point estimation is the attempt to provide the single “best” prediction of some quantity of interest. In general the quantity of interest can be a single parameter&#160;or a vector of parameters in some parametric model, such as the weights in our&#160;linear regression example in Sec. 5.1.4, but it can also be a whole function.</span></p>
<p><span class="font64">In order to distinguish estimates of parameters from their true value, our convention will be to denote a point estimate of a parameter 6 </span><span class="font64" style="font-weight:bold;font-style:italic;">by 6.</span></p>
<p><span class="font64">Let ,...,} be a set of m independent and identically distributed (i.i.d.) data points. A </span><span class="font64" style="font-weight:bold;font-style:italic;">point estimator</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">statistic</span><span class="font64"> is any function of the data:</span></p>
<p><span class="font64"><sup>e</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>m</sub></span><span class="font64"> = g(x<sup>(1)</sup>,..., x<sup>(m)</sup>). &#160;&#160;&#160;(5.19)</span></p>
<p><span class="font64">The definition does not require that g return a value that is close to the true 6 or even that the range of g is the same as the set of allowable values of 6.&#160;This definition of a point estimator is very general and allows the designer of an&#160;estimator great flexibility. While almost any function thus qualifies as an estimator,</span></p>
<p><span class="font64">Algorithm 5.1 The k-fold cross-validation algorithm. It can be used to estimate generalization error of a learning algorithm A when the given dataset D is too&#160;small for a simple train/test or train/valid split to yield accurate estimation of&#160;generalization error, because the mean of a loss L on a small test set may have too&#160;high variance. The dataset D contains as elements the abstract examples z<sup>(i)</sup> (for&#160;the i-th example), which could stand for an (input,target) pair z<sup>(i)</sup> = (x<sup>(i)</sup>, y<sup>(i)</sup>)&#160;in the case of supervised learning, or for just an input z<sup>(i)</sup> = x<sup>(i)</sup> in the case&#160;of unsupervised learning. The algorithm returns the vector of errors e for each&#160;example in D, whose mean is the estimated generalization error. The errors on&#160;individual examples can be used to compute a confidence interval around the mean&#160;(Eq. 5.47). While these confidence intervals are not well-justified after the use of&#160;cross-validation, it is still common practice to use them to declare that algorithm A&#160;is better than algorithm B only if the confidence interval of the error of algorithm&#160;A lies below and does not intersect the confidence interval of algorithm B.</span></p>
<p><span class="font64">Define KFoldXV(D, A, L, k):</span></p>
<p><span class="font64">Require: D, the given dataset, with elements z<sup>(i)</sup></span></p>
<p><span class="font64">Require: A, the learning algorithm, seen as a function that takes a dataset as input and outputs a learned function&#160;Require: L, the loss function, seen as a function from a learned function f and&#160;an example z<sup>(i)</sup> </span><span class="font64" style="font-weight:bold;">E </span><span class="font64">D to a scalar </span><span class="font64" style="font-weight:bold;font-style:italic;">E</span><span class="font64"> R&#160;Require: k, the number of folds</span></p>
<p><span class="font64">Split D into k mutually exclusive subsets D, whose union is D. for i from 1 to k do&#160;</span><span class="font31" style="font-style:italic;">f</span><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64"> = A(D</span><span class="font64" style="font-weight:bold;">\</span><span class="font64">D.)&#160;for z<sup>(j)</sup> in D<sub>i</sub> do</span></p>
<p><span class="font31" style="font-style:italic;"><sup>e</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">j = </span><span class="font31" style="font-style:italic;"><sup>L</sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(</sup></span><span class="font31" style="font-style:italic;">k </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>z</sup></span><span class="font64"><sup> ( </sup>end for&#160;end for&#160;Return e</span></p>
<p><span class="font64">a good estimator is a function whose output is close to the true underlying 0 that generated the training data.</span></p>
<p><span class="font64">For now, we take the frequentist perspective on statistics. That is, we assume that the true parameter value 0 is fixed but unknown, while the point estimate&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">0)</span><span class="font64"> is a function of the data. Since the data is drawn from a random process, any&#160;function of the data is random. Therefore </span><span class="font64" style="font-weight:bold;font-style:italic;">0</span><span class="font64"> is a random variable.</span></p>
<p><span class="font64">Point estimation can also refer to the estimation of the relationship between input and target variables. We refer to these types of point estimates as function&#160;estimators.</span></p>
<p><span class="font64">Function Estimation As we mentioned above, sometimes we are interested in performing function estimation (or function approximation). Here we are trying to&#160;predict a variable y given an input vector x. We assume that there is a function&#160;f (x) that describes the approximate relationship between y and x. For example,&#160;we may assume that y = f (x) + e, where e stands for the part of y that is not&#160;predictable from x. In function estimation, we are interested in approximating&#160;f with a model or estimate f. Function estimation is really just the same as&#160;estimating a parameter 0; the function estimator f is simply a point estimator in&#160;function space. The linear regression example (discussed above in Sec. 5.1.4) and&#160;the polynomial regression example (discussed in Sec. 5.2) are both examples of&#160;scenarios that may be interpreted either as estimating a parameter w or estimating&#160;a function f mapping from x to y.</span></p>
<p><span class="font64">We now review the most commonly studied properties of point estimators and discuss what they tell us about these estimators.</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">5.4.2 Bias</span></h5>
<p><span class="font64">The bias of an estimator is defined as:</span></p>
<p><span class="font64">bias(0<sub>m</sub>) = </span><span class="font64" style="font-weight:bold;font-style:italic;">E(0<sub>m</sub></span><span class="font64">) — 0 &#160;&#160;&#160;(5.20)</span></p>
<p><span class="font64">where the expectation is over the data (seen as samples from a random variable) and 0 is the true underlying value of 0 used to define the data generating distribution.&#160;An estimator 0<sub>m</sub> is said to be </span><span class="font64" style="font-weight:bold;font-style:italic;">unbiased</span><span class="font64"> if bias(0<sub>m</sub>) = 0, which implies that E(0<sub>m</sub>) =</span></p>
<p><span class="font64">0. An estimator </span><span class="font64" style="font-weight:bold;font-style:italic;">0<sub>m</sub></span><span class="font64"> is said to be </span><span class="font64" style="font-weight:bold;font-style:italic;">asymptotically unbiased</span><span class="font64"> if lim<sub>m</sub>^<sub>M</sub> bias(0<sub>m</sub>) = 0,&#160;which implies that lim<sub>m</sub>^<sub>M</sub> E</span><span class="font64" style="font-weight:bold;font-style:italic;">(0<sup>)</sup><sub>m</sub>)</span><span class="font64"> = </span><span class="font64" style="font-weight:bold;font-style:italic;">0.</span></p>
<p><span class="font64">Example: Bernoulli Distribution Consider a set of samples {x<sup>(1)</sup>,... ,x<sup>(m)</sup>} that are independently and identically distributed according to a Bernoulli distri-</span></p>
<p><span class="font64">A common estimator for the 0 parameter of this distribution is the mean of the training samples:</span></p><div>
<p><span class="font64">(5.22)</span></p></div><div>
<p><span class="font64">bution with mean 0:</span></p></div><div>
<p><span class="font64">P(x<sup>(i)</sup>; </span><span class="font64" style="font-weight:bold;font-style:italic;">0) =</span><span class="font64"> 0<sup>x(i)</sup> (1 - </span><span class="font64" style="font-weight:bold;font-style:italic;">0)<sup>(1</sup>־<sup>x(i))</sup>.</span></p></div><div>
<p><span class="font64">(5.21)</span></p></div>
<p><span class="font31" style="font-style:italic;">0</span><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font64"> </span><span class="font64" style="font-weight:bold;">= <sup>1</sup> V </span><span class="font64">X</span><span class="font64" style="font-weight:bold;">«</span></p>
<p><span class="font64">m</span></p>
<p><span class="font63">i=1</span></p>
<p><span class="font64">To determine whether this estimator is biased, we can substitute Eq. 5.22 into Eq. 5.20:</span></p><div>
<p><span class="font64">bias(°<sub>m</sub>) = </span><span class="font64" style="font-weight:bold;font-style:italic;">E[0<sub>m</sub>]</span><span class="font64"> — 0</span></p></div><div>
<p><span class="font64">= E</span></p></div><div>
<p><span class="font64">1</span></p>
<p><span class="font64">m</span></p></div><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64">(i)</span></p></div><div>
<p><span class="font64">m</span></p></div><div>
<p><span class="font64">x</span></p></div><div>
<p><span class="font64">(i)</span></p></div><div>
<p><span class="font64">0</span></p></div><div>
<p><span class="font64"><sup>E</sup></span></p>
<p><span class="font63">i=1 m</span></p>
<p><span class="font31" style="font-style:italic;">m</span><span class="font64" style="font-weight:bold;"> &#160;&#160;&#160;</span><span class="font64">x<sup>(i)</sup> 0*<sup>(1)</sup> (1—0)<sup>(1</sup>־<sup>x(1))</sup>)</span></p></div><div>
<p><span class="font64">0</span></p></div><div>
<p><span class="font64">0</span></p></div><div>
<p><span class="font64"><sup>i</sup>=<sup>1</sup> x<sup>(l)</sup>=0</span></p></div><div>
<p><span class="font64">m &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;">(</span><span class="font64"><sup>0</sup></span><span class="font64" style="font-weight:bold;">) — </span><span class="font64"><sup>0</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i= 1</span></p>
<p><span class="font31" style="font-style:italic;">=0-0=</span><span class="font64" style="font-weight:bold;">0</span></p></div><div>
<p><span class="font64">(5.23)</span></p>
<p><span class="font64">(5.24)</span></p>
<p><span class="font64">(5.25)</span></p>
<p><span class="font64">(5.26)</span></p>
<p><span class="font64">(5.27)</span></p>
<p><span class="font64">(5.28)</span></p></div>
<p><span class="font64">Since bias(0) = 0, we say that our estimator 0 is unbiased.</span></p>
<p><span class="font64" style="font-weight:bold;">Example: Gaussian Distribution Estimator of the Mean </span><span class="font64">Now, consider a set of samples {x<sup>(1)</sup>,..., x<sup>(m)</sup>} that are independently and identically distributed&#160;according to a Gaussian distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x</span><span class="font64"><sup>(i)</sup>) = N(x<sup>(i)</sup>; p, </span><span class="font64" style="font-weight:bold;font-style:italic;">a<sup>2</sup>),</span><span class="font64"> where i G {1,..., </span><span class="font64" style="font-weight:bold;font-style:italic;">m}.&#160;</span><span class="font64">Recall that the Gaussian probability density function is given by</span></p><div>
<p><span class="font64">p(x<sup>(i)</sup>; p, a <sup>2</sup>) =</span></p></div><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64">2</span></p></div><div>
<p><span class="font64">exp</span></p></div><div>
<p><span class="font67" style="font-weight:bold;">(■</span></p></div><div>
<p><span class="font64">1 (x<sup>(i)</sup> — p)</span></p></div><div>
<p><span class="font64">2 &#160;&#160;&#160;a<sup>2</sup></span></p></div><div>
<p><span class="font64"><sup>.</sup></span></p></div><div>
<p><span class="font64">(5.29)</span></p></div>
<p><span class="font64">A common estimator of the Gaussian mean parameter is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">sample mean</span><span class="font64">:</span></p><div>
<p><span class="font64">(i)</span></p></div><div>
<p><span class="font64">(5.30)</span></p></div>
<p><span class="font64">1</span></p>
<p><span class="font64">p0 m = &#160;&#160;&#160;x</span></p>
<p><span class="font64">m</span></p>
<p><span class="font63">i=1</span></p>
<p><span class="font64">To determine the bias of the sample mean, we are again interested in calculating its expectation:</span></p><div>
<p><span class="font64"><sup>bias(h</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font64"> <sup>)</sup> — <sup>E[</sup>Am<sup>]</sup> &#160;&#160;&#160;P</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">—</span><span class="font64"> E</span></p></div><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64">X</span></p></div><div>
<p><span class="font64">(i)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">m</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i</span><span class="font64">=1</span></p></div><div>
<p dir="rtl"><span class="font64">״ 1</span></p>
<p><span class="font64">—E m</span></p>
<p><span class="font64">V i=1</span></p>
<p><span class="font64"><sub>1</sub>m</span></p>
<p><span class="font64">m<sup>1</sup> i</span><span class="font18">=1</span></p>
<p><span class="font64">— p — </span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font64"> — 0</span></p></div><div>
<p><span class="font64">X</span></p></div><div>
<p><span class="font64">(i)</span></p></div><div>
<p><span class="font64">h</span></p></div><div>
<p><span class="font12" style="font-style:italic;">)</span></p></div><div>
<p><span class="font64" style="font-style:italic;">ll</span></p></div><div>
<p><span class="font64">(5.31)</span></p>
<p><span class="font64">(5.32)</span></p>
<p><span class="font64">(5.33)</span></p>
<p><span class="font64">(5.34)</span></p>
<p><span class="font64">(5.35)</span></p></div>
<p><span class="font64">Thus we find that the sample mean is an unbiased estimator of Gaussian mean parameter.</span></p>
<p><span class="font64" style="font-weight:bold;">Example: Estimators of the Variance of a Gaussian Distribution </span><span class="font64">As an</span></p>
<p><span class="font64">example, we compare two different estimators of the variance parameter a<sup>2</sup> of a Gaussian distribution. We are interested in knowing if either estimator is biased.&#160;The first estimator of a<sup>2</sup> we consider is known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">sample variance</span><span class="font64">:</span></p><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64">m</span></p></div><div>
<p><span class="font64"><sup>x (i) —</sup> Am ,</span></p></div><div>
<p><span class="font64">(5.36)</span></p></div>
<p><span class="font64">where pm. is the sample mean, defined above. More formally, we are interested in computing</span></p>
<p><span class="font64"><sup>bias(a</sup> m<sup>—E[a</sup>m<sup>]</sup> -<sup>a2 &#160;&#160;&#160;(5</sup>.<sup>37)</sup></span></p>
<p><span class="font64">We begin by evaluating the term E[a<sub>?</sub>“ ]:</span></p><div>
<p><span class="font64">E[am ] —e</span></p></div><div>
<p><span class="font61">ן m</span></p><h3><a id="bookmark5"></a><span class="font64">m e (x<sup>(i)</sup>—p״׳)</span></h3></div><div>
<p><span class="font64">2</span></p></div><div>
<p><span class="font64">m1</span></p></div><div>
<p><span class="font64">a</span></p></div><div>
<p><span class="font64">(5.38)</span></p>
<p><span class="font64">(5.39)</span></p></div>
<p><span class="font64">Returning to Eq. 5.37, we conclude that the bias of am is —a<sup>2</sup>/m. Therefore, the sample variance is a biased estimator.</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">unbiased sample variance</span><span class="font64"> estimator</span></p><div>
<p><span class="font64">a 2 _ &#160;&#160;&#160;</span><span class="font64" style="text-decoration:underline;"><sup>1</sup></span></p>
<p><span class="font64"><sup>m</sup> m- 1</span></p></div><div>
<p><span class="font63">m</span></p>
<p><span class="font64"><sup>x(i) -</sup> Am)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i= 1</span></p></div><div>
<p><span class="font64">2</span></p></div><div>
<p><span class="font64">(5.40)</span></p></div><div>
<p><span class="font64">״ m ] _ a<sup>2</sup>:</span></p></div>
<p><span class="font64">provides an alternative approach. As the name suggests this estimator is unbiased. That is, we find that E[5־m]</span></p><div>
<p><span class="font64">E[5mj _ E</span></p></div><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64">m1</span></p></div><div>
<p><span class="font64">(x<sup>(i) -</sup> Am)</span></p>
<p><span class="font64">i=1</span></p></div><div>
<p><span class="font64">2</span></p></div><div>
<p><span class="font64"><sup>m</sup>2 <sup>E[a</sup>m</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>]</sup></span></p></div><div>
<p><span class="font64">m - 1 <sup>m</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">m m — 1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">(</span><span class="font31" style="font-style:italic;">m</span><span class="font64" style="font-weight:bold;font-style:italic;">zl </span><span class="font31" style="font-style:italic;"><sub>a</sub></span><span class="font64" style="font-weight:bold;"> </span><span class="font64">2)</span></p>
<p><span class="font64">m</span></p></div><div>
<p><span class="font64">m 1 m</span></p></div><div>
<p><span class="font64">a <sup>2</sup>.</span></p></div><div>
<p><span class="font64">(5.41)</span></p>
<p><span class="font64">(5.42)</span></p>
<p><span class="font64">(5.43)</span></p>
<p><span class="font64">(5.44)</span></p></div>
<p><span class="font64">We have two estimators: one is biased and the other is not. While unbiased estimators are clearly desirable, they are not always the “best” estimators. As we&#160;will see we often use biased estimators that possess other important properties.</span></p><h5><a id="bookmark6"></a><span class="font64" style="font-weight:bold;">5.4.3 Variance and Standard Error</span></h5>
<p><span class="font64">Another property of the estimator that we might want to consider is how much we expect it to vary as a function of the data sample. Just as we computed the&#160;expectation of the estimator to determine its bias, we can compute its </span><span class="font64" style="font-weight:bold;font-style:italic;">variance.&#160;</span><span class="font64">The variance of an estimator is simply the variance</span></p>
<p><span class="font64">Var( J &#160;&#160;&#160;(5.45)</span></p>
<p><span class="font64">where the random variable is the training set. Alternately, the square root of the variance is called the </span><span class="font64" style="font-weight:bold;font-style:italic;">standard error,</span><span class="font64"> denoted SE(0).</span></p>
<p><span class="font64">The variance or the standard error of an estimator provides a measure of how we would expect the estimate we compute from data to vary as we independently&#160;resample the dataset from the underlying data generating process. Just as we&#160;might like an estimator to exhibit low bias we would also like it to have relatively&#160;low variance.</span></p>
<p><span class="font64">When we compute any statistic using a finite number of samples, our estimate of the true underlying parameter is uncertain, in the sense that we could have&#160;obtained other samples from the same distribution and their statistics would have&#160;been different. The expected degree of variation in any estimator is a source of&#160;error that we want to quantify.</span></p>
<p><span class="font64">The standard error of the mean is given by</span></p><div>
<p><span class="font64">SE(A </span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>m</sub>)</span></p></div><div>
<p><span class="font64">. Var[— &#160;&#160;&#160;x<sup>(i)</sup>] = —a=</span></p>
<p><span class="font31" style="font-style:italic;">m &#160;&#160;&#160;m</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p></div><div>
<p><span class="font64">(5.46)</span></p></div>
<p><span class="font64">where a<sup>2</sup> is the true variance of the samples x</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>l</sup>.</span><span class="font64"> The standard error is often estimated by using an estimate of a. Unfortunately, neither the square root of&#160;the sample variance nor the square root of the unbiased estimator of the variance&#160;provide an unbiased estimate of the standard deviation. Both approaches tend&#160;to underestimate the true standard deviation, but are still used in practice. The&#160;square root of the unbiased estimator of the variance is less of an underestimate.&#160;For large m, the approximation is quite reasonable.</span></p>
<p><span class="font64">The standard error of the mean is very useful in machine learning experiments. We often estimate the generalization error by computing the sample mean of the&#160;error on the test set. The number of examples in the test set determines the&#160;accuracy of this estimate. Taking advantage of the central limit theorem, which&#160;tells us that the mean will be approximately distributed with a normal distribution,&#160;we can use the standard error to compute the probability that the true expectation&#160;falls in any chosen interval. For example, the 95% confidence interval centered on&#160;the mean is Am is</span></p><div>
<p><span class="font64">(5.47)</span></p></div>
<p><span class="font64">(Am - 1.96SE(Am), Am + 1.96SE(Am)),</span></p>
<p><span class="font64">under the normal distribution with mean A<sub>m</sub> and variance SE( </span><span class="font64" style="font-weight:bold;font-style:italic;">Am)</span><span class="font64"> <sup>2</sup>. In machine learning experiments, it is common to say that algorithm A is better than algorithm&#160;B if the upper bound of the 95% confidence interval for the error of algorithm A is&#160;less than the lower bound of the 95% confidence interval for the error of algorithm&#160;B.</span></p>
<p><span class="font64" style="font-weight:bold;">Example: Bernoulli Distribution </span><span class="font64">We once again consider a set of samples ^,..., <sup>(m)</sup>} drawn independently and identically from a Bernoulli distribution&#160;(recall P(x<sup>(i)</sup>; 9) </span><span class="font64" style="font-weight:bold;font-style:italic;">= 9<sup>x</sup></span><span class="font64"><sup>W</sup>(1 — 9)<sup>(1-xW)</sup>). This time we are interested in computing&#160;the variance of the estimator </span><span class="font64" style="font-weight:bold;font-style:italic;">9<sub>m</sub> =</span><span class="font64"> — W </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>m</sup> ,</span><span class="font64"> x<sup>(i)</sup>.</span></p>
<p><span class="font64"><sup>m</sup> m </span><span class="font64" style="font-weight:bold;font-style:italic;">1=1</span></p><div>
<p><span class="font64">(5.48)</span></p></div>
<p><span class="font64"><sup>Var</sup> (<sup>9</sup>m) = <sup>Var</sup> ^m ^ <sup>x</sup>«^j</span></p>
<p><span class="font64">— &#160;&#160;&#160;m0(1 </span><span class="font64" style="font-weight:bold;font-style:italic;">— 0)&#160;m<sup>2</sup></span></p><div><div><img src="main-44.jpg" alt=""/>
<p><span class="font64">(5.51)</span></p>
<p><span class="font64">(5.52)</span></p></div></div>
<p><span class="font64">1</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">- &#160;&#160;&#160;0(1 — 0)&#160;m</span></p>
<p><span class="font64">The variance of the estimator decreases as a function of m, the number of examples in the dataset. This is a common property of popular estimators that we will&#160;return to when we discuss consistency (see Sec. 5.4.5).</span></p><h5><a id="bookmark7"></a><span class="font64" style="font-weight:bold;">5.4.4 Trading off Bias and Variance to Minimize Mean Squared&#160;Error</span></h5>
<p><span class="font64">Bias and variance measure two different sources of error in an estimator. Bias measures the expected deviation from the true value of the function or parameter.&#160;Variance on the other hand, provides a measure of the deviation from the expected&#160;estimator value that any particular sampling of the data is likely to cause.</span></p>
<p><span class="font64">What happens when we are given a choice between two estimators, one with more bias and one with more variance? How do we choose between them? For&#160;example, imagine that we are interested in approximating the function shown in&#160;Fig. 5.2 and we are only offered the choice between a model with large bias and&#160;one that suffers from large variance. How do we choose between them?</span></p>
<p><span class="font64">The most common way to negotiate this trade-off is to use cross-validation. Empirically, cross-validation is highly successful on many real-world tasks. Alternatively, we can also compare the </span><span class="font64" style="font-weight:bold;font-style:italic;">mean squared error</span><span class="font64"> (MSE) of the estimates:</span></p>
<p><span class="font64">MSE = E </span><span class="font64" style="font-weight:bold;font-style:italic;">[(9<sub>m</sub> —</span><span class="font64"> 0)<sup>2</sup> ] &#160;&#160;&#160;(5.53)</span></p>
<p><span class="font64">= Bias(#<sub>m</sub>)<sup>2</sup> + Var </span><span class="font64" style="font-weight:bold;font-style:italic;">(0<sub>m</sub>)</span><span class="font64"> &#160;&#160;&#160;(5.54)</span></p>
<p><span class="font64">The MSE measures the overall expected deviation—in a squared error sense— between the estimator and the true value of the parameter 0. As is clear from&#160;Eq. 5.54, evaluating the MSE incorporates both the bias and the variance. Desirable&#160;estimators are those with small MSE and these are estimators that manage to keep&#160;both their bias and variance somewhat in check.</span></p>
<p><span class="font64">The relationship between bias and variance is tightly linked to the machine learning concepts of capacity, underfitting and overfitting. In the case where gen-</span></p><div><img src="main-45.jpg" alt=""/>
<p><span class="font64">Figure 5.6: As capacity increases (x&gt;axis), bias (dotted) tends to decrease and variance (dashed) tends to increase, yielding another U-shaped curve for generalization error (bold&#160;curve). If we vary capacity along one axis, there is an optimal capacity, with underfitting&#160;when the capacity is below this optimum and overfitting when it is above. This relationship&#160;is similar to the relationship between capacity, underfitting, and overfitting, discussed in&#160;Sec. 5.2 and Fig. 5.3.</span></p></div>
<p><span class="font64">eralization error is measured by the MSE (where bias and variance are meaningful components of generalization error), increasing capacity tends to increase variance&#160;and decrease bias. This is illustrated in Fig. 5.6, where we see again the U-shaped&#160;curve of generalization error as a function of capacity.</span></p><h5><a id="bookmark8"></a><span class="font64" style="font-weight:bold;">5.4.5 Consistency</span></h5>
<p><span class="font64">So far we have discussed the properties of various estimators for a training set of fixed size. Usually, we are also concerned with the behavior of an estimator as the&#160;amount of training data grows. In particular, we usually wish that, as the number&#160;of data points m in our dataset increases, our point estimates converge to the true&#160;value of the corresponding parameters. More formally, we would like that</span></p>
<p><span class="font64">lim </span><span class="font64" style="font-weight:bold;font-style:italic;">6<sub>m</sub></span><span class="font64"> A 6. &#160;&#160;&#160;(5.55)</span></p>
<p><span class="font64">The symbol A means that the convergence is in probability, i.e. for any e &gt; 0, P(|6<sub>m</sub> — 6| &gt; e) A 0 as m A to . The condition described by Eq. 5.55 is&#160;known as </span><span class="font64" style="font-weight:bold;font-style:italic;">consistency.</span><span class="font64"> It is sometimes referred to as weak consistency, with&#160;strong consistency referring to the </span><span class="font64" style="font-weight:bold;font-style:italic;">almost sure</span><span class="font64"> convergence of 6 to 6. </span><span class="font64" style="font-weight:bold;font-style:italic;">Almost sure&#160;convergence</span><span class="font64"> of a sequence of random variables x<sup>(1)</sup>, x<sup>(2)</sup>,... to a value x occurs&#160;when p(lim</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>m</sub>^&lt;x</span><span class="font64"> x<sup>(m)</sup> = x) = 1.</span></p>
<p><span class="font64">Consistency ensures that the bias induced by the estimator is assured to diminish as the number of data examples grows. However, the reverse is not&#160;true—asymptotic unbiasedness does not imply consistency. For example, consider&#160;estimating the mean parameter </span><span class="font31" style="font-style:italic;">p</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">of a normal distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">N(</span><span class="font31" style="font-style:italic;">x</span><span class="font64">; </span><span class="font31" style="font-style:italic;">p, a</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>2</sup></span><span class="font64">), with a&#160;dataset consisting of m samples: {x<sup>(1)</sup>,..., x<sup>(m)</sup>}. We could use the first sample&#160;x<sup>(1)</sup> of the dataset as an </span><span class="font64" style="font-weight:bold;font-style:italic;">unbiased</span><span class="font64"> estimator: </span><span class="font31" style="font-style:italic;">0 </span><span class="font64" style="font-weight:bold;font-style:italic;">= </span><span class="font31" style="font-style:italic;">x</span><span class="font64"><sup>(1)</sup>. In that case, E(#<sub>m</sub>) = 0&#160;so the estimator is unbiased no matter how many data points are seen. This, of&#160;course, implies that the estimate is asymptotically unbiased. However, this is not&#160;a consistent estimator as it is </span><span class="font64" style="font-weight:bold;font-style:italic;">not</span><span class="font64"> the case that </span><span class="font31" style="font-style:italic;">0</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>m</sub> ^ </span><span class="font31" style="font-style:italic;">0</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">as m ^ rc&gt;.</span></p><h4><a id="bookmark9"></a><span class="font65" style="font-weight:bold;">5.5 Maximum Likelihood Estimation</span></h4>
<p><span class="font64">Previously, we have seen some definitions of common estimators and analyzed their properties. But where did these estimators come from? Rather than guessing&#160;that some function might make a good estimator and then analyzing its bias and&#160;variance, we would like to have some principle from which we can derive specific&#160;functions that are good estimators for different models.</span></p>
<p><span class="font64">The most common such principle is the maximum likelihood principle.</span></p>
<p><span class="font64">Consider a set of m examples X = {x<sup>(1)</sup>,..., x<sup>(m)</sup>} drawn independently from the true but unknown data generating distribution pd<sub>ata</sub> (x).</span></p>
<p><span class="font64">Let p<sub>mo</sub>d<sub>e</sub></span><span class="font18">1</span><span class="font64">(x; 6) be a parametric family of probability distributions over the same space indexed by 6. In other words, </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"><sub>mo</sub>d<sub>e</sub></span><span class="font18">1</span><span class="font64">(x; 6</span><span class="font64" style="font-weight:bold;font-style:italic;">)</span><span class="font64"> maps any configuration x&#160;to a real number estimating the true probability Pd<sub>ata</sub>(x).</span></p>
<p><span class="font64">The maximum likelihood estimator for 6 is then defined as</span></p>
<p><span class="font64" style="font-variant:small-caps;">6ml = arg max pmode</span><span class="font18">1</span><span class="font64">(X; 6) &#160;&#160;&#160;(5.56)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">e</span></p>
<p><span class="font64">m</span></p>
<p><span class="font64">= arg max &#160;&#160;&#160;Pmodei(x<sup>(i)</sup>; 6)&#160;&#160;&#160;&#160;(5.57)</span></p>
<p><span class="font64">e</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">1=1</span></p>
<p><span class="font64">This product over many probabilities can be inconvenient for a variety of reasons. For example, it is prone to numerical underflow. To obtain a more convenient&#160;but equivalent optimization problem, we observe that taking the logarithm of the&#160;likelihood does not change its arg max but does conveniently transform a product</span></p>
<p><span class="font64" style="font-variant:small-caps;">0ml = argmax^ logpmodel(x</span><span class="font64"><sup>(i)</sup>; 0).</span></p><div>
<p><span class="font64">(5.58)</span></p></div><div>
<p><span class="font64">into a sum:</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p>
<p><span class="font64">Because the argmax does not change when we rescale the cost function, we can divide by m to obtain a version of the criterion that is expressed as an expectation&#160;with respect to the empirical distribution pd<sub>ata</sub> defined by the training data:</span></p><div>
<p><span class="font64">0 <sub>ML</sub> = argmax E</span></p></div><div>
<p><span class="font63">8</span></p></div><div>
<p><span class="font64">X^Pdata</span></p></div><div>
<p><span class="font64">log</span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64">model(x; 0).</span></p></div><div>
<p><span class="font64">(5.59)</span></p></div>
<p><span class="font64">One way to interpret maximum likelihood estimation is to view it as minimizing the dissimilarity between the empirical distribution pd<sub>ata</sub> defined by the training&#160;set and the model distribution, with the degree of dissimilarity between the two&#160;measured by the KL divergence. The KL divergence is given by</span></p>
<p><span class="font64">■^KL <sup>(p</sup>data ||<sup>p</sup>modeO &#160;&#160;&#160;®X^pd<sub>ata</sub> <sup>[log p</sup>data<sup>(x)&#160;&#160;&#160;&#160;1ogp</sup>model<sup>(x)]</sup> .&#160;&#160;&#160;&#160;<sup>(5</sup>.<sup>60)</sup></span></p>
<p><span class="font64">The term on the left is a function only of the data generating process, not the model. This means when we train the model to minimize the KL divergence, we&#160;need only minimize</span></p>
<p><span class="font64"><sup>— E</sup>x~pdata <sup>[l0g</sup>Bnodel <sup>(x)] &#160;&#160;&#160;(5</sup>.<sup>61)</sup></span></p>
<p><span class="font64">which is of course the same as the maximization in Eq. 5.59.</span></p>
<p><span class="font64">Minimizing this KL divergence corresponds exactly to minimizing the crossentropy between the distributions. Many authors use the term “cross-entropy” to identify specifically the negative log-likelihood of a Bernoulli or softmax distribution,&#160;but that is a misnomer. Any loss consisting of a negative log-likelihood is a cross&#160;entropy between the empirical distribution defined by the training set and the&#160;model. For example, mean squared error is the cross-entropy between the empirical&#160;distribution and a Gaussian model.</span></p>
<p><span class="font64">We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution pd<sub>ata</sub>. Ideally, we would like to match the true data generating distribution pd<sub>ata</sub>, but we have no direct access to this&#160;distribution.</span></p>
<p><span class="font64">While the optimal 0 is the same regardless of whether we are maximizing the likelihood or minimizing the KL divergence, the values of the objective functions&#160;are different. In software, we often phrase both as minimizing a cost function.&#160;Maximum likelihood thus becomes minimization of the negative log-likelihood&#160;(NLL), or equivalently, minimization of the cross entropy. The perspective of&#160;maximum likelihood as minimum KL divergence becomes helpful in this case&#160;because the KL divergence has a known minimum value of zero. The negative&#160;log-likelihood can actually become negative when x is real-valued.</span></p><h5><a id="bookmark10"></a><span class="font64" style="font-weight:bold;">5.5.1 Conditional Log-Likelihood and Mean Squared Error</span></h5>
<p><span class="font64">The maximum likelihood estimator can readily be generalized to the case where our goal is to estimate a conditional probability </span><span class="font64" style="font-weight:bold;">P</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">y </span><span class="font64">| </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">; </span><span class="font64" style="font-weight:bold;">ff</span><span class="font64">) in order to predict </span><span class="font64" style="font-weight:bold;">y&#160;</span><span class="font64">given </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">. This is actually the most common situation because it forms the basis for&#160;most supervised learning. If </span><span class="font64" style="font-weight:bold;">X </span><span class="font64">represents all our inputs and </span><span class="font64" style="font-weight:bold;">Y </span><span class="font64">all our observed&#160;targets, then the conditional maximum likelihood estimator is</span></p><div>
<p><span class="font64">(5.62)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">Oml</span><span class="font64"> = arg max P(</span><span class="font64" style="font-weight:bold;">Y | X</span><span class="font64" style="font-weight:bold;font-style:italic;">;0</span><span class="font31" style="font-style:italic;">). </span><span class="font64" style="font-weight:bold;font-style:italic;">e</span></p>
<p><span class="font64">If the examples are assumed to be i.i.d., then this can be decomposed into</span></p>
<p><span class="font63">m</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">Oml</span><span class="font64"> = arg max &#160;&#160;&#160;log </span><span class="font64" style="font-weight:bold;">P</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">y</span><span class="font64"><sup>(i)</sup> | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(i)</sup>; </span><span class="font64" style="font-weight:bold;">ff</span><span class="font64">)</span><span class="font64" style="font-weight:bold;">.&#160;&#160;&#160;&#160;</span><span class="font64">(5.63)</span></p><div>
<p><span class="font64" style="font-weight:bold;">e</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p>
<p><span class="font64" style="font-weight:bold;">Example: Linear Regression as Maximum Likelihood </span><span class="font64">Linear regression, introduced earlier in Sec. 5.1.4, may be justified as a maximum likelihood procedure.&#160;Previously, we motivated linear regression as an algorithm that learns to take an&#160;input </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">and produce an output value y. The mapping from </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> to y is chosen to&#160;minimize mean squared error, a criterion that we introduced more or less arbitrarily.&#160;We now revisit linear regression from the point of view of maximum likelihood&#160;estimation. Instead of producing a single prediction y, we now think of the model&#160;as producing a conditional distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p(y</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">). We can imagine that with an&#160;infinitely large training set, we might see several training examples with the same&#160;input value </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">but different values of y. The goal of the learning algorithm is now to&#160;fit the distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">p (y</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) to all of those different y values that are all compatible&#160;with </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">. To derive the same linear regression algorithm we obtained before, we&#160;define p(y | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) = N (y; y(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">; </span><span class="font64" style="font-weight:bold;font-style:italic;">w</span><span class="font64">),<sup>2</sup>ס ). The function y(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">; </span><span class="font64" style="font-weight:bold;font-style:italic;">w)</span><span class="font64"> gives the prediction of&#160;the mean of the Gaussian. In this example, we assume that the variance is fixed to</span></p>
<p><span class="font12" style="font-style:italic;">C\</span></p>
<p><span class="font64">some constant </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>2</sup>ס</span><span class="font64"> chosen by the user. We will see that this choice of the functional form of p(y | </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) causes the maximum likelihood estimation procedure to yield the&#160;same learning algorithm as we developed before. Since the examples are assumed&#160;to be i.i.d., the conditional log-likelihood (Eq. 5.63) is given by</span></p><div>
<p><span class="font64">Eiog </span><span class="font64" style="font-weight:bold;font-style:italic;">p(y</span><span class="font64"><sup>(i) 1 x(i)</sup>; ff)</span></p></div><div>
<p><span class="font63">i=1</span></p></div><div>
<p><span class="font64">m</span></p></div><div>
<p><span class="font64">ו <sup>m</sup> <sub>l )</sub> &#160;&#160;&#160;<sup>|y(i) -</sup> y<sup>(i)|12</sup></span></p>
<p><span class="font64"><sup>10</sup>g <sup>10</sup>־ - <sup>ס</sup>g<sup>(2n) -</sup> &#160;&#160;&#160;202—</span></p></div><div>
<p><span class="font64">(5.64)</span></p>
<p><span class="font64">(5.65)</span></p></div><div>
<p><span class="font63">i=1</span></p></div>
<p><span class="font64">where is the output of the linear regression on the i-th input and m is the number of the training examples. Comparing the log-likelihood with the mean&#160;squared error,</span></p>
<p><span class="font63">m</span></p>
<p><span class="font64">MSEtrain = — V' || V <sup>W</sup> - V<sup>W</sup> ||<sup>2</sup> , &#160;&#160;&#160;(5.66)</span></p>
<p><span class="font64">m</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p>
<p><span class="font64">we immediately see that maximizing the log-likelihood with respect to </span><span class="font64" style="font-weight:bold;">w </span><span class="font64">yields the same estimate of the parameters </span><span class="font64" style="font-weight:bold;">w </span><span class="font64">as does minimizing the mean squared error.&#160;The two criteria have different values but the same location of the optimum. This&#160;justifies the use of the MSE as a maximum likelihood estimation procedure. As we&#160;will see, the maximum likelihood estimator has several desirable properties.</span></p><h5><a id="bookmark11"></a><span class="font64" style="font-weight:bold;">5.5.2 Properties of Maximum Likelihood</span></h5>
<p><span class="font64">The main appeal of the maximum likelihood estimator is that it can be shown to be the best estimator asymptotically, as the number of examples m ^ to ,in terms&#160;of its rate of convergence as m increases.</span></p>
<p><span class="font64">Under appropriate conditions, maximum likelihood estimator has the property of consistency (see Sec. 5.4.5 above), meaning that as the number of training&#160;examples approaches infinity, the maximum likelihood estimate of a parameter&#160;converges to the true value of the parameter. These conditions are:</span></p>
<p><span class="font64">• &#160;&#160;&#160;The true distribution pd<sub>ata</sub> must lie within the model family p<sub>mo</sub>d</span><span class="font18"><sub>e</sub>1</span><span class="font64"> (•; </span><span class="font64" style="font-weight:bold;">6</span><span class="font64">).&#160;Otherwise, no estimator can recover pd<sub>ata</sub>.</span></p>
<p><span class="font64">• &#160;&#160;&#160;The true distribution pd<sub>ata</sub> must correspond to exactly one value of </span><span class="font64" style="font-weight:bold;">6</span><span class="font64">. Otherwise, maximum likelihood can recover the correct pd<sub>ata</sub>, but will not be able&#160;to determine which value of </span><span class="font64" style="font-weight:bold;">6 </span><span class="font64">was used by the data generating processing.</span></p>
<p><span class="font64">There are other inductive principles besides the maximum likelihood estimator, many of which share the property of being consistent estimators. However, consistent estimators can differ in their </span><span class="font64" style="font-weight:bold;font-style:italic;">statistic efficiency,</span><span class="font64"> meaning that one consistent&#160;estimator may obtain lower generalization error for a fixed number of samples m,&#160;or equivalently, may require fewer examples to obtain a fixed level of generalization&#160;error.</span></p>
<p><span class="font64">Statistical efficiency is typically studied in the </span><span class="font64" style="font-weight:bold;font-style:italic;">parametric case</span><span class="font64"> (like in linear regression) where our goal is to estimate the value of a parameter (and assuming&#160;it is possible to identify the true parameter), not the value of a function. A way to&#160;measure how close we are to the true parameter is by the expected mean squared&#160;error, computing the squared difference between the estimated and true parameter&#160;values, where the expectation is over m training samples from the data generating&#160;distribution. That parametric mean squared error decreases as m increases, and&#160;for m large, the Cramer-Rao lower bound (Rao, 1945; Cramer, 1946) shows that no&#160;consistent estimator has a lower mean squared error than the maximum likelihood&#160;estimator.</span></p>
<p><span class="font64">For these reasons (consistency and efficiency), maximum likelihood is often considered the preferred estimator to use for machine learning. When the number&#160;of examples is small enough to yield overfitting behavior, regularization strategies&#160;such as weight decay may be used to obtain a biased version of maximum likelihood&#160;that has less variance when training data is limited.</span></p><h4><a id="bookmark12"></a><span class="font65" style="font-weight:bold;">5.6 Bayesian Statistics</span></h4>
<p><span class="font64">So far we have discussed </span><span class="font64" style="font-weight:bold;font-style:italic;">frequentist statistics</span><span class="font64"> and approaches based on estimating a single value of 6, then making all predictions thereafter based on that one estimate.&#160;Another approach is to consider all possible values of 6 when making a prediction.&#160;The latter is the domain of </span><span class="font64" style="font-weight:bold;font-style:italic;">Bayesian statistics.</span></p>
<p><span class="font64">As discussed in Sec. 5.4.1, the frequentist perspective is that the true parameter value 6 is fixed but unknown, while the point estimate 6 is a random variable on&#160;account of it being a function of the dataset (which is seen as random).</span></p>
<p><span class="font64">The Bayesian perspective on statistics is quite different. The Bayesian uses probability to reflect degrees of certainty of states of knowledge. The dataset is&#160;directly observed and so is not random. On the other hand, the true parameter 6&#160;is unknown or uncertain and thus is represented as a random variable.</span></p>
<p><span class="font64">Before observing the data, we represent our knowledge of 6 using the </span><span class="font64" style="font-weight:bold;font-style:italic;">prior probability distribution</span><span class="font64">,</span><span class="font64" style="font-weight:bold;font-style:italic;">p(6</span><span class="font64">) (sometimes referred to as simply “the prior”). Generally, the machine learning practitioner selects a prior distribution that is quite&#160;broad (i.e. with high entropy) to reflect a high degree of uncertainty in the value of&#160;6 before observing any data. For example, one might assume </span><span class="font64" style="font-weight:bold;font-style:italic;">a priori</span><span class="font64"> that 6 lies&#160;in some finite range or volume, with a uniform distribution. Many priors instead&#160;reflect a preference for “simpler” solutions (such as smaller magnitude coefficients,&#160;or a function that is closer to being constant).</span></p>
<p><span class="font64">Now consider that we have a set of data samples {x<sup>(1)</sup>,..., x<sup>( m)</sup>}. We can recover the effect of data on our belief about 6 by combining the data likelihood&#160;p(x<sup>(1)</sup>,..., x<sup>(m)</sup> | 6) with the prior via Bayes’ rule:</span></p><div>
<p><span class="font64">p(6 | x</span></p></div><div>
<p dir="rtl"><span class="font64" style="text-decoration:underline;">(<sup>6)</sup></span><span class="font64">1) &#160;&#160;&#160;<sub>x</sub>(m)<sub>)</sub> = </span><span class="font64" style="text-decoration:underline;">p</span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:underline;"><sup>(x(1),</sup>...<sup>,x(m)</sup></span><span class="font64" style="text-decoration:underline;"> | <sup>6)</sup>p</span><span class="font64">)</span></p>
<p dir="rtl"><span class="font64">(m׳)״ &#160;&#160;&#160;(1)&#160;&#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(&#160;&#160;&#160;&#160;x,</sup> . . . <sup>,</sup></span></p></div><div>
<p><span class="font64">p(x<sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;font-style:italic;">x<sup>(m)</sup>)</span></p></div><div>
<p><span class="font64">(5.67)</span></p></div>
<p><span class="font64">In the scenarios where Bayesian estimation is typically used, the prior begins as a relatively uniform or Gaussian distribution with high entropy, and the observation&#160;of the data usually causes the posterior to lose entropy and concentrate around a&#160;few highly likely values of the parameters.</span></p>
<p><span class="font64">Relative to maximum likelihood estimation, Bayesian estimation offers two important differences. First, unlike the maximum likelihood approach that makes&#160;predictions using a point estimate of 0, the Bayesian approach is to make predictions&#160;using a full distribution over 0. For example, after observing m examples, the&#160;predicted distribution over the next data sample,&#160;&#160;&#160;&#160;, is given by</span></p>
<p><span class="font64">p(x<sup>(m+1)</sup> | x<sup>(1)</sup>,..., x<sup>(m)</sup>) </span><span class="font64" style="font-weight:bold;font-style:italic;">= J</span><span class="font64"> p(x<sup>(m+1)</sup> | 0)p(0 | x<sup>(1)</sup>,..., x<sup>(m)</sup>) </span><span class="font64" style="font-weight:bold;font-style:italic;">d0.</span><span class="font64"> &#160;&#160;&#160;(5.68)</span></p>
<p><span class="font64">Here each value of 0 with positive probability density contributes to the prediction of the next example, with the contribution weighted by the posterior density itself.&#160;After having observed {x<sup>(1)</sup>,...,&#160;&#160;&#160;&#160;, if we are still quite uncertain about the</span></p>
<p><span class="font64">value of </span><span class="font64" style="font-weight:bold;font-style:italic;">0,</span><span class="font64"> then this uncertainty is incorporated directly into any predictions we might make.</span></p>
<p><span class="font64">In Sec. 5.4, we discussed how the frequentist approach addresses the uncertainty in a given point estimate of 0 by evaluating its variance. The variance of the&#160;estimator is an assessment of how the estimate might change with alternative&#160;samplings of the observed data. The Bayesian answer to the question of how to deal&#160;with the uncertainty in the estimator is to simply integrate over it, which tends to&#160;protect well against overfitting. This integral is of course just an application of&#160;the laws of probability, making the Bayesian approach simple to justify, while the&#160;frequentist machinery for constructing an estimator is based on the rather ad hoc&#160;decision to summarize all knowledge contained in the dataset with a single point&#160;estimate.</span></p>
<p><span class="font64">The second important difference between the Bayesian approach to estimation and the maximum likelihood approach is due to the contribution of the Bayesian&#160;prior distribution. The prior has an influence by shifting probability mass density&#160;towards regions of the parameter space that are preferred </span><span class="font64" style="font-weight:bold;font-style:italic;">a priori.</span><span class="font64"> In practice,&#160;the prior often expresses a preference for models that are simpler or more smooth.&#160;Critics of the Bayesian approach identify the prior as a source of subjective human&#160;judgment impacting the predictions.</span></p>
<p><span class="font64">Bayesian methods typically generalize much better when limited training data is available, but typically suffer from high computational cost when the number of&#160;training examples is large.</span></p>
<p><span class="font64">Example: Bayesian Linear Regression Here we consider the Bayesian estimation approach to learning the linear regression parameters. In linear regression, we learn a linear mapping from an input vector x E R<sup>n</sup> to predict the value of a&#160;scalar y E R. The prediction is parametrized by the vector </span><span class="font64" style="font-weight:bold;font-style:italic;">w E</span><span class="font64"> R<sup>n</sup>:</span></p><div>
<p><span class="font64">y _ w<sup>T</sup> x.</span></p></div>
<p><span class="font64">(5.69)</span></p>
<p><span class="font64">Given a set of m training samples (X (<sup>tram)</sup>, y (<sup>tram</sup>)), <sub>W</sub>e can express the prediction of y over the entire training set as:</span></p><div>
<p><span class="font64">(5.70)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">'</span><span class="font64"> (train)   x (train) ^</span></p>
<p><span class="font64">Expressed as a Gaussian conditional distribution on y<sup>(train)</sup>, we have</span></p><div>
<p><span class="font64">p(y<sup>(train)</sup> | X<sup>(train)</sup>, w) _ N(y<sup>(train)</sup>; X<sup>(train)</sup>w, I)</span></p>
<p><span class="font64">1 2</span></p></div>
<p><span class="font64">(5.71)</span></p><div>
<p><span class="font64">x exp</span></p></div>
<p><span class="font64">^_1 (y<sup>(train)</sup> __ X <sup>(train)</sup> <sub>W</sub>)<sup>T</sup> (y<sup>(train)</sup> __ X<sup>(train)</sup> <sub>w</sub>)^ ,</span></p>
<p><span class="font64">(5.72)</span></p>
<p><span class="font64">where we follow the standard MSE formulation in assuming that the Gaussian variance on y is one. In what follows, to reduce the notational burden, we refer to&#160;(X<sup>(train)</sup>, y<sup>(train)</sup>) as simply (X, y).</span></p>
<p><span class="font64">To determine the posterior distribution over the model parameter vector w, we first need to specify a prior distribution. The prior should reflect our naive belief&#160;about the value of these parameters. While it is sometimes difficult or unnatural&#160;to express our prior beliefs in terms of the parameters of the model, in practice we&#160;typically assume a fairly broad distribution expressing a high degree of uncertainty&#160;about 0. For real-valued parameters it is common to use a Gaussian as a prior&#160;distribution:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(w) _ N</span><span class="font64">(w; yo, a</span><span class="font18">0</span><span class="font64">) x exp ^- ^</span></p></div><div>
<p><span class="font64">w _ y 0)<sup>T</sup>A<sub>0</sub> <a id="footnote1"></a><sup><a href="#bookmark13">1</a></sup><sup></sup>(w _ yo</span></p></div><div>
<p><span class="font64"><sup>)</sup></span></p></div><div>
<p><span class="font64">(5.73)</span></p></div>
<p><span class="font64">where yo and A </span><span class="font18">0</span><span class="font64"> are the prior distribution mean vector and covariance matrix respectively.<a id="footnote1"></a><sup><a href="#bookmark13">1</a></sup><sup></sup></span></p>
<p><span class="font64">With the prior thus specified, we can now proceed in determining the </span><span class="font64" style="font-weight:bold;font-style:italic;">posterior </span><span class="font64">distribution over the model parameters.</span></p><div>
<p><span class="font64">(5.74)</span></p></div>
<p><span class="font64">p(w | X, </span><span class="font64" style="font-weight:bold;font-style:italic;">y) x p(y</span><span class="font64"> | X, w)p(w)</span></p>
<p><span class="font64">exp (<sup>-1 </sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(</sup>y <sup>- X</sup>w)<sup>T (</sup>y <sup>— X</sup>wU</span><span class="font64"> exp (<sup>-</sup> ^<sup>(</sup>w <sup>-</sup> y</span><span class="font18">0</span><span class="font64"><sup>)T</sup>A° <sup>1(</sup>w </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>-</sup></span><span class="font64"> y</span><span class="font18">0</span><span class="font64">) j</span></p><div>
<p><span class="font11">OC</span></p></div>
<p><span class="font64">(5.75)</span></p><div>
<p><span class="font64">(<sup>—</sup>2(</span></p></div><div>
<p><span class="font64">(X exp ( — ^ (—2y<sup>T</sup>Xw + w <sup>T</sup>X<sup>T</sup> Xw + w<sup>T</sup> A<sup>- 1</sup>w — 2y<sup>T</sup> A° <sup>1</sup>w</span></p></div><div>
<p><span class="font64">(5.76)</span></p></div>
<p><span class="font64">We now define A</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>m</sub></span><span class="font64"> — (X<sup>T</sup>X + A<sup>-1</sup> (<sup>1</sup>־ and </span><span class="font64" style="font-weight:bold;font-style:italic;">y<sub>m</sub></span><span class="font64"> — </span><span class="font64" style="font-weight:bold;font-style:italic;">A<sub>m</sub></span><span class="font64"> (X<sup>T</sup>y + A<sup>-</sup>&quot;<sup>1</sup>yo)• Using these new variables, we find that the posterior may be rewritten as a Gaussian&#160;distribution:</span></p><div>
<p><span class="font64">(5.77)</span></p></div>
<p><span class="font64">P<sup>(w</sup> | <sup>X</sup>, y<sup>) X ex</sup>P 2<sup>(w —</sup> ym)<sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">A<sub>m</sub><sup>1(w —</sup> y<sub>m</sub></span><span class="font64">) + 2 </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"> mAmV</span></p><div>
<p><span class="font64">1</span></p></div><div>
<p><span class="font64">X exp — 2(w — </span><span class="font64" style="font-weight:bold;font-style:italic;">ym</span><span class="font64">)<sup>T</sup> Am<sup>1</sup>(w — y<sub>7</sub></span></p></div><div>
<p><span class="font2">0</span><span class="font47">) •</span></p></div><div>
<p><span class="font64">(5.78)</span></p></div>
<p><span class="font64">All terms that do not include the parameter vector w have been omitted; they are implied by the fact that the distribution must be normalized to integrate to 1.&#160;Eq. 3.23 shows how to normalize a multivariate Gaussian distribution.</span></p>
<p><span class="font64">Examining this posterior distribution allows us to gain some intuition for the effect of Bayesian inference. In most situations, we set yo to 0. If we set <sup>A</sup>0 — a <sup>1&#160;</sup>then </span><span class="font64" style="font-weight:bold;font-style:italic;">y<sub>m</sub></span><span class="font64"> gives the same estimate of w as does frequentist linear regression with a&#160;weight decay penalty of </span><span class="font64" style="font-weight:bold;font-style:italic;">aw<sup>T</sup>w</span><span class="font64">. One difference is that the Bayesian estimate is&#160;undefined if a is set to zero—-we are not allowed to begin the Bayesian learning&#160;process with an infinitely wide prior on w. The more important difference is that&#160;the Bayesian estimate provides a covariance matrix, showing how likely all the&#160;different values of w are, rather than providing only the estimate y<sub>m</sub>.</span></p><h5><a id="bookmark14"></a><span class="font64">5.6.1 Maximum </span><span class="font64" style="font-weight:bold;font-style:italic;">A Posteriori</span><span class="font64"> (MAP) Estimation</span></h5>
<p><span class="font64">While the most principled approach is to make predictions using the full Bayesian posterior distribution over the parameter </span><span class="font64" style="font-weight:bold;font-style:italic;">6</span><span class="font64">, it is still often desirable to have a&#160;single point estimate. One common reason for desiring a point estimate is that&#160;most operations involving the Bayesian posterior for most interesting models are&#160;intractable, and a point estimate offers a tractable approximation. Rather than&#160;simply returning to the maximum likelihood estimate, we can still gain some of&#160;the benefit of the Bayesian approach by allowing the prior to influence the choice&#160;of the point estimate. One rational way to do this is to choose the </span><span class="font64" style="font-weight:bold;font-style:italic;">maximum a&#160;posteriori</span><span class="font64"> (MAP) point estimate. The MAP estimate chooses the point of maximal</span></p>
<p><span class="font64">posterior probability (or maximal probability density in the more common case of continuous 0):</span></p>
<p><span class="font64" style="font-variant:small-caps;">0<sub>M</sub>a<sub>P</sub></span><span class="font64"> = argmax</span><span class="font64" style="font-weight:bold;font-style:italic;">p(0</span><span class="font64"> | x) = argmaxlog</span><span class="font64" style="font-weight:bold;font-style:italic;">p(x</span><span class="font64"> | 0) + logp(0). &#160;&#160;&#160;(5.79)</span></p>
<p><span class="font63">0 </span><span class="font64" style="font-weight:bold;font-style:italic;">0</span></p>
<p><span class="font64">We recognize, above on the right hand side, logp(x | 0), i.e. the standard log-likelihood term, and logp(0), corresponding to the prior distribution.</span></p>
<p><span class="font64">As an example, consider a linear regression model with a Gaussian prior on the weights w. If this prior is given by N(w;0, 1־I<sup>2</sup>), then the log-prior term in Eq.&#160;5.79 is proportional to the familiar Aw<sup>T</sup> w weight decay penalty, plus a term that&#160;does not depend on w and does not affect the learning process. MAP Bayesian&#160;inference with a Gaussian prior on the weights thus corresponds to weight decay.</span></p>
<p><span class="font64">As with full Bayesian inference, MAP Bayesian inference has the advantage of leveraging information that is brought by the prior and cannot be found in the&#160;training data. This additional information helps to reduce the variance in the&#160;MAP point estimate (in comparison to the ML estimate). However, it does so at&#160;the price of increased bias.</span></p>
<p><span class="font64">Many regularized estimation strategies, such as maximum likelihood learning regularized with weight decay, can be interpreted as making the MAP approximation to Bayesian inference. This view applies when the regularization consists of&#160;adding an extra term to the objective function that corresponds to logp(0). Not&#160;all regularization penalties correspond to MAP Bayesian inference. For example,&#160;some regularizer terms may not be the logarithm of a probability distribution.&#160;Other regularization terms depend on the data, which of course a prior probability&#160;distribution is not allowed to do.</span></p>
<p><span class="font64">MAP Bayesian inference provides a straightforward way to design complicated yet interpretable regularization terms. For example, a more complicated penalty&#160;term can be derived by using a mixture of Gaussians, rather than a single Gaussian&#160;distribution, as the prior (Nowlan and Hinton, 1992).</span></p><h4><a id="bookmark15"></a><span class="font65" style="font-weight:bold;">5.7 Supervised Learning Algorithms</span></h4>
<p><span class="font64">Recall from Sec. 5.1.3 that supervised learning algorithms are, roughly speaking, learning algorithms that learn to associate some input with some output, given a&#160;training set of examples of inputs x and outputs y. In many cases the outputs&#160;y may be difficult to collect automatically and must be provided by a human&#160;“supervisor,” but the term still applies even when the training set targets were&#160;collected automatically.</span></p><h5><a id="bookmark16"></a><span class="font64" style="font-weight:bold;">5.7.1 &#160;&#160;&#160;Probabilistic Supervised Learning</span></h5>
<p><span class="font64">Most supervised learning algorithms in this book are based on estimating a probability distribution p(y </span><span class="font64" style="font-weight:bold;font-style:italic;">|</span><span class="font64"> x). We can do this simply by using maximum&#160;likelihood estimation to find the best parameter vector 0 for a parametric family&#160;of distributions p(y | x; 0).</span></p>
<p><span class="font64">We have already seen that linear regression corresponds to the family</span></p>
<p><span class="font64">p(y | x; 0) = N(y; 0<sup>Tx</sup>,<sup>1)</sup>. &#160;&#160;&#160;(5•<sup>80</sup>)</span></p>
<p><span class="font64">We can generalize linear regression to the classification scenario by defining a different family of probability distributions. If we have two classes, class 0 and&#160;class 1, then we need only specify the probability of one of these classes. The&#160;probability of class 1 determines the probability of class 0, because these two values&#160;must add up to 1.</span></p>
<p><span class="font64">The normal distribution over real-valued numbers that we used for linear regression is parametrized in terms of a mean. Any value we supply for this mean&#160;is valid. A distribution over a binary variable is slightly more complicated, because&#160;its mean must always be between 0 and 1. One way to solve this problem is to use&#160;the logistic sigmoid function to squash the output of the linear function into the&#160;interval (0, 1) and interpret that value as a probability:</span></p>
<p><span class="font64">p(y = 1 | x; 0) = a(0<sup>T</sup>x). &#160;&#160;&#160;(5.81)</span></p>
<p><span class="font64">This approach is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">logistic regression</span><span class="font64"> (a somewhat strange name since we use the model for classification rather than regression).</span></p>
<p><span class="font64">In the case of linear regression, we were able to find the optimal weights by solving the normal equations. Logistic regression is somewhat more difficult. There&#160;is no closed-form solution for its optimal weights. Instead, we must search for&#160;them by maximizing the log-likelihood. We can do this by minimizing the negative&#160;log-likelihood (NLL) using gradient descent.</span></p>
<p><span class="font64">This same strategy can be applied to essentially any supervised learning problem, by writing down a parametric family of conditional probability distributions over&#160;the right kind of input and output variables.</span></p><h5><a id="bookmark17"></a><span class="font64" style="font-weight:bold;">5.7.2 &#160;&#160;&#160;Support Vector Machines</span></h5>
<p><span class="font64">One of the most influential approaches to supervised learning is the support vector machine (Boser </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1992; Cortes and Vapnik, 1995). This model is similar to&#160;logistic regression in that it is driven by a linear function w<sup>T</sup> x + b. Unlike logistic&#160;regression, the support vector machine does not provide probabilities, but only&#160;outputs a class identity. The SVM predicts that the positive class is present when&#160;w<sup>T</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> + b is positive. Likewise, it predicts that the negative class is present when&#160;w<sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> + b is negative.</span></p>
<p><span class="font64">One key innovation associated with support vector machines is the </span><span class="font64" style="font-weight:bold;font-style:italic;">kernel trick. </span><span class="font64">The kernel trick consists of observing that many machine learning algorithms can&#160;be written exclusively in terms of dot products between examples. For example, it&#160;can be shown that the linear function used by the support vector machine can be&#160;re-written as</span></p>
<p><span class="font64">m</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">w</span><span class="font64"><sup>T</sup>x + b = b + &#160;&#160;&#160;a</span><span class="font64" style="font-weight:bold;font-style:italic;">{X<sup>T</sup>x</span><span class="font64"><sup>(i)</sup>&#160;&#160;&#160;&#160;(5.82)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p>
<p><span class="font64">where x<sup>(i)</sup> is a training example and a is a vector of coefficients. Rewriting the learning algorithm this way allows us to replace x by the output of a given feature&#160;function ^(x) and the dot product with a function k(x, x<sup>(i)</sup>) = ^(x) ■ ^ (x<sup>(i)</sup>) called&#160;a </span><span class="font64" style="font-weight:bold;font-style:italic;">kernel.</span><span class="font64"> The ■ operator represents an inner product analogous to ^(x )<sup>T</sup> ^(x<sup>(i)</sup>).&#160;For some feature spaces, we may not use literally the vector inner product. In&#160;some infinite dimensional spaces, we need to use other kinds of inner products, for&#160;example, inner products based on integration rather than summation. A complete&#160;development of these kinds of inner products is beyond the scope of this book.</span></p>
<p><span class="font64">After replacing dot products with kernel evaluations, we can make predictions using the function</span></p>
<p><span class="font64">f (x) = b + &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">a</span><span class="font64"> <sub>i</sub>k(x, x<sup>(i)</sup>).&#160;&#160;&#160;&#160;(5.83)</span></p>
<p><span class="font64">This function is nonlinear with respect to x, but the relationship between ^ (x) and f (x) is linear. Also, the relationship between a and f(x) is linear. The&#160;kernel-based function is exactly equivalent to preprocessing the data by applying&#160;^(x) to all inputs, then learning a linear model in the new transformed space.</span></p>
<p><span class="font64">The kernel trick is powerful for two reasons. First, it allows us to learn models that are nonlinear as a function of x using convex optimization techniques that are&#160;guaranteed to converge efficiently. This is possible because we consider ^ fixed and&#160;optimize only a, i.e., the optimization algorithm can view the decision function&#160;as being linear in a different space. Second, the kernel function k often admits&#160;an implementation that is significantly more computational efficient than naively&#160;constructing two ^(x) vectors and explicitly taking their dot product.</span></p>
<p><span class="font64">In some cases, &#160;&#160;&#160;x) can even be infinite dimensional, which would result in</span></p>
<p><span class="font64">an infinite computational cost for the naive, explicit approach. In many cases, k(x, </span><span class="font64" style="font-weight:bold;font-style:italic;">x')</span><span class="font64"> is a nonlinear, tractable function of x even when ^(x) is intractable. As&#160;an example of an infinite-dimensional feature space with a tractable kernel, we&#160;construct a feature mapping </span><span class="font31" style="font-style:italic;">^</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">(x) over the non-negative integers </span><span class="font31" style="font-style:italic;">x</span><span class="font64" style="font-weight:bold;font-style:italic;">.</span><span class="font64"> Suppose that&#160;this mapping returns a vector containing x ones followed by infinitely many zeros.&#160;We can write a kernel function k(x,x<sup>(i)</sup>) = mm(x,x<sup>(i)</sup>) that is exactly equivalent&#160;to the corresponding infinite-dimensional dot product.</span></p>
<p><span class="font64">The most commonly used kernel is the </span><span class="font64" style="font-weight:bold;font-style:italic;">Gaussian kernel</span></p>
<p><span class="font64">k(u, v) </span><span class="font64" style="font-weight:bold;font-style:italic;">= N (u — v</span><span class="font64"> ;0 , a<sup>2</sup>I) &#160;&#160;&#160;(5.84)</span></p>
<p><span class="font64">where N(x</span><span class="font64" style="font-weight:bold;font-style:italic;">; M</span><span class="font31" style="font-style:italic;">,</span><span class="font64" style="font-weight:bold;"> </span><span class="font64"><sup>s</sup>) is the standard normal density. This kernel is also known as the </span><span class="font64" style="font-weight:bold;font-style:italic;">radial basis function</span><span class="font64"> (RBF) kernel, because its value decreases along lines in&#160;v space radiating outward from u. The Gaussian kernel corresponds to a dot&#160;product in an infinite-dimensional space, but the derivation of this space is less&#160;straightforward than in our example of the min kernel over the integers.</span></p>
<p><span class="font64">We can think of the Gaussian kernel as performing a kind of </span><span class="font64" style="font-weight:bold;font-style:italic;">template matching. </span><span class="font64">A training example x associated with training label y becomes a template for class&#160;y. When a test point </span><span class="font64" style="font-weight:bold;font-style:italic;">x'</span><span class="font64"> is near x according to Euclidean distance, the Gaussian&#160;kernel has a large response, indicating that x' is very similar to the x template.&#160;The model then puts a large weight on the associated training label y. Overall,&#160;the prediction will combine many such training labels weighted by the similarity&#160;of the corresponding training examples.</span></p>
<p><span class="font64">Support vector machines are not the only algorithm that can be enhanced using the kernel trick. Many other linear models can be enhanced in this way. The&#160;category of algorithms that employ the kernel trick is known as </span><span class="font64" style="font-weight:bold;font-style:italic;">kernel machines&#160;</span><span class="font64">or </span><span class="font64" style="font-weight:bold;font-style:italic;">kernel methods</span><span class="font64"> (Williams and Rasmussen, 1996; Schdlkopf </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1999).</span></p>
<p><span class="font64">A major drawback to kernel machines is that the cost of evaluating the decision function is linear in the number of training examples, because the i-th example&#160;contributes a term a<sub>i</sub>k(x, x<sup>(i)</sup>) to the decision function. Support vector machines&#160;are able to mitigate this by learning an a vector that contains mostly zeros.&#160;Classifying a new example then requires evaluating the kernel function only for&#160;the training examples that have non-zero a</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>i</sub>.</span><span class="font64"> These training examples are known&#160;as </span><span class="font64" style="font-weight:bold;font-style:italic;">support vectors.</span></p>
<p><span class="font64">Kernel machines also suffer from a high computational cost of training when the dataset is large. We will revisit this idea in Sec. 5.9. Kernel machines with&#160;generic kernels struggle to generalize well. We will explain why in Sec. 5.11. The&#160;modern incarnation of deep learning was designed to overcome these limitations of&#160;kernel machines. The current deep learning renaissance began when Hinton </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.&#160;</span><span class="font64">(2006) demonstrated that a neural network could outperform the RBF kernel SVM&#160;on the MNIST benchmark.</span></p>
<p><a id="bookmark13"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font64"><sup></sup>Unless there is a reason to assume a particular covariance structure, we typically assume a diagonal covariance matrix A = diag(A </span><span class="font18">0</span><span class="font64">).</span></p>
</body>
</html>