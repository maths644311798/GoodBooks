<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 18</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Confronting the Partition Function</span></h2>
<p><span class="font64">In Sec. 16.2.2 we saw that many probabilistic models (commonly known as undirected graphical models) are defined by an unnormalized probability distribution p(x; 9). We must normalize p by dividing by a partition function </span><span class="font64" style="font-weight:bold;font-style:italic;">Z(</span><span class="font65" style="font-style:italic;">6</span><span class="font64" style="font-weight:bold;font-style:italic;">)</span><span class="font64"> in order to&#160;obtain a valid probability distribution:</span></p>
<p><span class="font64" style="font-weight:bold;">1</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p<sup>(x</sup>;<sup>e</sup>) = Z(</span><span class="font65" style="font-style:italic;">6</span><span class="font64" style="font-weight:bold;font-style:italic;">)<sup>p(x</sup>;</span><span class="font64"> </span><span class="font18"><sup>6</sup></span><span class="font64" style="font-weight:bold;"><sup>)</sup></span><span class="font64"><sup>18-1)</sup> &#160;&#160;&#160;־<sup>)</sup></span></p>
<p><span class="font64">The partition function is an integral (for continuous variables) or sum (for discrete variables) over the unnormalized probability of all states:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">j</span><span class="font64"> p(x)dx &#160;&#160;&#160;(18.2)</span></p>
<p><span class="font64">or</span></p><div>
<p><span class="font64">(18.3)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">J2<sup>p(x</sup>)■</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x</span></p>
<p><span class="font64">This operation is intractable for many interesting models.</span></p>
<p><span class="font64">As we will see in Chapter 20, several deep learning models are designed to have a tractable normalizing constant, or are designed to be used in ways that do&#160;not involve computing </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x)</span><span class="font64"> at all. However, other models directly confront the&#160;challenge of intractable partition functions. In this chapter, we describe techniques&#160;used for training and evaluating models that have intractable partition functions.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">18.1 The Log-Likelihood Gradient</span></h4>
<p><span class="font64">What makes learning undirected models by maximum likelihood particularly difficult is that the partition function depends on the parameters. The gradient of&#160;the log-likelihood with respect to the parameters has a term corresponding to the&#160;gradient of the partition function:</span></p><div>
<p><span class="font64">(18.4)</span></p></div>
<p><span class="font64">Ve logp(x; </span><span class="font18">6</span><span class="font64">) _ Ve log p(x; </span><span class="font64" style="font-weight:bold;font-style:italic;">6) -</span><span class="font64"> Ve log </span><span class="font64" style="font-weight:bold;font-style:italic;">Z(6).</span></p>
<p><span class="font64">This is a well-known decomposition into the </span><span class="font64" style="font-weight:bold;font-style:italic;">positive phase</span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">negative phase </span><span class="font64">of learning.</span></p>
<p><span class="font64">For most undirected models of interest, the negative phase is difficult. Models with no latent variables or with few interactions between latent variables typically&#160;have a tractable positive phase. The quintessential example of a model with a&#160;straightforward positive phase and difficult negative phase is the RBM, which has&#160;hidden units that are conditionally independent from each other given the visible&#160;units. The case where the positive phase is difficult, with complicated interactions&#160;between latent variables, is primarily covered in Chapter 19. This chapter focuses&#160;on the difficulties of the negative phase.</span></p>
<p><span class="font64">Let us look more closely at the gradient of log Z:</span></p><div>
<table border="1">
<tr><td>
<p><span class="font64">Ve log Z</span></p></td><td>
<p><span class="font64">(18.5)</span></p></td></tr>
<tr><td>
<p dir="rtl"><span class="font64">נא</span></p>
<p><span class="font64">II</span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(18.6)</span></p></td></tr>
<tr><td>
<p><span class="font64"><sup>V</sup>e Ex <sup>p(x) _</sup>Z</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">(18.7)</span></p></td></tr>
<tr><td>
<p><span class="font64">_ Ex <sup>V</sup> e<sup>p(x)</sup></span></p>
<p><span class="font64"><sup>_</sup>Z<sup>.</sup></span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(18.8)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">For models that guarantee p(x) &gt; 0 for all x,</span></p></td><td style="vertical-align:bottom;">
<p><span class="font64">we can substitute exp (log p(x))</span></p></td></tr>
</table>
<p><span class="font64">for p(x):</span></p></div><div>
<p><span class="font64">(18.9)</span></p>
<p><span class="font64">(18.10)</span></p>
<p><span class="font64">(18.11)</span></p>
<p><span class="font64">(18.12)</span></p></div>
<p><span class="font64" style="text-decoration:underline;">Ex <sup>V</sup> e <sup>ex</sup>P <sup>(</sup>l°g <sup>p</sup>(x))</span></p>
<p><span class="font64">Z</span></p>
<p><span class="font64" style="text-decoration:underline;">Ex <sup>ex</sup>P <sup>(lo</sup>g <sup>p(x)) V</sup>e log p(x) </span><span class="font64">Z</span></p>
<p><span class="font64">_ </span><span class="font64" style="text-decoration:underline;">Ex <sup>p(x)V</sup>e log p(x)</span></p>
<p><span class="font64">Z</span></p>
<p><span class="font64">_5Z P(x)Ve tog <sup>p(x)</sup></span></p>
<p><span class="font64">= Ex-p(x) <sup>V</sup>e log p(x).</span></p><div>
<p><span class="font64">(18.13)</span></p></div>
<p><span class="font64">This derivation made use of summation over discrete x, but a similar result applies using integration over continuous x. In the continuous version of the&#160;derivation, we use Leibniz’s rule for differentiation under the integral sign to obtain&#160;the identity</span></p>
<p><span class="font64">V<sub>e</sub> </span><span class="font64" style="font-weight:bold;font-style:italic;">Jp(x)dx = J</span><span class="font64"> V<sub>e</sub>p(x)dx. &#160;&#160;&#160;(18.14)</span></p>
<p><span class="font64">This identity is applicable only under certain regularity conditions on </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> and Vep(x). In measure theoretic terms, the conditions are: (i) The unnormalized distributionp&#160;must be a Lebesgue-integrable function of x for every value of 0; (ii) The gradient&#160;Vep(x) must exist for all 0 and almost all x; (iii) There must exist an integrable&#160;function R(x) that bounds V ep(x) in the sense that max* | dy p(x )| &lt; R(x) for all&#160;0 and almost all x. Fortunately, most machine learning models of interest have&#160;these properties.</span></p>
<p><span class="font64">This identity</span></p>
<p><span class="font64">Ve log </span><span class="font64" style="font-weight:bold;font-style:italic;">Z =</span><span class="font64"> Ex^(x) Ve log p(x) &#160;&#160;&#160;(18.15)</span></p>
<p><span class="font64">is the basis for a variety of Monte Carlo methods for approximately maximizing the likelihood of models with intractable partition functions.</span></p>
<p><span class="font64">The Monte Carlo approach to learning undirected models provides an intuitive framework in which we can think of both the positive phase and the negative&#160;phase. In the positive phase, we increase log p(x) for x drawn from the data. In&#160;the negative phase, we decrease the partition function by decreasing log p(x) drawn&#160;from the model distribution.</span></p>
<p><span class="font64">In the deep learning literature, it is common to parametrize log </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> in terms of an energy function (Eq. 16.7). In this case, we can interpret the positive phase&#160;as pushing down on the energy of training examples and the negative phase as&#160;pushing up on the energy of samples drawn from the model, as illustrated in Fig.</span></p>
<p><span class="font64">18.1.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">18.2 Stochastic Maximum Likelihood and Contrastive&#160;Divergence</span></h4>
<p><span class="font64">The naive way of implementing Eq. 18.15 is to compute it by burning in a set of Markov chains from a random initialization every time the gradient is needed.&#160;When learning is performed using stochastic gradient descent, this means the&#160;chains must be burned in once per gradient step. This approach leads to the&#160;training procedure presented in Algorithm 18.1. The high cost of burning in the&#160;Markov chains in the inner loop makes this procedure computationally infeasible,&#160;but this procedure is the starting point that other more practical algorithms aim&#160;to approximate.</span></p>
<p><span class="font64">Algorithm 18.1 A naive MCMC algorithm for maximizing the log-likelihood with an intractable partition function using gradient ascent.</span></p>
<p><span class="font64">Set </span><span class="font64" style="font-weight:bold;">e</span><span class="font64">, the step size, to a small positive number.</span></p>
<p><span class="font64">Set </span><span class="font64" style="font-weight:bold;">k</span><span class="font64">, the number of Gibbs steps, high enough to allow burn in. Perhaps 100 to train an RBM on a small image patch.&#160;while not converged do</span></p>
<p><span class="font64">Sample a minibatch of </span><span class="font64" style="font-weight:bold;">m </span><span class="font64">examples {x<sup>(1)</sup></span><span class="font64" style="font-weight:bold;">,..., </span><span class="font64">x<sup>(m)</sup>} from the training set. g ^ ״E</span><span class="font18">1</span><span class="font64">״ V log </span><span class="font64" style="font-weight:bold;">p</span><span class="font64">(x<sup>(i)</sup>;</span><span class="font18">6</span><span class="font64">).</span></p>
<p><span class="font64">Initialize a set of </span><span class="font64" style="font-weight:bold;">m </span><span class="font64">samples {X<sup>(1)</sup></span><span class="font64" style="font-weight:bold;">,..., </span><span class="font64">X<sup>(m)</sup>} to random values (e.g., from a uniform or normal distribution, or possibly a distribution with marginals&#160;matched to the model’s marginals).&#160;for </span><span class="font64" style="font-weight:bold;">i </span><span class="font64">= </span><span class="font18">1</span><span class="font64"> to </span><span class="font64" style="font-weight:bold;">k </span><span class="font64">do&#160;for </span><span class="font64" style="font-weight:bold;">j </span><span class="font64">= </span><span class="font18">1</span><span class="font64"> to </span><span class="font64" style="font-weight:bold;">m </span><span class="font64">do</span></p>
<p><span class="font64">X<sup>(j)</sup> ^ gibbs_update(5i<sup>(j)</sup>)</span><span class="font64" style="font-weight:bold;">. </span><span class="font64">end for&#160;end for</span></p>
<p><span class="font64">g ^ g - ״ E<sub>1</sub>=״ V </span><span class="font18">1</span><span class="font64">°g </span><span class="font64" style="font-weight:bold;"><sup>p</sup></span><span class="font64"><sup>(x (i)</sup>; <a id="footnote1"></a><sup><a href="#bookmark3">1</a></sup><sup></sup></span><span class="font64" style="font-weight:bold;">.</span></p>
<p><span class="font12" style="font-weight:bold;font-style:italic;">6 ^ 6</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">+ </span><span class="font64" style="font-weight:bold;">e</span><span class="font64">g</span><span class="font64" style="font-weight:bold;">. </span><span class="font64">end while</span></p><div><img src="main-181.jpg" alt=""/>
<p><span class="font64">Figure 18.1: The view of Algorithm 18.1 as having a “positive phase” and “negative phase.” </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> In the positive phase, we sample points from the data distribution, and push up on&#160;their unnormalized probability. This means points that are likely in the data get pushed&#160;up on more. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> In the negative phase, we sample points from the model distribution,&#160;and push down on their unnormalized probability. This counteracts the positive phase’s&#160;tendency to just add a large constant to the unnormalized probability everywhere. When&#160;the data distribution and the model distribution are equal, the positive phase has the&#160;same chance to push up at a point as the negative phase has to push down. When this&#160;occurs, there is no longer any gradient (in expectation) and training must terminate.</span></p></div>
<p><span class="font64">for dreaming in humans and other animals (Crick and Mitchison, 1983), the idea being that the brain maintains a probabilistic model of the world and follows the&#160;gradient of log </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> while experiencing real events while awake and follows the negative&#160;gradient of log </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> to minimize log Z while sleeping and experiencing events sampled&#160;from the current model. This view explains much of the language used to describe&#160;algorithms with a positive and negative phase, but it has not been proven to be&#160;correct with neuroscientific experiments. In machine learning models, it is usually&#160;necessary to use the positive and negative phase simultaneously, rather than in&#160;separate time periods of wakefulness and REM sleep. As we will see in Sec. 19.5,&#160;other machine learning algorithms draw samples from the model distribution for&#160;other purposes and such algorithms could also provide an account for the function&#160;of dream sleep.</span></p>
<p><span class="font64">Given this understanding of the role of the positive and negative phase of learning, we can attempt to design a less expensive alternative to Algorithm 18.1.&#160;The main cost of the naive MCMC algorithm is the cost of burning in the Markov&#160;chains from a random initialization at each step. A natural solution is to initialize&#160;the Markov chains from a distribution that is very close to the model distribution,&#160;so that the burn in operation does not take as many steps.</span></p>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">contrastive divergence</span><span class="font64"> (CD, or CD-k to indicate CD with </span><span class="font64" style="font-weight:bold;font-style:italic;">k</span><span class="font64"> Gibbs steps) algorithm initializes the Markov chain at each step with samples from the data&#160;distribution (Hinton, 2000, 2010). This approach is presented as Algorithm 18.2.&#160;Obtaining samples from the data distribution is free, because they are already&#160;available in the data set. Initially, the data distribution is not close to the model&#160;distribution, so the negative phase is not very accurate. Fortunately, the positive&#160;phase can still accurately increase the model’s probability of the data. After the&#160;positive phase has had some time to act, the model distribution is closer to the&#160;data distribution, and the negative phase starts to become accurate.</span></p>
<p><span class="font64">Algorithm 18.2 The contrastive divergence algorithm, using gradient ascent as the optimization procedure.</span></p>
<p><span class="font64">Set e, the step size, to a small positive number.</span></p>
<p><span class="font64">Set k, the number of Gibbs steps, high enough to allow a Markov chain sampling from p(x; </span><span class="font65" style="font-style:italic;">6</span><span class="font64">) to mix when initialized from </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64" style="font-weight:bold;">d<sub>ata</sub></span><span class="font64">. Perhaps 1-20 to train an RBM&#160;on a small image patch.&#160;while not converged do</span></p><div>
<p><span class="font64">x</span></p></div><div>
<p><span class="font64"><sup>(m)</sup>} from the training set.</span></p></div>
<p><span class="font64">Sample a minibatch of m examples {x<sup>(1)</sup>-g ^ mE׳=! v log p(x<sup>(i)</sup><sub>;</sub></span><span class="font18">6</span><span class="font64">). for </span><span class="font64" style="font-weight:bold;font-style:italic;">i =</span><span class="font64"> </span><span class="font18">1</span><span class="font64"> to m do&#160;x<sup>(i)</sup> ^ x<sup>(i)</sup>.&#160;end for</span></p>
<p><span class="font64">for i = </span><span class="font18">1</span><span class="font64"> to k do for </span><span class="font64" style="font-weight:bold;font-style:italic;">j = 1</span><span class="font64"> to m do</span></p>
<p><span class="font64">x<sup>(j)</sup> ^ gibbs_update(5i<sup>(j)</sup>). end for&#160;end for</span></p>
<p><span class="font64">g ^ g - </span><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font63" style="font-style:italic;">Tdl 1</span><span class="font63"> V l°g <sup>p(x(i)</sup>; <sup>6)</sup>.</span></p>
<p><span class="font18">6</span><span class="font64"> ^ </span><span class="font18">6</span><span class="font64"> + eg. end while</span></p>
<p><span class="font64">Of course, CD is still an approximation to the correct negative phase. The main way that CD qualitatively fails to implement the correct negative phase&#160;is that it fails to suppress regions of high probability that are far from actual&#160;training examples. These regions that have high probability under the model but&#160;low probability under the data generating distribution are called </span><span class="font64" style="font-weight:bold;font-style:italic;">spurious modes.&#160;</span><span class="font64">Fig. 18.2 illustrates why this happens. Essentially, it is because modes in the&#160;model distribution that are far from the data distribution will not be visited by</span></p><div>
<p><span class="font64">A spurious mode</span></p><img src="main-182.jpg" alt=""/></div>
<p><span class="font64">Figure 18.2: An illustration of how the negative phase of contrastive divergence (Algorithm 18.2) can fail to suppress spurious modes. A spurious mode is a mode that is present in&#160;the model distribution but absent in the data distribution. Because contrastive divergence&#160;initializes its Markov chains from data points and runs the Markov chain for only a&#160;few steps, it is unlikely to visit modes in the model that are far from the data points.&#160;This means that when sampling from the model, we will sometimes get samples that do&#160;not resemble the data. It also means that due to wasting some of its probability mass&#160;on these modes, the model will struggle to place high probability mass on the correct&#160;modes. For the purpose of visualization, this figure uses a somewhat simplified concept&#160;of distance—the spurious mode is far from the correct mode along the number line in&#160;R. This corresponds to a Markov chain based on making local moves with a single x&#160;variable in R. For most deep probabilistic models, the Markov chains are based on Gibbs&#160;sampling and can make non-local moves of individual variables but cannot move all of&#160;the variables simultaneously. For these problems, it is usually better to consider the edit&#160;distance between modes, rather than the Euclidean distance. However, edit distance in a&#160;high dimensional space is difficult to depict in a 2-D plot.</span></p>
<p><span class="font64">Markov chains initialized at training points, unless k is very large.</span></p>
<p><span class="font64">Carreira-Perpinan and Hinton (2005) showed experimentally that the CD estimator is biased for RBMs and fully visible Boltzmann machines, in that it&#160;converges to different points than the maximum likelihood estimator. They argue&#160;that because the bias is small, CD could be used as an inexpensive way to initialize&#160;a model that could later be fine-tuned via more expensive MCMC methods. Bengio&#160;and Delalleau (2009) showed that CD can be interpreted as discarding the smallest&#160;terms of the correct MCMC update gradient, which explains the bias.</span></p>
<p><span class="font64">CD is useful for training shallow models like RBMs. These can in turn be stacked to initialize deeper models like DBNs or DBMs. However, CD does not&#160;provide much help for training deeper models directly. This is because it is difficult&#160;to obtain samples of the hidden units given samples of the visible units. Since the&#160;hidden units are not included in the data, initializing from training points cannot&#160;solve the problem. Even if we initialize the visible units from the data, we will still&#160;need to burn in a Markov chain sampling from the distribution over the hidden&#160;units conditioned on those visible samples.</span></p>
<p><span class="font64">The CD algorithm can be thought of as penalizing the model for having a Markov chain that changes the input rapidly when the input comes from the data.&#160;This means training with CD somewhat resembles autoencoder training. Even&#160;though CD is more biased than some of the other training methods, it can be&#160;useful for pretraining shallow models that will later be stacked. This is because&#160;the earliest models in the stack are encouraged to copy more information up to&#160;their latent variables, thereby making it available to the later models. This should&#160;be thought of more of as an often-exploitable side effect of CD training rather than&#160;a principled design advantage.</span></p>
<p><span class="font64">Sutskever and Tieleman (2010) showed that the CD update direction is not the gradient of any function. This allows for situations where CD could cycle forever,&#160;but in practice this is not a serious problem.</span></p>
<p><span class="font64">A different strategy that resolves many of the problems with CD is to initialize the Markov chains at each gradient step with their states from the previous gradient&#160;step. This approach was first discovered under the name </span><span class="font64" style="font-weight:bold;font-style:italic;">stochastic maximum&#160;likelihood</span><span class="font64"> (SML) in the applied mathematics and statistics community (Younes,&#160;1998) and later independently rediscovered under the name </span><span class="font64" style="font-weight:bold;font-style:italic;">persistent contrastive&#160;divergence</span><span class="font64"> (PCD, or PCD-k to indicate the use of k Gibbs steps per update) in&#160;the deep learning community (Tieleman, 2008). See Algorithm 18.3. The basic&#160;idea of this approach is that, so long as the steps taken by the stochastic gradient&#160;algorithm are small, then the model from the previous step will be similar to the&#160;model from the current step. It follows that the samples from the previous model’s&#160;distribution will be very close to being fair samples from the current model’s&#160;distribution, so a Markov chain initialized with these samples will not require much&#160;time to mix.</span></p>
<p><span class="font64">Because each Markov chain is continually updated throughout the learning process, rather than restarted at each gradient step, the chains are free to wander&#160;far enough to find all of the model’s modes. SML is thus considerably more&#160;resistant to forming models with spurious modes than CD is. Moreover, because&#160;it is possible to store the state of all of the sampled variables, whether visible or&#160;latent, SML provides an initialization point for both the hidden and visible units.&#160;CD is only able to provide an initialization for the visible units, and therefore&#160;requires burn-in for deep models. SML is able to train deep models efficiently.</span></p>
<p><span class="font64">Marlin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2010) compared SML to many of the other criteria presented in this chapter. They found that SML results in the best test set log-likelihood for&#160;an RBM, and that if the RBM’s hidden units are used as features for an SVM&#160;classifier, SML results in the best classification accuracy.</span></p>
<p><span class="font64">SML is vulnerable to becoming inaccurate if the stochastic gradient algorithm can move the model faster than the Markov chain can mix between steps. This&#160;can happen if k is too small or e is too large. The permissible range of values is&#160;unfortunately highly problem-dependent. There is no known way to test formally&#160;whether the chain is successfully mixing between steps. Subjectively, if the learning&#160;rate is too high for the number of Gibbs steps, the human operator will be able&#160;to observe that there is much more variance in the negative phase samples across&#160;gradient steps rather than across different Markov chains. For example, a model&#160;trained on MNIST might sample exclusively 7s on one step. The learning process&#160;will then push down strongly on the mode corresponding to 7s, and the model&#160;might sample exclusively 9s on the next step.</span></p>
<p><span class="font64" style="font-weight:bold;">Algorithm 18.3 </span><span class="font64">The stochastic maximum likelihood / persistent contrastive divergence algorithm using gradient ascent as the optimization procedure.</span></p>
<p><span class="font64">Set e, the step size, to a small positive number.</span></p>
<p><span class="font64">Set k, the number of Gibbs steps, high enough to allow a Markov chain sampling from </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x;</span><span class="font64"> </span><span class="font18">6</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">eg</span><span class="font64">) to burn in, starting from samples from </span><span class="font64" style="font-weight:bold;font-style:italic;">p(x;</span><span class="font64"> </span><span class="font18">6</span><span class="font64">). Perhaps 1 for&#160;RBM on a small image patch, or 5-50 for a more complicated model like a DBM.&#160;Initialize a set of m samples {</span><span class="font64" style="font-weight:bold;">X</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;">X</span><span class="font64"><sup>(m)</sup>} to random values (e.g., from a&#160;uniform or normal distribution, or possibly a distribution with marginals matched&#160;to the model’s marginals).&#160;</span><span class="font64" style="font-weight:bold;">while </span><span class="font64">not converged </span><span class="font64" style="font-weight:bold;">do</span></p>
<p><span class="font64">Sample a minibatch of m examples {</span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(m)</sup>} from the training set.</span></p>
<p><span class="font63">g ^ m Ei=1 v<sup>10</sup>g <sup>p(xW</sup>; </span><span class="font18"><sup>6</sup></span><span class="font63"><sup>)</sup>.</span></p>
<p><span class="font64" style="font-weight:bold;">for </span><span class="font64" style="font-weight:bold;font-style:italic;">i =</span><span class="font64"> 1 to k </span><span class="font64" style="font-weight:bold;">do for </span><span class="font64" style="font-weight:bold;font-style:italic;">j = 1</span><span class="font64"> to m </span><span class="font64" style="font-weight:bold;">do</span></p>
<p><span class="font64" style="font-weight:bold;">x</span><span class="font64"><sup>(j)</sup> ^ gibbs_update(</span><span class="font64" style="font-weight:bold;">x</span><span class="font63"><sup>j)</sup>). </span><span class="font64" style="font-weight:bold;">end for</span></p>
<p><span class="font64" style="font-weight:bold;">end for</span></p>
<p><span class="font64" style="font-weight:bold;">g ^ g <sup>-</sup> </span><span class="font63">m </span><span class="font65" style="font-style:italic;">YZL1 </span><span class="font64" style="font-weight:bold;"><sup>v</sup>9</span><span class="font63"><sup>10</sup>g <sup>p(</sup></span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font64"><sup>(i)</sup>; </span><span class="font18"><sup>6</sup></span><span class="font63"><sup>)</sup>.</span></p>
<p><span class="font18">6</span><span class="font64"> ^ </span><span class="font18">6</span><span class="font64"> + e</span><span class="font64" style="font-weight:bold;">g</span><span class="font64">. </span><span class="font64" style="font-weight:bold;">end while</span></p>
<p><span class="font64">Care must be taken when evaluating the samples from a model trained with SML. It is necessary to draw the samples starting from a fresh Markov chain&#160;initialized from a random starting point after the model is done training. The&#160;samples present in the persistent negative chains used for training have been&#160;influenced by several recent versions of the model, and thus can make the model&#160;appear to have greater capacity than it actually does.</span></p>
<p><span class="font64">Berglund and Raiko (2013) performed experiments to examine the bias and variance in the estimate of the gradient provided by CD and SML. CD proves to&#160;have lower variance than the estimator based on exact sampling. SML has higher&#160;variance. The cause of CD’s low variance is its use of the same training points&#160;in both the positive and negative phase. If the negative phase is initialized from&#160;different training points, the variance rises above that of the estimator based on&#160;exact sampling.</span></p>
<p><span class="font64">All of these methods based on using MCMC to draw samples from the model can in principle be used with almost any variant of MCMC. This means that&#160;techniques such as SML can be improved by using any of the enhanced MCMC&#160;techniques described in Chapter 17, such as parallel tempering (Desjardins </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font18">2010</span><span class="font64">; Cho </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010).</span></p>
<p><span class="font64">One approach to accelerating mixing during learning relies not on changing the Monte Carlo sampling technology but rather on changing the parametrization of&#160;the model and the cost function. </span><span class="font64" style="font-weight:bold;font-style:italic;">Fast PCD</span><span class="font64"> or FPCD (Tieleman and Hinton, 2009)&#160;involves replacing the parameters </span><span class="font18">6</span><span class="font64"> of a traditional model with an expression</span></p>
<p><span class="font18">6</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">= 6</span><span class="font64"><sup>(slow)</sup> + </span><span class="font18">6</span><span class="font64"><sup>(fast)</sup>. &#160;&#160;&#160;(18.16)</span></p>
<p><span class="font64">There are now twice as many parameters as before, and they are added together element-wise to provide the parameters used by the original model definition. The&#160;fast copy of the parameters is trained with a much larger learning rate, allowing&#160;it to adapt rapidly in response to the negative phase of learning and push the&#160;Markov chain to new territory. This forces the Markov chain to mix rapidly, though&#160;this effect only occurs during learning while the fast weights are free to change.&#160;Typically one also applies significant weight decay to the fast weights, encouraging&#160;them to converge to small values, after only transiently taking on large values long&#160;enough to encourage the Markov chain to change modes.</span></p>
<p><span class="font64">One key benefit to the MCMC-based methods described in this section is that they provide an estimate of the gradient of log Z, and thus we can essentially&#160;decompose the problem into the log p contribution and the log Z contribution.&#160;We can then use any other method to tackle log p(x), and just add our negative&#160;phase gradient onto the other method’s gradient. In particular, this means that&#160;our positive phase can make use of methods that provide only a lower bound on&#160;p. Most of the other methods of dealing with log Z presented in this chapter are&#160;incompatible with bound-based positive phase methods.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">18.3 Pseudolikelihood</span></h4>
<p><span class="font64">Monte Carlo approximations to the partition function and its gradient directly confront the partition function. Other approaches sidestep the issue, by training&#160;the model without computing the partition function. Most of these approaches are&#160;based on the observation that it is easy to compute ratios of probabilities in an&#160;undirected probabilistic model. This is because the partition function appears in&#160;both the numerator and the denominator of the ratio and cancels out:</span></p><div>
<p><span class="font64">(18.17)</span></p></div>
<p><span class="font63" style="text-decoration:underline;">p(x)</span><span class="font63"> = </span><span class="font63" style="text-decoration:underline;">j <sup>p(x)</sup></span><span class="font63"> = </span><span class="font63" style="text-decoration:underline;">p(x) </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font11" style="font-style:italic;"><sup>(</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font11" style="font-style:italic;"><sup>)</sup></span><span class="font63">&#160;&#160;&#160;&#160;j </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font11" style="font-style:italic;">(</span><span class="font63"> y)&#160;&#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font11" style="font-style:italic;"><sup>(</sup></span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font11" style="font-style:italic;">)'</span></p>
<p><span class="font64">The pseudolikelihood is based on the observation that conditional probabilities take this ratio-based form, and thus can be computed without knowledge of the&#160;partition function. Suppose that we partition x into a, b and c, where a contains&#160;the variables we want to find the conditional distribution over, b contains the&#160;variables we want to condition on, and c contains the variables that are not part&#160;of our query.</span></p><div>
<p><span class="font63">P<sup>(a 1 b)</sup> =</span></p></div><div>
<p><span class="font64">p(a, b)</span></p></div><div>
<p><span class="font64">p(a, b)</span></p></div><div>
<p><span class="font63">p(a, b)</span></p></div><div>
<p><span class="font64">P<sup>(b)</sup> &#160;&#160;&#160;Ea,c </span><span class="font64" style="font-weight:bold;font-style:italic;">P<sup>(</sup>a <sup>h,</sup></span><span class="font64"><sup> c)</sup>&#160;&#160;&#160;&#160;Ea,c<sup>p(</sup>a </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>h,</sup></span><span class="font64"><sup> c)</sup>’</span></p></div><div>
<p><span class="font64">(18.18)</span></p></div>
<p><span class="font64">This quantity requires marginalizing out a, which can be a very efficient operation provided that a and c do not contain very many variables. In the extreme case, a&#160;can be a single variable and c can be empty, making this operation require only as&#160;many evaluations of </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> as there are values of a single random variable.</span></p>
<p><span class="font64">Unfortunately, in order to compute the log-likelihood, we need to marginalize out large sets of variables. If there are n variables total, we must marginalize a set&#160;of size n — 1. By the chain rule of probability,</span></p>
<p><span class="font64" style="font-weight:bold;">logp(</span><span class="font64">x</span><span class="font64" style="font-weight:bold;">) = log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(x</span><span class="font64"> 1</span><span class="font64" style="font-weight:bold;">) + log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(x</span><span class="font62" style="font-style:italic;">2</span><span class="font64"> \ </span><span class="font64" style="font-weight:bold;"><sup>x</sup></span><span class="font63">l</span><span class="font64" style="font-weight:bold;"><sup>)</sup> +</span><span class="font64">-----</span><span class="font64" style="font-weight:bold;">+ </span><span class="font64" style="font-weight:bold;font-style:italic;">p<sup>(x</sup>n </span><span class="font64" style="font-style:italic;"><sup>1</sup></span><span class="font64"><sup> x</sup>l:n-l</span><span class="font64" style="font-weight:bold;">). &#160;&#160;&#160;</span><span class="font64">(18.19)</span></p>
<p><span class="font64">In this case, we have made a maximally small, but c can be as large as x</span><span class="font18">2</span><span class="font64"><sub>:n</sub> . What if we simply move c into b to reduce the computational cost? This yields the&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">pseudolikelihood</span><span class="font64"> (Besag, 1975) objective function, based on predicting the value of&#160;feature </span><span class="font64" style="font-weight:bold;font-style:italic;">xi</span><span class="font64"> given all of the other features x<sub>-i</sub>:</span></p>
<p><span class="font63">n</span></p>
<p><span class="font18">5</span><span class="font64">&gt;g</span><span class="font64" style="font-weight:bold;">p</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x</span><span class="font63">i \ x-i)</span><span class="font64" style="font-weight:bold;">. &#160;&#160;&#160;</span><span class="font64">(18.20)</span></p>
<p><span class="font63">i</span><span class="font18">=1</span></p>
<p><span class="font64">If each random variable has k different values, this requires only </span><span class="font64" style="font-weight:bold;font-style:italic;">k</span><span class="font64"> xn evaluations of </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> to compute, as opposed to the k<sup>n</sup> evaluations needed to compute the partition&#160;function.</span></p>
<p><span class="font64">This may look like an unprincipled hack, but it can be proven that estimation by maximizing the pseudolikelihood is asymptotically consistent (Mase, 1995).&#160;Of course, in the case of datasets that do not approach the large sample limit,&#160;pseudolikelihood may display different behavior from the maximum likelihood&#160;estimator.</span></p>
<p><span class="font64">It is possible to trade computational complexity for deviation from maximum likelihood behavior by using the </span><span class="font64" style="font-weight:bold;font-style:italic;">generalized pseudolikelihood</span><span class="font64"> estimator (Huang and&#160;Ogata, 2002). The generalized pseudolikelihood estimator uses m different sets&#160;S<sup>(i)</sup>, </span><span class="font64" style="font-weight:bold;font-style:italic;">i =</span><span class="font64"> 1,..., m of indices of variables that appear together on the left side of the&#160;conditioning bar. In the extreme case of m = 1 and S<sup>(1)</sup> = 1,..., </span><span class="font64" style="font-weight:bold;font-style:italic;">n</span><span class="font64"> the generalized&#160;pseudolikelihood recovers the log-likelihood. In the extreme case of m = n and&#160;S<sup>(i)</sup> = {i}, the generalized pseudolikelihood recovers the pseudolikelihood. The&#160;generalized pseudolikelihood objective function is given by</span></p>
<p><span class="font63">m</span></p>
<p><span class="font64">5^<sup>lo</sup>g P<sup>(x</sup> s(i) </span><span class="font18"><sup>1</sup></span><span class="font64"><sup> x</sup></span><span class="font64" style="font-variant:small-caps;">-s«). &#160;&#160;&#160;<sup>(</sup></span><span class="font18">1<sup>8</sup></span><span class="font64">.</span><span class="font18">21</span><span class="font64">)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p>
<p><span class="font64">The performance of pseudolikelihood-based approaches depends largely on how the model will be used. Pseudolikelihood tends to perform poorly on tasks that&#160;require a good model of the full joint p(x), such as density estimation and sampling.&#160;However, it can perform better than maximum likelihood for tasks that require only&#160;the conditional distributions used during training, such as filling in small amounts&#160;of missing values. Generalized pseudolikelihood techniques are especially powerful if&#160;the data has regular structure that allows the S index sets to be designed to capture&#160;the most important correlations while leaving out groups of variables that only&#160;have negligible correlation. For example, in natural images, pixels that are widely&#160;separated in space also have weak correlation, so the generalized pseudolikelihood&#160;can be applied with each S set being a small, spatially localized window.</span></p>
<p><span class="font64">One weakness of the pseudolikelihood estimator is that it cannot be used with other approximations that provide only a lower bound on p(x), such as variational&#160;inference, which will be covered in Chapter 19. This is because p appears in the&#160;denominator. A lower bound on the denominator provides only an upper bound on&#160;the expression as a whole, and there is no benefit to maximizing an upper bound.&#160;This makes it difficult to apply pseudolikelihood approaches to deep models such&#160;as deep Boltzmann machines, since variational methods are one of the dominant&#160;approaches to approximately marginalizing out the many layers of hidden variables&#160;that interact with each other. However, pseudolikelihood is still useful for deep&#160;learning, because it can be used to train single layer models, or deep models using&#160;approximate inference methods that are not based on lower bounds.</span></p>
<p><span class="font64">Pseudolikelihood has a much greater cost per gradient step than SML, due to its explicit computation of all of the conditionals. However, generalized pseudolikelihood and similar criteria can still perform well if only one randomly selected&#160;conditional is computed per example (Goodfellow </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013b), thereby bringing&#160;the computational cost down to match that of SML.</span></p>
<p><span class="font64">Though the pseudolikelihood estimator does not explicitly minimize log Z, it can still be thought of as having something resembling a negative phase. The&#160;denominators of each conditional distribution result in the learning algorithm&#160;suppressing the probability of all states that have only one variable differing from&#160;a training example.</span></p>
<p><span class="font64">See Marlin and de Freitas (2011) for a theoretical analysis of the asymptotic efficiency of pseudolikelihood.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">18.4 Score Matching and Ratio Matching</span></h4>
<p><span class="font64">Score matching (Hyvarinen, 2005) provides another consistent means of training a model without estimating Z or its derivatives. The name score matching comes&#160;from terminology in which the derivatives of a log density with respect to its&#160;argument, V<sub>x</sub> log</span><span class="font64" style="font-weight:bold;font-style:italic;">p(</span><span class="font64">x), are called its </span><span class="font64" style="font-weight:bold;font-style:italic;">score.</span><span class="font64"> The strategy used by score matching is&#160;to minimize the expected squared difference between the derivatives of the model’s&#160;log density with respect to the input and the derivatives of the data’s log density&#160;with respect to the input:</span></p>
<p><span class="font18">1</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">L(x, 6)</span><span class="font64"> = ^ ||V </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> log Pmodel (x; </span><span class="font18">6</span><span class="font64">) - V* log p data(x)||</span><span class="font18">2</span><span class="font64"> &#160;&#160;&#160;(18.22)</span></p>
<p><span class="font18">1</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>J</sup> (<sup>6</sup>)</span><span class="font64"> </span><span class="font18">=<sub>2</sub></span><span class="font64"> <sup>E</sup>P<sub>d</sub>a<sub>ta</sub> (*) <sup>L</sup>(X, <sup>6</sup>) &#160;&#160;&#160;(<sup>18</sup>.<sup>23</sup>)</span></p>
<p><span class="font18">6</span><span class="font64">* = min J(</span><span class="font18">6</span><span class="font64">) &#160;&#160;&#160;(18.24)</span></p>
<p><span class="font64">This objective function avoids the difficulties associated with differentiating the partition function Z because Z is not a function of x and therefore V<sub>x</sub> Z = 0.&#160;Initially, score matching appears to have a new difficulty: computing the score&#160;of the data distribution requires knowledge of the true distribution generating&#160;the training data, pd<sub>ata</sub>. Fortunately, minimizing the expected value of L(x, </span><span class="font18">6</span><span class="font64">) is</span></p>
<p><span class="font64">equivalent to minimizing the expected value of</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>n</sup> &#160;&#160;&#160;d2</span></p>
<p><span class="font64"><sup>L(x</sup>'</span><span class="font18"><sup>0</sup></span><span class="font64"><sup>)</sup> = g( </span><span class="font64" style="font-weight:bold;font-style:italic;">dxj</span></p></div><div>
<p><span class="font64">d</span><span class="font64" style="font-weight:bold;"><sup>2</sup> &#160;&#160;&#160;</span><span class="font18">1</span></p>
<p><span class="font64">logP</span><span class="font64" style="font-weight:bold;">model </span><span class="font64">(x; </span><span class="font18">0</span><span class="font64">) +</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">2 dx</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">log Pmodel </span><span class="font12" style="font-weight:bold;font-style:italic;">(x</span></p></div><div>
<p dir="rtl"><span class="font67" style="font-weight:bold;">(</span><span class="font63">־</span><span class="font67" style="font-weight:bold;">(״</span></p></div><div>
<p><span class="font64">(18.25)</span></p></div>
<p><span class="font64">where n is the dimensionality of x.</span></p>
<p><span class="font64">Because score matching requires taking derivatives with respect to x, it is not applicable to models of discrete data. However, the latent variables in the model&#160;may be discrete.</span></p>
<p><span class="font64">Like the pseudolikelihood, score matching only works when we are able to evaluate log p(x) and its derivatives directly. It is not compatible with methods&#160;that only provide a lower bound on log p(x), because score matching requires&#160;the derivatives and second derivatives of log p(x) and a lower bound conveys no&#160;information about its derivatives. This means that score matching cannot be&#160;applied to estimating models with complicated interactions between the hidden&#160;units, such as sparse coding models or deep Boltzmann machines. While score&#160;matching can be used to pretrain the first hidden layer of a larger model, it has&#160;not been applied as a pretraining strategy for the deeper layers of a larger model.&#160;This is probably because the hidden layers of such models usually contain some&#160;discrete variables.</span></p>
<p><span class="font64">While score matching does not explicitly have a negative phase, it can be viewed as a version of contrastive divergence using a specific kind of Markov chain&#160;(Hyvarinen, 2007a). The Markov chain in this case is not Gibbs sampling, but&#160;rather a different approach that makes local moves guided by the gradient. Score&#160;matching is equivalent to CD with this type of Markov chain when the size of the&#160;local moves approaches zero.</span></p>
<p><span class="font64">Lyu (2009) generalized score matching to the discrete case (but made an error in their derivation that was corrected by Marlin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2010)). Marlin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2010)&#160;found that </span><span class="font64" style="font-weight:bold;font-style:italic;">generalized score matching</span><span class="font64"> (GSM) does not work in high dimensional&#160;discrete spaces where the observed probability of many events is </span><span class="font18">0</span><span class="font64">.</span></p>
<p><span class="font64">A more successful approach to extending the basic ideas of score matching to discrete data is </span><span class="font64" style="font-weight:bold;font-style:italic;">ratio matching</span><span class="font64"> (Hyvarinen, 2007b). Ratio matching applies&#160;specifically to binary data. Ratio matching consists of minimizing the average over&#160;examples of the following objective function:</span></p>
<p><span class="font63">2</span></p><div>
<p><span class="font64">L<sup>(rm)</sup> (x, 0)</span></p></div><div><h3><a id="bookmark6"></a><span class="font65" style="font-weight:bold;">£</span></h3>
<p><span class="font64" style="font-weight:bold;font-style:italic;">j<sup>=1</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;">1 +</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x</span></p></div><div>
<p><span class="font64">(18.26)</span></p></div><div>
<p><span class="font63">Pmodel(/</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(x</sup>),j );G)</span></p></div>
<p><span class="font64">where f (x, j) returns x with the bit at position j flipped. Ratio matching avoids the partition function using the same trick as the pseudolikelihood estimator: in a&#160;ratio of two probabilities, the partition function cancels out. Marlin </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2010)&#160;found that ratio matching outperforms SML, pseudolikelihood and GSM in terms&#160;of the ability of models trained with ratio matching to denoise test set images.</span></p>
<p><span class="font64">Like the pseudolikelihood estimator, ratio matching requires n evaluations of p per data point, making its computational cost per update roughly n times higher&#160;than that of SML.</span></p>
<p><span class="font64">As with the pseudolikelihood estimator, ratio matching can be thought of as pushing down on all fantasy states that have only one variable different from a&#160;training example. Since ratio matching applies specifically to binary data, this&#160;means that it acts on all fantasy states within Hamming distance 1 of the data.</span></p>
<p><span class="font64">Ratio matching can also be useful as the basis for dealing with high-dimensional sparse data, such as word count vectors. This kind of data poses a challenge for&#160;MCMC-based methods because the data is extremely expensive to represent in&#160;dense format, yet the MCMC sampler does not yield sparse values until the model&#160;has learned to represent the sparsity in the data distribution. Dauphin and Bengio&#160;(2013) overcame this issue by designing an unbiased stochastic approximation to&#160;ratio matching. The approximation evaluates only a randomly selected subset of&#160;the terms of the objective, and does not require the model to generate complete&#160;fantasy samples.</span></p>
<p><span class="font64">See Marlin and de Freitas (2011) for a theoretical analysis of the asymptotic efficiency of ratio matching.</span></p><h4><a id="bookmark7"></a><span class="font65" style="font-weight:bold;">18.5 Denoising Score Matching</span></h4>
<p><span class="font64">In some cases we may wish to regularize score matching, by fitting a distribution</span></p>
<p><span class="font64" style="font-weight:bold;">P</span><span class="font63">smoothed(x) = </span><span class="font64" style="font-weight:bold;font-style:italic;">J p</span><span class="font63">data(y)</span><span class="font64" style="font-weight:bold;">q</span><span class="font64">(x | y)</span><span class="font64" style="font-weight:bold;">d</span><span class="font64">y &#160;&#160;&#160;(18.27)</span></p>
<p><span class="font64">rather than the true </span><span class="font64" style="font-weight:bold;">p</span><span class="font63">d<sub>ata</sub>. The distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">q(x</span><span class="font64"> | y) is a corruption process, usually one that forms x by adding a small amount of noise to y.</span></p>
<p><span class="font64">Denoising score matching is especially useful because in practice we usually do not have access to the true </span><span class="font64" style="font-weight:bold;">p</span><span class="font63">d<sub>ata</sub> but rather only an empirical distribution defined&#160;by samples from it. Any consistent estimator will, given enough capacity, make&#160;</span><span class="font64" style="font-weight:bold;">P</span><span class="font64"><sub>mo</sub>d<sub>e</sub>i into a set of Dirac distributions centered on the training points. Smoothing&#160;by </span><span class="font64" style="font-weight:bold;">q </span><span class="font64">helps to reduce this problem, at the loss of the asymptotic consistency property&#160;described in Sec. 5.4.5. Kingma and LeCun (2010) introduced a procedure for&#160;performing regularized score matching with the smoothing distribution q being&#160;normally distributed noise.</span></p>
<p><span class="font64">Recall from Sec. 14.5.1 that several autoencoder training algorithms are equivalent to score matching or denoising score matching. These autoencoder&#160;training algorithms are therefore a way of overcoming the partition function&#160;problem.</span></p><h4><a id="bookmark8"></a><span class="font65" style="font-weight:bold;">18.6 Noise-Contrastive Estimation</span></h4>
<p><span class="font64">Most techniques for estimating models with intractable partition functions do not provide an estimate of the partition function. SML and CD estimate only the&#160;gradient of the log partition function, rather than the partition function itself.&#160;Score matching and pseudolikelihood avoid computing quantities related to the&#160;partition function altogether.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Noise-contrastive estimation (NCE)</span><span class="font64"> (Gutmann and Hyvarinen, 2010) takes a different strategy. In this approach, the probability distribution estimated by the&#160;model is represented explicitly as</span></p>
<p><span class="font64">logPmodel (x) = log pmodel (x; </span><span class="font18">6</span><span class="font64">) + C, &#160;&#160;&#160;(18.28)</span></p>
<p><span class="font64">where c is explicitly introduced as an approximation of — log </span><span class="font64" style="font-weight:bold;font-style:italic;">Z (6</span><span class="font64">). Rather than estimating only </span><span class="font18">6</span><span class="font64">, the noise contrastive estimation procedure treats c as just&#160;another parameter and estimates </span><span class="font18">6</span><span class="font64"> and c simultaneously, using the same algorithm&#160;for both. The resulting logp<sub>mo</sub>d<sub>e</sub>1(x) thus may not correspond exactly to a valid&#160;probability distribution, but will become closer and closer to being valid as the&#160;estimate of c improves.<a id="footnote2"></a><sup><a href="#bookmark9">2</a></sup><sup></sup></span></p>
<p><span class="font64">Such an approach would not be possible using maximum likelihood as the criterion for the estimator. The maximum likelihood criterion would choose to set&#160;c arbitrarily high, rather than setting c to create a valid probability distribution.</span></p>
<p><span class="font64">NCE works by reducing the unsupervised learning problem of estimating p(x) to that of learning a probabilistic binary classifier in which one of the categories&#160;corresponds to the data generated by the model. This supervised learning problem&#160;is constructed in such a way that maximum likelihood estimation in this supervised&#160;learning problem defines an asymptotically consistent estimator of the original&#160;problem.</span></p>
<p><span class="font64">Specifically, we introduce a second distribution, the </span><span class="font64" style="font-weight:bold;font-style:italic;">noise distribution p<sub>no</sub></span><span class="font64">i<sub>se</sub>(x). The noise distribution should be tractable to evaluate and to sample from. We&#160;can now construct a model over both x and a new, binary class variable y. In the&#160;new joint model, we specify that</span></p><div>
<p><span class="font64" style="font-weight:bold;">P</span><span class="font63">joint(</span><span class="font64" style="font-weight:bold;">y </span><span class="font64">= 1)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;">1</span></p>
<p><span class="font64" style="font-weight:bold;">2 ’</span></p></div><div>
<p><span class="font63"><sup>p</sup>joint<sup>(x 1 y</sup> — <sup>1)</sup> — <sup>p</sup>model<sup>(x)</sup>ל</span></p></div><div>
<p><span class="font64">(18.29)</span></p>
<p><span class="font64">(18.30)</span></p></div>
<p><span class="font64">and</span></p><div>
<p><span class="font64">(18.31)</span></p></div>
<p><span class="font63">Pjoint<sup>(x 1 y</sup> — <sup>0)</sup> — <sup>p</sup>noise(x).</span></p>
<p><span class="font64">In other words, </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">is a switch variable that determines whether we will generate x from the model or from the noise distribution.</span></p>
<p><span class="font64">We can construct a similar joint model of training data. In this case, the switch variable determines whether we draw x from the </span><span class="font64" style="font-weight:bold;font-style:italic;">data</span><span class="font64"> or from the noise&#160;distribution. Formally, </span><span class="font64" style="font-weight:bold;">p</span><span class="font63">train(</span><span class="font64" style="font-weight:bold;">y </span><span class="font64">— 1) — ^ , </span><span class="font64" style="font-weight:bold;">p</span><span class="font63">train (x | </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">— 1) — </span><span class="font64" style="font-weight:bold;">p</span><span class="font63">data(x), and</span></p>
<p><span class="font64" style="font-weight:bold;">p</span><span class="font63">train (x | </span><span class="font64" style="font-weight:bold;">y </span><span class="font64">— 0) — </span><span class="font64" style="font-weight:bold;">P</span><span class="font63">noise(x).</span></p>
<p><span class="font64">We can now just use standard maximum likelihood learning on the </span><span class="font64" style="font-weight:bold;font-style:italic;">supervised </span><span class="font64">learning problem of fitting </span><span class="font64" style="font-weight:bold;">p</span><span class="font63">j<sub>oint</sub> to </span><span class="font64" style="font-weight:bold;">p</span><span class="font63">t<sub>rain</sub>:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">6ל c</span><span class="font64"> — arg max Ex</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">6,c</span></p></div><div>
<p><span class="font63">,y^Ptrain</span></p></div><div>
<p><span class="font63">in <sup>1og</sup>pjoint <sup>(y 1 x)</sup>.</span></p></div><div>
<p><span class="font64">(18.32)</span></p></div>
<p><span class="font64">The distribution pj<sub>o</sub>i<sub>nt</sub> is essentially a logistic regression model applied to the difference in log probabilities of the model and the noise distribution:</span></p><div>
<p><span class="font64">pjoint(y — </span><span class="font18">1</span><span class="font64"> | x) —</span></p></div><div>
<p><span class="font63"><sup>p</sup>model <sup>(x)</sup></span></p></div><div>
<p><span class="font63"><sup>p</sup> model<sup>(x)</sup> + <sup>p</sup> noise <sup>(x)</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div>
<p><span class="font62">1 _1 <sup>p</sup>noise<sup>(x) p</sup>model <sup>(x)</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;">1</span></p></div><div>
<p><span class="font63"><sup>1</sup>+<sup>ex</sup>p (<sup>1og</sup> P</span><span class="font67" style="font-weight:bold;">mo</span><span class="font63">filXj)</span></p>
<p><span class="font63"><sup>p</sup>noise<sup>(x)</sup></span></p></div><div>
<p><span class="font67" style="font-weight:bold;">(</span></p></div><div>
<p><span class="font64">a - log</span></p></div><div>
<p><span class="font67" style="font-weight:bold;">)</span></p></div><div>
<p><span class="font64"><sup>p</sup>model<sup>(x)</sup> <sub>־</sub></span></p>
<p><span class="font64">— </span><span class="font64" style="font-weight:bold;font-style:italic;">a</span><span class="font64"> (logPmodel<sup>(x) - </sup></span><span class="font18"><sup>1</sup></span><span class="font64"><sup>o</sup>gPnoise(x)) .</span></p></div><div>
<p><span class="font64">(18.33)</span></p>
<p><span class="font64">(18.34)</span></p>
<p><span class="font64">(18.35)</span></p>
<p><span class="font64">(18.36)</span></p>
<p><span class="font64">(18.37)</span></p></div>
<p><span class="font64">NCE is thus simple to apply so long as log p<sub>mo</sub>d<sub>e</sub>i is easy to back-propagate through, and, as specified above, p<sub>no</sub>i<sub>se</sub> is easy to evaluate (in order to evaluate&#160;Pj<sub>o</sub>i<sub>nt</sub>) and sample from (in order to generate the training data).</span></p>
<p><span class="font64">NCE is most successful when applied to problems with few random variables, but can work well even if those random variables can take on a high number of&#160;values. For example, it has been successfully applied to modeling the conditional&#160;distribution over a word given the context of the word (Mnih and Kavukcuoglu,&#160;2013). Though the word may be drawn from a large vocabulary, there is only one&#160;word.</span></p>
<p><span class="font64">When NCE is applied to problems with many random variables, it becomes less efficient. The logistic regression classifier can reject a noise sample by identifying&#160;any one variable whose value is unlikely. This means that learning slows down&#160;greatly after p<sub>mo</sub>d<sub>e</sub>1 has learned the basic marginal statistics. Imagine learning a&#160;model of images of faces, using unstructured Gaussian noise as p<sub>no</sub>i<sub>se</sub>. If p<sub>mo</sub>d<sub>e</sub>1&#160;learns about eyes, it can reject almost all unstructured noise samples without&#160;having learned anything about other facial features, such as mouths.</span></p>
<p><span class="font64">The constraint that </span><span class="font64" style="font-weight:bold;font-style:italic;">p<sub>no</sub>i<sub>se</sub></span><span class="font64"> must be easy to evaluate and easy to sample from can be overly restrictive. When </span><span class="font64" style="font-weight:bold;font-style:italic;">p<sub>no</sub>-<sub>ise</sub></span><span class="font64"> is simple, most samples are likely to be too&#160;obviously distinct from the data to force p<sub>mo</sub>d<sub>e</sub>1 to improve noticeably.</span></p>
<p><span class="font64">Like score matching and pseudolikelihood, NCE does not work if only a lower bound on p is available. Such a lower bound could be used to construct a lower&#160;bound on </span><span class="font64" style="font-weight:bold;font-style:italic;">pj<sub>0</sub>i<sub>nt</sub>(y =</span><span class="font64"> </span><span class="font18">1</span><span class="font64"> | x), but it can only be used to construct an upper bound on&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">Pj<sub>o</sub>i<sub>nt</sub>(y =</span><span class="font64"> 0 | x), which appears in half the terms of the NCE objective. Likewise,&#160;a lower bound on p<sub>no</sub>i<sub>se</sub> is not useful, because it provides only an upper bound on&#160;Pjoint(y = 1 | x).</span></p>
<p><span class="font64">When the model distribution is copied to define a new noise distribution before each gradient step, NCE defines a procedure called </span><span class="font64" style="font-weight:bold;font-style:italic;">self-contrastive estimation,&#160;</span><span class="font64">whose expected gradient is equivalent to the expected gradient of maximum&#160;likelihood (Goodfellow, 2014). The special case of NCE where the noise samples&#160;are those generated by the model suggests that maximum likelihood can be&#160;interpreted as a procedure that forces a model to constantly learn to distinguish&#160;reality from its own evolving beliefs, while noise contrastive estimation achieves&#160;some reduced computational cost by only forcing the model to distinguish reality&#160;from a fixed baseline (the noise model).</span></p>
<p><span class="font64">Using the supervised task of classifying between training samples and generated samples (with the model energy function used in defining the classifier) to provide&#160;a gradient on the model was introduced earlier in various forms (Welling </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2003b; Bengio, 2009).</span></p>
<p><span class="font64">Noise contrastive estimation is based on the idea that a good generative model should be able to distinguish data from noise. A closely related idea is that&#160;a good generative model should be able to generate samples that no classifier&#160;can distinguish from data. This idea yields generative adversarial networks (Sec.&#160;20.10.4).</span></p><h4><a id="bookmark10"></a><span class="font65" style="font-weight:bold;">18.7 Estimating the Partition Function</span></h4>
<p><span class="font64">While much of this chapter is dedicated to describing methods that avoid needing to compute the intractable partition function Z(</span><span class="font18">6</span><span class="font64">) associated with an undirected&#160;graphical model, in this section we discuss several methods for directly estimating&#160;the partition function.</span></p>
<p><span class="font64">Estimating the partition function can be important because we require it if we wish to compute the normalized likelihood of data. This is often important in&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">evaluating</span><span class="font64"> the model, monitoring training performance, and comparing models to&#160;each other.</span></p>
<p><span class="font64">For example, imagine we have two models: model Ma defining a probability distribution </span><span class="font64" style="font-weight:bold;font-style:italic;">pa</span><span class="font64">(x; </span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">a) = Z&quot;Pa (x; </span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">a) and model Mb defining a probability</span></p>
<p><span class="font64">distribution </span><span class="font64" style="font-style:italic;font-variant:small-caps;">p</span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">b(</span><span class="font64">x; </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">6b</span><span class="font64">) = </span><span class="font64" style="font-weight:bold;font-style:italic;">1־ pB</span><span class="font64"> (x; </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">6b</span><span class="font64"> ). A common way to compare the models is to evaluate and compare the likelihood that both models assign to an i.i.d.&#160;test dataset. Suppose the test set consists of m examples {x<sup>(1)</sup>,..., </span><span class="font64" style="font-weight:bold;font-style:italic;">X<sup>(m)</sup></span><span class="font64"> }. jf&#160;</span><span class="font14">n</span><span class="font64">. PA<sup>(x(i)</sup>; </span><span class="font18"><sup>6</sup></span><span class="font64" style="font-variant:small-caps;">a<sup>)</sup></span><span class="font64"> &gt; rX pb(x</span><span class="font64" style="font-variant:small-caps;"><sup>(.)</sup> ; </span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">b ) or equivalently if</span></p><div>
<p><span class="font64">(18.38)</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Y</span><span class="font64"> logPA (x<sup>(i)</sup>; </span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">a) &#160;&#160;&#160;</span><span class="font64"><sup>lQ</sup>g</span><span class="font64" style="font-weight:bold;font-style:italic;">PB</span><span class="font64"> (x<sup>(i)</sup>; </span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">b) &gt; 0,</span></p>
<p><span class="font64">then we say that Ma is a better model than Mb (or, at least, it is a better model of the test set), in the sense that it has a better test log-likelihood. Unfortunately,&#160;testing whether this condition holds requires knowledge of the partition function.&#160;Unfortunately, Eq. 18.38 seems to require evaluating the log probability that&#160;the model assigns to each point, which in turn requires evaluating the partition&#160;function. We can simplify the situation slightly by re-arranging Eq. 18.38 into a&#160;form where we need to know only the </span><span class="font64" style="font-weight:bold;">ratio </span><span class="font64">of the two model’s partition functions:</span></p><div>
<p><span class="font65" style="font-style:italic;">Y</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">log PA (x<sup>(i)</sup>; </span><span class="font65" style="font-style:italic;">6 </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">a) — </span><span class="font65" style="font-style:italic;">Y</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">log PB (x<sup>(i)</sup>; </span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">b ) = </span><span class="font65" style="font-style:italic;">Y</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">log . . .</span></p></div><div>
<p><span class="font64" style="font-variant:small-caps;">Pa(x<sup>(.)</sup>; </span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">a) Pb(x<sup>(.)</sup> ; </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">6b)</span></p></div><div>
<p><span class="font60">)</span></p></div><div>
<p><span class="font64">—m log</span></p></div><div>
<p><span class="font64">Z (</span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">a )</span></p></div><div>
<p><span class="font64"><sup>,</sup>Z (</span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">b )־ (18.39)</span></p>
<p><span class="font64">We can thus determine whether Ma is a better model than Mb without knowing the partition function of either model but only their ratio. As we will see shortly,</span></p></div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">we</span><span class="font64"> can estimate this ratio using importance sampling, provided that the two models are similar.</span></p>
<p><span class="font64">If, however, we wanted to compute the actual probability of the test data under either Ma or Mb, we would need to compute the actual value of the partition&#160;functions. That said, if we knew the ratio of two partition functions, </span><span class="font64" style="font-weight:bold;font-style:italic;">r</span><span class="font64"> ־&#160;&#160;&#160;&#160;,</span></p>
<p><span class="font64">and we knew the actual value of just one of the two, say </span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">Z (6 a</span><span class="font64"> ), we could compute the value of the other:</span></p><div>
<p><span class="font64">Z (</span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">b ) = rZ (</span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">a)</span></p></div><div>
<p><span class="font64">Z (</span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">b ) Z (</span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">a )</span></p></div><div>
<p><span class="font64">Z (</span><span class="font18">6</span><span class="font64" style="font-variant:small-caps;">a).</span></p></div><div>
<p><span class="font64">(18.40)</span></p></div>
<p><span class="font64">A simple way to estimate the partition function is to use a Monte Carlo method such as simple importance sampling. We present the approach in terms&#160;of continuous variables using integrals, but it can be readily applied to discrete&#160;variables by replacing the integrals with summation. We use a proposal distribution&#160;</span><span class="font64" style="font-weight:bold;">p</span><span class="font64">o(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64" style="font-variant:small-caps;">) ־ Zq</span><span class="font64" style="font-weight:bold;">P</span><span class="font64">o(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) which supports tractable sampling and tractable evaluation of&#160;both the partition function </span><span class="font64" style="font-weight:bold;">Z</span><span class="font18">0</span><span class="font64"> and the unnormalized distribution po(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">).</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font64">Z</span><span class="font18">1</span><span class="font64"> = </span><span class="font64" style="font-weight:bold;font-style:italic;">J p</span><span class="font65" style="font-style:italic;">i</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">(x) </span><span class="font64" style="font-weight:bold;font-style:italic;">dx</span></p></td><td>
<p><span class="font64">(18.41)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">־ <sub>W</sub>(x</span><span class="font18">)<sup>p1</sup></span><span class="font64"><sup>(x) dx</sup></span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(18.42)</span></p></td></tr>
<tr><td>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sub>Z</sub> f</span><span class="font64"> <sub>( v</sub></span><span class="font18"><sup>p</sup>1</span><span class="font64"><sup>(x)</sup> <sub>d</sub></span></p>
<p><span class="font64">־ Z </span><span class="font18">0</span><span class="font64"> po (x) dx</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64"> po<sup>(x)</sup></span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(18.43)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font64">Z <sup>Z</sup>o </span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font65" style="font-style:italic;">i</span><span class="font64" style="font-weight:bold;"> </span><span class="font64"><sup>(x(k</sup>&gt; <sup>)</sup> <sub>t</sub> </span><span class="font65" style="font-style:italic;">(</span><span class="font64" style="font-weight:bold;font-style:italic;">k</span><span class="font65" style="font-style:italic;">&gt;</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>Z</sup></span><span class="font65" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;"> </span><span class="font64">־ <sub>(k</sub>&gt;</span><span class="font64" style="font-weight:bold;">x </span><span class="font64"><sup>s</sup>־<sup>t</sup>• <sup>: x(</sup> &gt; </span><span class="font64" style="font-weight:bold;"><sup>-</sup> </span><span class="font64" style="font-variant:small-caps;">p</span><span class="font64" style="font-weight:bold;font-variant:small-caps;">o </span><span class="font64"><sup>K</sup> k</span><span class="font18">=1</span><span class="font64"> p^o <sup>(x(k</sup>&gt;<sup>)</sup></span></p></td><td style="vertical-align:middle;">
<p><span class="font64">(18.44)</span></p></td></tr>
</table>
<p><span class="font64">In the last line, we make a Monte Carlo estimator, </span><span class="font64" style="font-weight:bold;font-style:italic;">Z</span><span class="font62" style="font-style:italic;">1</span><span class="font64">, of the integral using samples drawn from p</span><span class="font18">0</span><span class="font64">(</span><span class="font64" style="font-weight:bold;">x</span><span class="font64">) and then weight each sample with the ratio of the unnormalized&#160;p </span><span class="font18">1</span><span class="font64"> and the proposal po.</span></p>
<p><span class="font64">We see also that this approach allows us to estimate the ratio between the partition functions as</span></p><div>
<p><span class="font64" style="font-weight:bold;">1 &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">p<sub>1</sub></span><span class="font64"> </span><span class="font64" style="font-weight:bold;">(x<sup>(k</sup>&gt;)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;"><sup>1</sup> &#160;&#160;&#160;s.t. : x<sup>(k</sup>&gt; - po.</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>K</sup> t</span><span class="font62" style="font-style:italic;">=1</span><span class="font64" style="font-weight:bold;font-style:italic;"> po<sup>(x</sup></span></p></div><div>
<p><span class="font64">(18.45)</span></p></div>
<p><span class="font64">This value can then be used directly to compare two models as described in Eq. 18.39.</span></p>
<p><span class="font64">If the distribution po is close to p<sub>1</sub>, Eq. 18.44 can be an effective way of estimating the partition function (Minka, 2005). Unfortunately, most of the time</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">pi</span><span class="font64"> is both complicated (usually multimodal) and defined over a high dimensional space. It is difficult to find a tractable po that is simple enough to evaluate while&#160;still being close enough to pi to result in a high quality approximation. If po and&#160;pi are not close, most samples from po will have low probability under p </span><span class="font18">1</span><span class="font64"> and&#160;therefore make (relatively) negligible contribution to the sum in Eq. 18.44.</span></p>
<p><span class="font64">Having few samples with significant weights in this sum will result in an estimator that is of poor quality due to high variance. This can be understood&#160;quantitatively through an estimate of the variance of our estimate Z</span><span class="font18">1</span><span class="font64">:</span></p><div>
<p><span class="font64" style="font-weight:bold;">(*) = K2</span></p></div><div>
<p><span class="font64">pi(x<sup>(</sup></span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> </span><span class="font18">0</span><span class="font64">(x<sup>(k)</sup>)</span></p></div><div><h3><a id="bookmark11"></a><span class="font15" style="font-weight:bold;">■)’</span></h3></div><div>
<p><span class="font64">(18.46)</span></p></div>
<p><span class="font64">This quantity is largest when there is significant deviation in the values of the importance weights |־jx(fcr).</span></p>
<p><span class="font64">We now turn to two related strategies developed to cope with the challenging task of estimating partition functions for complex distributions over highdimensional spaces: annealed importance sampling and bridge sampling. Both start with the simple importance sampling strategy introduced above and both&#160;attempt to overcome the problem of the proposal po being too far from p </span><span class="font18">1</span><span class="font64"> by&#160;introducing intermediate distributions that attempt to </span><span class="font64" style="font-weight:bold;font-style:italic;">bridge the gap</span><span class="font64"> between po&#160;and p<sub>i</sub>.</span></p>
<p><a id="bookmark3"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font64">We can view the MCMC approach to maximum likelihood as trying to achieve balance between two forces, one pushing up on the model distribution where the&#160;data occurs, and another pushing down on the model distribution where the model&#160;samples occur. Fig. 18.1 illustrates this process. The two forces correspond to&#160;maximizing log </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font64"> and minimizing log Z. Several approximations to the negative&#160;phase are possible. Each of these approximations can be understood as making&#160;the negative phase computationally cheaper but also making it push down in the&#160;wrong locations.</span></p>
<p><span class="font64">Because the negative phase involves drawing samples from the model’s distribution, we can think of it as finding points that the model believes in strongly. Because the negative phase acts to reduce the probability of those points, they&#160;are generally considered to represent the model’s incorrect beliefs about the world.&#160;They are frequently referred to in the literature as “hallucinations” or “fantasy&#160;particles.” In fact, the negative phase has been proposed as a possible explanation</span></p>
<p><a id="bookmark9"><sup><a href="#footnote2">2</a></sup></a></p>
<p><span class="font64"><sup></sup>NCE is also applicable to problems with a tractable partition function, where there is no need to introduce the extra parameter c. However, it has generated the most interest as a means&#160;of estimating models with difficult partition functions.</span></p>
</body>
</html>