<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h2><span class="font66" style="font-weight:bold;">Chapter 9</span></h2><h2><a id="bookmark0"></a><span class="font67" style="font-weight:bold;">Convolutional Networks</span></h2>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Convolutional networks</span><span class="font64"> (LeCun, 1989), also known as </span><span class="font64" style="font-weight:bold;font-style:italic;">convolutional neural networks </span><span class="font64">or </span><span class="font64" style="font-weight:bold;font-style:italic;">CNNs</span><span class="font64">, are a specialized kind of neural network for processing data that has&#160;a known, grid-like topology. Examples include time-series data, which can be&#160;thought of as a 1D grid taking samples at regular time intervals, and image data,&#160;which can be thought of as a 2D grid of pixels. Convolutional networks have been&#160;tremendously successful in practical applications. The name “convolutional neural&#160;network” indicates that the network employs a mathematical operation called&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">convolution.</span><span class="font64"> Convolution is a specialized kind of linear operation. </span><span class="font64" style="font-weight:bold;">Convolutional&#160;networks are simply neural networks that use convolution in place of&#160;general matrix multiplication in at least one of their layers.</span></p>
<p><span class="font64">In this chapter, we will first describe what convolution is. Next, we will explain the motivation behind using convolution in a neural network. We will&#160;then describe an operation called </span><span class="font64" style="font-weight:bold;font-style:italic;">pooling,</span><span class="font64"> which almost all convolutional networks&#160;employ. Usually, the operation used in a convolutional neural network does not&#160;correspond precisely to the definition of convolution as used in other fields such&#160;as engineering or pure mathematics. We will describe several variants on the&#160;convolution function that are widely used in practice for neural networks. We&#160;will also show how convolution may be applied to many kinds of data, with&#160;different numbers of dimensions. We then discuss means of making convolution&#160;more efficient. Convolutional networks stand out as an example of neuroscientific&#160;principles influencing deep learning. We will discuss these neuroscientific principles,&#160;then conclude with comments about the role convolutional networks have played&#160;in the history of deep learning. One topic this chapter does not address is how to&#160;choose the architecture of your convolutional network. The goal of this chapter is&#160;to describe the kinds of tools that convolutional networks provide, while Chapter 11&#160;describes general guidelines for choosing which tools to use in which circumstances.&#160;Research into convolutional network architectures proceeds so rapidly that a new&#160;best architecture for a given benchmark is announced every few weeks to months,&#160;rendering it impractical to describe the best architecture in print. However, the&#160;best architectures have consistently been composed of the building blocks described&#160;here.</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">9.1 The Convolution Operation</span></h4>
<p><span class="font64">In its most general form, convolution is an operation on two functions of a realvalued argument. To motivate the definition of convolution, we start with examples of two functions we might use.</span></p>
<p><span class="font64">Suppose we are tracking the location of a spaceship with a laser sensor. Our laser sensor provides a single output </span><span class="font64" style="font-weight:bold;font-style:italic;">x(t),</span><span class="font64"> the position of the spaceship at time&#160;t. Both </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> and t are real-valued, i.e., we can get a different reading from the laser&#160;sensor at any instant in time.</span></p>
<p><span class="font64">Now suppose that our laser sensor is somewhat noisy. To obtain a less noisy estimate of the spaceship’s position, we would like to average together several&#160;measurements. Of course, more recent measurements are more relevant, so we will&#160;want this to be a weighted average that gives more weight to recent measurements.&#160;We can do this with a weighting function w(a), where a is the age of a measurement.&#160;If we apply such a weighted average operation at every moment, we obtain a new&#160;function s providing a smoothed estimate of the position of the spaceship:</span></p><div>
<p><span class="font64">(9.1)</span></p></div>
<p><span class="font64"><sup>s(t)</sup> = /<sup>x(a)w(t - a)da</sup></span></p>
<p><span class="font64">This operation is called </span><span class="font64" style="font-weight:bold;font-style:italic;">convolution.</span><span class="font64"> The convolution operation is typically denoted with an asterisk:</span></p>
<p><span class="font64">s(t) = (x * w)(t) &#160;&#160;&#160;(9.2)</span></p>
<p><span class="font64">In our example, w needs to be a valid probability density function, or the output is not a weighted average. Also, w needs to be 0 for all negative arguments,&#160;or it will look into the future, which is presumably beyond our capabilities. These&#160;limitations are particular to our example though. In general, convolution is defined&#160;for any functions for which the above integral is defined, and may be used for other&#160;purposes besides taking weighted averages.</span></p>
<p><span class="font64">In convolutional network terminology, the first argument (in this example, the function x )to the convolution is often referred to as the </span><span class="font64" style="font-weight:bold;font-style:italic;">input</span><span class="font64"> and the second</span></p>
<p><span class="font64">argument (in this example, the function </span><span class="font64" style="font-weight:bold;font-style:italic;">w)</span><span class="font64"> as the </span><span class="font64" style="font-weight:bold;font-style:italic;">kernel.</span><span class="font64"> The output is sometimes referred to as the </span><span class="font64" style="font-weight:bold;font-style:italic;">feature map.</span></p>
<p><span class="font64">In our example, the idea of a laser sensor that can provide measurements at every instant in time is not realistic. Usually, when we work with data on a&#160;computer, time will be discretized, and our sensor will provide data at regular&#160;intervals. In our example, it might be more realistic to assume that our laser&#160;provides a measurement once per second. The time index t can then take on only&#160;integer values. If we now assume that x and w are defined only on integer t, we&#160;can define the discrete convolution:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">s(t)</span><span class="font64"> = (x * w)(t) = &#160;&#160;&#160;x(a)w(t — a)&#160;&#160;&#160;&#160;(9.3)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">a=—oo</span></p>
<p><span class="font64">In machine learning applications, the input is usually a multidimensional array of data and the kernel is usually a multidimensional array of parameters that are&#160;adapted by the learning algorithm. We will refer to these multidimensional arrays&#160;as tensors. Because each element of the input and kernel must be explicitly stored&#160;separately, we usually assume that these functions are zero everywhere but the&#160;finite set of points for which we store the values. This means that in practice we&#160;can implement the infinite summation as a summation over a finite number of&#160;array elements.</span></p>
<p><span class="font64">Finally, we often use convolutions over more than one axis at a time. For example, if we use a two-dimensional image I as our input, we probably also want&#160;to use a two-dimensional kernel K:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">S(i,j) = (I * K)(i,j)</span><span class="font64"> = &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">1</span><span class="font64">(m,n)K(i — </span><span class="font64" style="font-weight:bold;font-style:italic;">m,j — n).</span><span class="font64">&#160;&#160;&#160;&#160;(9.4)</span></p>
<p><span class="font63">m n</span></p>
<p><span class="font64">Convolution is commutative, meaning we can equivalently write:</span></p>
<p><span class="font64">S(i, j) = (K * I)(i, j) = &#160;&#160;&#160;I(i — m, j — n)K(m, n).&#160;&#160;&#160;&#160;(9.5)</span></p>
<p><span class="font63">mn</span></p>
<p><span class="font64">Usually the latter formula is more straightforward to implement in a machine learning library, because there is less variation in the range of valid values of m&#160;and n.</span></p>
<p><span class="font64">The commutative property of convolution arises because we have </span><span class="font64" style="font-weight:bold;font-style:italic;">flipped</span><span class="font64"> the kernel relative to the input, in the sense that as m increases, the index into the&#160;input increases, but the index into the kernel decreases. The only reason to flip&#160;the kernel is to obtain the commutative property. While the commutative property&#160;is useful for writing proofs, it is not usually an important property of a neural&#160;network implementation. Instead, many neural network libraries implement a&#160;related function called the </span><span class="font64" style="font-weight:bold;font-style:italic;">cross-correlation,</span><span class="font64"> which is the same as convolution but&#160;without flipping the kernel:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">S</span><span class="font64">(i,j) = </span><span class="font64" style="font-weight:bold;font-style:italic;">(I</span><span class="font64"> * </span><span class="font64" style="font-weight:bold;font-style:italic;">K</span><span class="font64">)(i,j) = </span><span class="font64" style="font-weight:bold;font-style:italic;">^2^2</span><span class="font64"> I+ <sup>m</sup>’ j + <sup>n</sup>)K</span><span class="font64" style="font-weight:bold;font-style:italic;">(m,n).</span><span class="font64"> &#160;&#160;&#160;(9.6)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font63"> n</span></p>
<p><span class="font64">Many machine learning libraries implement cross-correlation but call it convolution. In this text we will follow this convention of calling both operations convolution,&#160;and specify whether we mean to flip the kernel or not in contexts where kernel&#160;flipping is relevant. In the context of machine learning, the learning algorithm will&#160;learn the appropriate values of the kernel in the appropriate place, so an algorithm&#160;based on convolution with kernel flipping will learn a kernel that is flipped relative&#160;to the kernel learned by an algorithm without the flipping. It is also rare for&#160;convolution to be used alone in machine learning; instead convolution is used&#160;simultaneously with other functions, and the combination of these functions does&#160;not commute regardless of whether the convolution operation flips its kernel or&#160;not.</span></p>
<p><span class="font64">See Fig. 9.1 for an example of convolution (without kernel flipping) applied to a 2-D tensor.</span></p>
<p><span class="font64">Discrete convolution can be viewed as multiplication by a matrix. However, the matrix has several entries constrained to be equal to other entries. For example,&#160;for univariate discrete convolution, each row of the matrix is constrained to be&#160;equal to the row above shifted by one element. This is known as a </span><span class="font64" style="font-weight:bold;font-style:italic;">Toeplitz matrix.&#160;</span><span class="font64">In two dimensions, a </span><span class="font64" style="font-weight:bold;font-style:italic;">doubly block circulant matrix</span><span class="font64"> corresponds to convolution.&#160;In addition to these constraints that several elements be equal to each other,&#160;convolution usually corresponds to a very sparse matrix (a matrix whose entries are&#160;mostly equal to zero). This is because the kernel is usually much smaller than the&#160;input image. Any neural network algorithm that works with matrix multiplication&#160;and does not depend on specific properties of the matrix structure should work&#160;with convolution, without requiring any further changes to the neural network.&#160;Typical convolutional neural networks do make use of further specializations in&#160;order to deal with large inputs efficiently, but these are not strictly necessary from&#160;a theoretical perspective.</span></p><div>
<p><span class="font63">Input</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">a</span></p></td><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">b</span></p></td><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">c</span></p></td><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">d</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">e</span></p></td><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">j</span></p></td><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">g</span></p></td><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">h</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">k</span></p></td><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">1</span></p></td></tr>
</table>
<p><span class="font63">Output</span></p></div><div>
<p><span class="font63">Kernel</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">w</span></p></td><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">x</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">y</span></p></td><td style="vertical-align:middle;">
<p><span class="font50" style="font-style:italic;">z</span></p></td></tr>
</table></div><div>
<p><span class="font11" style="font-style:italic;">aw + bx +</span></p>
<p><span class="font63">ey + </span><span class="font11" style="font-style:italic;">fz</span></p></div><div>
<p><span class="font63">bw + </span><span class="font11" style="font-style:italic;">cx +</span></p>
<p><span class="font11" style="font-style:italic;">fy + 9<sup>z</sup></span></p></div><div>
<p><span class="font11" style="font-style:italic;">cw + dx + gy + hz</span></p></div><div>
<p><span class="font63">ew + </span><span class="font11" style="font-style:italic;">fx + iy + jz</span></p></div><div>
<p><span class="font11" style="font-style:italic;">fw + gx + jy + kz</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font11" style="font-style:italic;">gw</span></p></td><td style="vertical-align:bottom;">
<p><span class="font11" style="font-style:italic;">+</span></p></td><td style="vertical-align:bottom;">
<p><span class="font11" style="font-style:italic;">hx +</span></p></td></tr>
<tr><td>
<p><span class="font11" style="font-style:italic;">ky</span></p></td><td>
<p><span class="font11" style="font-style:italic;">+</span></p></td><td>
<p><span class="font11" style="font-style:italic;">Iz</span></p></td></tr>
</table></div>
<p><span class="font64">Figure 9.1: An example of 2-D convolution without kernel-flipping. In this case we restrict the output to only positions where the kernel lies entirely within the image, called “valid”&#160;convolution in some contexts. We draw boxes with arrows to indicate how the upper-left&#160;element of the output tensor is formed by applying the kernel to the corresponding&#160;upper-left region of the input tensor.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">9.2 Motivation</span></h4>
<p><span class="font64">Convolution leverages three important ideas that can help improve a machine learning system: </span><span class="font64" style="font-weight:bold;font-style:italic;">sparse interactions, parameter sharing</span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">equivariant representations.</span><span class="font64"> Moreover, convolution provides a means for working with inputs of variable&#160;size. We now describe each of these ideas in turn.</span></p>
<p><span class="font64">Traditional neural network layers use matrix multiplication by a matrix of parameters with a separate parameter describing the interaction between each&#160;input unit and each output unit. This means every output unit interacts with every&#160;input unit. Convolutional networks, however, typically have </span><span class="font64" style="font-weight:bold;font-style:italic;">sparse interactions&#160;</span><span class="font64">(also referred to as </span><span class="font64" style="font-weight:bold;font-style:italic;">sparse connectivity</span><span class="font64"> or </span><span class="font64" style="font-weight:bold;font-style:italic;">sparse weights).</span><span class="font64"> This is accomplished by&#160;making the kernel smaller than the input. For example, when processing an image,&#160;the input image might have thousands or millions of pixels, but we can detect small,&#160;meaningful features such as edges with kernels that occupy only tens or hundreds of&#160;pixels. This means that we need to store fewer parameters, which both reduces the&#160;memory requirements of the model and improves its statistical efficiency. It also&#160;means that computing the output requires fewer operations. These improvements&#160;in efficiency are usually quite large. If there are m inputs and </span><span class="font64" style="font-weight:bold;font-style:italic;">n</span><span class="font64"> outputs, then&#160;matrix multiplication requires m x n parameters and the algorithms used in practice&#160;have O(m </span><span class="font64" style="font-weight:bold;font-style:italic;">x n</span><span class="font64">) runtime (per example). If we limit the number of connections&#160;each output may have to </span><span class="font64" style="font-weight:bold;font-style:italic;">k,</span><span class="font64"> then the sparsely connected approach requires only&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">k x n</span><span class="font64"> parameters and O(k x n) runtime. For many practical applications, it is&#160;possible to obtain good performance on the machine learning task while keeping&#160;k several orders of magnitude smaller than m. For graphical demonstrations of&#160;sparse connectivity, see Fig. 9.2 and Fig. 9.3. In a deep convolutional network,&#160;units in the deeper layers may </span><span class="font64" style="font-weight:bold;font-style:italic;">indirectly</span><span class="font64"> interact with a larger portion of the input,&#160;as shown in Fig. 9.4. This allows the network to efficiently describe complicated&#160;interactions between many variables by constructing such interactions from simple&#160;building blocks that each describe only sparse interactions.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Parameter sharing</span><span class="font64"> refers to using the same parameter for more than one function in a model. In a traditional neural net, each element of the weight matrix&#160;is used exactly once when computing the output of a layer. It is multiplied by one&#160;element of the input and then never revisited. As a synonym for parameter sharing,&#160;one can say that a network has </span><span class="font64" style="font-weight:bold;font-style:italic;">tied weights,</span><span class="font64"> because the value of the weight applied&#160;to one input is tied to the value of a weight applied elsewhere. In a convolutional&#160;neural net, each member of the kernel is used at every position of the input (except&#160;perhaps some of the boundary pixels, depending on the design decisions regarding&#160;the boundary). The parameter sharing used by the convolution operation means&#160;that rather than learning a separate set of parameters for every location, we learn</span></p><div><img src="main-90.jpg" alt=""/>
<p><span class="font64">Figure 9.2: </span><span class="font64" style="font-style:italic;">Sparse connectivity, viewed from below:</span><span class="font64"> We highlight one input unit,x</span><span class="font19">3</span><span class="font64">, and also highlight the output units in s that are affected by this unit. </span><span class="font64" style="font-style:italic;">(Top)</span><span class="font64"> When s is formed&#160;by convolution with a kernel of width 3, only three outputs are affected by x. </span><span class="font64" style="font-style:italic;">(Bottom)&#160;</span><span class="font64">When s is formed by matrix multiplication, connectivity is no longer sparse, so all of the&#160;outputs are affected by x</span><span class="font19">3</span><span class="font64">.</span></p></div><div><img src="main-91.jpg" alt=""/></div><div><img src="main-92.jpg" alt=""/>
<p><span class="font64">Figure 9.5: </span><span class="font64" style="font-style:italic;">Parameter sharing:</span><span class="font64"> Black arrows indicate the connections that use a particular parameter in two different models. </span><span class="font64" style="font-style:italic;">(Top)</span><span class="font64"> The black arrows indicate uses of the central&#160;element of a 3-element kernel in a convolutional model. Due to parameter sharing, this&#160;single parameter is used at all input locations. </span><span class="font64" style="font-style:italic;">(Bottom)</span><span class="font64"> The single black arrow indicates&#160;the use of the central element of the weight matrix in a fully connected model. This model&#160;has no parameter sharing so the parameter is used only once.</span></p></div>
<p><span class="font64">only one set. This does not affect the runtime of forward propagation—it is still O(k </span><span class="font64" style="font-weight:bold;font-style:italic;">x</span><span class="font64"> n)—but it does further reduce the storage requirements of the model to&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">k</span><span class="font64"> parameters. Recall that k is usually several orders of magnitude less than m.&#160;Since m and n are usually roughly the same size, k is practically insignificant&#160;compared to m x n. Convolution is thus dramatically more efficient than dense&#160;matrix multiplication in terms of the memory requirements and statistical efficiency.&#160;For a graphical depiction of how parameter sharing works, see Fig. 9.5.</span></p>
<p><span class="font64">As an example of both of these first two principles in action, Fig. 9.6 shows how sparse connectivity and parameter sharing can dramatically improve the efficiency&#160;of a linear function for detecting edges in an image.</span></p>
<p><span class="font64">In the case of convolution, the particular form of parameter sharing causes the layer to have a property called </span><span class="font64" style="font-weight:bold;font-style:italic;">equivariance</span><span class="font64"> to translation. To say a function is&#160;equivariant means that if the input changes, the output changes in the same way.&#160;Specifically, a function f(x) is equivariant to a function g if f (g(x)) = g(f(x)).&#160;In the case of convolution, if we let g be any function that translates the input,&#160;i.e., shifts it, then the convolution function is equivariant to </span><span class="font64" style="font-weight:bold;font-style:italic;">g</span><span class="font64">. For example, let </span><span class="font64" style="font-weight:bold;font-style:italic;">I&#160;</span><span class="font64">be a function giving image brightness at integer coordinates. Let g be a function&#160;mapping one image function to another image function, such that </span><span class="font64" style="font-weight:bold;font-style:italic;">I' = g (I)</span><span class="font64"> is&#160;the image function with </span><span class="font64" style="font-weight:bold;font-style:italic;">I'</span><span class="font64">(x,y) = I(x — 1,y). This shifts every pixel of I one&#160;unit to the right. If we apply this transformation to I, then apply convolution,&#160;the result will be the same as if we applied convolution to I', then applied the&#160;transformation g to the output. When processing time series data, this means&#160;that convolution produces a sort of timeline that shows when different features&#160;appear in the input. If we move an event later in time in the input, the exact&#160;same representation of it will appear in the output, just later in time. Similarly&#160;with images, convolution creates a 2-D map of where certain features appear in&#160;the input. If we move the object in the input, its representation will move the&#160;same amount in the output. This is useful for when we know that some function&#160;of a small number of neighboring pixels is useful when applied to multiple input&#160;locations. For example, when processing images, it is useful to detect edges in&#160;the first layer of a convolutional network. The same edges appear more or less&#160;everywhere in the image, so it is practical to share parameters across the entire&#160;image. In some cases, we may not wish to share parameters across the entire&#160;image. For example, if we are processing images that are cropped to be centered&#160;on an individual’s face, we probably want to extract different features at different&#160;locations—the part of the network processing the top of the face needs to look for&#160;eyebrows, while the part of the network processing the bottom of the face needs to&#160;look for a chin.</span></p>
<p><span class="font64">Convolution is not naturally equivariant to some other transformations, such as changes in the scale or rotation of an image. Other mechanisms are necessary&#160;for handling these kinds of transformations.</span></p>
<p><span class="font64">Finally, some kinds of data cannot be processed by neural networks defined by matrix multiplication with a fixed-shape matrix. Convolution enables processing&#160;of some of these kinds of data. We discuss this further in Sec. 9.7.</span></p><h4><a id="bookmark3"></a><span class="font65" style="font-weight:bold;">9.3 Pooling</span></h4>
<p><span class="font64">A typical layer of a convolutional network consists of three stages (see Fig. 9.7). In the first stage, the layer performs several convolutions in parallel to produce a set&#160;of linear activations. In the second stage, each linear activation is run through a&#160;nonlinear activation function, such as the rectified linear activation function. This&#160;stage is sometimes called the </span><span class="font64" style="font-weight:bold;font-style:italic;">detector</span><span class="font64"> stage. In the third stage, we use a </span><span class="font64" style="font-weight:bold;font-style:italic;">pooling&#160;function</span><span class="font64"> to modify the output of the layer further.</span></p>
<p><span class="font64">A pooling function replaces the output of the net at a certain location with a summary statistic of the nearby outputs. For example, the </span><span class="font64" style="font-weight:bold;font-style:italic;">max pooling</span><span class="font64"> (Zhou&#160;and Chellappa, 1988) operation reports the maximum output within a rectangular</span></p><div><img src="main-93.jpg" alt=""/></div>
<p><span class="font64">Figure 9.6: </span><span class="font64" style="font-style:italic;">Efficiency of edge detection.</span><span class="font64"> The image on the right was formed by taking each pixel in the original image and subtracting the value of its neighboring pixel on the&#160;left. This shows the strength of all of the vertically oriented edges in the input image,&#160;which can be a useful operation for object detection. Both images are 280 pixels tall.&#160;The input image is 320 pixels wide while the output image is 319 pixels wide. This&#160;transformation can be described by a convolution kernel containing two elements, and&#160;requires 319 x 280 x 3 = 267, 960 floating point operations (two multiplications and&#160;one addition per output pixel) to compute using convolution. To describe the same&#160;transformation with a matrix multiplication would take 320 x 280 x 319 x 280, or over&#160;eight billion, entries in the matrix, making convolution four billion times more efficient for&#160;representing this transformation. The straightforward matrix multiplication algorithm&#160;performs over sixteen billion floating point operations, making convolution roughly 60,000&#160;times more efficient computationally. Of course, most of the entries of the matrix would be&#160;zero. If we stored only the nonzero entries of the matrix, then both matrix multiplication&#160;and convolution would require the same number of floating point operations to compute.&#160;The matrix would still need to contain 2 x 319 x 280 = 178, 640 entries. Convolution&#160;is an extremely efficient way of describing transformations that apply the same linear&#160;transformation of a small, local region across the entire input. (Photo credit: Paula&#160;Goodfellow)</span></p><div><div><img src="main-94.jpg" alt=""/>
<p><span class="font64">Figure 9.3: </span><span class="font64" style="font-style:italic;">Sparse connectivity, viewed from above:</span><span class="font64"> We highlight one output unit,s</span><span class="font19">3</span><span class="font64">, and also highlight the input units in x that affect this unit. These units are known as the&#160;</span><span class="font64" style="font-style:italic;">receptive field</span><span class="font64"> of s</span><span class="font19">3</span><span class="font64">. </span><span class="font64" style="font-style:italic;">(Top)</span><span class="font64"> When s is formed by convolution with a kernel of width 3, only&#160;three inputs affect S</span><span class="font19">3</span><span class="font64">. </span><span class="font64" style="font-style:italic;">(Bottom)</span><span class="font64"> When s is formed by matrix multiplication, connectivity&#160;is no longer sparse, so all of the inputs affect S</span><span class="font19">3</span><span class="font64">.</span></p></div></div><div><div><img src="main-95.jpg" alt=""/>
<p><span class="font64">Figure 9.4: The receptive field of the units in the deeper layers of a convolutional network is larger than the receptive field of the units in the shallow layers. This effect increases if&#160;the network includes architectural features like strided convolution (Fig. 9.12) or pooling&#160;(Sec. 9.3). This means that even though </span><span class="font64" style="font-style:italic;">direct</span><span class="font64"> connections in a convolutional net are very&#160;sparse, units in the deeper layers can be </span><span class="font64" style="font-style:italic;">indirectly</span><span class="font64"> connected to all or most of the input&#160;image.</span></p></div></div><div><div><img src="main-96.jpg" alt=""/>
<p><span class="font64">Figure 9.7: The components of a typical convolutional neural network layer. There are two commonly used sets of terminology for describing these layers. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> In this terminology,&#160;the convolutional net is viewed as a small number of relatively complex layers, with each&#160;layer having many “stages.” In this terminology, there is a one-to-one mapping between&#160;kernel tensors and network layers. In this book we generally use this terminology. </span><span class="font64" style="font-style:italic;">(Right)&#160;</span><span class="font64">In this terminology, the convolutional net is viewed as a larger number of simple layers;&#160;every step of processing is regarded as a layer in its own right. This means that not every&#160;“layer” has parameters.</span></p></div></div>
<p><span class="font64">neighborhood. Other popular pooling functions include the average of a rectangular neighborhood, the L</span><span class="font18"><sup>2</sup></span><span class="font64"> norm of a rectangular neighborhood, or a weighted average&#160;based on the distance from the central pixel.</span></p>
<p><span class="font64">In all cases, pooling helps to make the representation become approximately </span><span class="font64" style="font-weight:bold;font-style:italic;">invariant</span><span class="font64"> to small translations of the input. Invariance to translation means that if&#160;we translate the input by a small amount, the values of most of the pooled outputs&#160;do not change. See Fig. 9.8 for an example of how this works. </span><span class="font64" style="font-weight:bold;">Invariance to&#160;local translation can be a very useful property if we care more about&#160;whether some feature is present than exactly where it is. </span><span class="font64">For example,&#160;when determining whether an image contains a face, we need not know the location&#160;of the eyes with pixel-perfect accuracy, we just need to know that there is an eye on&#160;the left side of the face and an eye on the right side of the face. In other contexts,&#160;it is more important to preserve the location of a feature. For example, if we want&#160;to find a corner defined by two edges meeting at a specific orientation, we need to&#160;preserve the location of the edges well enough to test whether they meet.</span></p>
<p><span class="font64">The use of pooling can be viewed as adding an infinitely strong prior that the function the layer learns must be invariant to small translations. When this&#160;assumption is correct, it can greatly improve the statistical efficiency of the network.</span></p>
<p><span class="font64">Pooling over spatial regions produces invariance to translation, but if we pool over the outputs of separately parametrized convolutions, the features can learn&#160;which transformations to become invariant to (see Fig. 9.9).</span></p>
<p><span class="font64">Because pooling summarizes the responses over a whole neighborhood, it is possible to use fewer pooling units than detector units, by reporting summary&#160;statistics for pooling regions spaced k pixels apart rather than 1 pixel apart. See&#160;Fig. 9.10 for an example. This improves the computational efficiency of the network&#160;because the next layer has roughly k times fewer inputs to process. When the&#160;number of parameters in the next layer is a function of its input size (such as&#160;when the next layer is fully connected and based on matrix multiplication) this&#160;reduction in the input size can also result in improved statistical efficiency and&#160;reduced memory requirements for storing the parameters.</span></p>
<p><span class="font64">For many tasks, pooling is essential for handling inputs of varying size. For example, if we want to classify images of variable size, the input to the classification&#160;layer must have a fixed size. This is usually accomplished by varying the size of an&#160;offset between pooling regions so that the classification layer always receives the&#160;same number of summary statistics regardless of the input size. For example, the&#160;final pooling layer of the network may be defined to output four sets of summary&#160;statistics, one for each quadrant of an image, regardless of the image size.</span></p>
<p><span class="font64">Some theoretical work gives guidance as to which kinds of pooling one should</span></p><div>
<p><span class="font63">POOLING STAGE</span></p><img src="main-97.jpg" alt=""/></div><div><div>
<p><span class="font63">POOLING STAGE</span></p><img src="main-98.jpg" alt=""/>
<p><span class="font64">Figure 9.8: Max pooling introduces invariance. </span><span class="font64" style="font-style:italic;">(Top)</span><span class="font64"> A view of the middle of the output of a convolutional layer. The bottom row shows outputs of the nonlinearity. The top&#160;row shows the outputs of max pooling, with a stride of one pixel between pooling regions&#160;and a pooling region width of three pixels. </span><span class="font64" style="font-style:italic;">(Bottom)</span><span class="font64"> A view of the same network, after&#160;the input has been shifted to the right by one pixel. Every value in the bottom row has&#160;changed, but only half of the values in the top row have changed, because the max pooling&#160;units are only sensitive to the maximum value in the neighborhood, not its exact location.</span></p></div></div><div><div><img src="main-99.jpg" alt=""/></div></div><div><div><img src="main-100.jpg" alt=""/></div></div>
<p><span class="font64">Figure 9.9: </span><span class="font64" style="font-style:italic;">Example of learned invariances:</span><span class="font64"> A pooling unit that pools over multiple features that are learned with separate parameters can learn to be invariant to transformations of&#160;the input. Here we show how a set of three learned filters and a max pooling unit can learn&#160;to become invariant to rotation. All three filters are intended to detect a hand-written 5.&#160;Each filter attempts to match a slightly different orientation of the 5. When a 5 appears in&#160;the input, the corresponding filter will match it and cause a large activation in a detector&#160;unit. The max pooling unit then has a large activation regardless of which pooling unit&#160;was activated. We show here how the network processes two different inputs, resulting&#160;in two different detector units being activated. The effect on the pooling unit is roughly&#160;the same either way. This principle is leveraged by maxout networks (Goodfellow </span><span class="font64" style="font-style:italic;">et al.,&#160;</span><span class="font64">2013a) and other convolutional networks. Max pooling over spatial positions is naturally&#160;invariant to translation; this multi-channel approach is only necessary for learning other&#160;transformations.</span></p><div><div><img src="main-101.jpg" alt=""/>
<p><span class="font64">Figure 9.10: </span><span class="font64" style="font-style:italic;">Pooling with downsampling.</span><span class="font64"> Here we use max-pooling with a pool width of three and a stride between pools of two. This reduces the representation size by a factor&#160;of two, which reduces the computational and statistical burden on the next layer. Note&#160;that the rightmost pooling region has a smaller size, but must be included if we do not&#160;want to ignore some of the detector units.</span></p></div></div>
<p><span class="font64">use in various situations (Boureau </span><span class="font64" style="font-weight:bold;font-style:italic;">et al</span><span class="font64">2010). It is also possible to dynamically pool features together, for example, by running a clustering algorithm on the&#160;locations of interesting features (Boureau </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011). This approach yields a&#160;different set of pooling regions for each image. Another approach is to </span><span class="font64" style="font-weight:bold;">learn </span><span class="font64">a&#160;single pooling structure that is then applied to all images (Jia </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2012).</span></p>
<p><span class="font64">Pooling can complicate some kinds of neural network architectures that use top-down information, such as Boltzmann machines and autoencoders. These&#160;issues will be discussed further when we present these types of networks in Part&#160;III. Pooling in convolutional Boltzmann machines is presented in Sec. 20.6. The&#160;inverse-like operations on pooling units needed in some differentiable networks will&#160;be covered in Sec. 20.10.6.</span></p>
<p><span class="font64">Some examples of complete convolutional network architectures for classification using convolution and pooling are shown in Fig. 9.11.</span></p><h4><a id="bookmark4"></a><span class="font65" style="font-weight:bold;">9.4 Convolution and Pooling as an Infinitely Strong&#160;Prior</span></h4>
<p><span class="font64">Recall the concept of a </span><span class="font64" style="font-weight:bold;font-style:italic;">prior probability distribution</span><span class="font64"> from Sec. 5.2. This is a probability distribution over the parameters of a model that encodes our beliefs&#160;about what models are reasonable, before we have seen any data.</span></p>
<p><span class="font64">Priors can be considered weak or strong depending on how concentrated the probability density in the prior is. A weak prior is a prior distribution with high&#160;entropy, such as a Gaussian distribution with high variance. Such a prior allows&#160;the data to move the parameters more or less freely. A strong prior has very low&#160;entropy, such as a Gaussian distribution with low variance. Such a prior plays a&#160;more active role in determining where the parameters end up.</span></p>
<p><span class="font64">An infinitely strong prior places zero probability on some parameters and says that these parameter values are completely forbidden, regardless of how much&#160;support the data gives to those values.</span></p>
<p><span class="font64">We can imagine a convolutional net as being similar to a fully connected net, but with an infinitely strong prior over its weights. This infinitely strong prior&#160;says that the weights for one hidden unit must be identical to the weights of its&#160;neighbor, but shifted in space. The prior also says that the weights must be zero,&#160;except for in the small, spatially contiguous receptive field assigned to that hidden&#160;unit. Overall, we can think of the use of convolution as introducing an infinitely&#160;strong prior probability distribution over the parameters of a layer. This prior&#160;says that the function the layer should learn contains only local interactions and is</span></p><div><img src="main-102.jpg" alt=""/></div>
<p><span class="font64">Figure 9.11: Examples of architectures for classification with convolutional networks. The specific strides and depths used in this figure are not advisable for real use; they are&#160;designed to be very shallow in order to fit onto the page. Real convolutional networks&#160;also often involve significant amounts of branching, unlike the chain structures used&#160;here for simplicity. </span><span class="font64" style="font-style:italic;">(Left)</span><span class="font64"> A convolutional network that processes a fixed image size.&#160;After alternating between convolution and pooling for a few layers, the tensor for the&#160;convolutional feature map is reshaped to flatten out the spatial dimensions. The rest&#160;of the network is an ordinary feedforward network classifier, as described in Chapter 6.&#160;</span><span class="font64" style="font-style:italic;">(Center)</span><span class="font64"> A convolutional network that processes a variable-sized image, but still maintains&#160;a fully connected section. This network uses a pooling operation with variably-sized pools&#160;but a fixed number of pools, in order to provide a fixed-size vector of 576 units to the&#160;fully connected portion of the network. </span><span class="font64" style="font-style:italic;">(Right)</span><span class="font64"> A convolutional network that does not&#160;have any fully connected weight layer. Instead, the last convolutional layer outputs one&#160;feature map per class. The model presumably learns a map of how likely each class is to&#160;occur at each spatial location. Averaging a feature map down to a single value provides&#160;the argument to the softmax classifier at the top.</span></p>
<p><span class="font64">equivariant to translation. Likewise, the use of pooling is an infinitely strong prior that each unit should be invariant to small translations.</span></p>
<p><span class="font64">Of course, implementing a convolutional net as a fully connected net with an infinitely strong prior would be extremely computationally wasteful. But thinking&#160;of a convolutional net as a fully connected net with an infinitely strong prior can&#160;give us some insights into how convolutional nets work.</span></p>
<p><span class="font64">One key insight is that convolution and pooling can cause underfitting. Like any prior, convolution and pooling are only useful when the assumptions made&#160;by the prior are reasonably accurate. If a task relies on preserving precise spatial&#160;information, then using pooling on all features can increase the training error.&#160;Some convolutional network architectures (Szegedy </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014a) are designed to&#160;use pooling on some channels but not on other channels, in order to get both&#160;highly invariant features and features that will not underfit when the translation&#160;invariance prior is incorrect. When a task involves incorporating information from&#160;very distant locations in the input, then the prior imposed by convolution may be&#160;inappropriate.</span></p>
<p><span class="font64">Another key insight from this view is that we should only compare convolutional models to other convolutional models in benchmarks of statistical learning performance. Models that do not use convolution would be able to learn even if&#160;we permuted all of the pixels in the image. For many image datasets, there are&#160;separate benchmarks for models that are </span><span class="font64" style="font-weight:bold;font-style:italic;">permutation invariant</span><span class="font64"> and must discover&#160;the concept of topology via learning, and models that have the knowledge of spatial&#160;relationships hard-coded into them by their designer.</span></p><h4><a id="bookmark5"></a><span class="font65" style="font-weight:bold;">9.5 Variants of the Basic Convolution Function</span></h4>
<p><span class="font64">When discussing convolution in the context of neural networks, we usually do not refer exactly to the standard discrete convolution operation as it is usually&#160;understood in the mathematical literature. The functions used in practice differ&#160;slightly. Here we describe these differences in detail, and highlight some useful&#160;properties of the functions used in neural networks.</span></p>
<p><span class="font64">First, when we refer to convolution in the context of neural networks, we usually actually mean an operation that consists of many applications of convolution in&#160;parallel. This is because convolution with a single kernel can only extract one kind&#160;of feature, albeit at many spatial locations. Usually we want each layer of our&#160;network to extract many kinds of features, at many locations.</span></p>
<p><span class="font64">Additionally, the input is usually not just a grid of real values. Rather, it is a grid of vector-valued observations. For example, a color image has a red, green&#160;and blue intensity at each pixel. In a multilayer convolutional network, the input&#160;to the second layer is the output of the first layer, which usually has the output&#160;of many different convolutions at each position. When working with images, we&#160;usually think of the input and output of the convolution as being 3-D tensors, with&#160;one index into the different channels and two indices into the spatial coordinates&#160;of each channel. Software implementations usually work in batch mode, so they&#160;will actually use 4-D tensors, with the fourth axis indexing different examples in&#160;the batch, but we will omit the batch axis in our description here for simplicity.</span></p>
<p><span class="font64">Because convolutional networks usually use multi-channel convolution, the linear operations they are based on are not guaranteed to be commutative, even if&#160;kernel-flipping is used. These multi-channel operations are only commutative if&#160;each operation has the same number of output channels as input channels.</span></p>
<p><span class="font64">Assume we have a 4-D kernel tensor K with element K</span><span class="font64" style="font-weight:bold;font-style:italic;">i,j,k,l</span><span class="font63"> giving the connection strength between a unit in channel i of the output and a unit in channel j of the&#160;input, with an offset of k rows and l columns between the output unit and the&#160;input unit. Assume our input consists of observed data V with element </span><span class="font64" style="font-weight:bold;font-style:italic;">V<sub>i</sub>,j,k</span><span class="font63"> giving&#160;the value of the input unit within channel i at row j and column k. Assume our&#160;output consists of Z with the same format as V. If Z is produced by convolving K&#160;across V without flipping K, then</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>Z</sup>i,j,k</span><span class="font63"> — </span><span class="font63" style="text-decoration:underline;">^ '</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>V</sup>lj+m—l,k+n—l<sup>K</sup>i,l,m,n</span><span class="font63"> &#160;&#160;&#160;<sup>(9</sup>.<sup>7)</sup></span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">l,m,n</span></p>
<p><span class="font64">where the summation over l, m and n is over all values for which the tensor indexing operations inside the summation is valid. In linear algebra notation, we index into&#160;arrays using a 1 for the first entry. This necessitates the — 1 in the above formula.&#160;Programming languages such as C and Python index starting from 0, rendering&#160;the above expression even simpler.</span></p>
<p><span class="font64">We may want to skip over some positions of the kernel in order to reduce the computational cost (at the expense of not extracting our features as finely). We&#160;can think of this as downsampling the output of the full convolution function. If&#160;we want to sample only every s pixels in each direction in the output, then we can&#160;define a downsampled convolution function c such that</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>Z</sup>i,j,k</span><span class="font63"> &#160;&#160;&#160;<sup>c(K</sup>, <sup>V</sup>, <sup>s)</sup>i,j,k&#160;&#160;&#160;&#160;</span><span class="font64" style="text-decoration:underline;">^ '</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">\_<sup>V</sup>l,(j-l)xs+m,(k — l)xs+n<sup>K</sup>i,l,m,n '</span><span class="font64">&#160;&#160;&#160;&#160;<sup>(9</sup>.<sup>8)</sup></span></p>
<p><span class="font63">l,m,n</span></p>
<p><span class="font64">We refer to s as the </span><span class="font64" style="font-weight:bold;font-style:italic;">stride</span><span class="font64"> of this downsampled convolution. It is also possible to define a separate stride for each direction of motion. See Fig. 9.12 for an&#160;illustration.</span></p><div><img src="main-103.jpg" alt=""/></div><div><div><img src="main-104.jpg" alt=""/>
<p><span class="font64">Figure 9.12: Convolution with a stride. In this example, we use a stride of two. </span><span class="font64" style="font-style:italic;">(Top) </span><span class="font64">Convolution with a stride length of two implemented in a single operation. </span><span class="font64" style="font-style:italic;">(Bottom)&#160;</span><span class="font64">Convolution with a stride greater than one pixel is mathematically equivalent to convolution&#160;with unit stride followed by downsampling. Obviously, the two-step approach involving&#160;downsampling is computationally wasteful, because it computes many values that are&#160;then discarded.</span></p></div></div>
<p><span class="font64">One essential feature of any convolutional network implementation is the ability to implicitly zero-pad the input V in order to make it wider. Without this feature,&#160;the width of the representation shrinks by one pixel less than the kernel width&#160;at each layer. Zero padding the input allows us to control the kernel width and&#160;the size of the output independently. Without zero padding, we are forced to&#160;choose between shrinking the spatial extent of the network rapidly and using small&#160;kernels—both scenarios that significantly limit the expressive power of the network.&#160;See Fig. 9.13 for an example.</span></p>
<p><span class="font64">Three special cases of the zero-padding setting are worth mentioning. One is the extreme case in which no zero-padding is used whatsoever, and the convolution&#160;kernel is only allowed to visit positions where the entire kernel is contained entirely&#160;within the image. In MATLAB terminology, this is called </span><span class="font64" style="font-weight:bold;font-style:italic;">valid</span><span class="font64"> convolution. In&#160;this case, all pixels in the output are a function of the same number of pixels in&#160;the input, so the behavior of an output pixel is somewhat more regular. However,&#160;the size of the output shrinks at each layer. If the input image has width m and&#160;the kernel has width k, the output will be of width m — </span><span class="font64" style="font-weight:bold;font-style:italic;">k</span><span class="font64"> + 1. The rate of this&#160;shrinkage can be dramatic if the kernels used are large. Since the shrinkage is&#160;greater than </span><span class="font18">0</span><span class="font64">, it limits the number of convolutional layers that can be included&#160;in the network. As layers are added, the spatial dimension of the network will&#160;eventually drop to </span><span class="font18">1</span><span class="font64"> x </span><span class="font18">1</span><span class="font64">, at which point additional layers cannot meaningfully&#160;be considered convolutional. Another special case of the zero-padding setting is&#160;when just enough zero-padding is added to keep the size of the output equal to the&#160;size of the input. MATLAB calls this </span><span class="font64" style="font-weight:bold;font-style:italic;">same</span><span class="font64"> convolution. In this case, the network&#160;can contain as many convolutional layers as the available hardware can support,&#160;since the operation of convolution does not modify the architectural possibilities&#160;available to the next layer. However, the input pixels near the border influence&#160;fewer output pixels than the input pixels near the center. This can make the&#160;border pixels somewhat underrepresented in the model. This motivates the other&#160;extreme case, which MATLAB refers to as </span><span class="font64" style="font-weight:bold;font-style:italic;">full convolution,</span><span class="font64"> in which enough zeroes&#160;are added for every pixel to be visited k times in each direction, resulting in an&#160;output image of width m + k — 1. In this case, the output pixels near the border&#160;are a function of fewer pixels than the output pixels near the center. This can&#160;make it difficult to learn a single kernel that performs well at all positions in&#160;the convolutional feature map. Usually the optimal amount of zero padding (in&#160;terms of test set classification accuracy) lies somewhere between “valid” and “same”&#160;convolution.</span></p>
<p><span class="font64">In some cases, we do not actually want to use convolution, but rather locally connected layers (LeCun, 1986, 1989). In this case, the adjacency matrix in the&#160;graph of our MLP is the same, but every connection has its own weight, specified</span></p><div><div><img src="main-105.jpg" alt=""/>
<p><span class="font64">Figure 9.13: </span><span class="font64" style="font-style:italic;">The effect of zero padding on network size:</span><span class="font64"> Consider a convolutional network with a kernel of width six at every layer. In this example, we do not use any pooling, so&#160;only the convolution operation itself shrinks the network size. </span><span class="font64" style="font-style:italic;">(Top)</span><span class="font64"> In this convolutional&#160;network, we do not use any implicit zero padding. This causes the representation to&#160;shrink by five pixels at each layer. Starting from an input of sixteen pixels, we are only&#160;able to have three convolutional layers, and the last layer does not ever move the kernel,&#160;so arguably only two of the layers are truly convolutional. The rate of shrinking can&#160;be mitigated by using smaller kernels, but smaller kernels are less expressive and some&#160;shrinking is inevitable in this kind of architecture. </span><span class="font64" style="font-style:italic;">(Bottom)</span><span class="font64"> By adding five implicit zeroes&#160;to each layer, we prevent the representation from shrinking with depth. This allows us to&#160;make an arbitrarily deep convolutional network.</span></p></div></div>
<p><span class="font64">by a </span><span class="font18">6</span><span class="font64">-D tensor W. The indices into W are respectively: </span><span class="font64" style="font-weight:bold;font-style:italic;">i,</span><span class="font64"> the output channel, j, the output row, </span><span class="font64" style="font-weight:bold;font-style:italic;">k,</span><span class="font64"> the output column, l, the input channel, m, the row offset&#160;within the input, and n, the column offset within the input. The linear part of a&#160;locally connected layer is then given by</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>Z</sup>i,j,k</span><span class="font63"> ~ </span><span class="font63" style="text-decoration:underline;">^ ^</span><span class="font63"> <sup>[</sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>V</sup>l,j+m- 1,k+n- 1'<sup>w</sup>i,j,k,l,m,n]</span><span class="font63"> • &#160;&#160;&#160;(</span><span class="font18"><sup>99</sup></span><span class="font64">י)</span></p>
<p><span class="font63">l,m,n</span></p>
<p><span class="font64">This is sometimes also called </span><span class="font64" style="font-weight:bold;font-style:italic;">unshared convolution,</span><span class="font64"> because it is a similar operation to discrete convolution with a small kernel, but without sharing parameters across&#160;locations. Fig. 9.14 compares local connections, convolution, and full connections.</span></p>
<p><span class="font64">Locally connected layers are useful when we know that each feature should be a function of a small part of space, but there is no reason to think that the same&#160;feature should occur across all of space. For example, if we want to tell if an image&#160;is a picture of a face, we only need to look for the mouth in the bottom half of the&#160;image.</span></p>
<p><span class="font64">It can also be useful to make versions of convolution or locally connected layers in which the connectivity is further restricted, for example to constrain that each&#160;output channel i be a function of only a subset of the input channels l. A common&#160;way to do this is to make the first m output channels connect to only the first&#160;n input channels, the second m output channels connect to only the second n&#160;input channels, and so on. See Fig. 9.15 for an example. Modeling interactions&#160;between few channels allows the network to have fewer parameters in order to&#160;reduce memory consumption and increase statistical efficiency, and also reduces&#160;the amount of computation needed to perform forward and back-propagation. It&#160;accomplishes these goals without reducing the number of hidden units.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Tiled convolution</span><span class="font64"> (Gregor and LeCun, 2010a; Le </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010) offers a compromise between a convolutional layer and a locally connected layer. Rather than learning&#160;a separate set of weights at </span><span class="font64" style="font-weight:bold;font-style:italic;">every</span><span class="font64"> spatial location, we learn a set of kernels that&#160;we rotate through as we move through space. This means that immediately&#160;neighboring locations will have different filters, like in a locally connected layer, but&#160;the memory requirements for storing the parameters will increase only by a factor&#160;of the size of this set of kernels, rather than the size of the entire output feature&#160;map. See Fig. 9.16 for a comparison of locally connected layers, tiled convolution,&#160;and standard convolution.</span></p>
<p><span class="font64">To define tiled convolution algebraically, let k be a </span><span class="font18">6</span><span class="font64">-D tensor, where two of the dimensions correspond to different locations in the output map. Rather than&#160;having a separate index for each location in the output map, output locations cycle&#160;through a set of </span><span class="font64" style="font-weight:bold;font-style:italic;">t</span><span class="font64"> different choices of kernel stack in each direction. If </span><span class="font64" style="font-weight:bold;font-style:italic;">t</span><span class="font64"> is equal to</span></p><div><img src="main-106.jpg" alt=""/></div><div><img src="main-107.jpg" alt=""/></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">m</span></p>
<p><span class="font64">a;</span></p>
<p><span class="font62">&lt;d</span></p>
<p><span class="font17">Pi</span></p>
<p><span class="font39">•H</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">U</span></p>
<p><span class="font11" style="font-style:italic;">o</span></p>
<p><span class="font11" style="font-style:italic;">o</span></p>
<p><span class="font64">o</span></p>
<p><span class="font39">CD</span></p>
<p><span class="font17">Pi</span></p>
<p><span class="font10">fd</span></p>
<p><span class="font62">&lt;d</span></p>
<p><span class="font64">o</span></p><img src="main-108.jpg" alt=""/>
<p><span class="font64">Figure 9.15: A convolutional network with the first two output channels connected to only the first two input channels, and the second two output channels connected to only&#160;the second two input channels.</span></p></div><div><div><img src="main-109.jpg" alt=""/>
<p><span class="font64">Figure 9.14: Comparison of local connections, convolution, and full connections.</span></p>
<p><span class="font64" style="font-style:italic;">(Top)</span><span class="font64"> A locally connected layer with a patch size of two pixels. Each edge is labeled with a unique letter to show that each edge is associated with its own weight parameter.</span></p>
<p><span class="font64" style="font-style:italic;">(Center)</span><span class="font64"> A convolutional layer with a kernel width of two pixels. This model has exactly the same connectivity as the locally connected layer. The difference lies not in which units&#160;interact with each other, but in how the parameters are shared. The locally connected layer&#160;has no parameter sharing. The convolutional layer uses the same two weights repeatedly&#160;across the entire input, as indicated by the repetition of the letters labeling each edge.&#160;</span><span class="font64" style="font-style:italic;">(Bottom)</span><span class="font64"> A fully connected layer resembles a locally connected layer in the sense that&#160;each edge has its own parameter (there are too many to label explicitly with letters in this&#160;diagram). However, it does not have the restricted connectivity of the locally connected&#160;layer.</span></p></div></div><div><img src="main-110.jpg" alt=""/></div><div><img src="main-111.jpg" alt=""/></div>
<p><span class="font64">the output width, this is the same as a locally connected layer.</span></p>
<p><span class="font64"><sup>Z</sup>,j,k = </span><span class="font64" style="text-decoration:underline;">^ '</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>V</sup>l,j+m-1,k+n-1<sup>K</sup>i,l,m,n,j%t+1,k%t+1 </span><span class="font64"><sup>9</sup>.<sup>10</sup>) &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">י</span><span class="font64">)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">l,m,n</span></p>
<p><span class="font64">where % is the modulo operation, with t</span><span class="font64" style="font-weight:bold;font-style:italic;">%t</span><span class="font63"> =0, (t + 1)%t = 1, etc. It is straightforward to generalize this equation to use a different tiling range for each&#160;dimension.</span></p>
<p><span class="font64">Both locally connected layers and tiled convolutional layers have an interesting interaction with max-pooling: the detector units of these layers are driven by&#160;different filters. If these filters learn to detect different transformed versions of&#160;the same underlying features, then the max-pooled units become invariant to the&#160;learned transformation (see Fig. 9.9). Convolutional layers are hard-coded to be&#160;invariant specifically to translation.</span></p>
<p><span class="font64">Other operations besides convolution are usually necessary to implement a convolutional network. To perform learning, one must be able to compute the&#160;gradient with respect to the kernel, given the gradient with respect to the outputs.&#160;In some simple cases, this operation can be performed using the convolution&#160;operation, but many cases of interest, including the case of stride greater than </span><span class="font18">1</span><span class="font64">,&#160;do not have this property.</span></p>
<p><span class="font64">Recall that convolution is a linear operation and can thus be described as a matrix multiplication (if we first reshape the input tensor into a flat vector). The&#160;matrix involved is a function of the convolution kernel. The matrix is sparse and&#160;each element of the kernel is copied to several elements of the matrix. This view&#160;helps us to derive some of the other operations needed to implement a convolutional&#160;network.</span></p>
<p><span class="font64">Multiplication by the transpose of the matrix defined by convolution is one such operation. This is the operation needed to back-propagate error derivatives&#160;through a convolutional layer, so it is needed to train convolutional networks&#160;that have more than one hidden layer. This same operation is also needed if we&#160;wish to reconstruct the visible units from the hidden units (Simard </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 1992).&#160;Reconstructing the visible units is an operation commonly used in the models&#160;described in Part III of this book, such as autoencoders, RBMs, and sparse coding.</span></p>
<p><span class="font64">Transpose convolution is necessary to construct convolutional versions of those models. Like the kernel gradient operation, this input gradient operation can be&#160;implemented using a convolution in some cases, but in the general case requires&#160;a third operation to be implemented. Care must be taken to coordinate this&#160;transpose operation with the forward propagation. The size of the output that the&#160;transpose operation should return depends on the zero padding policy and stride of&#160;the forward propagation operation, as well as the size of the forward propagation’s&#160;output map. In some cases, multiple sizes of input to forward propagation can&#160;result in the same size of output map, so the transpose operation must be explicitly&#160;told what the size of the original input was.</span></p>
<p><span class="font64">These three operations—convolution, backprop from output to weights, and backprop from output to inputs—are sufficient to compute all of the gradients&#160;needed to train any depth of feedforward convolutional network, as well as to train&#160;convolutional networks with reconstruction functions based on the transpose of&#160;convolution. See Goodfellow (2010) for a full derivation of the equations in the&#160;fully general multi-dimensional, multi-example case. To give a sense of how these&#160;equations work, we present the two dimensional, single example version here.</span></p>
<p><span class="font64">Suppose we want to train a convolutional network that incorporates strided convolution of kernel stack K applied to multi-channel image V with stride s as&#160;defined by c(K, V, s) as in Eq. 9.8. Suppose we want to minimize some loss function&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64">(V, K). During forward propagation, we will need to use c itself to output Z,&#160;which is then propagated through the rest of the network and used to compute&#160;the cost function J. During back-propagation, we will receive a tensor G such that</span></p>
<p><span class="font64"><sup>Gi,j,k</sup> ~ </span><span class="font64" style="font-weight:bold;font-style:italic;">3Z־~k</span><span class="font64"> <sup>J(V</sup>, <sup>K)</sup>•</span></p>
<p><span class="font64">To train the network, we need to compute the derivatives with respect to the weights in the kernel. To do so, we can use a function</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;"><sup>g(G</sup>,</span><span class="font64"> <sup>V</sup>, </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>s</sup>)i,j,k,l ~</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span></p></div><div>
<p><span class="font64"><sup>J(V</sup>, <sup>K)</sup> ~ </span><span class="font64" style="text-decoration:underline;">^ '</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>G</sup>i,m,n <sup>V</sup>j,(m-1)xs+k,(n-1)xs+l•</span><span class="font64"> &#160;&#160;&#160;(9.11)</span></p></div>
<p><span class="font64">If this layer is not the bottom layer of the network, we will need to compute the gradient with respect to V in order to back-propagate the error farther down.&#160;To do so, we can use a function</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">h<sup>(K</sup>,</span><span class="font64"> <sup>G</sup>, <sup>s)</sup>i,j,k</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">d</span></p></div><div>
<p><span class="font64"><sup>dV</sup>i,j,k = £</span></p></div><div><h5><a id="bookmark6"></a><span class="font64">J(V, K)</span></h5></div><div>
<p><span class="font64">(9.12)</span></p>
<p><span class="font64">(9.13)</span></p></div><div>
<p><span class="font64">l,m</span></p>
<p><span class="font64">s.t.</span></p></div><div>
<p><span class="font63"><sup>n</sup>,p</span></p>
<p><span class="font64">s.t.</span></p></div><div>
<p><span class="font64">(l-l)xs+m=j (<sup>n-</sup>l<sup>)</sup>x<sup>s</sup>+p=<sup>k</sup></span></p></div>
<p><span class="font64">Autoencoder networks, described in Chapter 14, are feedforward networks trained to copy their input to their output. A simple example is the PCA algorithm,&#160;that copies its input </span><span class="font64" style="font-weight:bold;">x </span><span class="font64">to an approximate reconstruction </span><span class="font64" style="font-weight:bold;">r </span><span class="font64">using the function&#160;</span><span class="font64" style="font-weight:bold;">W</span><span class="font64"><sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">Wx.</span><span class="font64"> It is common for more general autoencoders to use multiplication&#160;by the transpose of the weight matrix just as PCA does. To make such models&#160;convolutional, we can use the function h to perform the transpose of the convolution&#160;operation. Suppose we have hidden units H in the same format as Z and we define&#160;a reconstruction</span></p>
<p><span class="font64">R = h(K, H,s). &#160;&#160;&#160;(9.14)</span></p>
<p><span class="font64">In order to train the autoencoder, we will receive the gradient with respect to R as a tensor E. To train the decoder, we need to obtain the gradient with&#160;respect to K. This is given by </span><span class="font64" style="font-weight:bold;font-style:italic;">g</span><span class="font64"> (H, E , s). To train the encoder, we need to obtain&#160;the gradient with respect to H. This is given by c(K, E, s). It is also possible to&#160;differentiate through g using c and h, but these operations are not needed for the&#160;back-propagation algorithm on any standard network architectures.</span></p>
<p><span class="font64">Generally, we do not use only a linear operation in order to transform from the inputs to the outputs in a convolutional layer. We generally also add some&#160;bias term to each output before applying the nonlinearity. This raises the question&#160;of how to share parameters among the biases. For locally connected layers it is&#160;natural to give each unit its own bias, and for tiled convolution, it is natural to&#160;share the biases with the same tiling pattern as the kernels. For convolutional&#160;layers, it is typical to have one bias per channel of the output and share it across&#160;all locations within each convolution map. However, if the input is of known, fixed&#160;size, it is also possible to learn a separate bias at each location of the output map.&#160;Separating the biases may slightly reduce the statistical efficiency of the model, but&#160;also allows the model to correct for differences in the image statistics at different&#160;locations. For example, when using implicit zero padding, detector units at the&#160;edge of the image receive less total input and may need larger biases.</span></p><h4><a id="bookmark7"></a><span class="font65" style="font-weight:bold;">9.6 Structured Outputs</span></h4>
<p><span class="font64">Convolutional networks can be used to output a high-dimensional, structured object, rather than just predicting a class label for a classification task or a real&#160;value for a regression task. Typically this object is just a tensor, emitted by a&#160;standard convolutional layer. For example, the model might emit a tensor S, where&#160;is the probability that pixel (j, k) of the input to the network belongs to class&#160;i. This allows the model to label every pixel in an image and draw precise masks&#160;that follow the outlines of individual objects.</span></p>
<p><span class="font64">One issue that often comes up is that the output plane can be smaller than the input plane, as shown in Fig. 9.13. In the kinds of architectures typically used for&#160;classification of a single object in an image, the greatest reduction in the spatial&#160;dimensions of the network comes from using pooling layers with large stride. In</span></p>
<table border="1">
<tr><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font63">Y (<sup>1</sup>A</span></p></td><td style="vertical-align:bottom;">
<p><span class="font12" style="font-style:italic;">(</span><span class="font63"> Y<sup>(2)</sup> '</span></p></td><td style="vertical-align:bottom;">
<p><span class="font48">(15</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font63">V</span></p></td><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font63" style="font-variant:small-caps;">. V w\</span></p></td><td style="vertical-align:middle;">
<p><span class="font63">V</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font40">(</span></p></td><td style="vertical-align:bottom;">
<p><span class="font63">H<sup>(1)</sup>)</span></p></td><td>
<p><span class="font12" style="font-style:italic;">f</span><span class="font63"> H<sup>(2)</sup> </span><span class="font12" style="font-style:italic;">'</span></p></td><td style="vertical-align:bottom;">
<p><span class="font63" style="font-variant:small-caps;">( h<sup>(3)</sup> J</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font63">U</span></p></td><td style="vertical-align:middle;">
<p><span class="font63">U</span></p></td><td style="vertical-align:middle;">
<p><span class="font63">U</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font63">X </span><span class="font12" style="font-style:italic;">\</span></p></td><td>
<p></p></td><td>
<p></p></td></tr>
</table>
<p><span class="font64">Figure 9.17: An example of a recurrent convolutional network for pixel labeling. The input is an image tensor X, with axes corresponding to image rows, image columns, and&#160;channels (red, green, blue). The goal is to output a tensor of labelsY, with a probability&#160;distribution over labels for each pixel. This tensor has axes corresponding to image rows,&#160;image columns, and the different classes. Rather than outputtingY in a single shot, the&#160;recurrent network iteratively refines its estimate Y by using a previous estimate of Y&#160;as input for creating a new estimate. The same parameters are used for each updated&#160;estimate, and the estimate can be refined as many times as we wish. The tensor of&#160;convolution kernels U is used on each step to compute the hidden representation given the&#160;input image. The kernel tensor V is used to produce an estimate of the labels given the&#160;hidden values. On all but the first step, the kernels W are convolved over </span><span class="font64" style="font-style:italic;">Y</span><span class="font64"> to provide&#160;input to the hidden layer. On the first time step, this term is replaced by zero. Because&#160;the same parameters are used on each step, this is an example of a recurrent network, as&#160;described in Chapter 10.</span></p>
<p><span class="font64">order to produce an output map of similar size as the input, one can avoid pooling altogether (Jain </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2007). Another strategy is to simply emit a lower-resolution&#160;grid of labels (Pinheiro and Collobert, 2014, 2015). Finally, in principle, one could&#160;use a pooling operator with unit stride.</span></p>
<p><span class="font64">One strategy for pixel-wise labeling of images is to produce an initial guess of the image labels, then refine this initial guess using the interactions between&#160;neighboring pixels. Repeating this refinement step several times corresponds to&#160;using the same convolutions at each stage, sharing weights between the last layers&#160;of the deep net (Jain </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2007). This makes the sequence of computations&#160;performed by the successive convolutional layers with weights shared across layers&#160;a particular kind of recurrent network (Pinheiro and Collobert, 2014, 2015). Fig.&#160;9.17 shows the architecture of such a recurrent convolutional network.</span></p>
<p><span class="font64">Once a prediction for each pixel is made, various methods can be used to further process these predictions in order to obtain a segmentation of the image&#160;into regions (Briggman </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2009; Turaga </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2010; Farabet </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2013).</span></p>
<p><span class="font64">The general idea is to assume that large groups of contiguous pixels tend to be associated with the same label. Graphical models can describe the probabilistic&#160;relationships between neighboring pixels. Alternatively, the convolutional network&#160;can be trained to maximize an approximation of the graphical model training&#160;objective (Ning </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2005; Thompson </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2014).</span></p>
</body>
</html>