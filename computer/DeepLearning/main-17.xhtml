<?xml version="1.0" encoding="UTF-8"?>

<html xmlns="http://www.w3.org/1999/xhtml">
<head><meta name="charset" content="UTF-8"/><title></title><link rel="stylesheet" href="main.css" type="text/css"/>
</head>
<body><h5><a id="bookmark0"></a><span class="font64" style="font-weight:bold;">8.3.3 Nesterov Momentum</span></h5>
<p><span class="font64">Sutskever </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013) introduced a variant of the momentum algorithm that was inspired by Nesterov’s accelerated gradient method (Nesterov, 1983, 2004). The&#160;update rules in this case are given by:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">v ^ av — eV<sub>i </sub>0 ^ 0</span><span class="font64"> + v,</span></p></div><div>
<p><span class="font64">1</span></p>
<p><span class="font64">m</span></p></div><div>
<p><span class="font64">m</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Y</span><span class="font64"> L </span><span class="font64" style="font-weight:bold;font-style:italic;">(f</span><span class="font64"> (x<sup>(i)</sup>; </span><span class="font18">0</span><span class="font64"> + av), y<sup>(i</sup>&gt;)</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i=1</span></p></div><div>
<p><span class="font64">(</span><span class="font18">8</span><span class="font64">.</span><span class="font18">21</span><span class="font64">)</span></p>
<p><span class="font64">(</span><span class="font18">8</span><span class="font64">.</span><span class="font18">22</span><span class="font64">)</span></p></div>
<p><span class="font64">where the parameters a and e play a similar role as in the standard momentum method. The difference between Nesterov momentum and standard momentum is&#160;where the gradient is evaluated. With Nesterov momentum the gradient is evaluated&#160;after the current velocity is applied. Thus one can interpret Nesterov momentum&#160;as attempting to add a </span><span class="font64" style="font-weight:bold;font-style:italic;">correction factor</span><span class="font64"> to the standard method of momentum.&#160;The complete Nesterov momentum algorithm is presented in Algorithm 8.3.</span></p>
<p><span class="font64">In the convex batch gradient case, Nesterov momentum brings the rate of convergence of the excess error from </span><span class="font64" style="font-weight:bold;font-style:italic;">O(1/k)</span><span class="font64"> (after k steps) to O</span><span class="font18">(1</span><span class="font64"> /k<sup>2</sup>) as shown&#160;by Nesterov (1983). Unfortunately, in the stochastic gradient case, Nesterov&#160;momentum does not improve the rate of convergence.</span></p>
<p><span class="font64">Algorithm 8.3 Stochastic gradient descent (SGD) with Nesterov momentum</span></p>
<p><span class="font64">Require: Learning rate e, momentum parameter a.</span></p>
<p><span class="font64">Require: Initial parameter 0, initial velocity v. while stopping criterion not met do</span></p>
<p><span class="font64">Sample a minibatch of m examples from the training set {x<sup>(1</sup>&gt;,..., x<sup>(m</sup>&gt;} with</span></p><div>
<p><span class="font64">corresponding labels y<sup>(i&gt;</sup>.</span></p>
<p><span class="font64">Apply interim update: </span><span class="font64" style="font-weight:bold;font-style:italic;">0 ^ 0</span><span class="font64"> + av Compute gradient (at interim point): g&#160;Compute velocity update: v ^ av — eg&#160;Apply update: 0 ^ 0 + v&#160;end while</span></p></div><div>
<p><span class="font18">1</span></p>
<p><span class="font7">m</span></p></div>
<p><span class="font64">viEi </span><span class="font64" style="font-weight:bold;font-style:italic;">L(f</span><span class="font64">(x<sup>(i</sup>&gt;; </span><span class="font18">0</span><span class="font64">),y<sup>(i</sup>&gt;)</span></p><h4><a id="bookmark1"></a><span class="font65" style="font-weight:bold;">8.4 Parameter Initialization Strategies</span></h4>
<p><span class="font64">Some optimization algorithms are not iterative by nature and simply solve for a solution point. Other optimization algorithms are iterative by nature but, when&#160;applied to the right class of optimization problems, converge to acceptable solutions&#160;in an acceptable amount of time regardless of initialization. Deep learning training&#160;algorithms usually do not have either of these luxuries. Training algorithms for deep&#160;learning models are usually iterative in nature and thus require the user to specify&#160;some initial point from which to begin the iterations. Moreover, training deep&#160;models is a sufficiently difficult task that most algorithms are strongly affected by&#160;the choice of initialization. The initial point can determine whether the algorithm&#160;converges at all, with some initial points being so unstable that the algorithm&#160;encounters numerical difficulties and fails altogether. When learning does converge,&#160;the initial point can determine how quickly learning converges and whether it&#160;converges to a point with high or low cost. Also, points of comparable cost&#160;can have wildly varying generalization error, and the initial point can affect the&#160;generalization as well.</span></p>
<p><span class="font64">Modern initialization strategies are simple and heuristic. Designing improved initialization strategies is a difficult task because neural network optimization is&#160;not yet well understood. Most initialization strategies are based on achieving some&#160;nice properties when the network is initialized. However, we do not have a good&#160;understanding of which of these properties are preserved under which circumstances&#160;after learning begins to proceed. A further difficulty is that some initial points&#160;may be beneficial from the viewpoint of optimization but detrimental from the&#160;viewpoint of generalization. Our understanding of how the initial point affects&#160;generalization is especially primitive, offering little to no guidance for how to select&#160;the initial point.</span></p>
<p><span class="font64">Perhaps the only property known with complete certainty is that the initial parameters need to “break symmetry” between different units. If two hidden&#160;units with the same activation function are connected to the same inputs, then&#160;these units must have different initial parameters. If they have the same initial&#160;parameters, then a deterministic learning algorithm applied to a deterministic cost&#160;and model will constantly update both of these units in the same way. Even if the&#160;model or training algorithm is capable of using stochasticity to compute different&#160;updates for different units (for example, if one trains with dropout), it is usually&#160;best to initialize each unit to compute a different function from all of the other&#160;units. This may help to make sure that no input patterns are lost in the null&#160;space of forward propagation and no gradient patterns are lost in the null space&#160;of back-propagation. The goal of having each unit compute a different function&#160;motivates random initialization of the parameters. We could explicitly search&#160;for a large set of basis functions that are all mutually different from each other,&#160;but this often incurs a noticeable computational cost. For example, if we have at&#160;most as many outputs as inputs, we could use Gram-Schmidt orthogonalization&#160;on an initial weight matrix, and be guaranteed that each unit computes a very&#160;different function from each other unit. Random initialization from a high-entropy&#160;distribution over a high-dimensional space is computationally cheaper and unlikely&#160;to assign any units to compute the same function as each other.</span></p>
<p><span class="font64">Typically, we set the biases for each unit to heuristically chosen constants, and initialize only the weights randomly. Extra parameters, for example, parameters&#160;encoding the conditional variance of a prediction, are usually set to heuristically&#160;chosen constants much like the biases are.</span></p>
<p><span class="font64">We almost always initialize all the weights in the model to values drawn randomly from a Gaussian or uniform distribution. The choice of Gaussian&#160;or uniform distribution does not seem to matter very much, but has not been&#160;exhaustively studied. The scale of the initial distribution, however, does have a&#160;large effect on both the outcome of the optimization procedure and on the ability&#160;of the network to generalize.</span></p>
<p><span class="font64">Larger initial weights will yield a stronger symmetry breaking effect, helping to avoid redundant units. They also help to avoid losing signal during forward or&#160;back-propagation through the linear component of each layer—larger values in the&#160;matrix result in larger outputs of matrix multiplication. Initial weights that are&#160;too large may, however, result in exploding values during forward propagation or&#160;back-propagation. In recurrent networks, large weights can also result in </span><span class="font64" style="font-weight:bold;font-style:italic;">chaos&#160;</span><span class="font64">(such extreme sensitivity to small perturbations of the input that the behavior&#160;of the deterministic forward propagation procedure appears random). To some&#160;extent, the exploding gradient problem can be mitigated by gradient clipping&#160;(thresholding the values of the gradients before performing a gradient descent step).&#160;Large weights may also result in extreme values that cause the activation function&#160;to saturate, causing complete loss of gradient through saturated units. These&#160;competing factors determine the ideal initial scale of the weights.</span></p>
<p><span class="font64">The perspectives of regularization and optimization can give very different insights into how we should initialize a network. The optimization perspective&#160;suggests that the weights should be large enough to propagate information successfully, but some regularization concerns encourage making them smaller. The use&#160;of an optimization algorithm such as stochastic gradient descent that makes small&#160;incremental changes to the weights and tends to halt in areas that are nearer to&#160;the initial parameters (whether due to getting stuck in a region of low gradient, or&#160;due to triggering some early stopping criterion based on overfitting) expresses a&#160;prior that the final parameters should be close to the initial parameters. Recall&#160;from Sec. 7.8 that gradient descent with early stopping is equivalent to weight&#160;decay for some models. In the general case, gradient descent with early stopping is&#160;not the same as weight decay, but does provide a loose analogy for thinking about&#160;the effect of initialization. We can think of initializing the parameters </span><span class="font64" style="font-weight:bold;">0 </span><span class="font64">to </span><span class="font64" style="font-weight:bold;font-style:italic;">0</span><span class="font18">0</span><span class="font64"> as&#160;being similar to imposing a Gaussian prior </span><span class="font64" style="font-weight:bold;font-style:italic;">p(0</span><span class="font64" style="font-weight:bold;">) </span><span class="font64">with mean </span><span class="font64" style="font-weight:bold;">0</span><span class="font18">0</span><span class="font64">. From this point&#160;of view, it makes sense to choose </span><span class="font64" style="font-weight:bold;">0</span><span class="font18">0</span><span class="font64"> to be near 0. This prior says that it is more&#160;likely that units do not interact with each other than that they do interact. Units&#160;interact only if the likelihood term of the objective function expresses a strong&#160;preference for them to interact. On the other hand, if we initialize </span><span class="font64" style="font-weight:bold;">0</span><span class="font18">0</span><span class="font64"> to large&#160;values, then our prior specifies which units should interact with each other, and&#160;how they should interact.</span></p>
<p><span class="font64">Some heuristics are available for choosing the initial scale of the weights. One heuristic is to initialize the weights of a fully connected layer with m inputs and&#160;n outputs by sampling each weight from U (— -ק-, =ק= ), while Glorot and Bengio&#160;(</span><span class="font18">2010</span><span class="font64">) suggest using the </span><span class="font64" style="font-weight:bold;font-style:italic;">normalized initialization</span></p><div>
<p><span class="font64">(8.23)</span></p></div><div>
<p><span class="font64">)•</span></p></div><div>
<p><span class="font64">Wij - U(-</span></p></div>
<p><span class="font64">6 6</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">\Jm</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">n y/m</span><span class="font64"> + </span><span class="font64" style="font-weight:bold;font-style:italic;">n</span></p>
<p><span class="font64">This latter heuristic is designed to compromise between the goal of initializing all layers to have the same activation variance and the goal of initializing all&#160;layers to have the same gradient variance. The formula is derived using the&#160;assumption that the network consists only of a chain of matrix multiplications,&#160;with no nonlinearities. Real neural networks obviously violate this assumption,&#160;but many strategies designed for the linear model perform reasonably well on its&#160;nonlinear counterparts.</span></p>
<p><span class="font64">Saxe </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2013) recommend initializing to random orthogonal matrices, with a carefully chosen scaling or </span><span class="font64" style="font-weight:bold;font-style:italic;">gain</span><span class="font64"> factor g that accounts for the nonlinearity applied&#160;at each layer. They derive specific values of the scaling factor for different types of&#160;nonlinear activation functions. This initialization scheme is also motivated by a&#160;model of a deep network as a sequence of matrix multiplies without nonlinearities.&#160;Under such a model, this initialization scheme guarantees that the total number of&#160;training iterations required to reach convergence is independent of depth.</span></p>
<p><span class="font64">Increasing the scaling factor g pushes the network toward the regime where activations increase in norm as they propagate forward through the network and&#160;gradients increase in norm as they propagate backward. Sussillo (2014) showed&#160;that setting the gain factor correctly is sufficient to train networks as deep as&#160;1,000 layers, without needing to use orthogonal initializations. A key insight of&#160;this approach is that in feedforward networks, activations and gradients can grow&#160;or shrink on each step of forward or back-propagation, following a random walk&#160;behavior. This is because feedforward networks use a different weight matrix at&#160;each layer. If this random walk is tuned to preserve norms, then feedforward&#160;networks can mostly avoid the vanishing and exploding gradients problem that&#160;arises when the same weight matrix is used at each step, described in Sec. 8.2.5.</span></p>
<p><span class="font64">Unfortunately, these optimal criteria for initial weights often do not lead to optimal performance. This may be for three different reasons. First, we may&#160;be using the wrong criteria—it may not actually be beneficial to preserve the&#160;norm of a signal throughout the entire network. Second, the properties imposed&#160;at initialization may not persist after learning has begun to proceed. Third, the&#160;criteria might succeed at improving the speed of optimization but inadvertently&#160;increase generalization error. In practice, we usually need to treat the scale of the&#160;weights as a hyperparameter whose optimal value lies somewhere roughly near but&#160;not exactly equal to the theoretical predictions.</span></p>
<p><span class="font64">One drawback to scaling rules that set all of the initial weights to have the same standard deviation, such as </span><span class="font64" style="font-weight:bold;font-style:italic;">Jm</span><span class="font64">, is that every individual weight becomes extremely&#160;small when the layers become large. Martens (2010) introduced an alternative&#160;initialization scheme called </span><span class="font64" style="font-weight:bold;font-style:italic;">sparse initialization</span><span class="font64"> in which each unit is initialized to&#160;have exactly k non-zero weights. The idea is to keep the total amount of input to&#160;the unit independent from the number of inputs </span><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font64"> without making the magnitude&#160;of individual weight elements shrink with m. Sparse initialization helps to achieve&#160;more diversity among the units at initialization time. However, it also imposes&#160;a very strong prior on the weights that are chosen to have large Gaussian values.&#160;Because it takes a long time for gradient descent to shrink “incorrect” large values,&#160;this initialization scheme can cause problems for units such as maxout units that&#160;have several filters that must be carefully coordinated with each other.</span></p>
<p><span class="font64">When computational resources allow it, it is usually a good idea to treat the initial scale of the weights for each layer as a hyperparameter, and to choose these&#160;scales using a hyperparameter search algorithm described in Sec. 11.4.2, such&#160;as random search. The choice of whether to use dense or sparse initialization&#160;can also be made a hyperparameter. Alternately, one can manually search for&#160;the best initial scales. A good rule of thumb for choosing the initial scales is to&#160;look at the range or standard deviation of activations or gradients on a single&#160;minibatch of data. If the weights are too small, the range of activations across the&#160;minibatch will shrink as the activations propagate forward through the network.&#160;By repeatedly identifying the first layer with unacceptably small activations and&#160;increasing its weights, it is possible to eventually obtain a network with reasonable&#160;initial activations throughout. If learning is still too slow at this point, it can be&#160;useful to look at the range or standard deviation of the gradients as well as the&#160;activations. This procedure can in principle be automated and is generally less&#160;computationally costly than hyperparameter optimization based on validation set&#160;error because it is based on feedback from the behavior of the initial model on a&#160;single batch of data, rather than on feedback from a trained model on the validation&#160;set. While long used heuristically, this protocol has recently been specified more&#160;formally and studied by Mishkin and Matas (2015).</span></p>
<p><span class="font64">So far we have focused on the initialization of the weights. Fortunately, initialization of other parameters is typically easier.</span></p>
<p><span class="font64">The approach for setting the biases must be coordinated with the approach for settings the weights. Setting the biases to zero is compatible with most weight&#160;initialization schemes. There are a few situations where we may set some biases to&#160;non-zero values:</span></p>
<p><span class="font64">• &#160;&#160;&#160;If a bias is for an output unit, then it is often beneficial to initialize the bias to&#160;obtain the right marginal statistics of the output. To do this, we assume that&#160;the initial weights are small enough that the output of the unit is determined&#160;only by the bias. This justifies setting the bias to the inverse of the activation&#160;function applied to the marginal statistics of the output in the training set.&#160;For example, if the output is a distribution over classes and this distribution&#160;is a highly skewed distribution with the marginal probability of class i given&#160;by element c of some vector </span><span class="font64" style="font-weight:bold;">c</span><span class="font64">, then we can set the bias vector </span><span class="font64" style="font-weight:bold;">b </span><span class="font64">by solving&#160;the equation softmax(</span><span class="font64" style="font-weight:bold;">b</span><span class="font64">) = </span><span class="font64" style="font-weight:bold;">c</span><span class="font64">. This applies not only to classifiers but also to&#160;models we will encounter in Part III, such as autoencoders and Boltzmann&#160;machines. These models have layers whose output should resemble the input&#160;data </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">, and it can be very helpful to initialize the biases of such layers to&#160;match the marginal distribution over </span><span class="font64" style="font-weight:bold;">x</span><span class="font64">.</span></p>
<p><span class="font64">• &#160;&#160;&#160;Sometimes we may want to choose the bias to avoid causing too much&#160;saturation at initialization. For example, we may set the bias of a ReLU&#160;hidden unit to 0.1 rather than 0 to avoid saturating the ReLU at initialization.&#160;This approach is not compatible with weight initialization schemes that do&#160;not expect strong input from the biases though. For example, it is not&#160;recommended for use with random walk initialization (Sussillo, 2014).</span></p>
<p><span class="font64">• &#160;&#160;&#160;Sometimes a unit controls whether other units are able to participate in a&#160;function. In such situations, we have a unit with output u and another unit&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font64"> G </span><span class="font18">[0</span><span class="font64">,</span><span class="font18">1</span><span class="font64">], then we can view </span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font64"> as a gate that determines whether </span><span class="font64" style="font-weight:bold;font-style:italic;">uh</span><span class="font64"> ~ </span><span class="font18">1</span><span class="font64"> or&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">uh</span><span class="font64"> ~ </span><span class="font18">0</span><span class="font64">. In these situations, we want to set the bias for h so that h ~ </span><span class="font18">1</span><span class="font64"> most</span></p>
<p><span class="font64">of the time at initialization. Otherwise u does not have a chance to learn. For example, Jozefowicz </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015) advocate setting the bias to 1 for the&#160;forget gate of the LSTM model, described in Sec. 10.10.</span></p>
<p><span class="font64">Another common type of parameter is a variance or precision parameter. For example, we can perform linear regression with a conditional variance estimate&#160;using the model</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">p(y</span><span class="font64"> | </span><span class="font64" style="font-weight:bold;font-style:italic;">x)</span><span class="font64"> = </span><span class="font64" style="font-weight:bold;font-style:italic;">N(y</span><span class="font64"> | w</span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>T</sup>x</span><span class="font64"> + b, </span><span class="font64" style="font-weight:bold;font-style:italic;">l/fi)</span><span class="font64"> &#160;&#160;&#160;(8.24)</span></p>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;font-style:italic;">(3</span><span class="font64"> is a precision parameter. We can usually initialize variance or precision parameters to 1 safely. Another approach is to assume the initial weights are close&#160;enough to zero that the biases may be set while ignoring the effect of the weights,&#160;then set the biases to produce the correct marginal mean of the output, and set&#160;the variance parameters to the marginal variance of the output in the training set.</span></p>
<p><span class="font64">Besides these simple constant or random methods of initializing model parameters, it is possible to initialize model parameters using machine learning. A common strategy discussed in Part III of this book is to initialize a supervised model with&#160;the parameters learned by an unsupervised model trained on the same inputs.&#160;One can also perform supervised training on a related task. Even performing&#160;supervised training on an unrelated task can sometimes yield an initialization that&#160;offers faster convergence than a random initialization. Some of these initialization&#160;strategies may yield faster convergence and better generalization because they&#160;encode information about the distribution in the initial parameters of the model.&#160;Others apparently perform well primarily because they set the parameters to have&#160;the right scale or set different units to compute different functions from each other.</span></p><h4><a id="bookmark2"></a><span class="font65" style="font-weight:bold;">8.5 Algorithms with Adaptive Learning Rates</span></h4>
<p><span class="font64">Neural network researchers have long realized that the learning rate was reliably one of the hyperparameters that is the most difficult to set because it has a significant&#160;impact on model performance. As we have discussed in Sec. 4.3 and Sec. 8.2, the&#160;cost is often highly sensitive to some directions in parameter space and insensitive&#160;to others. The momentum algorithm can mitigate these issues somewhat, but&#160;does so at the expense of introducing another hyperparameter. In the face of this,&#160;it is natural to ask if there is another way. If we believe that the directions of&#160;sensitivity are somewhat axis-aligned, it can make sense to use a separate learning&#160;rate for each parameter, and automatically adapt these learning rates throughout&#160;the course of learning.</span></p>
<p><span class="font64">The delta-bar-delta algorithm (Jacobs, 1988) is an early heuristic approach to adapting individual learning rates for model parameters during training. The&#160;approach is based on a simple idea: if the partial derivative of the loss, with respect&#160;to a given model parameter, remains the same sign, then the learning rate should&#160;increase. If the partial derivative with respect to that parameter changes sign,&#160;then the learning rate should decrease. Of course, this kind of rule can only be&#160;applied to full batch optimization.</span></p>
<p><span class="font64">More recently, a number of incremental (or mini-batch-based) methods have been introduced that adapt the learning rates of model parameters. This section&#160;will briefly review a few of these algorithms.</span></p><h5><a id="bookmark3"></a><span class="font64" style="font-weight:bold;">8.5.1 &#160;&#160;&#160;AdaGrad</span></h5>
<p><span class="font64">The AdaGrad algorithm, shown in Algorithm 8.4, individually adapts the learning rates of all model parameters by scaling them inversely proportional to the square&#160;root of the sum of all of their historical squared values (Duchi </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,</span><span class="font64"> 2011). The&#160;parameters with the largest partial derivative of the loss have a correspondingly&#160;rapid decrease in their learning rate, while parameters with small partial derivatives&#160;have a relatively small decrease in their learning rate. The net effect is greater&#160;progress in the more gently sloped directions of parameter space.</span></p>
<p><span class="font64">In the context of convex optimization, the AdaGrad algorithm enjoys some desirable theoretical properties. However, empirically it has been found that—for&#160;training deep neural network models—the accumulation of squared gradients from&#160;the beginning of training can result in a premature and excessive decrease&#160;in the effective learning rate. AdaGrad performs well for some but not all deep&#160;learning models.</span></p><h5><a id="bookmark4"></a><span class="font64" style="font-weight:bold;">8.5.2 &#160;&#160;&#160;RMSProp</span></h5>
<p><span class="font64">The RMSProp algorithm (Hinton, 2012) modifies AdaGrad to perform better in the non-convex setting by changing the gradient accumulation into an exponentially&#160;weighted moving average. AdaGrad is designed to converge rapidly when applied&#160;to a convex function. When applied to a non-convex function to train a neural&#160;network, the learning trajectory may pass through many different structures and&#160;eventually arrive at a region that is a locally convex bowl. AdaGrad shrinks the&#160;learning rate according to the entire history of the squared gradient and may&#160;have made the learning rate too small before arriving at such a convex structure.&#160;RMSProp uses an exponentially decaying average to discard history from the</span></p>
<p><span class="font64">Algorithm 8.4 The AdaGrad algorithm Require: Global learning rate e&#160;Require: Initial parameter </span><span class="font18">6</span></p>
<p><span class="font64">Require: Small constant 5, perhaps 10<sup>-</sup></span><span class="font18"><sup>7</sup></span><span class="font64">, for numerical stability Initialize gradient accumulation variable r = 0&#160;while stopping criterion not met do</span></p>
<p><span class="font64">Sample a minibatch of m examples from the training set {x<sup>(1)</sup>,..., &#160;&#160;&#160;} with</span></p>
<p><span class="font64">corresponding targets</span></p>
<p><span class="font64">Compute gradient: g ^ ^<sup>V</sup></span><span class="font62" style="font-style:italic;">0</span><span class="font64" style="font-weight:bold;font-style:italic;">YU <sup>L</sup>(f</span><span class="font64"> (*<sup>W</sup>; </span><span class="font18">6</span><span class="font64">), y<sup>(i)</sup>)</span></p>
<p><span class="font64">Accumulate squared gradient: r ^ r + g </span><span class="font18">0</span><span class="font64"> g</span></p>
<p><span class="font64">Compute update: A</span><span class="font18">6</span><span class="font64"> ^ — </span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>s</sub>_+^<sub>r</sub> 0 g■</span><span class="font64"> &#160;&#160;&#160;(Division and square root applied</span></p>
<p><span class="font64">element-wise)</span></p>
<p><span class="font64">Apply update: </span><span class="font18">6</span><span class="font64"> ^ </span><span class="font18">6</span><span class="font64"> + A</span><span class="font18">6 </span><span class="font64">end while</span></p>
<p><span class="font64">extreme past so that it can converge rapidly after finding a convex bowl, as if it were an instance of the AdaGrad algorithm initialized within that bowl.</span></p>
<p><span class="font64">RMSProp is shown in its standard form in Algorithm 8.5 and combined with Nesterov momentum in Algorithm </span><span class="font18">8</span><span class="font64">.</span><span class="font18">6</span><span class="font64">. Compared to AdaGrad, the use of the&#160;moving average introduces a new hyperparameter, p, that controls the length scale&#160;of the moving average.</span></p>
<p><span class="font64">Empirically, RMSProp has been shown to be an effective and practical optimization algorithm for deep neural networks. It is currently one of the go-to optimization methods being employed routinely by deep learning practitioners.</span></p><h5><a id="bookmark5"></a><span class="font64" style="font-weight:bold;">8.5.3 Adam</span></h5>
<p><span class="font64" style="font-weight:bold;font-style:italic;">Adam</span><span class="font64"> (Kingma and Ba, 2014) is yet another adaptive learning rate optimization algorithm and is presented in Algorithm 8.7. The name “Adam” derives from&#160;the phrase “adaptive moments.” In the context of the earlier algorithms, it is&#160;perhaps best seen as a variant on the combination of RMSProp and momentum&#160;with a few important distinctions. First, in Adam, momentum is incorporated&#160;directly as an estimate of the first order moment (with exponential weighting) of&#160;the gradient. The most straightforward way to add momentum to RMSProp is to&#160;apply momentum to the rescaled gradients. The use of momentum in combination&#160;with rescaling does not have a clear theoretical motivation. Second, Adam includes&#160;bias corrections to the estimates of both the first-order moments (the momentum&#160;term) and the (uncentered) second-order moments to account for their initialization</span></p>
<p><span class="font64">Algorithm 8.5 The RMSProp algorithm</span></p>
<p><span class="font64">Require: Global learning rate e, decay rate p.</span></p>
<p><span class="font64">Require: Initial parameter </span><span class="font18">6 </span><span class="font64">Require: Small constant S, usually 10<sup>-</sup></span><span class="font18"><sup>6</sup></span><span class="font64">, used to stabilize division by small&#160;numbers.</span></p>
<p><span class="font64">Initialize accumulation variables r = 0 while stopping criterion not met do</span></p>
<p><span class="font64">Sample a minibatch of m examples from the training set {x<sup>(1)</sup>,..., &#160;&#160;&#160;} with</span></p>
<p><span class="font64">corresponding targets y<sup>(i)</sup>.</span></p>
<p><span class="font64">Compute gradient: g ^ ^<sup>V</sup></span><span class="font18">0</span><span class="font64"> La </span><span class="font64" style="font-weight:bold;font-style:italic;">L(f</span><span class="font64"> (x<sup>(i)</sup>; </span><span class="font18">6</span><span class="font64">), y<sup>(i)</sup>)</span></p>
<p><span class="font64">Accumulate squared gradient: r ^ pr + (1 — p)g </span><span class="font18">0</span><span class="font64"> g</span></p>
<p><span class="font64">Compute parameter update: A</span><span class="font18">6</span><span class="font64"> = — &#160;&#160;&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">0 g</span><span class="font64">. </span><span class="font64" style="font-weight:bold;font-style:italic;">(</span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:line-through;">^§<sub>+r</sub></span><span class="font64"> applied element-wise)</span></p>
<p><span class="font64">Apply update: </span><span class="font18">6</span><span class="font64"> ^ </span><span class="font18">6</span><span class="font64"> + A</span><span class="font18">6 </span><span class="font64">end while</span></p>
<p><span class="font64">at the origin (see Algorithm 8.7). RMSProp also incorporates an estimate of the (uncentered) second-order moment, however it lacks the correction factor. Thus,&#160;unlike in Adam, the RMSProp second-order moment estimate may have high bias&#160;early in training. Adam is generally regarded as being fairly robust to the choice&#160;of hyperparameters, though the learning rate sometimes needs to be changed from&#160;the suggested default.</span></p><h5><a id="bookmark6"></a><span class="font64" style="font-weight:bold;">8.5.4 Choosing the Right Optimization Algorithm</span></h5>
<p><span class="font64">In this section, we discussed a series of related algorithms that each seek to address the challenge of optimizing deep models by adapting the learning rate for each&#160;model parameter. At this point, a natural question is: which algorithm should one&#160;choose?</span></p>
<p><span class="font64">Unfortunately, there is currently no consensus on this point. Schaul </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2014) presented a valuable comparison of a large number of optimization algorithms&#160;across a wide range of learning tasks. While the results suggest that the family of&#160;algorithms with adaptive learning rates (represented by RMSProp and AdaDelta)&#160;performed fairly robustly, no single best algorithm has emerged.</span></p>
<p><span class="font64">Currently, the most popular optimization algorithms actively in use include SGD, SGD with momentum, RMSProp, RMSProp with momentum, AdaDelta&#160;and Adam. The choice of which algorithm to use, at this point, seems to depend&#160;largely on the user’s familiarity with the algorithm (for ease of hyperparameter&#160;tuning).</span></p>
<p><span class="font64">Algorithm </span><span class="font18">8.6</span><span class="font64"> RMSProp algorithm with Nesterov momentum Require: Global learning rate e, decay rate p, momentum coefficient a.&#160;Require: Initial parameter </span><span class="font18">6</span><span class="font64">, initial velocity v.</span></p>
<p><span class="font64">Initialize accumulation variable r = 0 while stopping criterion not met do</span></p>
<p><span class="font64">Sample a minibatch of </span><span class="font64" style="font-weight:bold;font-style:italic;">m</span><span class="font64"> examples from the training set {x<sup>(1)</sup>,..., x<sup>(m)</sup>} with corresponding targets y<sup>(i)</sup>.</span></p>
<p><span class="font64">Compute interim update: </span><span class="font62" style="font-style:italic;">6</span><span class="font64" style="font-weight:bold;font-style:italic;"> ^ </span><span class="font62" style="font-style:italic;">6</span><span class="font64"> + av Compute gradient: g ^ my - ^. L(f (x<sup>(i)</sup>;</span><span class="font18">6</span><span class="font64">), </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"><sup>(i)</sup>)</span></p>
<p><span class="font64">Accumulate gradient: r ^ pr + (1 — p)g </span><span class="font18">0</span><span class="font64"> g</span></p>
<p><span class="font64">Compute velocity update: v ^ av — &#160;&#160;&#160;0 g • (yr applied element-wise)</span></p>
<p><span class="font64">Apply update: </span><span class="font18">6</span><span class="font64"> ^ </span><span class="font18">6</span><span class="font64"> + v end while</span></p><h4><a id="bookmark7"></a><span class="font65" style="font-weight:bold;">8.6 Approximate Second-Order Methods</span></h4>
<p><span class="font64">In this section we discuss the application of second-order methods to the training of deep networks. See LeCun </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (1998a) for an earlier treatment of this subject.&#160;For simplicity of exposition, the only objective function we examine is the empirical&#160;risk:</span></p><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64"> (</span><span class="font18">6</span><span class="font64">) = Ex,</span></p></div><div>
<p><span class="font64">y~<sup>p</sup>data </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(x,</sup>y</span></p></div><div>
<p><span class="font64">)<sup>[L(f</sup></span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>(</sup>x; </span><span class="font64"><sup>6)</sup>, y<sup>)]</sup> = m^ </span><span class="font64" style="font-weight:bold;font-style:italic;">L<sup>(</sup>f <sup>(</sup>x</span><span class="font64"><sup>(i)</sup>; </span><span class="font18"><sup>6</sup></span><span class="font64"><sup>),</sup>y<sup>(i)</sup>).</span></p></div><div>
<p><span class="font64">(8.25)</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">i= 1</span></p></div>
<p><span class="font64">However the methods we discuss here extend readily to more general objective functions that, for instance, include parameter regularization terms such as those&#160;discussed in Chapter 7.</span></p><h5><a id="bookmark8"></a><span class="font64" style="font-weight:bold;">8.6.1 Newton’s Method</span></h5>
<p><span class="font64">In Sec. 4.3, we introduced second-order gradient methods. In contrast to first-order methods, second-order methods make use of second derivatives to improve optimization. The most widely used second-order method is Newton’s method. We&#160;now describe Newton’s method in more detail, with emphasis on its application to&#160;neural network training.</span></p>
<p><span class="font64">Newton’s method is an optimization scheme based on using a second-order Taylor series expansion to approximate J(</span><span class="font18">6</span><span class="font64">) near some point </span><span class="font18">60</span><span class="font64">, ignoring derivatives</span></p>
<p><span class="font64">Algorithm 8.7 The Adam algorithm</span></p><div>
<p><span class="font64">Require: Step size e (Suggested default: 0.001)</span></p>
<p><span class="font64">Require: Exponential decay rates for moment estimates, p! and p</span><span class="font18">2</span><span class="font64"> in [0, 1).</span></p>
<p><span class="font64">(Suggested defaults: 0.9 and 0.999 respectively)</span></p>
<p><span class="font64">Require: Small constant S used for numerical stabilization. (Suggested default: </span><span class="font18">10</span><span class="font64"><sup>-</sup></span><span class="font18"><sup>8</sup></span><span class="font64">)</span></p>
<p><span class="font64">Require: Initial parameters </span><span class="font18">6</span></p>
<p><span class="font64">Initialize 1st and 2nd moment variables </span><span class="font64" style="font-weight:bold;font-style:italic;">s =</span><span class="font64"> 0, r = 0</span></p>
<p><span class="font64">Initialize time step </span><span class="font64" style="font-weight:bold;font-style:italic;">t =</span><span class="font64"> 0</span></p>
<p><span class="font64">while stopping criterion not met do</span></p>
<p><span class="font64">Sample a minibatch of m examples from the training set {x<sup>(!)</sup>,..., &#160;&#160;&#160;} with</span></p>
<p><span class="font64">corresponding targets y<sup>(i)</sup>.</span></p>
<p><span class="font64">Compute gradient: g — ^V</span><span class="font62" style="font-style:italic;">0</span><span class="font64"> Xa </span><span class="font64" style="font-weight:bold;font-style:italic;">L(f</span><span class="font64"> (x<sup>(i)</sup>; </span><span class="font18">6</span><span class="font64">), y<sup>(i)</sup>) t i— t + </span><span class="font18">1</span></p>
<p><span class="font64">Update biased first moment estimate: s — </span><span class="font64" style="font-weight:bold;font-style:italic;">p!s</span><span class="font64"> + (1 — p! )g Update biased second moment estimate: r — </span><span class="font64" style="font-weight:bold;font-style:italic;">p</span><span class="font62" style="font-style:italic;">2</span><span class="font64" style="font-weight:bold;font-style:italic;">r</span><span class="font64"> + (1 — p</span><span class="font18">2</span><span class="font64">)g </span><span class="font18">0</span><span class="font64"> g&#160;Correct bias in first moment: S</span></p></div><div>
<p><span class="font64">Correct bias in second moment: r Compute update: A</span><span class="font18">6</span><span class="font64"> = — e-</span></p></div><div>
<p><span class="font64"><sup>!</sup>-pi</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">r</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;">!-p</span><span class="font62" style="font-style:italic;">2</span></p></div><div>
<p><span class="font64">Apply update: </span><span class="font18">6</span><span class="font64"> — </span><span class="font18">6</span><span class="font64"> + A</span><span class="font18">6 </span><span class="font64">end while</span></p></div><div>
<p><span class="font64" style="font-weight:bold;font-style:italic;">r+S</span></p></div><div>
<p><span class="font64">(operations applied element-wise)</span></p></div>
<p><span class="font64">of higher order:</span></p><div>
<p><span class="font18">1</span></p></div><div>
<p><span class="font64">J(</span><span class="font18">6</span><span class="font64">) « J(</span><span class="font18">60</span><span class="font64">) + (</span><span class="font18">6</span><span class="font64"> — </span><span class="font18">60</span><span class="font64">)' VeJ(</span><span class="font18">6</span><span class="font64">)) + <sub>2</sub>(</span><span class="font18">6</span><span class="font64"> — </span><span class="font18">60</span><span class="font64">)</span><span class="font18"><sup>1</sup></span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">H</span><span class="font64">(</span><span class="font18">6</span><span class="font64"> — </span><span class="font62" style="font-style:italic;">60</span><span class="font64">),</span></p></div><div>
<p><span class="font64">(8.26)</span></p></div>
<p><span class="font64">where H is the Hessian of J with respect to </span><span class="font18">6</span><span class="font64"> evaluated at </span><span class="font18">60</span><span class="font64">. If we then solve for the critical point of this function, we obtain the Newton parameter update rule:</span></p><div>
<p><span class="font64">(8.27)</span></p></div>
<p><span class="font18">6</span><span class="font64">* = </span><span class="font18">60</span><span class="font64"> — </span><span class="font64" style="font-weight:bold;font-style:italic;">H<sup>-!</sup>V eJ </span><span class="font62" style="font-style:italic;">(6</span><span class="font64"> </span><span class="font18">0</span><span class="font64">)</span></p>
<p><span class="font64">Thus for a locally quadratic function (with positive definite H), by rescaling the gradient by H<sup>-!</sup>, Newton’s method jumps directly to the minimum. If the&#160;objective function is convex but not quadratic (there are higher-order terms), this&#160;update can be iterated, yielding the training algorithm associated with Newton’s&#160;method, given in Algorithm </span><span class="font18">8</span><span class="font64">.</span><span class="font18">8</span><span class="font64">.</span></p>
<p><span class="font64">For surfaces that are not quadratic, as long as the Hessian remains positive definite, Newton’s method can be applied iteratively. This implies a two-step</span></p>
<p><span class="font64">Algorithm </span><span class="font18">8.8</span><span class="font64"> Newton’s method with objective &#160;&#160;&#160;J (0)</span></p>
<p><span class="font64">i </span><span class="font18" style="text-decoration:underline;">&quot;1</span><span class="font64" style="text-decoration:underline;"> </span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:underline;">L(f</span><span class="font64" style="text-decoration:underline;"> (*»; </span><span class="font62" style="font-style:italic;text-decoration:underline;">0</span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:underline;">)^).</span><span class="font64">_</span></p>
<p><span class="font64">Require: Initial parameter </span><span class="font18">03 </span><span class="font64">Require: Training set of m examples&#160;while stopping criterion not met do</span></p>
<p><span class="font64">Compute gradient: g ^ &quot;Ve , </span><span class="font64" style="font-weight:bold;font-style:italic;">L(f</span><span class="font64"> (x<sup>(i)</sup>; 0), y<sup>(i)</sup>)</span></p>
<p><span class="font64">Compute Hessian: </span><span class="font64" style="font-weight:bold;font-style:italic;">H ^</span><span class="font64"> &quot; Ve </span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>i</sub> L(f</span><span class="font64"> (x<sup>(i)</sup>; 0), </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"><sup>(i)</sup>)</span></p>
<p><span class="font64">Compute Hessian inverse: H<sup>-</sup></span><span class="font18"><sup>1 </sup></span><span class="font64">Compute update: A0 = —H<sup>-</sup></span><span class="font18"><sup>1</sup></span><span class="font64">g&#160;Apply update: 0 = 0 + A0&#160;end while</span></p>
<p><span class="font64">iterative procedure. First, update or compute the inverse Hessian (i.e. by updating the quadratic approximation). Second, update the parameters according to Eq.&#160;8.27.</span></p>
<p><span class="font64">In Sec. 8.2.3, we discussed how Newton’s method is appropriate only when the Hessian is positive definite. In deep learning, the surface of the objective&#160;function is typically non-convex with many features, such as saddle points, that&#160;are problematic for Newton’s method. If the eigenvalues of the Hessian are not&#160;all positive, for example, near a saddle point, then Newton’s method can actually&#160;cause updates to move in the wrong direction. This situation can be avoided&#160;by regularizing the Hessian. Common regularization strategies include adding a&#160;constant, a, along the diagonal of the Hessian. The regularized update becomes</span></p>
<p><span class="font18">0</span><span class="font64">* = </span><span class="font18">00</span><span class="font64"> — </span><span class="font64" style="font-weight:bold;font-style:italic;">[H</span><span class="font64"> (f (</span><span class="font18">00</span><span class="font64">)) + </span><span class="font64" style="font-weight:bold;font-style:italic;">all<sup>1</sup>־ Ve f (</span><span class="font62" style="font-style:italic;">00</span><span class="font64">). &#160;&#160;&#160;(8.28)</span></p>
<p><span class="font64">This regularization strategy is used in approximations to Newton’s method, such as the Levenberg-Marquardt algorithm (Levenberg, 1944; Marquardt, 1963), and&#160;works fairly well as long as the negative eigenvalues of the Hessian are still relatively&#160;close to zero. In cases where there are more extreme directions of curvature, the&#160;value of a would have to be sufficiently large to offset the negative eigenvalues.&#160;However, as a increases in size, the Hessian becomes dominated by the </span><span class="font64" style="font-weight:bold;font-style:italic;">al</span><span class="font64"> diagonal&#160;and the direction chosen by Newton’s method converges to the standard gradient&#160;divided by a. When strong negative curvature is present, a may need to be so&#160;large that Newton’s method would make smaller steps than gradient descent with&#160;a properly chosen learning rate.</span></p>
<p><span class="font64">Beyond the challenges created by certain features of the objective function, such as saddle points, the application of Newton’s method for training large neural&#160;networks is limited by the significant computational burden it imposes. The&#160;number of elements in the Hessian is squared in the number of parameters, so with&#160;k parameters (and for even very small neural networks the number of parameters&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">k</span><span class="font64"> can be in the millions), Newton’s method would require the inversion of a k x k&#160;matrix—with computational complexity of O(k<sup>3</sup>). Also, since the parameters&#160;will change with every update, the inverse Hessian has to be computed at every&#160;training iteration. As a consequence, only networks with a very small number&#160;of parameters can be practically trained via Newton’s method. In the remainder&#160;of this section, we will discuss alternatives that attempt to gain some of the&#160;advantages of Newton’s method while side-stepping the computational hurdles.</span></p><h5><a id="bookmark9"></a><span class="font64" style="font-weight:bold;">8.6.2 Conjugate Gradients</span></h5>
<p><span class="font64">Conjugate gradients is a method to efficiently avoid the calculation of the inverse Hessian by iteratively descending </span><span class="font64" style="font-weight:bold;font-style:italic;">conjugate directions.</span><span class="font64"> The inspiration for this&#160;approach follows from a careful study of the weakness of the method of steepest&#160;descent (see Sec. 4.3 for details), where line searches are applied iteratively in&#160;the direction associated with the gradient. Fig. </span><span class="font18">8.6</span><span class="font64"> illustrates how the method of&#160;steepest descent, when applied in a quadratic bowl, progresses in a rather ineffective&#160;back-and-forth, zig-zag pattern. This happens because each line search direction,&#160;when given by the gradient, is guaranteed to be orthogonal to the previous line&#160;search direction.</span></p>
<p><span class="font64">Let the previous search direction be d<sub>t</sub>_</span><span class="font18">1</span><span class="font64">. At the minimum, where the line search terminates, the directional derivative is zero in direction d<sub>t</sub>_ </span><span class="font18">1</span><span class="font64">: V</span><span class="font62" style="font-style:italic;">0</span><span class="font64" style="font-weight:bold;font-style:italic;"> J</span><span class="font64">(</span><span class="font18">0</span><span class="font64">) </span><span class="font64" style="font-weight:bold;font-style:italic;">•&#160;d<sub>t</sub>_</span><span class="font64"> </span><span class="font18">1</span><span class="font64"> = 0. Since the gradient at this point defines the current search direction,&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">d<sub>t</sub> = V</span><span class="font62" style="font-style:italic;">0</span><span class="font64" style="font-weight:bold;font-style:italic;">J(0</span><span class="font64">) will have no contribution in the direction </span><span class="font64" style="font-weight:bold;font-style:italic;">dt_ </span><span class="font62" style="font-style:italic;">1</span><span class="font64">. Thus d<sub>t</sub> is orthogonal&#160;to d<sub>t-1</sub>. This relationship between d<sub>t-</sub></span><span class="font18"><sub>1</sub></span><span class="font64"> and d<sub>t</sub> is illustrated in Fig. </span><span class="font18">8.6</span><span class="font64"> for&#160;multiple iterations of steepest descent. As demonstrated in the figure, the choice of&#160;orthogonal directions of descent do not preserve the minimum along the previous&#160;search directions. This gives rise to the zig-zag pattern of progress, where by&#160;descending to the minimum in the current gradient direction, we must re-minimize&#160;the objective in the previous gradient direction. Thus, by following the gradient at&#160;the end of each line search we are, in a sense, undoing progress we have already&#160;made in the direction of the previous line search. The method of conjugate gradients&#160;seeks to address this problem.</span></p>
<p><span class="font64">In the method of conjugate gradients, we seek to find a search direction that is </span><span class="font64" style="font-weight:bold;font-style:italic;">conjugate</span><span class="font64"> to the previous line search direction, i.e. it will not undo progress made&#160;in that direction. At training iteration </span><span class="font64" style="font-weight:bold;font-style:italic;">t</span><span class="font64">, the next search direction d<sub>t</sub> takes the&#160;form:</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dt = <sup>V</sup> </span><span class="font62" style="font-style:italic;">0</span><span class="font64" style="font-weight:bold;font-style:italic;">J</span><span class="font64"> (0) + </span><span class="font64" style="font-weight:bold;font-style:italic;">f3td t_ </span><span class="font62" style="font-style:italic;">1</span><span class="font64"> &#160;&#160;&#160;(8.29)</span></p><div><div><img src="main-86.jpg" alt=""/>
<p><span class="font64">Figure 8.6: The method of steepest descent applied to a quadratic cost surface. The method of steepest descent involves jumping to the point of lowest cost along the line&#160;defined by the gradient at the initial point on each step. This resolves some of the problems&#160;seen with using a fixed learning rate in Fig. 4.6, but even with the optimal step size the&#160;algorithm still makes back-and-forth progress toward the optimum. By definition, at&#160;the minimum of the objective along a given direction, the gradient at the final point is&#160;orthogonal to that direction.</span></p></div></div>
<p><span class="font64">were </span><span class="font64" style="font-weight:bold;font-style:italic;">fi<sub>t</sub></span><span class="font64"> is a coefficient whose magnitude controls how much of the direction, </span><span class="font64" style="font-weight:bold;font-style:italic;">d<sub>t-</sub></span><span class="font18">1</span><span class="font64">, we should add back to the current search direction.</span></p>
<p><span class="font64">Two directions, d and d<sub>t-</sub></span><span class="font18">1</span><span class="font64">, are defined as conjugate if d<sup>J</sup>H(J</span><span class="font64" style="font-weight:bold;font-style:italic;">)d<sub>t-</sub></span><span class="font18">1</span><span class="font64"> = 0.</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">dj Hd<sub>t-</sub></span><span class="font62" style="font-style:italic;"><sub>1</sub></span><span class="font64" style="font-weight:bold;font-style:italic;"> =</span><span class="font64"> 0 &#160;&#160;&#160;(8.30)</span></p>
<p><span class="font64">The straightforward way to impose conjugacy would involve calculation of the eigenvectors of H to choose </span><span class="font64" style="font-weight:bold;font-style:italic;">fi<sub>t</sub></span><span class="font64">, which would not satisfy our goal of developing&#160;a method that is more computationally viable than Newton’s method for large&#160;problems. Can we calculate the conjugate directions without resorting to these&#160;calculations? Fortunately the answer to that is yes.</span></p>
<p><span class="font64">Two popular methods for computing the </span><span class="font64" style="font-weight:bold;font-style:italic;">fi<sub>t</sub></span><span class="font64"> are:</span></p><div>
<p><span class="font64">1. Fletcher-Reeves:</span></p></div><div>
<p><span class="font64">fi t =</span></p></div><div>
<p><span class="font64">_Ve J (ft )<sup>T</sup> VeJ </span><span class="font64" style="font-weight:bold;font-style:italic;">(O</span><span class="font64"> t)_ VeJ (Ot-</span><span class="font18">1</span><span class="font64"> )<sup>T</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">VJ (O t</span><span class="font62" style="font-style:italic;">-1</span><span class="font64" style="font-weight:bold;font-style:italic;">)</span></p></div><div>
<p><span class="font64">(8.31)</span></p></div>
<p><span class="font64">For a quadratic surface, the conjugate directions ensure that the gradient along the previous direction does not increase in magnitude. We therefore stay at the&#160;minimum along the previous directions. As a consequence, in a k-dimensional&#160;parameter space, conjugate gradients only requires k line searches to achieve the&#160;minimum. The conjugate gradient algorithm is given in Algorithm 8.9.</span></p>
<p><span class="font64">Algorithm 8.9 Conjugate gradient method</span></p>
<p><span class="font64">Require: Initial parameters </span><span class="font62" style="font-style:italic;">60 </span><span class="font64">Require: Training set of m examples&#160;Initialize </span><span class="font64" style="font-weight:bold;font-style:italic;">po</span><span class="font64"> = 0&#160;Initialize go = 0&#160;Initialize </span><span class="font64" style="font-weight:bold;font-style:italic;">t =</span><span class="font64"> 1</span></p>
<p><span class="font64">while stopping criterion not met do Initialize the gradient </span><span class="font64" style="font-weight:bold;font-style:italic;">g<sub>t</sub></span><span class="font64"> = 0</span></p>
<p><span class="font64">Compute gradient: </span><span class="font64" style="font-weight:bold;font-style:italic;">g<sub>t</sub> —</span><span class="font64"> ^<sub>i</sub></span><span class="font64" style="font-weight:bold;font-style:italic;">L(f</span><span class="font64"> (x<sup>(i)</sup>; </span><span class="font18">6</span><span class="font64">), y<sup>(i)</sup>)</span></p>
<p><span class="font64">Compute fy </span><span class="font64" style="font-weight:bold;font-style:italic;">= </span><span class="font64" style="font-weight:bold;font-style:italic;text-decoration:line-through;">gf<sup>—</sup></span><span class="font64" style="text-decoration:line-through;"> ^ </span><span class="font62" style="font-style:italic;text-decoration:line-through;">1</span><span class="font64" style="text-decoration:line-through;"> *</span><span class="font64"> (Polak-Ribiere)</span></p>
<p><span class="font64">(Nonlinear conjugate gradient: optionally reset fy to zero, for example if t is a multiple of some constant k, such as k = 5)</span></p>
<p><span class="font64">Compute search direction: p<sub>t</sub> = —g<sub>t</sub> + </span><span class="font64" style="font-weight:bold;font-style:italic;">fy<sub>t</sub>p<sub>t</sub>_</span><span class="font62" style="font-style:italic;">1</span></p>
<p><span class="font64">Perform line search to find: e* = argmin <sub>e</sub> &#160;&#160;&#160;L(f (x <sup>(i)</sup>; </span><span class="font18">6</span><span class="font64"><sub>t</sub> + </span><span class="font64" style="font-weight:bold;font-style:italic;">ept), y</span><span class="font64"><sup>(i)</sup>)</span></p>
<p><span class="font64">(On a truly quadratic cost function, analytically solve for e* rather than explicitly searching for it)</span></p>
<p><span class="font64">Apply update: </span><span class="font18">6</span><span class="font64"><sub>t</sub>+! = </span><span class="font18">6</span><span class="font64"><sub>t</sub> + e *p<sub>t </sub>t —— t + </span><span class="font18">1&#160;</span><span class="font64">end while</span></p>
<p><span class="font64">Nonlinear Conjugate Gradients: So far we have discussed the method of conjugate gradients as it is applied to quadratic objective functions. Of course,&#160;our primary interest in this chapter is to explore optimization methods for training&#160;neural networks and other related deep learning models where the corresponding&#160;objective function is far from quadratic. Perhaps surprisingly, the method of&#160;conjugate gradients is still applicable in this setting, though with some modification.&#160;Without any assurance that the objective is quadratic, the conjugate directions are&#160;no longer assured to remain at the minimum of the objective for previous directions.&#160;As a result, the </span><span class="font64" style="font-weight:bold;font-style:italic;">nonlinear conjugate gradients</span><span class="font64"> algorithm includes occasional resets&#160;where the method of conjugate gradients is restarted with line search along the&#160;unaltered gradient.</span></p>
<p><span class="font64">Practitioners report reasonable results in applications of the nonlinear conjugate</span></p>
<p><span class="font64">gradients algorithm to training neural networks, though it is often beneficial to initialize the optimization with a few iterations of stochastic gradient descent before&#160;commencing nonlinear conjugate gradients. Also, while the (nonlinear) conjugate&#160;gradients algorithm has traditionally been cast as a batch method, minibatch&#160;versions have been used successfully for the training of neural networks (Le </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.,&#160;</span><span class="font64">2011). Adaptations of conjugate gradients specifically for neural networks have&#160;been proposed earlier, such as the scaled conjugate gradients algorithm (Moller,&#160;1993).</span></p><h5><a id="bookmark10"></a><span class="font64" style="font-weight:bold;">8.6.3 BFGS</span></h5>
<p><span class="font64">The </span><span class="font64" style="font-weight:bold;font-style:italic;">Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm</span><span class="font64"> attempts to bring some of the advantages of Newton’s method without the computational burden. In that&#160;respect, BFGS is similar to CG. However, BFGS takes a more direct approach to&#160;the approximation of Newton’s update. Recall that Newton’s update is given by</span></p>
<p><span class="font64">9* = 00 - </span><span class="font64" style="font-weight:bold;font-style:italic;">H-'Vg</span><span class="font64"> J(</span><span class="font18">6</span><span class="font64">b), &#160;&#160;&#160;(8.33)</span></p>
<p><span class="font64">where H is the Hessian of J with respect to 9 evaluated at </span><span class="font18">90</span><span class="font64">. The primary computational difficulty in applying Newton’s update is the calculation of the&#160;inverse Hessian H<sup>-1</sup>. The approach adopted by quasi-Newton methods (of which&#160;the BFGS algorithm is the most prominent) is to approximate the inverse with&#160;a matrix </span><span class="font64" style="font-weight:bold;font-style:italic;">M<sub>t</sub></span><span class="font64"> that is iteratively refined by low rank updates to become a better&#160;approximation of H<sup>-1</sup>.</span></p>
<p><span class="font64">The specification and derivation of the BFGS approximation is given in many textbooks on optimization, including Luenberger (1984).</span></p>
<p><span class="font64">Once the inverse Hessian approximation M<sub>t</sub> is updated, the direction of descent </span><span class="font64" style="font-weight:bold;font-style:italic;">P</span><span class="font64"> is determined by p<sub>t</sub> = </span><span class="font64" style="font-weight:bold;font-style:italic;">M<sub>t</sub>g</span><span class="font62"><sub>t</sub>. A line search is performed in this direction to&#160;determine the size of the step, e*, taken in this direction. The final update to the&#160;parameters is given by:</span></p>
<p><span class="font18">9</span><span class="font62">t</span><span class="font18">+1</span><span class="font64"> = </span><span class="font64" style="font-weight:bold;font-style:italic;">9t +</span><span class="font64"> e* </span><span class="font64" style="font-weight:bold;font-style:italic;">pt</span><span class="font64">. &#160;&#160;&#160;(8.34)</span></p>
<p><span class="font64">Like the method of conjugate gradients, the BFGS algorithm iterates a series of line searches with the direction incorporating second-order information. However&#160;unlike conjugate gradients, the success of the approach is not heavily dependent&#160;on the line search finding a point very close to the true minimum along the line.&#160;Thus, relative to conjugate gradients, BFGS has the advantage that it can spend&#160;less time refining each line search. On the other hand, the BFGS algorithm must&#160;store the inverse Hessian matrix, M, that requires O(n</span><span class="font18"><sup>2</sup></span><span class="font64">) memory, making BFGS&#160;impractical for most modern deep learning models that typically have millions of&#160;parameters.</span></p>
<p><span class="font64">Limited Memory BFGS (or L-BFGS) The memory costs of the BFGS algorithm can be significantly decreased by avoiding storing the complete inverse&#160;Hessian approximation M. The L-BFGS algorithm computes the approximation M&#160;using the same method as the BFGS algorithm, but beginning with the assumption&#160;that M<sup>(t-1)</sup> is the identity matrix, rather than storing the approximation from one&#160;step to the next. If used with exact line searches, the directions defined by L-BFGS&#160;are mutually conjugate. However, unlike the method of conjugate gradients, this&#160;procedure remains well behaved when the minimum of the line search is reached&#160;only approximately. Th L-BFGS strategy with no storage described here can be&#160;generalized to include more information about the Hessian by storing some of the&#160;vectors used to update M at each time step, which costs only O(n) per step.</span></p><h4><a id="bookmark11"></a><span class="font65" style="font-weight:bold;">8.7 Optimization Strategies and Meta-Algorithms</span></h4>
<p><span class="font64">Many optimization techniques are not exactly algorithms, but rather general templates that can be specialized to yield algorithms, or subroutines that can be&#160;incorporated into many different algorithms.</span></p><h5><a id="bookmark12"></a><span class="font64" style="font-weight:bold;">8.7.1 Batch Normalization</span></h5>
<p><span class="font64">Batch normalization (Ioffe and Szegedy, 2015) is one of the most exciting recent innovations in optimizing deep neural networks and it is actually not an optimization&#160;algorithm at all. Instead, it is a method of adaptive reparametrization, motivated&#160;by the difficulty of training very deep models.</span></p>
<p><span class="font64">Very deep models involve the composition of several functions or layers. The gradient tells how to update each parameter, under the assumption that the other&#160;layers do not change. In practice, we update all of the layers simultaneously.&#160;When we make the update, unexpected results can happen because many functions&#160;composed together are changed simultaneously, using updates that were computed&#160;under the assumption that the other functions remain constant. As a simple&#160;example, suppose we have a deep neural network that has only one unit per layer&#160;and does not use an activation function at each hidden layer: y = </span><span class="font64" style="font-weight:bold;font-style:italic;">xw </span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;"> w</span><span class="font62" style="font-style:italic;">2</span><span class="font64"> w</span><span class="font18">3</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">... wi.&#160;</span><span class="font64">Here, w<sub>i</sub> provides the weight used by layer i. The output of layer i is </span><span class="font64" style="font-weight:bold;font-style:italic;">h<sub>i</sub> = h,_ </span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;"> w</span><span class="font64">^.&#160;The output y is a linear function of the input x, but a nonlinear function of the&#160;weights </span><span class="font64" style="font-weight:bold;font-style:italic;">wi.</span><span class="font64"> Suppose our cost function has put a gradient of 1 on y, so we wish to&#160;decrease </span><span class="font64" style="font-weight:bold;font-style:italic;">y</span><span class="font64"> slightly. The back-propagation algorithm can then compute a gradient&#160;g = </span><span class="font64" style="font-weight:bold;font-style:italic;">V<sub>w</sub>y.</span><span class="font64"> Consider what happens when we make an update </span><span class="font64" style="font-weight:bold;font-style:italic;">w ^ w — eg.</span><span class="font64"> The&#160;first-order Taylor series approximation of y predicts that the value of y will decrease&#160;by eg<sup>T</sup>g. If we wanted to decrease y by .1, this first-order information available in&#160;the gradient suggests we could set the learning rate e to . However, the actual&#160;update will include second-order and third-order effects, on up to effects of order l.&#160;The new value of y is given by</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">x(w</span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;"> —</span><span class="font64"> egi )(w</span><span class="font18">2</span><span class="font64"> — </span><span class="font64" style="font-weight:bold;font-style:italic;">eg</span><span class="font18">2</span><span class="font64">)... (wi </span><span class="font64" style="font-weight:bold;font-style:italic;">— egi</span><span class="font64">). &#160;&#160;&#160;(8.35)</span></p>
<p><span class="font64">An example of one second-order term arising from this update is e</span><span class="font18"><sup>2</sup></span><span class="font64">gi g</span><span class="font18">2</span><span class="font64"> ni=3 </span><span class="font64" style="font-weight:bold;font-style:italic;"><sup>w</sup>i</span><span class="font64">. This term might be negligible if ni</span><span class="font18">=3</span><span class="font64"> </span><span class="font64" style="font-weight:bold;font-style:italic;">w<sub>i</sub></span><span class="font64"> is small, or might be exponentially large&#160;if the weights on layers 3 through l are greater than 1. This makes it very hard&#160;to choose an appropriate learning rate, because the effects of an update to the&#160;parameters for one layer depends so strongly on all of the other layers. Second-order&#160;optimization algorithms address this issue by computing an update that takes these&#160;second-order interactions into account, but we can see that in very deep networks,&#160;even higher-order interactions can be significant. Even second-order optimization&#160;algorithms are expensive and usually require numerous approximations that prevent&#160;them from truly accounting for all significant second-order interactions. Building&#160;an n-th order optimization algorithm for </span><span class="font64" style="font-weight:bold;font-style:italic;">n &gt;</span><span class="font64"> 2 thus seems hopeless. What can we&#160;do instead?</span></p>
<p><span class="font64">Batch normalization provides an elegant way of reparametrizing almost any deep network. The reparametrization significantly reduces the problem of coordinating&#160;updates across many layers. Batch normalization can be applied to any input&#160;or hidden layer in a network. Let H be a minibatch of activations of the layer&#160;to normalize, arranged as a design matrix, with the activations for each example&#160;appearing in a row of the matrix. To normalize H, we replace it with</span></p>
<p><span class="font64" style="font-weight:bold;font-style:italic;">H =</span><span class="font64"> &#160;&#160;&#160;,&#160;&#160;&#160;&#160;(8.36)</span></p>
<p><span class="font64">G</span></p>
<p><span class="font64">where </span><span class="font18">1</span><span class="font64"> is a vector containing the mean of each unit and </span><span class="font64" style="font-weight:bold;font-variant:small-caps;">g </span><span class="font64">is a vector containing the standard deviation of each unit. The arithmetic here is based on broadcasting&#160;the vector </span><span class="font18">1</span><span class="font64"> and the vector </span><span class="font64" style="font-weight:bold;font-variant:small-caps;">g </span><span class="font64">to be applied to every row of the matrix H. Within&#160;each row, the arithmetic is element-wise, so </span><span class="font64" style="font-weight:bold;font-style:italic;">Hij</span><span class="font64"> is normalized by subtracting gj&#160;and dividing by aj. The rest of the network then operates on H</span><span class="font64" style="font-weight:bold;font-style:italic;">'</span><span class="font64"> in exactly the&#160;same way that the original network operated on H.</span></p>
<p><span class="font64">At training time,</span></p>
<p><span class="font18">1</span></p>
<p><span class="font64">1 = - &#160;&#160;&#160;Hi,:&#160;&#160;&#160;&#160;(8.37)</span></p>
<p><span class="font64">m</span></p>
<p><span class="font64">and</span></p>
<p><span class="font64"><sup>a</sup> </span><span class="font64" style="font-weight:bold;font-style:italic;">= </span><span class="font66" style="font-style:italic;">^J<sup>5</sup></span><span class="font64"> + </span><span class="font66" style="font-style:italic;">^J2</span><span class="font64" style="font-weight:bold;font-style:italic;font-variant:small-caps;"><sup>(h -</sup></span><span class="font64" style="font-weight:bold;font-style:italic;"> vt</span><span class="font64">, &#160;&#160;&#160;<sup>(8</sup>-<sup>38)</sup></span></p>
<p><span class="font64">where </span><span class="font64" style="font-weight:bold;font-style:italic;">5</span><span class="font64"> is a small positive value such as </span><span class="font18">10<sup>-8</sup></span><span class="font64"> imposed to avoid encountering the undefined gradient </span><span class="font64" style="font-weight:bold;font-style:italic;">of y/z</span><span class="font64"> at z = 0. Crucially, we back-propagate through&#160;these operations for computing the mean and the standard deviation, and for&#160;applying them to normalize H. This means that the gradient will never propose&#160;an operation that acts simply to increase the standard deviation or mean of&#160;</span><span class="font64" style="font-weight:bold;font-style:italic;">hi</span><span class="font64">; the normalization operations remove the effect of such an action and zero&#160;out its component in the gradient. This was a major innovation of the batch&#160;normalization approach. Previous approaches had involved adding penalties to&#160;the cost function to encourage units to have normalized activation statistics or&#160;involved intervening to renormalize unit statistics after each gradient descent step.&#160;The former approach usually resulted in imperfect normalization and the latter&#160;usually resulted in significant wasted time as the learning algorithm repeatedly&#160;proposed changing the mean and variance and the normalization step repeatedly&#160;undid this change. Batch normalization reparametrizes the model to make some&#160;units always be standardized by definition, deftly sidestepping both problems.</span></p>
<p><span class="font64">At test time, </span><span class="font64" style="font-weight:bold;font-style:italic;">fi</span><span class="font64"> and </span><span class="font64" style="font-weight:bold;font-style:italic;">a</span><span class="font64"> may be replaced by running averages that were collected during training time. This allows the model to be evaluated on a single example,&#160;without needing to use definitions of i and </span><span class="font64" style="font-weight:bold;font-style:italic;">a</span><span class="font64"> that depend on an entire minibatch.</span></p>
<p><span class="font64">Revisiting the y </span><span class="font64" style="font-weight:bold;font-style:italic;">=</span><span class="font64"> xw!w</span><span class="font18">2</span><span class="font64"> ... </span><span class="font64" style="font-weight:bold;font-style:italic;">wi</span><span class="font64"> example, we see that we can mostly resolve the difficulties in learning this model by normalizing </span><span class="font64" style="font-weight:bold;font-style:italic;">h-</span><span class="font62" style="font-style:italic;">1</span><span class="font64">. Suppose that x is drawn&#160;from a unit Gaussian. Then hi— will also come from a Gaussian, because the&#160;transformation from x to hi is linear. However, h </span><span class="font62" style="font-style:italic;">1</span><span class="font64" style="font-weight:bold;font-style:italic;"><sub>-</sub></span><span class="font62" style="font-style:italic;">1</span><span class="font64"> will no longer have zero mean&#160;and unit variance. After applying batch normalization, we obtain the normalized&#160;hi<sub>-</sub>! that restores the zero mean and unit variance properties. For almost any&#160;update to the lower layers, hi— will remain a unit Gaussian. The output y may&#160;then be learned as a simple linear function y = </span><span class="font64" style="font-weight:bold;font-style:italic;">uih</span><span class="font64"> i—. Learning in this model is&#160;now very simple because the parameters at the lower layers simply do not have an&#160;effect in most cases; their output is always renormalized to a unit Gaussian. In&#160;some corner cases, the lower layers can have an effect. Changing one of the lower&#160;layer weights to </span><span class="font18">0</span><span class="font64"> can make the output become degenerate, and changing the sign&#160;of one of the lower weights can flip the relationship between hi— and y. These&#160;situations are very rare. Without normalization, nearly every update would have&#160;an extreme effect on the statistics of </span><span class="font64" style="font-weight:bold;font-style:italic;">h</span><span class="font62" style="font-style:italic;">1</span><span class="font64">_!. Batch normalization has thus made&#160;this model significantly easier to learn. In this example, the ease of learning of&#160;course came at the cost of making the lower layers useless. In our linear example,&#160;the lower layers no longer have any harmful effect, but they also no longer have&#160;any beneficial effect. This is because we have normalized out the first and second&#160;order statistics, which is all that a linear network can influence. In a deep neural&#160;network with nonlinear activation functions, the lower layers can perform nonlinear&#160;transformations of the data, so they remain useful. Batch normalization acts to&#160;standardize only the mean and variance of each unit in order to stabilize learning,&#160;but allows the relationships between units and the nonlinear statistics of a single&#160;unit to change.</span></p>
<p><span class="font64">Because the final layer of the network is able to learn a linear transformation, we may actually wish to remove all linear relationships between units within a&#160;layer. Indeed, this is the approach taken by Desjardins </span><span class="font64" style="font-weight:bold;font-style:italic;">et al.</span><span class="font64"> (2015), who provided&#160;the inspiration for batch normalization. Unfortunately, eliminating all linear&#160;interactions is much more expensive than standardizing the mean and standard&#160;deviation of each individual unit, and so far batch normalization remains the most&#160;practical approach.</span></p>
<p><span class="font64">Normalizing the mean and standard deviation of a unit can reduce the expressive power of the neural network containing that unit. In order to maintain the&#160;expressive power of the network, it is common to replace the batch of hidden unit&#160;activations </span><span class="font64" style="font-weight:bold;font-style:italic;">H</span><span class="font64"> with </span><span class="font18">7</span><span class="font64">H' + fl rather than simply the normalized H'. The variables&#160;</span><span class="font18">7</span><span class="font64"> and fl are learned parameters that allow the new variable to have any mean&#160;and standard deviation. At first glance, this may seem useless—why did we set&#160;the mean to </span><span class="font18">0</span><span class="font64">, and then introduce a parameter that allows it to be set back to&#160;any arbitrary value fl ? The answer is that the new parametrization can represent&#160;the same family of functions of the input as the old parametrization, but the new&#160;parametrization has different learning dynamics. In the old parametrization, the&#160;mean of H was determined by a complicated interaction between the parameters&#160;in the layers below H. In the new parametrization, the mean of </span><span class="font18">7</span><span class="font64">H' + fl is&#160;determined solely by fl. The new parametrization is much easier to learn with&#160;gradient descent.</span></p>
<p><span class="font64">Most neural network layers take the form of ^(XW + </span><span class="font64" style="font-weight:bold;font-style:italic;">b)</span><span class="font64"> where ^ is some fixed nonlinear activation function such as the rectified linear transformation. It&#160;is natural to wonder whether we should apply batch normalization to the input&#160;X, or to the transformed value XW + b. Ioffe and Szegedy ( 2015) recommend&#160;the latter. More specifically, XW + b should be replaced by a normalized version&#160;of </span><span class="font64" style="font-weight:bold;font-style:italic;">XW</span><span class="font64">. The bias term should be omitted because it becomes redundant with&#160;the </span><span class="font64" style="font-weight:bold;font-style:italic;">f3</span><span class="font64"> parameter applied by the batch normalization reparametrization. The input&#160;to a layer is usually the output of a nonlinear activation function such as the&#160;rectified linear function in a previous layer. The statistics of the input are thus&#160;more non-Gaussian and less amenable to standardization by linear operations.</span></p>
<p><span class="font64">In convolutional networks, described in Chapter 9, it is important to apply the same normalizing </span><span class="font64" style="font-weight:bold;font-style:italic;">/a</span><span class="font64"> and a at every spatial location within a feature map, so that&#160;the statistics of the feature map remain the same regardless of spatial location.</span></p>
</body>
</html>